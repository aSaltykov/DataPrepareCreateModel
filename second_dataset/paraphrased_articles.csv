Title,Abstract,Introduction,ParaphrasedTitle,ParaphrasedAbstract,ParaphraseIntroduction,URL
Detecting Euphemisms with Literal Descriptions and Visual Imagery,"This paper describes our two-stage system1 for the Euphemism Detection shared task hosted by the 3rd Workshop on Figurative Language Processing in conjunction with EMNLP 2022. Euphemisms tone down expressions about sensitive or unpleasant issues like addiction and death. The ambiguous nature of euphemistic words or expressions makes it challenging to detect their actual meaning within a context. In the first stage, we seek to mitigate this ambiguity by incorporating literal descriptions into input text prompts to our baseline model. It turns out that this kind of direct supervision yields remarkable performance improvement. In the second stage, we integrate visual supervision into our system using visual imageries, two sets of images generated by a text-to-image model by taking terms and descriptions as input. Our experiments demonstrate that visual supervision also gives a statistically significant performance boost. Our system achieved the second place with an F1 score of 87.2%, only about 0.9% worse than the best submission.","Recent advances in large pretrained language models allowed the computational linguistics community to tackle more knowledge-intensive tasks which require commonsense reasoning (Talmor et al., 2019; Bisk et al., 2020; Lin et al., 2021), and figurative language understanding (Pedinotti et al., 2021; Liu et al., 2022). In this work, we focus on a figurative language understanding task called euphemism detection. Euphemisms attempt to smooth harsh, impolite, or blunt expressions about taboo or sensitive topics like death and unemployment (Holder, 2008). For instance, when we speak of older people we often refer to senior citizens instead of a direct expression that can be seen as offensive. 1Code is available at github.com/ilkerkesen/euphemism Identifying euphemisms is challenging due to their natural ambiguity, i.e., the meaning of the term shifts depending on the context: ‘Over the hill’ could either mean someone or something is physically over some hill (literal), or someone or something is old, past one’s prime (figurative) (Lee et al., 2022). One cannot distinguish these two different senses without sufficient context. Thus, these terms are referred as potentially euphemistic terms (PETs) (Gavidia et al., 2022). Here, we propose a two-stage method for the Euphemism Detection shared task hosted by the 3rd Workshop on Figurative Language Processing at EMNLP 2022. In the first stage, we manually collect literal descriptions for each PET. We then incorporate these descriptions into input text prompts to help the model distinguish figurative from literal usage. We demonstrate that this kind of extraneous linguistic supervision improves a strong baseline by a large margin. In the second stage, we attempt to answer the question, “Is visual supervision also useful to infer the meaning behind a PET?” To answer this question, we use a text-to-image model which takes terms and descriptions as input, and we generate two sets of images, which we denote as visual imageries. Our experiments show that using visual imagery provides the best results. A paired t-test points out that the improvement is statistically significant. Our qualitative analysis also suggests visual imageries are beneficial for analyzing PETs. The rest of this paper is organized as follows. Section 2 describes our proposed solution. In Section 3, we share the details of our evaluation setup and design choices. Section 4 reports our experimental results. In Section 5, we briefly review the relevant literature. Section 6 outlines our conclusions and discuss the limitations of our approach.",Finding Euphemisms with Literal Interpretations and Pictures,"Our two-stage system for the Euphemism Detection shared task hosted by the 3rd Workshop on Figurative Language Processing in conjunction with EMNLP 2022 is presented in this paper. It is difficult to identify the true meaning of euphemistic words or phrases due to their ambiguous nature when it comes to sensitive or unpleasant topics such as addiction and death. To reduce this ambiguity, we added literal descriptions to the input text prompts of our baseline model in the first stage, which resulted in significant performance improvement. In the second stage, visual supervision was integrated by using two sets of images generated by a text-to-image model that takes terms and descriptions as input. Our experiments showed that visual supervision gives a statistically significant performance boost. We achieved the second place with an F1 score of 87.2%, only about 0.9% worse than the best submission.","Recent advances in large pretrained language models have enabled the computational linguistics community to address knowledge-intensive tasks which require commonsense reasoning and understanding of figurative language (Talmor et al., 2019; Bisk et al., 2020; Lin et al., 2021; Pedinotti et al., 2021; Liu et al., 2022). We focus on a task involving figurative language called euphemism detection. Euphemisms are used to soften harsh, impolite, or blunt expressions about topics like death and unemployment (Holder, 2008). For example, we might refer to older people as 'senior citizens' instead of a more direct expression that could be seen as offensive. Identifying euphemisms is difficult because their meaning can change depending on the context (Lee et al., 2022). Terms with this kind of ambiguity are known as potentially euphemistic terms (PETs) (Gavidia et al., 2022). We propose a two-stage method for the Euphemism Detection shared task hosted by the 3rd Workshop on Figurative Language Processing at EMNLP 2022. In the first stage, we manually collect literal descriptions for each PET and incorporate them into input text prompts to assist the model in distinguishing figurative from literal usage. We demonstrate that this type of linguistic supervision improves a strong baseline significantly. In the second stage, we investigate if visual supervision is also useful in inferring the meaning behind a PET. To do so, we use a text-to-image model which takes terms and descriptions as input, and generates two sets of images, referred to as visual imageries. Our experiments show that using visual imagery leads to the best results, and a paired t-test confirms the improvement is statistically significant. Our qualitative analysis further suggests visual imageries are beneficial for analyzing PETs. Section 2 details our proposed solution, Section 3 covers the evaluation setup and design choices, Section 4 reports the experimental results, Section 5 reviews the relevant literature, and Section 6 outlines our conclusions and discusses the limitations of our approach. Code is available at github.com/ilkerkesen/euphemism.",https://arxiv.org/pdf/2211.04576.pdf
Disentangling Content and Motion for Text-Based Neural Video Manipulation,"Giving machines the ability to imagine possible new objects or scenes from linguistic descriptions and produce their realistic renderings is arguably one of the most challenging problems in computer vision. Recent advances in deep generative models have led to new approaches that give promising results towards this goal. In this paper, we introduce a new method called DiCoMoGAN for manipulating videos with natural language, aiming to perform local and semantic edits on a video clip to alter the appearances of an object of interest. Our GAN architecture allows for better utilization of multiple observations by disentangling content and motion to enable controllable semantic edits. To this end, we introduce two tightly coupled networks: (i) a representation network for constructing a concise understanding of motion dynamics and temporally invariant content, and (ii) a translation network that exploits the extracted latent content representation to actuate the manipulation according to the target description. Our qualitative and quantitative evaluations demonstrate that DiCoMoGAN significantly outperforms existing frame-based methods, producing temporally coherent and semantically more meaningful results.","Making desired edits on an image or video using tools like Adobe Photoshop, Adobe Premiere Pro and Apple Final Cut Pro is quite challenging and requires extensive training and experience. Thanks to the proliferation of deep learning, some user-friendly solutions are proposed for editing images [43]. Yet, democratizing the video editing process to improve accessibility and empower the non-experts still requires rethinking modern architectures. Towards this end, we set off to ask: Can we learn to semantically manipulate videos through natural language descriptions in a temporally consistent way? (c.f. Fig. 1) The existing literature approaches this problem on a frame-by-frame basis applying minimal necessary modifications specified by the input text, disjointly to the each and every input frame. Almost all of the image editing methods use encoder-decoder architectures [21, 43, 44, 53, 59, 64, 80] and employ adversarial learning strategies [35, 39] to provide the agreement between the resulting images and the target text and to generate photo-realistic outcomes. However, performing language-driven edits on videos requires models to not only understand the frame content but also be aware of the global video context and its temporal unidirectionality. Moreover, a harmonious editing demands the target descriptions to be related not only to static frames but to the entire video to achieve good gestalt. To achieve all these, we propose a new data-driven text based video manipulation model called DiCoMoGAN. The key to our approach is a unified network model consisting of a representation network (RepNet) and a translation network (TraNet), which jointly learn to disentangle video content and motion dynamics and to perform the text-specified edits on a given video sequence. Under the assumption that textual description is strongly related to appearance, we create a structured latent space composed of text relevant, text irrelevant and dynamic subspaces (c.f. Fig. 2). To ensure the former, we steer the latent subspace to be shared between global video descriptor and the text features, encoded by CLIP [59]. We then use the features from this structured latent space along with text features to condition multifeature modulation (MFMOD) blocks. We train this integrated architecture via a multi-task loss function in an end to end manner to encode scene specific transformations effectively while capturing the relationships between the spatiotemporal data and the text input. Our experiments on the standard 3D Shapes benchmark [6] as well as on our new dataset Fashion Videos demonstrate that DiCoMoGAN can produce high quality, temporally consistent videos faithfully reflecting the intentions stated in the target descriptions. In summary, our contributions are as follows: (1) Our representation network, RepNet, implements a neural architecture that explicitly enforces the separation of static and dynamic features via a setbased β-VAE model [32] equipped with a Latent ODE [67]. (2) Our translation network, TraNet, follows an encoder-decoder architecture which is guided by the representation network through a novel multi-feature modulation block called MFMOD where the residual activation maps are modulated based on both the given textual description and the disentangled content code. (3) To test the capabilities of our model in a more realistic setting, we collect a new dataset containing Fashion Videos with the related textual descriptions.",Separating Content and Motion for Neural Video Manipulation with Text,"Developing machines that can generate realistic renderings of new objects or scenes from linguistic descriptions is one of the most difficult tasks in computer vision. Recently, deep generative models have been utilized to produce promising results. In this paper, we propose a new method, DiCoMoGAN, for manipulating videos with natural language, to modify the look of a particular object. Our GAN architecture permits better utilization of multiple observations by separating content and motion to allow for controllable semantic edits. To accomplish this, two networks are used: (i) a representation network for creating a succinct understanding of motion dynamics and content that is temporally invariant, and (ii) a translation network that uses the latent content representation to effect the manipulation according to the target description. Our qualitative and quantitative evaluations show that DiCoMoGAN significantly surpasses existing frame-based methods, producing results that are temporally coherent and semantically more meaningful.","Creating desired edits on images or videos using tools such as Adobe Photoshop, Adobe Premiere Pro and Apple Final Cut Pro is complicated and requires skill and experience. Thanks to the development of deep learning, user-friendly solutions for image editing have been proposed [43]. However, making video editing more accessible and empowering non-experts still requires reconsidering modern architectures. To this end, we asked: Can we learn to manipulate videos through natural language descriptions in a consistent manner? (c.f. Fig. 1) Existing approaches to this problem focus on frame-by-frame modifications specified by the input text, independently for each frame. Most image editing methods use encoder-decoder architectures [21, 43, 44, 53, 59, 64, 80] and adversarial learning strategies [35, 39] to match the resulting images to the target text and produce realistic outcomes. Editing videos with language, however, necessitates models that understand frame content and global video context, as well as temporal unidirectionality. Furthermore, a successful edit demands that the target descriptions be related to the entire video. To meet these requirements, we present DiCoMoGAN, a data-driven text-based video manipulation model. Our model consists of a representation network (RepNet) and a translation network (TraNet). The former separates video content and motion dynamics, while the latter uses a multi-feature modulation block (MFMOD) to condition the residual activation maps based on the given text and the disentangled content code. We train the integrated architecture with a multi-task loss function, and demonstrate its effectiveness on 3D Shapes [6] and our new Fashion Videos dataset. Our contributions are as follows: (1) RepNet implements a neural architecture with a set-based β-VAE model [32] and Latent ODE [67], to enforce the separation of static and dynamic features. (2) TraNet follows an encoder-decoder architecture guided by RepNet through MFMOD. (3) We collect a Fashion Videos dataset with related textual descriptions.",https://arxiv.org/pdf/2211.02980.pdf
PERCEPTION-DISTORTION TRADE-OFF IN THE SR SPACE SPANNED BY FLOW MODELS,"Flow-based generative super-resolution (SR) models learn to produce a diverse set of feasible SR solutions, called the SR space. Diversity of SR solutions increases with the temperature (τ ) of latent variables, which introduces random variations of texture among sample solutions, resulting in visual artifacts and low fidelity. In this paper, we present a simple but effective image ensembling/fusion approach to obtain a single SR image eliminating random artifacts and improving fidelity without significantly compromising perceptual quality. We achieve this by benefiting from a diverse set of feasible photorealistic solutions in the SR space spanned by flow models. We propose different image ensembling and fusion strategies which offer multiple paths to move sample solutions in the SR space to more desired destinations in the perception-distortion plane in a controllable manner depending on the fidelity vs. perceptual quality requirements of the task at hand. Experimental results demonstrate that our image ensembling/fusion strategy achieves more promising perception-distortion tradeoff compared to sample SR images produced by flow models and adversarially trained models in terms of both quantitative metrics and visual quality.","Deep-learning based super-resolution (SR) methods have produced astonishing results [1–8]. Given LR-HR paired training data, early deep SR methods [1–6] considered SR as a regression problem and produced a single deterministic output. Recently, several flow-based methods [9, 10] and challenges [11] recognized the ill-posed nature of the SR problem and proposed learning a one-to-many stochastic generative mapping to produce a diverse set of plausible SR images, called the SR space. These formulations consider photo-realism of solutions, their consistency with the LR image, and how well they span the SR space. Even though generative models can generate a diverse set of possible SR solutions, they introduce a new problem: selecting a single solution when the goal is to extract critical information from the SR image, e.g., whether a digit is 3 or 8. In such information-centric applications, generating multiple feasible solutions may not lead to a decisive result and a single high fidelity solution is desirable. While in some other applications, obtaining the best (single) photorealistic natural image may be more desirable. To this effect, we propose first generating finitely many samples in the SR space spanned by flow models, and then merging or fusing them as a means of obtaining the best perception-distortion trade-off for the application at hand, instead of searching for a single sample realization among infinitely many feasible solutions. Interestingly, our perceptiondistortion trade-off results summarized in Fig. 1 show that fusing multiple images in the SR space via simple integration techniques leads to better PSNR than that of the best regressive convolutional network (ConvNet) model, e.g., RRDB [7] and any feasible SRFlow sample without a significant compromise in perceptual quality of solutions with the best perceptual quality. Furthermore, we achieve these results by simple image integration techniques in the SR space instead of retraining the entire generative model as would be necessary in adversarially trained models, e.g., ESRGAN+ [12]. In summary, our main contributions are: 1. We propose a novel approach for perception-distortion trade-off in the SR space spanned by flow models by means of ensembling or fusing multiple sample SR solutions. 2. We show that ensembling by simple averaging or median operations over samples in the SR space lead to higher fidelity results without significant degradation of perceptual quality. 3. We propose a simple ConvNet consisting of 2-4 residual blocks that learns fusing samples in the SR space using LPIPS loss to favor the perception dimension for natural images.",The Trade-off between Perception and Distortion in the Flow Model SR Space,"Flow-based generative super-resolution (SR) models learn to generate a wide range of SR solutions, referred to as the SR space. As the temperature (τ ) of latent variables increases, the diversity of SR solutions also increases, resulting in visible artifacts and low accuracy. In this paper, we present a straightforward yet effective image ensembling/fusion approach to obtain a single SR image that eliminates random artifacts and boosts fidelity without significantly reducing perceptual quality. We take advantage of the wide range of realistic solutions in the SR space generated by flow models. We propose several image ensembling and fusion strategies that provide multiple paths to move sample solutions in the SR space to the desired locations in the perception-distortion plane depending on the task's fidelity vs. perceptual quality requirements. Experimental results demonstrate that our image ensembling/fusion strategy offers a better perception-distortion tradeoff compared to sample SR images produced by flow models and adversarially trained models in terms of both quantitative metrics and visual quality.","Deep-learning based super-resolution (SR) methods have yielded impressive results [1–8]. Earlier deep SR models [1–6] addressed SR as a regression issue and outputted a single definite answer. Recently, flow-based approaches [9, 10] and challenges [11] acknowledged the ill-posed character of the SR challenge and proposed learning a one-to-many random generative mapping to create a wide variety of plausible SR pictures, referred to as the SR space. These formulations take into account photo-realism of solutions, their consistency with the LR image, and how well they cover the SR space. Despite the fact that generative models can generate a broad selection of possible SR solutions, they bring up a fresh problem: picking one solution when the goal is to extract essential information from the SR image, e.g., whether a digit is 3 or 8. In such information-centric applications, generating multiple possible solutions may not lead to a decisive result and a single high fidelity solution is desirable. Alternatively, in some other applications, obtaining the best (single) photorealistic natural image may be more preferable. To this end, we propose first generating a finite number of samples in the SR space spanned by flow models, and then combining or fusing them as a means of achieving the best perception-distortion trade-off for the application at hand, rather than searching for a single sample realization among infinitely many feasible solutions. Our perception-distortion trade-off results summarized in Fig. 1 demonstrate that fusing multiple images in the SR space via simple integration techniques leads to higher PSNR than that of the top regressive convolutional network (ConvNet) model, e.g., RRDB [7] and any feasible SRFlow sample without a major sacrifice in perceptual quality of solutions with the best perceptual quality. Furthermore, we accomplish these results by straightforward image integration techniques in the SR space instead of retraining the entire generative model as would be necessary in adversarially trained models, e.g., ESRGAN+ [12]. In summary, our primary contributions are: 1. We present a novel approach for perception-distortion trade-off in the SR space spanned by flow models through ensembling or fusing multiple sample SR solutions. 2. We prove that ensembling by simple averaging or median operations over samples in the SR space lead to higher fidelity results without considerable deterioration of perceptual quality. 3. We propose a simple ConvNet consisting of 2-4 residual blocks that learns fusing samples in the SR space using LPIPS loss to favor the perception dimension for natural images.",https://arxiv.org/pdf/2209.08564.pdf
"""BE Y O N D T H E I M I TAT I O N G A M E : QU A N T I F YI N G A N D E X T R A P O L AT I N G T H E C A PA B I L I T I E S O F L A N G U A G E M O D E L S ""","Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 444 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI’s GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit “breakthrough” behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting. ","Generative language models have as their core capability the production of the most likely continuation for a text sequence. This seemingly simple skill is remarkably general. Any task that can be specified and executed via text can be framed as text continuation. This encompasses a wide range of cognitive tasks, including tasks that can be resolved over chat or email, for example, or in a web forum. A recent consensus is that as generative language models are made larger, and are trained on more data, they perform better in predictable ways. Their cross entropy on a test set scales as a power law with regards to model size, training data size, and the amount of compute used in training (Hestness et al., 2017; 2019; Rosenfeld et al., 2019; Kaplan et al., 2020; Brown et al., 2020). Motivated by this predictable improvement, researchers have now scaled language models to more than one trillion parameters (Fedus et al., 2021), and we expect models to grow orders of magnitude larger over the next several years. We also expect continued performance gains from improvements in architecture and training methods.Massive increases in quantity often imbue systems with qualitatively new behavior. In science, increases in scale often require or enable novel descriptions or even the creation of new fields (Anderson, 1972). Consider, for instance, the hierarchy from quantum field theory to atomic physics, to chemistry, to biology, and to ecology. Each level demonstrates new behavior and is the subject of a rich discipline, despite being reducible to a bulk system obeying the rules of the levels below. Language models similarly demonstrate qualitatively new behavior as they increase in size (Zhang et al., 2020e). For instance, they demonstrate nascent abilities in writing computer code (Hendrycks et al., 2021a; Chen et al., 2021; Austin et al., 2021; Schuster et al., 2021b; Biderman & Raff, 2022), playing chess (Noever et al., 2020; Stöckl, 2021), diagnosing medical conditions (Rasmy et al., 2021), and translating between languages (Sutskever et al., 2014), though they are currently less capable at all of these things than human beings with modest domain knowledge. These breakthrough capabilities (Ganguli et al., 2022) have been observed empirically, but we are unable to reliably predict the scale at which new breakthroughs will happen. We may also be unaware of additional breakthroughs that have already occurred but not yet been noticed experimentally. The quantitative and qualitative changes that occur in language models as they become larger are potentially transformative (Bommasani et al., 2021; Black et al., 2022). Large language models may augment or replace humans for broad sets of tasks that can be framed in terms of textual response. They may enable entirely new applications. Without proper care, they may also embed undesirable social bias deep into technology stacks and decision-making processes—but with proper care, they may enable decision-making to be automated with less human bias. Due to the potentially transformative effects of language models, it is vitally important that we understand their capabilities and limitations, and that we understand how those capabilities and limitations are likely to change as models are improved. This understanding will directly motivate the development of new technologies; allow us to identify and mitigate potential harmful social effects, ranging from job loss to automation of social bias (Bender et al., 2021); enable us to predict other ways in which model behavior may be subtly misaligned with human intent (Kenton et al., 2021); allow us to direct our research energies in the most promising directions (Bommasani et al., 2021, sec. 3); and enable us to avoid devoting research resources to problems that are likely to be solved by scale alone (Sutton, 2019).Current language-modeling benchmarks are insufficient to satisfy our need to understand the behavior of language models and to predict their future behavior. These existing benchmarks suffer from several limitations. First, many benchmarks have restricted scope, largely targeting a single or a few capabilities on which language models have already proven some proficiency. For instance, benchmarks often propose tasks that codify narrow subsets of areas, such as language understanding (Wang et al., 2019a), summarization (See et al., 2017; Hermann et al., 2015; Narayan et al., 2018; Koupaee & Wang, 2018; Rush et al., 2015; Graff et al., 2003), or trivia-question answering (Joshi et al., 2017; Kwiatkowski et al., 2019; Rajpurkar et al., 2016). Because they are narrowly targeted, and because their targets are often ones that language models are already known to perform, they are ill-suited to identify new and unexpected capabilities that language models may develop with increased scale, or to characterize the breadth of current capabilities. Second, recent language-modeling benchmarks have often had short useful lifespans (MartínezPlumed et al., 2021). When human-equivalent performance is reached for these benchmarks, they are often either discontinued and replaced or extended through the inclusion of more challenging benchmarks, in a kind of “challenge-solve-and-replace” evaluation dynamic (Schlangen, 2019), or a “dataset-solve-and-patch” adversarial benchmark co-evolution (Zellers et al., 2019a). For instance, superhuman performance was achieved on the common SuperGLUE benchmark (Wang et al., 2019a) less than 18 months after it was produced (Figure 2). Such short useful lifespans are likely due to the restricted scope of these benchmarks, which prevents them from including tasks that lie far beyond the capabilities of current language models. Finally, many current benchmarks use data collected through human labeling that is not performed by experts or by the task authors. The costs and challenges associated with such data-labeling significantly impact the difficulty of the chosen tasks, since a task many need to be easy to explain and perform. This often results in easier tasks, with noise, correctness, and distributional issues that can reduce the interpretability of results (Bowman & Dahl, 2021).models, and by the limitations of current benchmarks, we introduce a large-scale, extremely difficult and diverse benchmark. We then measure model performance on this benchmark. We provide a human-evaluator baseline and expert human evaluations on this diverse set of tasks, in order to measure whether model performance is broadly distinguishable from human-evaluator performance. Furthermore, models are measured across scales to facilitate a naive extrapolation of the scale at which they may be indistinguishable from human evaluators. In homage to Alan Turing’s imitation game (Turing, 1950), and because we aim to extract information about model behavior beyond what is available from a binary judgement of whether a model is distinguishable from a human, we call this benchmark the Beyond the Imitation Game benchmark, or BIG-bench. We also introduce BIG-bench Lite (Section 2.2), a curated subset of 24 tasks, intended for lightweight evaluation. The benchmark was developed openly on GitHub, with contributors adding tasks by way of GitHub pull requests. Peer review of the proposed tasks was performed by discussion on the pull request. In order to incentivize contributions, all authors of accepted tasks were given the opportunity to be co-authors on this paper introducing BIG-bench. We use this benchmark to analyze dense and sparse transformer models, from Google and OpenAI, across six orders of magnitude of model scale. We are especially interested in anticipating the future capabilities of language models. In our analysis we therefore focus not on the performance of any single model, but rather on how the performance changes as a function of model scale. For select tasks, we investigate how specific model capabilities evolve with scale. Figure 1 shows the aggregate performance of models ranging over five orders of magnitude in size on all BIG-bench tasks. ",Language Models' Capabilities Quantified and Extrapolated.,"Language models show improvement and new capabilities with size increase. These are not well understood, so to be prepared for future developments and to avoid negative effects, we must know the current and near-future abilities and limitations of these models. To do this, BIG-bench was created with 204 tasks from 444 authors at 132 institutions, covering linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and more. It focuses on tasks that are believed to be too difficult for existing language models. We tested OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, with sizes ranging from millions to hundreds of billions of parameters. Furthermore, human experts completed all tasks to set a strong baseline. Results showed that performance and calibration increase with size, but are still low in absolute terms (and compared to the experts). Performance was similar across model types, but sparsity was beneficial. Tasks that improve gradually usually have large knowledge or memorization components, while tasks that show dramatic improvement at a certain size usually involve multiple steps or components, or have fragile metrics. Social bias usually rises with scale in ambiguous contexts, but can be lessened with prompting.","Generative language models have the capability to produce likely continuations of a text sequence. This general ability can be used for tasks that are specified and executed via text, such as those in chat, email, or web forums. Increasing model size, training data, and compute used in training has been found to predictably improve performance (Hestness et al., 2017; 2019; Rosenfeld et al., 2019; Kaplan et al., 2020; Brown et al., 2020), leading to models with more than one trillion parameters (Fedus et al., 2021). Qualitatively new behavior is also observed as size increases, such as nascent abilities to write computer code (Hendrycks et al., 2021a; Chen et al., 2021; Austin et al., 2021; Schuster et al., 2021b; Biderman & Raff, 2022), play chess (Noever et al., 2020; Stöckl, 2021), diagnose medical conditions (Rasmy et al., 2021), and translate languages (Sutskever et al., 2014). These breakthroughs have the potential to transform technology, but understanding the capabilities and limitations of language models, and how they may change as models are improved, is essential to ensure that these transformations are beneficial and do not embed undesirable social bias. Existing benchmarks are insufficient for this purpose, as they are narrowly targeted, often reach human-equivalent performance quickly, and use data collected through human labeling. Therefore, we introduce Beyond the Imitation Game (BIG-bench), a large-scale, extremely difficult and diverse benchmark, which is measured across scales to enable extrapolation of when models may be indistinguishable from human evaluators. We also introduce BIG-bench Lite, a subset of 24 tasks for lightweight evaluation. We analyze dense and sparse transformer models from Google and OpenAI, across six orders of magnitude of model scale, to anticipate future capabilities and identify how specific capabilities evolve with scale.",https://arxiv.org/pdf/2206.04615.pdf
Multi-Contrast MRI Synthesis with Channel-Exchanging-Network,"Magnetic resonance imaging (MRI) is used in many diagnostic applications as it has a high soft-tissue contrast and is a non-invasive medical imaging method. MR signal levels differs according to the parameters T1, T2 and PD that change with respect to the chemical structure of the tissues. However, long scan times might limit acquiring images from multiple contrasts or if the multi-contrasts images are acquired, the contrasts are noisy. To overcome this limitation of MRI, multi-contrast synthesis can be utilized. In this paper, we propose a deep learning method based on Channel-Exchanging-Network (CEN) for multi-contrast image synthesis. Demonstrations are provided on IXI dataset. The proposed model based on CEN is compared against alternative methods based on CNNs and GANs. Our results show that the proposed model achieves superior performance to the competing methods. ","Magnetic resonance imaging (MRI) is used in many diagnostic applications as it has a high soft-tissue contrast and is a non-invasive medical imaging method. MR signal levels differs according to the parameters T1, T2 and PD that change with respect to the chemical structure of the tissues. Thus, images of the same anatomy from multiple contrasts can be obtained via MRI. According to the anatomical differences, the acquired images can be T1-weighted, T2-weighted, or PD-weighted. For instance, T1-weighted brain scans can distinguish white and gray matter better while PD-weighted images distinguish cortical tissue from fluids better. Evaluating the images of the same tissue from different contrasts also increases the accuracy of the clinical diagnosis. Although multi-contrast images provide more information for clinical diagnosis, the required scan durations are long. For patients at an advanced or very early age, the durations might even be longer. Thus, acquiring images from multiple contrasts might not be possible. Even if the images are acquired, they would be corrupted with noise and have low quality due to patient motion [1]. To overcome this limitation of multi-contrast imaging, multi-contrast image acquisitions should be accelerated without decreasing the quality of the images. A common approach is image reconstruction from under-sampled data to accelerate MR scans via compressed sensing (CS) [2]–[5]. CS enforces sparsity of images in a transform domain to recover from randomly sampled data. Another popular approach is image reconstruction via deep neural networks [6]–[9]. Since deep models require training on fully-sampled acquisitions that can be costly to collect, recent methods have aimed to lower reliance on large, paired training datasets. Domain-transferred models are firstly trained in a source domain where data is abundant, then transferred to the target domain for reconstruction [7]. A fundamental limitation of acceleration by reconstruction is that one must have undersampled acquisitions of the target image for recovery. In many cases, however, high-quality data from the target might not be available due to scan time limitations or artifacts that corrupt the scan. Synthesis is an alternative framework to cope with these cases, where missing or corrupted contrasts are recollected from the set of acquired contrasts in a multi-contrast MRI protocol. Multi-contrast MRI methods typically use one-to-one or many-to-one synthesis procedures according to the input when the target contrast is 978-1-6654-5092-8/22/$31.00 ©2022 IEEE single. One-to-one approaches [10]–[13] use a single source  contrast as input and develop a latent representation that is sensitive to the source’s unique properties. Many-to-one approaches [13]–[17], on the other hand, accept several distinct sources and develop a shared latent representation that is more sensitive to common characteristics across sources [18]. Apart from many-to-one and one-to-one methods, a joint many-toone and a combination of several one-to-one streams have been used [18]. In this work, we propose a deep learning method based on Channel-Exchanging-Network (CEN) for multi-contrast image synthesis. The proposed method enables aggregation of information from multiple different source contrasts during manyto-one mapping without introducing additional parameters related to fusion modules. Demonstrations are provided on IXI dataset containing T1-, T2-, and PD-weighted images. The proposed model based on CEN is compared against alternative methods based on CNNs and GANs. Our results show that the proposed model achieves superior performance to the competing methods.",Synthesizing MRI Images with Channel-Exchanging-Network,"MRI has high soft-tissue contrast and is a non-invasive method, making it ideal for diagnostic purposes. Its signal levels are affected by T1, T2 and PD, which vary depending on the tissue's chemical structure. However, long scan times can make it difficult to acquire multiple contrasts. To address this limitation, multi-contrast synthesis using deep learning based on Channel-Exchanging-Network (CEN) is proposed in this paper. Demonstrations on IXI dataset show that CEN outperforms CNNs and GANs in multi-contrast image synthesis.","MRI is a non-invasive medical imaging method with high soft-tissue contrast, and is used for many diagnostic applications. Images of the same anatomy can be obtained via MRI by changing the parameters T1, T2, and PD that correspond to the chemical structure of the tissues, resulting in T1-weighted, T2-weighted, and PD-weighted images. For example, T1-weighted brain scans can distinguish white and gray matter better while PD-weighted images distinguish cortical tissue from fluids better. Multi-contrast images provide more information for clinical diagnosis but the scan duration may be too long for patients at an advanced or very early age, resulting in noisy and low-quality images due to patient motion. To overcome this limitation of multi-contrast imaging, image reconstruction from undersampled data via compressed sensing (CS) or deep neural networks can be used to accelerate MR scans. Furthermore, synthesis is an alternative framework to cope with missing or corrupted contrasts, where missing or corrupted contrasts are recollected from the set of acquired contrasts in a multi-contrast MRI protocol. In this work, a deep learning method based on Channel-Exchanging-Network (CEN) is proposed for multi-contrast image synthesis. The proposed method enables aggregation of information from multiple different source contrasts during many-to-one mapping without introducing additional parameters related to fusion modules. Demonstrations on the IXI dataset containing T1-, T2-, and PD-weighted images show that the proposed model based on CEN achieves superior performance to the competing methods based on CNNs and GANs.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9864937
Leveraging semantic saliency maps for query-specific video summarization,"The immense amount of videos being uploaded to video sharing platforms makes it impossible for a person to watch all the videos understand what happens in them. Hence, machine learning techniques are now deployed to index videos by recognizing key objects, actions and scenes or places. Summarization is another alternative as it offers to extract only important parts while covering the gist of the video content. Ideally, the user may prefer to analyze a certain action or scene by searching a query term within the video. Current summarization methods generally do not take queries into account or require exhaustive data labeling. In this work, we present a weakly supervised query-focused video summarization method. Our proposed approach makes use of semantic attributes as an indicator of query relevance and semantic attention maps to locate related regions in the frames and utilizes both within a submodular maximization framework. We conducted experiments on the recently introduced RAD dataset and obtained highly competitive results. Moreover, to better evaluate the performance of our approach on longer videos, we collected a new dataset, which consists of 10 videos from YouTube and annotated with shot-level multiple attributes. Our dataset enables much diverse set of queries that can be used to summarize a video from different perspectives with more degrees of freedom.","Recent advancements in digital imaging technologies and their increasing involvement in people’s everyday lives has led to a massive increase in the amount of visual data being uploaded to the Internet. In addition, public and private institutions are now heavily using visual surveillance systems to constantly monitor different areas of cities and buildings thereby further adding to this surplus. Handling this much visual data and providing ways to make them easier to digest are important more than ever, which pose many challenges for computer vision researchers. Video summarization has gained interest as a prominent research problem which aims at finding the most essential part of a video, eliminating as much redundancy as possible [26]. Video summarization approaches first split a given video into pieces in the form of video frames or video shots, and then extract features from each of these pieces. Then, they select a subset of them by considering notions such as relevance, diversity, and coherency [44]. Most recent studies cast video summarization as a supervised learning problem, and additionally learn to estimate importance of each video piece from a training set containing a number of videos with their groundtruth annotations [6, 7]. A key difficulty with these generic video summarization methods lies in their evaluation [34, 35, 41]. Researchers commonly employ certain evaluation metrics such as F1-score and prediction accuracy to assess the summarization performance, yet these metrics are not highly correlated with the human judgments. The primary reason for this comes from the subjectivity of the summarization process in that each user has certain preferences over the importance of video pieces to be included in the summary, resulting in different summaries by different individuals that are not fully coincide with each other. Hence, designing a summarization approach that can fulfill the preference of each user is almost impossible. Very recently, the so-called query-specific or query-focused video summarization approaches have been proposed as an attempt to alleviate the aforementioned issue [9, 33– 35, 41, 48]. These approaches differ from the generic video summarization techniques in one important aspect. The summarization process is carried out by considering a set of preference terms in the form of textual queries. For instance, for a video shot in a restaurant, if the input query is given as ‘food and drink’, the parts of the video showing either the food or the drinks get higher importance, instead of close ups of people conversing with each other. As an example, Fig. 1 demonstrates two different summaries obtained from the same video sequence by considering two different query terms. As can be seen, each query term encodes a different concept and the extracted query-specific summary includes only the synopsis of the video relevant to the given query. Hence, this makes the evaluation process much more objective than that of generic video summarization. Query-specific summarization has its own challenges. First and foremost, it requires a common understanding of given textual queries and the existing visual data. To succeed, the models need to form a bridge between these two different modalities and select the summary shots accordingly by integrating the information extracted from them. Using textual queries allows for a more personalized way of summarizing videos, which widens the range of its real-life applications. As the information considered important could vary from one person to another and from an application domain to a different one, one can use these query-specific summarization models for various reasons such that obtaining snapshots of important and/or interesting events in news media coverage or surveillance videos. Most existing query-specific summarization methods are supervised approaches and heavily utilize labels associated with video parts together with groundtruth summaries in learning to summarize according to queries [9, 33–35, 41, 48]. In this study, we propose a weakly-supervised video summarization approach, which does not require a large set of training videos with specific query terms and the relevant ground truth summaries. We rather assume that some pre-defined visual classifiers are available during summarization for every textual query. We use these visual classifiers to determine the relevance scores between the concepts seen in each video shot and the query terms. Moreover, utilizing these classifiers, we extract class-specific saliency maps that are then used to select the relevant image regions and visual features from video frames. We use these to define novel objective functions that encode different aspects of a good summary, in which we cast the summarization process as a submodular optimization task and employ a greedy search algorithm to select video shots for the summary in relation to given query terms. To our knowledge, we are the first to follow such a strategy for summarizing videos. There are some previous attempts to fuse semantical and visual information through class-specific saliency maps [27], yet they are too limited. They only estimate shot-level importance scores by computing the average saliency scores and then use the topmost important shots as the video summary. In summary, our contributions can be summarized as follows: – We propose a new query-specific video summarization approach, which leverage weak supervision in the form of semantic saliency maps obtained from predictions of pretrained attribute/action classifiers. Hence, our approach can be easily extended to novel domains without any training. – We collect the Activity Related Summaries (ARS) dataset which consists of videos involving group and individual activities. Compared to similar datasets, our ARS dataset involves videos that are significantly much longer and thus we may utilize multiple query terms to obtain fundamentally different summaries of each video. – We demonstrate the effectiveness of the proposed summarization approach on our ARS dataset as well as on RAD dataset [41], another query-specific video summarization dataset. Our ARS dataset together with groundtruth attribute annotations and reference summaries are publicly available at the project website.1 The rest of the paper is organized as follows: In Section 2, we give a brief overview of the existing generic and query-specific video summarization approaches along with the semantic saliency models. In Section 3, we introduce our ARS dataset by examining our data collection strategy and providing its statistics. In Section 4, we present the details of our proposed approach for weakly-supervised query-specific video summarization. In Section 5.3, we show the results of our experimental evaluation. Finally, in the last section, we offer some concluding remarks and discuss possible research directions for future work.",Utilizing semantic relevance maps for query-specific video summary,"It is infeasible for a person to watch all the videos on video sharing platforms and comprehend their content. Therefore, machine learning techniques are employed to index videos by identifying essential objects, actions, places and scenes. Summarization is another choice that enables extraction of relevant parts while maintaining the essence of the video. The user may prefer to search a specific action or scene with a query term. Existing summarization methods generally do not take queries into account or require exhaustive data labelling. This paper proposes a weakly supervised query-focused video summarization technique that makes use of semantic attributes as a marker of query relevance and semantic attention maps to locate related regions in the frames. This approach is employed in a submodular maximization framework. Experiments on the RAD dataset produced promising results. Furthermore, a new dataset was created which consists of 10 YouTube videos with shot-level multiple attributes. This dataset enables a more diverse set of queries to summarize videos from various perspectives with more freedom.","Advancements in digital imaging tech and its use in daily life have caused a surge in visual data online. Plus, institutions are using surveillance systems to monitor places, making the need to manage and make sense of the data even greater. Video summarization is an active research topic that seeks to find the most essential part of a video, removing redundancy. It splits the video into frames/shots and extracts features from each one, then chooses a subset considering relevance, diversity, and coherency. Recent studies have seen it as a supervised learning problem, using a training set of videos and groundtruth annotations to estimate the importance of each piece. But, there is a difficulty in evaluation due to the subjectivity of the summarization process. Query-specific or query-focused video summarization approaches have been proposed in an attempt to address this issue, by considering preference terms in the form of textual queries. For instance, for a restaurant video, the query 'food and drink' would prioritize shots with food or drinks. Recently, a weakly-supervised video summarization approach was proposed, which does not require a large set of training videos and relevant ground truth summaries. Rather, it assumes some pre-defined visual classifiers are available during summarization for each textual query. Saliency maps are used to select the relevant image regions and visual features from video frames, and the summarization process is cast as a submodular optimization task. The Activity Related Summaries (ARS) dataset was collected, containing videos of group and individual activities. Experiments showed the effectiveness of the proposed summarization approach on the ARS and RAD datasets. The ARS dataset, groundtruth attribute annotations, and reference summaries are publicly available.",https://link.springer.com/content/pdf/10.1007/s11042-022-12442-w.pdf
"Neural Natural Language Generation: A Survey on Multilinguality, Multimodality, Controllability and Learning","Developing artificial learning systems that can understand and generate natural language has been one of the long-standing goals of artificial intelligence. Recent decades have witnessed an impressive progress on both of these problems, giving rise to a new family of approaches. Especially, the advances in deep learning over the past couple of years have led to neural approaches to natural language generation (NLG). These methods combine generative language learning techniques with neural-networks based frameworks. With a wide range of applications in natural language processing, neural NLG (NNLG) is a new and fast growing field of research. In this state-of-the-art report, we investigate the recent developments and applications of NNLG in its full extent from a multidimensional view, covering critical perspectives such as multimodality, multilinguality, controllability and learning strategies. We summarize the fundamental building blocks of NNLG approaches from these aspects and provide detailed reviews of commonly used preprocessing steps and basic neural architectures. This report also focuses on the seminal applications of these NNLG models such as machine translation, description generation, automatic speech recognition, abstractive summarization, text simplification, question answering and generation, and dialogue generation. Finally, we conclude with a thorough discussion of the described frameworks by pointing out some open research directions. ","Humans communicate and express information through natural language. Research within Artificial Intelligence, in particular Natural Language Processing (NLP), is concerned with the automatic analysis, representation and generation of human language. Generation of language is the focus of the subfield of Natural Language Generation (NLG). Generation is at the heart of human-machine interfaces. Examples of tasks that facilitate this are dialogue systems, question answering, machine translation, summarization and image captioning. Traditionally, NLG has been approached as a pipeline of several stages (Reiter & Dale, 2000), involving (i) macroplanning – deciding “what to say?”, (ii) microplanning – choosing the appropriate structures and vocabulary, and (iii) surface realization – determining the final output, or “how to say it?”, given the information provided in the previous stages. While early work on language generation relied on linguistic patterns which had been defined a priori, the field has witnessed a revolution in the past few years. The development and evaluation of statistical models based on neural architectures —Neural Natural Language Generation (NNLG) —has shifted the research focus away from knowledge-based approaches motivated by linguistic theories, which predominated in the 90s (Bateman & Zock, 2003). The increasing number of neural approaches for NLG has been also reported and make evident in the recent NLG surveys, e.g., (Gatt & Krahmer, 2018; Iqbal & Qureshi, 2020), further detailed in Section 2. In parallel, another development has been the exponential increase in information, both in terms of volume, and in terms of type, format, language, etc. This has increased the importance of multilinguality and multimodality in recent NLG approaches. Two further developments which have an impact on NLG are learning strategies and controllability. These four dimensions—multilinguality, multimodality, learning strategies, controllability— are important in the context of NNLG to maximize its potential. Tasks such as Machine Translation, Question Generation, and Abstractive Summarization, among others, are examples of applications in which these dimensions can play a crucial role, both individually and in tandem. Therefore, the objective of this survey is to provide the reader with an overview of recent advances in NNLG from a multidimensional perspective, focusing on the most recent neural approaches. In this survey, both aspects—dimensions and tasks—are discussed in detail, demonstrating their implementation in NNLG tasks and applications. To provide the reader with a general overview of the state of the art in NNLG, the rest of the paper is structured as follows: We first provide a brief list of the most recent surveys related to different aspects of NLG with an emphasis on neural approaches, together with the main scope of the present survey (Sections 2 and 3). In Section 4, we present the four main dimensions in which NNLG can be studied. In Section 5, we describe the fundamental building blocks, including common preprocessing steps, and basic neural architectures. After that, in Section 6, we investigate all major language- and speech-related NNLG applications widely studied in the community. In particular, we provide an in-depth review of seven popular tasks, namely machine translation, description generation,1 automatic speech recognition, abstractive summarization, text simplification, question answering/generation, and dialogue generation. Finally, we give concluding remarks in Section 7 where we outline some open research directions.","Survey of Multilinguality, Multimodality, Controllability & Learning in Neural Natural Language Generation","Advances in deep learning have enabled artificial intelligence to achieve the long-standing goal of understanding and generating natural language. This has led to the emergence of a new family of approaches known as neural natural language generation (NNLG). With applications in natural language processing, NNLG is a rapidly growing field of research. This report provides a multidimensional overview of NNLG, including preprocessing steps, basic neural architectures, multimodality, multilinguality, controllability, and learning strategies. It also covers seminal applications of NNLG such as machine translation, description generation, automatic speech recognition, abstractive summarization, text simplification, question answering and generation, and dialogue generation. Lastly, the report concludes with a discussion of open research directions.","Humans communicate by natural language, which is studied in Artificial Intelligence (AI) field of Natural Language Processing (NLP). A subset of NLP, Natural Language Generation (NLG), focuses on the production of language for human-machine interfaces. Traditional NLG consists of three stages: macroplanning (deciding “what to say?”), microplanning (choosing structures and vocabulary), and surface realization (determining output, or “how to say it?”). Recent NLG research has shifted from knowledge-based approaches to statistical models based on neural architectures, or Neural Natural Language Generation (NNLG). This shift is due to the increase in information volume and type, resulting in the need for multilinguality and multimodality in NLG. In addition, learning strategies and controllability are also important dimensions of NNLG. Tasks such as Machine Translation, Question Generation, and Abstractive Summarization are examples of applications that can benefit from these dimensions. This survey provides an overview of recent advances in NNLG, focusing on the most recent neural approaches, and includes discussion of the four main dimensions, building blocks, and seven popular tasks.",https://dl.acm.org/doi/pdf/10.1613/jair.1.12918
Stochastic Video Prediction with Structure and Motion,"While stochastic video prediction models enable future prediction under uncertainty, they mostly fail to model the complex dynamics of real-world scenes. For example, they cannot provide reliable predictions for scenes with a moving camera and independently moving foreground objects in driving scenarios. The existing methods fail to fully capture the dynamics of the structured world by only focusing on changes in pixels. In this paper, we assume that there is an underlying process creating observations in a video and propose to factorize it into static and dynamic components. We model the static part based on the scene structure and the ego-motion of the vehicle, and the dynamic part based on the remaining motion of the dynamic objects. By learning separate distributions of changes in foreground and background, we can decompose the scene into static and dynamic parts and separately model the change in each. Our experiments demonstrate that disentangling structure and motion helps stochastic video prediction, leading to better future predictions in complex driving scenarios on two real-world driving datasets, KITTI and Cityscapes. ","Videos contain visual information enriched by motion. Motion is a useful cue for reasoning about human activities or interactions between objects in a video. Given a few initial frames of a video, our goal is to predict several frames into the future, as realistically as possible. By looking at a few frames, humans can predict what will happen next. Surprisingly, they can even attribute semantic meanings to random dots and recognize motion patterns [1]. This shows the importance of motion to infer the dynamics of the video and to predict the future frames. Motion cues have been heavily utilized for future frame prediction in computer vision. A common approach is to factorize the video into static and dynamic components [2, 3, 4, 5, 6, 7, 8, 9]. First, most of the previous methods are deterministic and fail to model the uncertainty of the future. Second, motion is typically interpreted as local changes from one frame to the next. However, changes in motion follow certain patterns when observed over some time interval. Consider scenarios where objects move with nearconstant velocity, or humans repeating atomic actions in videos. Regularities in motion can be very informative for future frame prediction. The world observed from a moving vehicle can be decomposed into a static part which moves only according to the motion of the vehicle, or the ego-motion, and a dynamic part containing independently moving objects. With two different types of motion, the future is quite uncertain and hard to predict but also crucial for the high-level decision making process of an autonomous vehicle. The existing future frame prediction methods either ignore the uncertainty of the future or fail to fully capture the dynamics of the structured world by only focusing on changes in pixels. Despite drastic changes in the appearance of pixels, there are some common factors creating the observations which are shared across frames We compare the future prediction results of previous methods (top) to ours (bottom) while the vehicle makes a right turn. While previous methods fail due to a less frequent scenario in the dataset, our method can generate a better future prediction due to explicit modeling of the structure and ego-motion. This figure shows a single frame after the conditioning frames, please see Fig. 7 for the whole sequence. in a sequence. In this paper, we model, relate, and predict these factors to generate better predictions of future frames in a video. Reliable future predictions can help autonomous vehicle anticipate future and plan accordingly. Observations from a moving vehicle depend on some factors which smoothly evolve through time. We exploit this continuity to predict future frames matching the observed sequences. We first factorize the underlying process leading to observations as the scene structure, the ego-motion of the vehicle, and the motion of the dynamic objects. After obtaining this factorization for the previous frames, we make future predictions for each component conditioned on the past. Then, based on these predictions of the structure and the two types of motion, we generate future frames. In other words, rather than modelling the stochasticity of the future in the noisy pixel space, we model the stochasticity in terms of the underlying factors generating the pixels. In our experiments, we show that the structure and motion of the scene are continuous and can be propagated to the future more reliably than the pixels in real-world sequences. The inherent uncertainty of the future has been addressed by the stochastic video prediction methods. Earlier methods encode the dynamics of a video in stochastic latent variables which are then decoded to predict future frames [10]. Our previous work [11] proposes to incorporate the motion history by explicitly predicting motion between consecutive frames. In particular, we learn two separate distributions representing the changes in the pixel space and the motion space. Similarly, in this work, we decompose the scene into static and dynamic components but we focus on driving scenarios where the static part also moves. By using the domain knowledge [12, 13], we model the structure and the ego-motion together for the static part. Then, we observe that the changes to the foreground objects are created by both the ego-motion and the independent motion of the object itself. In order to learn the object motion as residual motion on top of the ego-motion, we condition the dynamic latent variables on the static [14]. To the best of our knowledge, our method is the first to decompose the motion in a scene by separating ego-motion and object motion for stochastic video prediction. We show that this separation improves the performance of future prediction in real-world scenes with a moving background and independently moving foreground objects on two real-world driving datasets, KITTI [15, 16] and Cityscapes [17]. Furthermore, conditioning the object motion on the ego-motion improves the results, especially for foreground objects in dynamic scenes of Cityscapes. Our method performs on-par with the state-of-the-art method, Improved-VRNN [18] while being 40 × faster. Moreover, overall performance gaps compared to Improved-VRNN are due to background regions. Improved-VRNN performs better in the background regions, whereas our method’s performance in the foreground objects is better, which shows our model’s ability to capture dynamic objects. Our model is designed to predict future frames, but it can also generate future depth, pose, and optical flow to synthesize the target frame without even seeing the target frame. We evaluate our depth predictions in comparison to the state-of-theart monocular depth estimation method [13]. Moreover, we evaluate our model in terms of diversity compared to Improved-VRNN. Our results show that our model can pinpoint the uncertainty into foreground regions or mostly moving objects whereas ImprovedVRNN uniformly distributes the uncertainty over the whole scene. A preliminary, technical report version of our work is available online [19]. ",Video Prediction with Stochasticity and Structure/Motion,"Stochastic video prediction models are inadequate for modeling the intricacies of real-world scenes, such as those with moving cameras and independently moving foreground objects in driving settings. To address this, we propose factoring the process of generating observations in a video into static and dynamic components. The static part is based on the scene structure and the vehicle's ego-motion, while the dynamic part is dependent on the motion of the dynamic objects. Our technique of disentangling structure and motion helps stochastic video prediction, thereby enhancing future predictions in complex driving scenarios on KITTI and Cityscapes datasets.","Videos contain motion-enriched visual information. We aim to predict future frames as realistically as possible given initial frames. Humans can do this even with random dots and recognize motion patterns, proving the importance of motion for inferring video dynamics and predicting future frames. Computer vision uses motion cues heavily for this purpose, often by factoring the video into static and dynamic components. These methods are deterministic and lack uncertainty modelling, as well as only focusing on changes between frames instead of over intervals. In this paper, we model, relate, and predict these factors to generate better predictions, with reliable predictions helping autonomous vehicles anticipate and plan. We factorize the underlying process, then make predictions for each component based on the past. By modelling the stochasticity in terms of the generating factors rather than pixels, we show the scene structure and motion are continuous and can be propagated to the future more reliably. To address uncertainty, we learn two distributions for changes in pixel space and motion space. We incorporate the motion history, explicitly predicting motion between frames, and decompose the scene into static and dynamic components, modelling the structure and ego-motion together for the static part. To capture object motion, we condition dynamic latent variables on the static. We evaluate our depth predictions, diversity, and performance compared to Improved-VRNN. Our results demonstrate our model's ability to capture dynamic objects, pinpointing the uncertainty into moving objects.",https://arxiv.org/pdf/2203.10528.pdf
Modulating Bottom-Up and Top-Down Visual Processing via Language-Conditional Filters,"How to best integrate linguistic and perceptual processing in multi-modal tasks that involve language and vision is an important open problem. In this work, we argue that the common practice of using language in a topdown manner, to direct visual attention over high-level visual features, may not be optimal. We hypothesize that the use of language to also condition the bottom-up processing from pixels to high-level features can provide benefits to the overall performance. To support our claim, we propose a U-Net-based model and perform experiments on two language-vision dense-prediction tasks: referring expression segmentation and language-guided image colorization. We compare results where either one or both of the top-down and bottom-up visual branches are conditioned on language. Our experiments reveal that using language to control the filters for bottom-up visual processing in addition to top-down attention leads to better results on both tasks and achieves competitive performance. Our linguistic analysis suggests that bottom-up conditioning improves segmentation of objects especially when input text refers to low-level visual concepts","As human beings, we can easily perceive our surroundings with our visual system and interact with each other using language. Since the work of Winograd [75], developing a system that understands human language in a situated environment has been one of the long-standing goals of artificial intelligence. Recent successes of deep learning studies in both language and vision domains have increased the interest in tasks that combine language and vision [2, 3, 33, 41, 69, 77]. However, how to best integrate linguistic and perceptual processing is still an important open problem. Towards this end, we investigate whether language should be used for conditioning bottom-up visual processing as well as top-down attention. In the human visual system, attention is driven by both top-down cognitive processes (e.g. focusing on a given shape) and bottom-up salient, behaviourally relevant stimuli (e.g. fast moving objects and contrasting colors) [13,14,71]. Studies on embodied language explore the link between linguistic and perceptual representations [21,64,73] and often assume that language has a high-level effect on perception and drives the top-down visual attention [5, 18, 36]. However, recent studies from cognitive science point out that language comprehension also affects low-level visual processing [6, 52, 59]. Motivated by this, we propose a model that can modulate either or both of bottom-up and top-down visual processing with language and compare different designs for language modulation. Current deep learning systems for language-vision tasks typically start with low-level image processing, then connect the language representation with high-level visual features to control the visual focus. To integrate both modalities, concatenation [54], element-wise multiplication [40, 49], attention from language to vision [1, 48, 76, 83, 88] and transformers [17, 19, 70] are commonly used. These studies typically do not condition low-level visual features on language. Some methods [15,63] do the opposite by conditioning only the bottom-up visual processing on language. To evaluate language-modulation on the bottom-up and top-down visual branches independently, we develop an architecture that clearly separates these two branches (based on U-Net [66]) and allows us to experiment with modulating one or both branches with language. The bottomup branch starts from low-level visual features and applies a sequence of contracting filters that result in successively higher level feature maps with lower spatial resolution. Following this, a top-down branch takes the final low resolution feature map and applies a sequence of expanding filters that eventually result in a map with the original image resolution. Information flows between branches through skip connections between contracting and expanding filters at the same level. Our proposed architecture is task-agnostic and it can be used for various vision-language tasks involving dense prediction. We evaluate our model with different language-modulation settings on two different tasks: referring expression segmentation and language-guided image colorization. In the referring expression segmentation (RES) task, given an image and a natural language description, the aim is to obtain the segmentation mask that marks the object(s) described. We can contrast this with pure image based object detection [24, 65] and semantic segmentation [9, 47] tasks which are limited to predefined semantic classes. The language input may contain various visual attributes (e.g. shape), spatial information (e.g. in front of), actions (e.g. running) and interactions/relations between different objects (e.g. arm of the chair that the cat is sitting on). In language-guided image colorization (LIC) task, given a grayscale image and a description, the aim is to predict pixel color values. The absence of color information in the input images makes this problem interesting to experiment with because color words do not help in conditioning the bottom-up branch when the input image is grayscale. We find that conditioning both branches leads to better results, achieving competitive performance on both tasks. Our experiments suggest that conditioning the bottom-up branch on language is important to ground low-level visual information. On RES, we find that modulating only the bottom-up branch performs significantly better than modulating only the top-down branch especially when colordependent language is present in the input. Our findings on LIC show that when color information absent in input images, the bottom-up baseline naturally fails to predict and manipulate colors of target objects specified by input language. That said, conditioning the bottom-up branch still improves the colorization quality by helping our model to accurately segment and colorize the target objects as a whole. The rest of the paper is structured as follows: We summarize related work and compare it to our approach in Section 2. We describe our model in detail in Section 3. We share the details of our experiments in Section 4. Section 5 summarizes our contributions. ",Altering Visual Processing Through Language-Based Filters,"Integrating linguistic and perceptual processing in multi-modal tasks involving language and vision is an open problem. Commonly, language is used to direct attention to high-level visual features in a top-down manner, which may not be optimal. We propose a U-Net-based model and explore the use of language to condition bottom-up processing from pixels to high-level features in two language-vision dense-prediction tasks: referring expression segmentation and language-guided image colorization. Results show that conditioning both top-down attention and bottom-up visual processing with language leads to better results on both tasks and competitive performance. Further linguistic analysis reveals that bottom-up conditioning improves segmentation of objects when input text refers to low-level visual concepts.","Human beings can detect their environment and communicate using language. AI has a long-term goal of comprehending human language in a specific setting since Winograd's work. Deep learning studies in language and vision have increased focus on tasks that combine the two. The problem of integrating linguistic and perceptual processing is still present. We examine if language should control both top-down and bottom-up visual processing. Attention is directed by both cognitive processes and behaviourally significant stimuli. Studies explore the link between linguistic and perceptual representations, and often suggest language affects perception and directs visual attention. Recent studies in cognitive science also demonstrate language comprehension affects low-level visual processing. We propose a model to modulate either or both of bottom-up and top-down visual processing with language and compare different designs for language modulation. We develop an architecture to isolate these two branches and allow us to experiment with modulating one or both branches with language. We evaluate our model with different language-modulation settings on two different tasks: referring expression segmentation and language-guided image colorization. Results suggest conditioning both branches leads to better performance, and conditioning the bottom-up branch is important to ground low-level visual information. Modulating only the bottom-up branch performs better than modulating only the top-down branch, particularly when color-dependent language is present. Our findings on LIC show that when color information is absent in input images, the bottom-up baseline fails to predict and manipulate colors of target objects specified by input language. Conditioning the bottom-up branch still improves the colorization quality by helping our model to accurately segment and colorize the target objects as a whole. As humans, we are able to recognize our environment with our visual system and communicate with each other via language. Ever since Winograd's work, developing a system to understand human language in a situated environment has been an ongoing goal of artificial intelligence. The success of deep learning studies in both language and vision has increased attention to tasks that incorporate language and vision. Nevertheless, the best way to merge linguistic and perceptual processing remains an open issue. To address this, we explore if language should be used to control top-down and bottom-up visual processing. Attention is driven by both cognitive processes and salient, behaviorally relevant stimuli. Studies on embodied language explore the connection between linguistic and perceptual representations and often assume language has a high-level effect on perception and guides top-down visual attention. However, cognitive science studies suggest language comprehension also affects low-level visual processing. Thus, we present a model that can modulate either or both of bottom-up and top-down visual processing with language and compare different designs for language modulation. We develop an architecture that separates these two branches (based on U-Net) and enables us to experiment with modulating one or both branches with language. We evaluate our model with different language-modulation settings on two different tasks: referring expression segmentation and language-guided image colorization. Results show conditioning both branches leads to better results, and conditioning the bottom-up branch is essential to ground low-level visual information. Modulating only the bottom-up branch performs significantly better than modulating only the top-down branch particularly when color-dependent language is present in the input. Our findings on LIC show that when color information is absent in input images, the bottom-up baseline fails to predict and manipulate colors of target objects specified by input language. Conditioning the bottom-up branch still improves the colorization quality by helping our model to accurately segment and colorize the target objects as a whole.",https://openaccess.thecvf.com/content/CVPR2022W/MULA/papers/Kesen_Modulating_Bottom-Up_and_Top-Down_Visual_Processing_via_Language-Conditional_Filters_CVPRW_2022_paper.pdf
Burst Photography for Learning to Enhance Extremely Dark Images,"Capturing images under extremely low-light conditions poses significant challenges for the standard camera pipeline. Images become too dark and too noisy, which makes traditional enhancement techniques almost impossible to apply. Recently, learning-based approaches have shown very promising results for this task since they have substantially more expressive capabilities to allow for improved quality. Motivated by these studies, in this paper, we aim to leverage burst photography to boost the performance and obtain much sharper and more accurate RGB images from extremely dark raw images. The backbone of our proposed framework is a novel coarse-to-fine network architecture that generates high-quality outputs progressively. The coarse network predicts a low-resolution, denoised raw image, which is then fed to the fine network to recover fine-scale details and realistic textures. To further reduce the noise level and improve the color accuracy, we extend this network to a permutation invariant structure so that it takes a burst of low-light images as input and merges information from multiple images at the feature-level. Our experiments demonstrate that our approach leads to perceptually more pleasing results than the state-of-the-art methods by producing more detailed and considerably higher quality images.","CAPTURING images in low-light conditions is a challenging task – the main difficulty being that the level of the signal measured by the camera sensors is generally much lower than the noise in the measurements [1]. The fundamental factors causing the noise are the variations in the number of photons entering the camera lens and the sensor-based measurement errors occurred when reading the signal [2], [3]. In addition, noise present in a low-light image also affects various image characteristics such as fine-scale structures and color balance, further degrading the image quality. Direct approaches for capturing bright photos in low light conditions include widening the aperture of the camera lens, lengthening the exposure time, or using camera flash [1], [4]. These methods, however, do not solve the problem completely as each of these hacks has its own drawbacks. Opening the aperture is limited by the hardware constraints, and when the camera flash is used, the objects closer to the camera are brightened more than the objects or the scene elements that are far away [5]. Images captured with long exposure times might have unwanted image blur due to camera shake or object movements in the scene [6]. Hence, in the literature, there has been a wide range of studies which try to improve the quality of low-light images, ranging from traditional denoising and enhancement methods to learning-based approaches. Image denoising is one of the classical problems in image processing, where the aim is to restore a clean image from a noisy image. Several methods have been proposed over the years to denoise images [7]–[19]. Most of these approaches rely on the images with Gaussian noise for developing a denoising model. Recently, deep learning-based methods that can deal with real image noise have been proposed [3], [20]. However, these approaches are not specialized to extremely low-light images which are harder to restore than a standard noisy image. Image enhancement is another active field of research, which has seen tremendous progress in the past few years with deep learning [21]–[27]. Usually, these methods work with low dynamic range (LDR) input images and hence, their performance is also limited due to the errors accumulated in the camera processing pipeline. When compared to LDR images, raw images straight from the camera are more suitable to use for enhancing extremely low-light images since they contain more information and are processed minimally. In the context of enhancing extremely dark images, Seein-the-Dark (SID) [28] is the first learning-based attempt to replace the standard camera pipeline, training a convolutional neural network (CNN) model to produce an enhanced RGB image from a single raw low-light image. For this purpose, the authors collected a dataset of short-exposure, dark raw photos and their corresponding long-exposure references. Their method is further improved by Maharjan et al. [29] and Zamir et al. [30] with some changes in the CNN architecture and the objective functions utilized in training. In a similar fashion, in our study, we develop a new multi-scale architecture for single image enhancement and use a different objective by combining contextual and pixel-wise losses. While the previous methods obtain an RGB image from a single dark raw image, we further explore whether the results can be improved by integrating multiple observations regarding the scene. Bracketing is a well-known technique in photography that relies on rapidly taking several shots of the same scene. These shots usually differ from each other in terms of some ",Enhancing Dark Images with Burst Photography,"Capturing images in dim lighting is difficult for the standard camera pipeline. Images become too dark and noisy, making traditional enhancement difficult. Recently, learning-based approaches have demonstrated potential for this task due to their ability to create improved quality. To boost performance, this paper proposes a novel coarse-to-fine network architecture that generates high-quality outputs gradually. This network predicts a low-resolution, denoised raw image, which is then fed to the fine network to recover small-scale details and textures. To reduce noise and improve color accuracy, the network is extended to a permutation invariant structure which takes a burst of low-light images as input and combines information from multiple images at the feature-level. Results show that our approach produces more detailed and higher quality images than existing methods.","Capturing images in low-light is difficult due to signal from camera sensors being much lower than noise. Variations in number of photons entering lens and sensor-based measurement errors cause the noise. Low-light images also affect image characteristics like color balance, degrading quality. Widening aperture, increasing exposure time or using camera flash are direct methods to capture bright photos, but have drawbacks. Literature has various studies to improve quality of low-light images, including traditional denoising/enhancement and learning-based approaches. Denoising is a classical problem in image processing with aim of restoring clean image from noisy one. Several methods exist, mostly relying on Gaussian noise for developing denoising model. Deep learning-based methods for real image noise have been proposed, but not specialized for extremely low-light images. Image enhancement also has seen progress with deep learning, usually with LDR input images, which can be limited due to errors in camera processing pipeline. Raw images from camera are more suitable for enhancing extremely low-light images due to more info and minimal processing. Seein-the-Dark (SID) was first attempt to replace standard camera pipeline, training CNN to produce enhanced RGB image from single raw low-light image. Improved by Maharjan et al. and Zamir et al. with changes in CNN architecture and objective functions used in training. We develop new multi-scale architecture for single image enhancement and use different objective combining contextual and pixel-wise losses. Results can be improved by integrating multiple observations of scene, using bracketing technique in photography of taking several shots of same scene with difference in some parameters.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9614963
"From Noon to Sunset: Interactive Rendering, Relighting, and Recolouring of Landscape Photographs by Modifying Solar Position ","Image editing is a commonly studied problem in computer graphics. Despite the presence of many advanced editing tools, there is no satisfactory solution to controllably update the position of the sun using a single image. This problem is made complicated by the presence of clouds, complex landscapes, and the atmospheric effects that must be accounted for. In this paper, we tackle this problem starting with only a single photograph. With the user clicking on the initial position of the sun, our algorithm performs several estimation and segmentation processes for finding the horizon, scene depth, clouds, and the sky line. After this initial process, the user can make both fine- and large-scale changes on the position of the sun: it can be set beneath the mountains or moved behind the clouds practically turning a midday photograph into a sunset (or vice versa). We leverage a precomputed atmospheric scattering algorithm to make all of these changes not only realistic but also in real-time. We demonstrate our results using both clear and cloudy skies, showing how to add, remove, and relight clouds, all the while allowing for advanced effects such as scattering, shadows, light shafts, and lens flares","Sunsets and sunrises are among the most popular photographic subjects. However, capturing those moments are not simple especially if the photographer is also concerned with adjusting other scene elements to achieve a good composition. All important scene elements must be positioned properly and desired poses must be attained for all live subjects. The camera parameters must be set correctly, all the while accounting for the overall appearance shifts due to the continuous motion of the sun. Most importantly, decisions must be made quickly as there is generally a limited time-frame during which an ideal composition can be obtained. It is not uncommon for professional photographers to plan and wait for hours, even days, to capture compelling sunset and sunrise pictures. In this paper, we propose an image editing technique particularly suited for modifying the appearance of images with large skies and visible sun. With our technique, the position of the sun in the sky can be updated freely, which allows for both small and large scale modifications to the original picture (Figure 1). Small scale modifications can be made if the user is generally satisfied with the photograph, but wants to make some subtle adjustments to the sun’s position. Our technique also allows drastic modifications such as moving the sun behind the horizon, mountains, and clouds, bringing it backup, changing its both azimuth and elevation as well as entirely removing the sun or adding it to a picture that is devoid of it. Such modifications entail overall changes to the photograph so that the entire photograph remains consistent with respect to the sun’s updated state. We show that such effects are not only possible but can be applied in real-time to allow an interactive solution. Our algorithm is comprised of three key stages namely segmentation, rendering, and recolouring. In the segmentation stage, the important components such as the foreground, sun, sky, and horizon are detected, and a soft segmentation mask for the clouds are computed. In the rendering stage, using the sun’s position and the estimated camera parameters, a precomputed atmospheric scattering algorithm [Ele09] is applied to re-render the sky together with the clouds [Sch16]. The recolouring stage involves colour transfer from the original sky and recolouring of the foreground to make the entire photograph consistent with the updated position of the sun and the sky. With these three stages, the proposed algorithm can produce compelling results for a wide range of input photographs. To this end, the key contributions of the current work are: • The first single-image-based sun position modification algorithm, • Realistic handling of various appearance effects that stem from updating the sun’s position, • A real-time implementation allowing all modifications to be fully interactive. In the following, we first review the related work followed by an algorithmic overview and details of each step. We then demonstrate our visual results, compare it with alternatives, discuss its limitations, and outline future research directions.","Modifying Solar Position to Render, Relight, and Recolor Landscape Photos from Noon to Sunset","Despite advanced editing tools, there is no satisfactory way to adjust the sun's position in a single image, due to clouds, complex landscapes, and atmospheric effects. This paper proposes a solution, starting with a single photograph. With the user clicking the initial position of the sun, the algorithm performs estimation and segmentation to find the horizon, scene depth, clouds, and sky line. Subsequently, the user can make fine- and large-scale changes to the sun's position - beneath mountains or behind clouds - practically transforming a midday photo into a sunset (or vice versa). Leveraging precomputed atmospheric scattering, these changes are both realistic and real-time. Results using clear and cloudy skies demonstrate the ability to add, remove, and relight clouds, while also allowing for scattering, shadows, light shafts, and lens flares.","Sunsets and sunrises are among the most beloved photographic subjects, but taking such pictures is difficult due to various scene elements that must be adjusted for a good composition. All important components must be arranged properly and poses must be obtained for any living subjects, while camera settings and the changing motion of the sun must be taken into account. Professional photographers may plan and wait for hours or days to get the ideal shot. In this paper, we present an image editing technique suitable for altering the appearance of images with wide skies and visible sun. Our technique allows for minor or large-scale modifications, like moving the sun behind the horizon, mountains, or clouds, changing its azimuth and elevation, or removing/adding the sun. This includes overall changes to the photo to ensure consistency with the sun's updated state. The algorithm is composed of three stages: segmentation, rendering, and recolouring. In segmentation, components like the foreground, sun, sky, and horizon are detected, with a soft segmentation mask for the clouds computed. In rendering, a precomputed atmospheric scattering algorithm is applied to re-render the sky and clouds, based on the sun's position and estimated camera parameters. The recolouring stage involves colour transfer from the original sky and recolouring of the foreground to make the entire photograph match the updated sun position. Our technique can produce compelling results for a range of input photographs, making the following key contributions: the first single-image-based sun position modification algorithm, realistic handling of various appearance effects from updating the sun's position, and a real-time implementation allowing all modifications to be interactive. We review related work, provide an algorithmic overview and details of each step, demonstrate visual results, compare it with alternatives, discuss its limitations, and outline future research directions.",https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.14392
Synthetic18K: Learning better representations for person re-ID and attribute recognition from 1.4 million synthetic images,"Learning robust representations is critical for the success of person re-identification and attribute recognition systems. However, to achieve this, we must use a large dataset of diverse person images as well as annotations of identity labels and/or a set of different attributes. Apart from the obvious concerns about privacy issues, the manual annotation process is both time consuming and too costly. In this paper, we instead propose to use synthetic person images for addressing these difficulties. Specifically, we first introduce Synthetic18K, a large-scale dataset of over 1 million computer generated person images of 18K unique identities with relevant attributes. Moreover, we demonstrate that pretraining of simple deep architectures on Synthetic18K for person re-identification and attribute recognition and then fine-tuning on real data leads to significant improvements in prediction performances, giving results better than or comparable to state-of-the-art models.","n developed countries, video surveillance systems have become a vital component of public security, constantly monitoring cameras installed at various different key locations. Person re-identification (person re-ID) is one of the important tasks in video surveillance, which refers to the problem of automatically re-identifying across multiple different cameras with the help of computers. Different from other person-centric classification problems in computer vision such as face recognition, person re-ID systems use identity information only during training and assume the identities are unknown at test time. Person re-ID basically combines three different tasks, which are pedestrian detection, person tracking and person retrieval. Hence, it is extremely challenging due to large variations in lighting conditions, differences in pose and viewpoint changes. The person re-ID models proposed in the literature can be divided into two main groups: image-based methods and video-based methods. All these approaches are usually evaluated on benchmark datasets annotated with either detected or ground truth human boxes so the problem mostly reduces to the person image retrieval where the aim is to retrieve images of a specific person identity from a large gallery of images involving many different identities. Therefore, success can be defined by comparing the identities of the retrieved images with the identity of the query image. In video-based approaches to person re-ID, the models use multiple bounding boxes for a video query and the videos in the gallery, and additionally try to integrate the temporal information between these boxes. Although research in both groups can benefit from each other, they are considered as different problems. In this work, we tackle the problem of image based re-ID. As another critical task in video surveillance, person attribute recog- nition aims at detecting various attributes of a person such as hair color, clothing type and color. Although person attribute recognition has been relatively less studied as compared to person re-ID, these tasks are, in fact, closely related since the mid-level semantic attributes, once identified, provide an intuitive way to describe a specific individual. Hence, a recent direction being explored in recent years is to consider these two challenging tasks in a joint manner in order to improve the performances of each other. In the past few years, person re-ID research has reached a saturated point where researchers gently enhance the performances on popular benchmark datasets by designing more and more complex architectures or by applying complicated data augmentation schemes. That being said, most of the methods in the literature fail to generalize well to in-the-wild settings because of the fact that existing datasets cover a limited range of samples that could be faced in real life. However, obtaining a comprehensive dataset is very expensive to gather for which you have to use multiple camera sources located in very different environments. Even if you collect the right amount of visual data, the effort required for manual annotation is quite costly. Hence, the existing datasets are not challenging enough in demonstrating different weather and lighting conditions, varieties in person attributes and/or body types. Finally, privacy of the individuals is an important issue for video surveillance. Although these datasets are initially collected for academic purposes, the intention of users may be different when the data become public, leading invasion of privacy of the individual. For example, DukeMTMC-reID dataset [1] has been recently shut down and cannot be downloaded publicly to conform to privacy regulations. In this study, we deal with the problem of learning simple yet effective representations for the person re-identification and attribute recognition tasks. In particular, in both of these two tasks, the main challenge lies in learning discriminative features which are not sen- sitive to the changes in the appearance of the person of interest due to illumination variations, scale and viewpoint changes. To overcome these difficulties, in this study, we first introduce a new synthetic dataset called Synthetic18K. The proposed dataset, compared to the existing synthetic datasets mentioned above, is much larger in scale in terms of the number of identities/virtual persons it contains and the number of images that each virtual person has. That is, it contains approximately 1.4 million images of 18K unique virtual persons cap- tured in four synthetic environments (three outdoor and one indoor) as well as with various cubemaps taken in real-life. While generating these virtual persons and obtaining their images, we follow a procedural gen- eration method which gives us the ability to play with both the low and high-level attributes of these synthetic persons and the characteristics of the scenes (weather conditions, times of day, etc.). These aspects are of critical importance for feature learning as the existing real- world data are generally not diverse in various factors like illumination conditions, scenes, clothing, etc. are still considered as challenging tasks. Moreover, covering each one of these factors in a dataset in a balanced manner could be very difficult to achieve, resulting in heavy- tailed data distributions and poor performances for the rare cases. Tackling person re-identification and attribute recognition in a joint manner also introduces certain advantages as these tasks are considered complementary tasks. Yet, no other synthetic datasets handles these two in a combined manner. Motivated with these, in our work, we also propose pretraining strategies and simple yet effective deep neural architectures for both person re-identification and attribute recognition tasks. We show that our proposed Synthetic18K dataset can be used to learn more ro- bust feature representations for these two tasks. In particular, we proposed three different pretraining schemes, one for solely person re-identification, one for only attribute recognition and one final for a combination of these two tasks. We demonstrate that even simple neural architectures which are pretrained on our synthetically gen- erated images using these strategies and later on fine-tuned on real data, perform competitively compared to complex state-of-the-art mod- els. Our experiments also show handling person re-identification and attribute recognition together gives more accurate results than their single-task counterparts, indicating the importance of the proposed joint pretraining strategy. Our dataset and models will be available at the project website.",Improving Person Re-ID & Attribute Recognition with 1.4M Synthetic Images,"Obtaining strong representations is essential for the success of person re-identification and attribute recognition systems. To accomplish this, a considerable amount of diverse person images and annotations of identity labels/attributes must be employed. This manual annotation process is expensive and time consuming, as well as a potential privacy issue. We propose using synthetic person images to address these problems. To this end, we introduce Synthetic18K, a large dataset of 1 million computer generated person images of 18K distinct identities with corresponding attributes. We show that pretraining simple deep architectures on Synthetic18K for person re-identification and attribute recognition, followed by fine-tuning on real data, leads to enhanced prediction performances, surpassing or equaling state-of-the-art models.","Video surveillance systems have become essential for public safety in developed countries, with computers aiding in the task of person re-identification (person re-ID) across multiple cameras. This is different from other computer vision-based person-centric classification problems such as face recognition, as person re-ID systems only use identity information during training and assume identities are unknown at test time. Person re-ID involves pedestrian detection, person tracking and person retrieval, making it a difficult challenge due to large variations in lighting, pose and viewpoint. Person re-ID models can be divided into image- and video-based approaches, which are evaluated using detected or ground truth human boxes, thereby reducing the problem to image retrieval of a specific person identity from a gallery of images of many identities. Video-based approaches use multiple bounding boxes for the video query and gallery videos, and attempt to integrate temporal information between these boxes. Person attribute recognition, another critical task in video surveillance, detects attributes such as hair color, clothing type and color, and is closely related to person re-ID since the identified semantic attributes can describe an individual. To learn simple yet effective representations for person re-ID and attribute recognition, we introduce Synthetic18K, a large-scale synthetic dataset containing 1.4 million images of 18K virtual persons in four environments, with real-life cubemaps. We follow a procedural generation method, enabling control over low and high-level attributes, as well as characteristics of the scenes. We propose pretraining strategies and deep neural architectures for both tasks, demonstrating that even simple models pretrained on Synthetic18K and fine-tuned on real data can perform competitively compared to complex state-of-the-art models. Additionally, our experiments show joint pretraining of person re-ID and attribute recognition gives more accurate results than their single-task counterparts.",https://reader.elsevier.com/reader/sd/pii/S0923596521001491?token=3B8BD3557F1036071C523A3EBF25620F35426342EE5D83A64BA9AA84135115837600772B11C1FA052274748390881F2A&originRegion=eu-west-1&originCreation=20221228191659
NOVA: Rendering Virtual Worlds with Humans for Computer Vision Tasks,"Today, the cutting edge of computer vision research greatly depends on the availability of large datasets, which are critical for effectively training and testing new methods. Manually annotating visual data, however, is not only a labor-intensive process but also prone to errors. In this study, we present NOVA, a versatile framework to create realistic-looking 3D rendered worlds containing procedurally generated humans with rich pixel-level ground truth annotations. NOVA can simulate various environmental factors such as weather conditions or different times of day, and bring an exceptionally diverse set of humans to life, each having a distinct body shape, gender and age. To demonstrate NOVA’s capabilities, we generate two synthetic datasets for person tracking. The first one includes 108 sequences, each with different levels of difficulty like tracking in crowded scenes or at nighttime and aims for testing the limits of current state-of-the-art trackers. A second dataset of 97 sequences with normal weather conditions is used to show how our synthetic sequences can be utilized to train and boost the performance of deep-learning based trackers. Our results indicate that the synthetic data generated by NOVA represents a good proxy of the real-world and can be exploited for computer vision tasks.","The rapid progress in the field of computer vision and other AI related disciplines has been significantly driven by learning based methods, most notably those based on deep learning. Getting the best out of these approaches, however, broadly depends on the availability of large training data, and hence a major bottleneck on the way towards solving many computer vision tasks is the lack of diverse, accurate and large-scale datasets. Manually curating such large datasets is labour-intensive and often error-prone. Although Amazon’s Mechanical Turk or similar services can alleviate those issues, these tools are very expensive, especially for small research groups, if one wishes to capture the real-world in its full glory. But maybe more importantly, such crowdsourcing platforms become impractical for collecting ground truth data for some computer vision tasks (e.g. optical flow estimation). A neat idea to overcome these difficulties is to utilize synthetic data for machine learning, which has gained momentum over the past few years. Recent improvements in game technologies have made the creation of photorealistic and physically accurate games possible. Since designing virtual worlds from scratch can be very expensive and requires highly skilled artists, it is possible to make use of the games that are already available. Making modifications on an open-sourced game or capturing the information sent by the game to graphics card can help to generate large synthetic datasets. However, the fact that commercial games do not represent a proxy of many real-world scenarios poses an essential problem with this approach, limiting its benefits. Another way to create large synthetic datasets is to design the virtual world based on the needs. While it usually requires more effort  to create and configure, this approach makes it possible to produce a high-fidelity proxy of the targeted scenarios. With the advances in graphics engine capabilities within the past decade, the photorealistic and physically-based simulations realized by using these engines allowed to minimize the gap between real and virtual world data. Procedural generation has been proposed as a solution for creating realistic looking environments in relatively short amounts of time, making it easier and cheaper for users to generate virtual worlds from scratch. In its simplest form, a procedural generation framework follows some systematic recipes and generates scenes, populations and actions, based on the given set of instructions. Our work contributes to this line of research, in which we pay special attention to the human generation aspect – in addition to offering a comprehensive variety of automatic ground truth annotation features that are partially available in other synthetic data generation frameworks. The large-scale benchmark datasets that were collected in the past few years [DDS*09, LMB*14, KH09, GZW*] has lead to the unprecedented progress in deep learning based computer vision approaches. Although the exponential increase in the amount of digital data today can make data collection easier than before, manual labelling of large volumes of examples with high quality and accurate labels still requires too much effort and comes with a tremendous cost. Our proposed NOVA framework, with its procedural and automated generation capabilities, provides a solution to this daunting data collection/annotation challenge by letting the users create and render 3D virtual worlds containing human agents with different characteristics in real-time. The authors in [DSGCP17] previously proposed a similar framework but their focus is mainly on human action recognition and thus their framework has limited functionalities. On the other hand, in our proposed NOVA framework, the users have full control of the scenes, scene elements and humans, along with the illumination and weather conditions, allowing to study various factors affecting the success of their algorithms during development time and opening up a possibility being used in a wider range of computer vision tasks. The main contributions of this work can be summarized as follows: • We present a novel procedural content generation engine called NOVA. It is capable of generating large-scale and photo-realistic videos of human agents performing various actions on many different scenes along with the annotations for various computer vision tasks. • Using our NOVA rendering engine, we generate two synthetic datasets specifically designed for person tracking. While we use the first dataset to assess the performance of existing visual trackers on various conditions, we employ the second one to train deep visual trackers to boost their performances on real sequences. • Our experiments demonstrate that the existing trackers perform poorly in highly crowded scenes, or in scenes captured at night and in foggy weather conditions. Moreover, our generated synthetic sequences present a good proxy of the real sequences in that when used as training data, it improves the performances of deep visual trackers.",Creating Human-Populated Virtual Environments for Computer Vision,"Research in computer vision depends on large datasets, which are essential for training and testing new methods. Creating these datasets manually, however, is time-consuming and prone to errors. This study presents NOVA, a framework for generating 3D rendered worlds with procedurally generated humans with accurate pixel-level annotations. NOVA can simulate various environmental factors, such as weather and time of day, and create distinct humans in terms of body shape, gender, and age. To showcase NOVA's potential, two synthetic datasets for person tracking were generated. The first, consisting of 108 sequences with varying levels of difficulty, was used to test the limits of current trackers. The second, with 97 sequences in normal weather conditions, was used to train and improve the performance of deep-learning based trackers. Results indicate that the synthetic data generated by NOVA is a good representation of the real world and can be used for computer vision tasks.","Rapid progress in AI-related fields is mainly driven by learning-based methods, such as deep learning. A major bottleneck for many computer vision tasks is the lack of diverse, accurate, and large datasets. Manual curation of such datasets is costly and error-prone. Amazon's Mechanical Turk and similar services can help, but are expensive and impractical for certain tasks (e.g. optical flow estimation). Synthetic data has been proposed as a solution, with improvements in game technologies allowing for photorealistic and physically accurate simulations. Designing virtual worlds from scratch is expensive and requires skilled artists, so existing games can be modified or information sent to graphics cards can be captured to generate datasets. However, commercial games do not represent all real-world scenarios. Another way to create synthetic datasets is to design virtual worlds based on needs, though this requires more effort. Graphics engine capabilities in the past decade have allowed for photorealistic and physically-based simulations to minimize the gap between real and virtual worlds. Procedural generation has been proposed as a solution to create realistic environments quickly and cheaply. Our work focuses on human generation and offers automatic ground truth annotation features. The benchmark datasets collected in recent years have lead to progress in deep learning-based computer vision approaches. Manual labelling of large volumes of examples is expensive, so our proposed NOVA framework provides a solution to this challenge by letting users create and render 3D virtual worlds containing human agents with different characteristics in real-time. We compare our framework to [DSGCP17], which focuses on human action recognition and has limited functionalities. Our proposed NOVA framework provides full control of scenes, scene elements, humans, illumination, and weather conditions, allowing users to study various factors affecting the success of their algorithms and being used in a wider range of computer vision tasks.",https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.14271
Leveraging Frequency Based Salient Spatial Sound Localization to Improve 360◦ Video Saliency Prediction,"Virtual and augmented reality (VR/AR) systems dramatically gained in popularity with various application areas such as gaming, social media, and communication. It is therefore a crucial task to have the knowhow to efficiently utilize, store or deliver 360◦ videos for end-users. Towards this aim, researchers have been developing deep neural network models for 360◦ multimedia processing and computer vision fields. In this line of work, an important research direction is to build models that can learn and predict the observers’ attention on 360◦ videos to obtain so-called saliency maps computationally. Although there are a few saliency models proposed for this purpose, these models generally consider only visual cues in video frames by neglecting audio cues from sound sources. In this study, an unsupervised frequency-based saliency model is presented for predicting the strength and location of saliency in spatial audio. The prediction of salient audio cues is then used as audio bias on the video saliency predictions of state-of-the-art models. Our experiments yield promising results and show that integrating the proposed spatial audio bias into the existing video saliency models consistently improves their performance. ","Several studies [1–3], show that auditory inputs influence human attention mechanism, yet many visual saliency estimation models neglect the effects of auditory cues when estimating saliency maps. In addition, from a computational perspective, some of the earlier works [4–8] also demonstrated that visually salient cues are partly correlated with the audio source location and semantics. In particular, Tavakoli et al. [4] proposed an audio-visual deep learning model (DAVE), which has an encoder-decoder architecture for video saliency prediction. Min et al. [5] suggested a novel multimodal saliency (MMS) model for audio-visual attention, which is proposed to be combined with the existing deep learning-based saliency models with a late-fusion, and to promote their performance by an average of 5%. Tsiami et al. [6] proposed a single multimodal network (STAViS) for audio-visual saliency, which learns to localize sound sources and to fuse the audio and visual saliency maps. Chen et al. [7] proposed a deep neural network architecture for feature extraction, semantic interaction, and their fusion for auditory and visual inputs.On the other hand, to the best of our knowledge, spatial audio information has just begun to be used for saliency prediction in 360◦ videos [9]. Moreover, with the advent of 360◦ videos, the saliency prediction task has faced new challenges. Humans do not discover their 360° environments at a glance, but starting within a narrower viewport, and then they continuously work out the peripheries with their head/eye movements. In contrast, 360◦ video sequences are represented as fully observable to computers. This results in a contradictory behavior for perception between computers and humans. At this point, spatial audio cues, which provide directional information in 360◦ space can be incorporated alongside the visual cues to supply additional insight for localizing the saliency predictions in 360◦ videos. In this paper, the 360◦ video saliency prediction task is addressed by leveraging the spatial audio information to localize the audio saliency and enhance the output of the existing (audio-)visual saliency prediction models. For this purpose, we adapt the mel-cepstrum based spectral residual saliency detection model (MCSR) proposed by Imamoglu et al. [10], to spatial audio. Despite it was presented as an image saliency model, it is highly adaptable to audio processing since it consists of mel-frequency cepstral coefficients (MFCCs). The proposed audio saliency localization model is built for first-order ambisonics (FOA) in 4-channel B-format, which is widely used in VR applications for spatial audio experience. Each channel (W, X, Y, Z) in FOAencoded audio represents a different directionality in the 360◦ space: center, forward-backward, left-right and up-down, respectively [11]. The MCSR model is adapted to FOA by applying on each channel to localize the salient sound in 360◦ space, and also detect the strength of audio saliency in the time domain. The produced audio saliency maps are further combined with the existing five video saliency models with a late-fusion. The contributions of our work can be summarized as follows: • We show that MFCC-based signal analysis can provide information about audio saliency in the time domain. This can be used in any audio saliency model to improve its precision by weighting (suppressing/boosting) the audio predictions according to their saliency values for any instant. • By extending the MFCC analysis for FOA, we show that it is possible to localize salient sounds in 360◦ audio. • Combining the first two findings, we show that integrating this unsupervised salient spatial sound localization (SSSL) method as a bias to the existing traditional/deep learning-based (audio-)visual saliency models can improve their performance by an average of 14%, on 360◦ videos. The rest of the paper is organized as follows: Section 2 describes our method. The dataset, experimental details, performance analysis, and comparison with the state-of-the-art models are given in Section 3. Section 4 covers the conclusion and future work.",Utilizing Frequency-Based Spatial Audio Localization to Enhance 360° Video Saliency Prediction,"The utilization, storage and delivery of 360◦ videos for users has become increasingly important with the rise of VR/AR systems in gaming, social media and communication. To achieve this, researchers have been devising deep neural network models for 360◦ multimedia processing and computer vision. A key focus is to create models that can ascertain the attention of observers on 360◦ videos, known as saliency maps. Existing saliency models mainly take visual cues from video frames into account, ignoring audio cues from sound sources. This paper presents an unsupervised frequency-based saliency model to predict the strength and location of saliency in spatial audio. This prediction of salient audio cues is then used to augment the video saliency predictions of state-of-the-art models. Experiments show that integrating the proposed spatial audio bias into existing video saliency models improves their performance.","Studies [1–3] have shown auditory inputs influence human attention. However, many visual saliency estimation models disregard auditory cues. Some works [4–8] also indicated visually salient cues are related to audio source location and semantics. To address this, Tavakoli et al. [4] developed an audio-visual deep learning model (DAVE), Min et al. [5] proposed a novel multimodal saliency (MMS) model for audio-visual attention, Tsiami et al. [6] suggested a single multimodal network (STAViS) for audio-visual saliency, and Chen et al. [7] proposed a deep neural network architecture for audio and visual inputs. Spatial audio information has just begun to be used for saliency prediction in 360◦ videos [9]. Since humans do not discover their 360° environments at a glance, but instead focus on a narrower viewport and slowly explore the peripheries, this poses a challenge for saliency prediction. To meet this, spatial audio cues, providing directional information in 360◦ space, can be added to visual cues to help localize saliency predictions in 360◦ videos. For this, we adapted the mel-cepstrum based spectral residual saliency detection model (MCSR) proposed by Imamoglu et al. [10], to spatial audio. This model is capable of localizing salient sound in 360◦ space, and detecting the strength of audio saliency in the time domain. The produced audio saliency maps are then fused with the existing five video saliency models. Our findings show that integrating this unsupervised salient spatial sound localization (SSSL) method as a bias to existing traditional/deep learning-based (audio-)visual saliency models can improve their performance by an average of 14%, on 360◦ videos.",http://www.mva-org.jp/Proceedings/2021/papers/O1-3-4.pdf
A Gated Fusion Network for Dynamic Saliency Prediction,"Predicting saliency in videos is a challenging problem due to complex modeling of interactions between spatial and temporal information, especially when ever-changing, dynamic nature of videos is considered. Recently, researchers have proposed large-scale data sets and models that take advantage of deep learning as a way to understand what is important for video saliency. These approaches, however, learn to combine spatial and temporal features in a static manner and do not adapt themselves much to the changes in the video content. In this article, we introduce the gated fusion network for dynamic saliency (GFSalNet), the first deep saliency model capable of making predictions in a dynamic way via the gated fusion mechanism. Moreover, our model also exploits spatial and channelwise attention within a multiscale architecture that further allows for highly accurate predictions. We evaluate the proposed approach on a number of data sets, and our experimental analysis demonstrates that it outperforms or is highly competitive with the state of the art. Importantly, we show that it has a good generalization ability, and moreover, exploits temporal information more effectively via its adaptive fusion scheme.","HUMAN visual system employs visual attention mechanisms to effectively deal with huge amount of information by focusing only on salient or attention grabbing parts of a scene, and thus filtering out irrelevant stimuli. Saliency estimation methods offer different computational models of attention to mimic this key component of our visual system. These methods generate a so-called saliency map within which a pixel value indicates the likelihood of that pixel being fixated by a human. Since the pioneering work of [1], this research area has gained a lot of interest in the last few decades (refer to [2] for an overview), and it has found to have practical use in a variety of computer vision tasks, such as visual quality assessment [3], [4], image and video resizing [5], [6], and video summarization [7], to name a few. Early saliency prediction approaches use low-level (color, orientation, and intensity) and/or high-level (pedestrians, faces, text, etc.) image features to estimate salient regions. While low-level cues are used to detect regions that are different from their surroundings, top-down cues are used to infer high-level semantics to guide the model. For example, humans tend to focus some object classes more than others. Recently, deep learning-based models have started to dominate over the traditional approaches as they can directly learn both low- and high-level features relevant for saliency prediction [8], [9]. Most of the literature on saliency estimation focuses on static images. Lately, predicting saliency in videos has also gained some attraction, but it still remains a largely unexplored field of research. Video saliency models (also called dynamic saliency models) aim to predict attention grabbing regions in dynamically changing scenes. While static saliency estimation considers only low-level and high-level spatial cues, dynamic saliency needs to take into account temporal information too as there is evidence that moving objects or object parts can also guide our attention. Motion and appearance play complementary roles in human attention and their significance can change over time. As we illustrate in Fig. 1, in dynamic scenes, humans tend to focus more on moving parts of the scene and the eye fixations change over time, showing the importance of motion cues (bottom row). On the other hand, when there is practically no motion in the scene, low-level appearance cues dominantly guide our attention and we focus more on the regions showing different visual characteristics than their surroundings (top row). Motivated by these observations, in  this work, we develop a deep dynamic saliency model which handles spatial and temporal changes in the visual stimuli in an adaptive manner. The first generation of dynamic saliency methods was simply extensions of the static saliency approaches, e.g., [10]–[14]. In other words, these methods adapted the strategies proposed for static scenes and mostly modified them to work on either 3-D feature maps that are formed by stacking 2-D spatial features over time or 2-D feature maps encoding motion information like optical flow images. Several follow-up works, however, have approached the problem from a fresh perspective and developed specialized methods for dynamic saliency detection, e.g., [15]–[23]. These models either utilize novel spatiotemporal features or employ data-driven techniques to learn relevant features from data. As with the case of state-of-the-art static saliency models, approaches based on deep learning have also shown promise for dynamic saliency. These studies basically explore different neural architectures used for processing temporal and spatial information in a joint manner, and they either use 3-D convolutions [24], long short-term memories (LSTMs) [24], [25], or multistream architectures that encode temporal information separately [26]–[28]. In this work, we introduce gated fusion network for video saliency (GFSalNet). Our proposed network model is radically different from the previously proposed deep models in that it includes a novel content-driven fusion scheme to combine spatial and temporal streams in a more dynamic manner. In particular, our model is based on two-stream CNNs [29], [30], which have been successfully applied to various video analysis tasks. To our interest, these architectures are inspired by the ventral and dorsal pathways, which are suggested to subserve object identification and motion perception, respectively, [31], [32], in the human visual cortex [33]. Although the use of two-stream CNNs in video saliency prediction has been investigated before [27], the main novelty of our work lies in the ability to fuse appearance and motion information in a spatiotemporally coordinated manner by estimating the importance of each cue with respect based on the current video content. The remainder of this article is organized as follows. In Section II, we give a brief overview of the existing dynamic saliency approaches. In Section III, we present the details of our proposed deep architecture for video saliency. In Section IV, we give the details of our experimental setup, including evaluation metrics, data sets, and the competing dynamic saliency models, and discuss the results of our experiments. Finally, in Section V, we offer some concluding remarks. Our codes and predefined models, along with the saliency maps extracted with our approach, will be publicly available at the project website ",Dynamic Saliency Prediction Using a Gated Fusion Network,"Predicting saliency in videos is a difficult task due to complexity of spatial and temporal interactions, especially when video content is constantly changing. To understand what is important for video saliency, researchers have proposed large-scale data sets and models using deep learning. These models, however, combine spatial and temporal features in a static manner, not adapting to changes in video content. This article introduces GFSalNet, the first deep saliency model with an adaptive fusion mechanism and multiscale architecture that can make predictions dynamically. Experiments show that GFSalNet outperforms or is highly competitive with the state of the art, has good generalization ability, and exploits temporal information more effectively.","The HUMAN visual system employs attention mechanisms to filter out irrelevant stimuli by focusing on salient parts of a scene. Saliency estimation methods generate a saliency map, with a pixel value indicating the likelihood of that pixel being fixated by a human. This research area has been popular in recent decades and has practical applications in computer vision tasks, such as visual quality assessment, image/video resizing, and video summarization. Low-level (color, orientation, and intensity) and/or high-level (pedestrians, faces, text, etc.) image features are used to estimate salient regions. Deep learning-based models have started to dominate traditional approaches as they can directly learn relevant features for saliency prediction. Most of the literature focuses on static images; however, predicting saliency in videos has recently gained some attention. Video saliency models (also called dynamic saliency models) aim to predict attention grabbing regions in dynamically changing scenes, which requires temporal information to be taken into account. Recently, gated fusion network for video saliency (GFSalNet) has been developed, which includes a novel content-driven fusion scheme to combine spatial and temporal streams in a more dynamic manner, based on two-stream CNNs. Experiments were conducted to evaluate the proposed model and results discussed.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9475536
Using synthetic data for person tracking under adverse weather conditions,"Robust visual tracking plays a vital role in many areas such as autonomous cars, surveillance and robotics. Recent trackers were shown to achieve adequate results under normal tracking scenarios with clear weather conditions, standard camera setups and lighting conditions. Yet, the performance of these trackers, whether they are corre- lation filter-based or learning-based, degrade under adverse weather conditions. The lack of videos with such weather conditions, in the available visual object tracking datasets, is the prime issue behind the low perfor- mance of the learning-based tracking algorithms. In this work, we provide a new person tracking dataset of real-world sequences (PTAW172Real) captured under foggy, rainy and snowy weather conditions to assess the performance of the current trackers. We also introduce a novel person tracking dataset of synthetic sequences (PTAW217Synth) procedurally generated by our NOVA framework spanning the same weather conditions in varying severity to mitigate the problem of data scarcity. Our experimental results demonstrate that the perfor- mances of the state-of-the-art deep trackers under adverse weather conditions can be boosted when the avail- able real training sequences are complemented with our synthetically generated dataset during training.","Recently, convolutional neural networks (CNN) have shown a re- markable progress in various computer vision tasks such as object de- tection [1], object tracking [2], semantic segmentation [3], depth estimation [4], optical flow estimation [5], and person re-identification (ReID) [6]. Compared to the traditional shallow approaches, CNN- based models exhibit better generalization ability and perform much more accurately. Moreover, their performance usually increases as their expressive power increases, i.e. having more layers and/or more parameters. Yet, this introduces another difficulty as training such big networks requires more data and more powerful computing devices. The introduction of cheap general purpose graphics processing units (GPGPUs) have alleviated the hardware limitations. However, the scar- city of large-scale datasets for training supervised learning methods still remains as the main bottleneck for many computer vision tasks, espe- cially, the ones that require enormous efforts for annotation, such as se- mantic segmentation and visual object tracking. Besides, for some others, e.g. optical flow and depth estimation, it becomes virtually im- possible to provide large-scale densely annotated datasets. In addition to the aforementioned need for large-scale benchmark datasets, another requirement is to have a high level of diversity to allow deep learning models to perform well in the wild and not to overfit to training data. On the other hand, obtaining large-scale diverse data in a real-world setting, especially under rare circumstances, is not a simple task. As a consequence, small scale and mostly normal attributes tend to be the common features of the available datasets. Unfortunately, training computer vision models under normal scenarios, such as clear sky, optimal lighting, and standard recording conditions, causes unex- pected behavior or complete failure in much challenging adverse conditions. In this work, we focus on person tracking under adverse snowy, rainy and foggy weather conditions. The general problem of visual ob- ject tracking (VOT) is one of the major tasks in computer vision field that is essential for solving other higher-level tasks such as pedestrian detection, action recognition, or trajectory estimation. Therefore, it is vital for many real-world systems such as self-driving vehicles, auto- mated retail or visual surveillance. Failure of such systems under ad- verse conditions can lead to property damages or human injuries. Thereby, to assess the performance of the state-of-the-art trackers in person tracking in video feeds taken under such circumstances, we col- lect a novel real dataset, PTAW172Real, that consists of 172 videos featuring weather with heavy snow, rain or fog. Our experiments ex- pose the poor performance of the state-of-the-art trackers when tested on PTAW172Real and this can be linked to the limited number of videos taken under adverse weather conditions in the current VOT datasets that these trackers were trained with. We offer a remedy for the lack of data availability by using our NOVA engine to generate a synthetic dataset, PTAW217Synth, that provides diverse and rich training se- quences featuring adverse weather conditions. We show that using syn- thetic data, we can bridge the aforementioned gap and improve the performance of the learning-based trackers in such conditions. To the best of our knowledge, no work has been done to validate the usability of synthetic data for this purpose. Our main contributions in this paper can be summarized as follows: • We present a novel real dataset called PTAW172Real for visual object tracking under adverse weather conditions. The dataset contains 172 videos manually annotated covering snowy, rainy and foggy weather conditions. • We highlight the poor performance of the state-of-the-art trackers under adverse weather conditions with PTAW172Real. • Using our NOVA rendering engine, we procedurally generate a new dataset called PTAW217Synth made up of synthetic sequences under adverse weather conditions complete with automatically- generated per-frame annotations including bounding boxes at pixel- level accuracy, occlusion state and other relevant metadata such as time-of-day and camera type. The dataset consists of 217 sequences for person tracking spanning the three adverse weather conditions. • We show that fine-tuning the pre-trained models on our synthetic dataset PTAW217Synth is able to improve the performance of the deep trackers. Similarly, we also show that training from scratch on only our synthetic training dataset can achieve comparable results to training on large-scale real datasets.",Utilizing synthetic info to monitor individuals in harsh weather,"Visual tracking is essential in autonomous cars, surveillance and robotics. Recent trackers can achieve satisfactory results in normal conditions, but their performance deteriorates in adverse weather. A major obstacle to improving deep tracker performance in such conditions is the lack of videos in available datasets. To address this issue, we present two datasets: PTAW172Real, containing real-world sequences in foggy, rainy and snowy weather, and PTAW217Synth, composed of synthetically generated sequences of varying severity. Our results demonstrate that the performance of state-of-the-art deep trackers can be improved when both real and synthetic datasets are used for training.","Recently, CNNs have shown great improvement in computer vision tasks such as object detection, tracking, semantic segmentation, depth estimation, optical flow estimation, and person re-identification (ReID). Compared to traditional shallow approaches, CNN-based models are more accurate and their performance increases with increased expressive power. Training such big networks requires more data and powerful computing devices, which have been aided by GPGPUs. However, the lack of large-scale datasets for supervised learning methods is still a main bottleneck, especially for tasks requiring much annotation, such as semantic segmentation and visual object tracking. Moreover, having large-scale diverse data in a real-world setting is hard, particularly for rare circumstances. Thus, datasets tend to have small scale and mostly normal attributes. As a result, models trained under normal scenarios fail in challenging adverse conditions. To address this issue, we introduce a novel real dataset, PTAW172Real, featuring weather with heavy snow, rain or fog. We expose the poor performance of the state-of-the-art trackers when tested on PTAW172Real due to the limited number of videos taken under adverse weather conditions in current VOT datasets. To remedy the lack of data availability, we use our NOVA engine to generate a synthetic dataset, PTAW217Synth, providing diverse and rich training sequences with adverse weather conditions. We show that using synthetic data can bridge the gap and improve the performance of the deep trackers. We offer the novel real dataset PTAW172Real for visual object tracking under adverse weather conditions, containing 172 videos manually annotated covering snowy, rainy and foggy weather conditions. We highlight the poor performance of the state-of-the-art trackers under such conditions with PTAW172Real. We use our NOVA rendering engine to generate the synthetic dataset PTAW217Synth made up of sequences under adverse weather conditions, complete with automatically-generated per-frame annotations including bounding boxes at pixel-level accuracy, occlusion state and other relevant metadata such as time-of-day and camera type. The dataset consists of 217 sequences for person tracking spanning the three adverse weather conditions. We show that fine-tuning the pre-trained models on our synthetic dataset PTAW217Synth is able to improve the performance of the deep trackers, and that training from scratch on only our synthetic training dataset can achieve comparable results to training on large-scale real datasets. To the best of our knowledge, no work has been done to validate the usability of synthetic data for this purpose.",https://reader.elsevier.com/reader/sd/pii/S0262885621000925?token=B70C80B32274004A312CFCBA7748F86F164DDA6B0215F7C9F3539C9E4C0FA8E40D6942E212E64D7439965548EE6D00D3&originRegion=eu-west-1&originCreation=20221228192436
MSVD‑Turkish: a comprehensive multimodal video dataset for integrated vision and language research in Turkish,"Automatic generation of video descriptions in natural language, also called video captioning, aims to understand the visual content of the video and produce a natural language sentence depicting the objects and actions in the scene. This challenging integrated vision and language problem, however, has been predominantly addressed for English. The lack of data and the linguistic properties of other languages limit the success of existing approaches for such languages. In this paper we target Turkish, a morphologically rich and agglutinative language that has very different properties compared to English. To do so, we create the frst large-scale video captioning dataset for this language by carefully translating the English descriptions of the videos in the MSVD (Microsoft Research Video Description Corpus) dataset into Turkish. In addition to enabling research in video captioning in Turkish, the parallel English–Turkish descriptions also enable the study of the role of video context in (multimodal) machine translation. In our experiments, we build models for both video captioning and multimodal machine translation and investigate the efect of diferent word segmentation approaches and diferent neural architectures to better address the properties of Turkish. We hope that the MSVD-Turkish dataset and the results reported in this work will lead to better video captioning and multimodal machine translation models for Turkish and other morphology rich and agglutinative languages.","Recent developments in computer vision (CV) and natural language processing (NLP) have led to a surge of new problems which lie at the intersection of these two felds, creating a new area of research in general called “integrated vision and language” (iVL). Video captioning is one of the important problems in iVL research, which has gained signifcant attention in both the CV and NLP communities. It aims at understanding the visual content of a given video clip and contextually generating a natural language description of this clip. Although a considerable amount of literature has revolved around this challenging task in recent years, all existing work is monolingual that it has mainly focused on the English language. Hence, whether or not the state-of-the-art video captioning methods can be efectively adapted to languages other than English, especially for low-resource languages, remains an open problem. Moreover, linguistic diferences between English and other languages, particularly those that are morphologically richer than English, introduce new challenges that need to be addressed. Before these questions can be answered, however, we require video datasets containing descriptions from languages other than English to further enable iVL research. As a frst step towards this direction, in this paper, we extend the MSVD (Microsoft Research Video Description Corpus) (Chen and Dolan 2011) dataset and introduce a new multilingual dataset that we call MSVD-Turkish which contains approximately 2k video clips and a total of 80k Turkish video descriptions. In particular, we collect these Turkish descriptions by manually translating the original English video descriptions from MSVD into Turkish. Compared to the original English descriptions, Turkish descriptions have a larger vocabulary size and more importantly refect the highly infected and highly agglutinative nature of Turkish. We demonstrate the multilingual, multimodal capabilities of the proposed MSVD-Turkish dataset, by exploring two distinct iVL tasks shown in Fig.  1, namely video captioning and multimodal machine translation (MT), but with a special focus on Turkish. As far as we are aware of, this work is the frst to investigate generating Turkish descriptions depicting visual content of videos. To this end, we analyse diferent segmentation strategies for Turkish. Additionally, we explore multimodal MT as the second task on MSVD-Turkish where we examine the use of supplementary visual cues within videos to potentially improve the translation quality. Our primary contributions in this paper are: – To foster research in multilingual, multimodal language generation, we collect a new large-scale dataset called MSVD-Turkish by translating the English descriptions of the videos from the well-known MSVD dataset into Turkish. – We investigate the performance of several (multimodal) MT and video captioning models on the proposed MSVD-Turkish dataset. – To address the rich and agglutinative morphology of Turkish, we explore different word segmentation approaches in Turkish.The paper is organised as follows: In Sect. 2, we briefy review the state-of-theart in multimodal MT and video captioning. In Sect. 3, we introduce the MSVDTurkish dataset, examine our data collection strategy and provide some statistics regarding the dataset. We introduce the details regarding the visual and textual representations considered in our machine translation and video captioning models in Sect. 4, and describe the models themselves in Sect. 5. In Sect. 6, we present our experimental results and discuss our fndings and fnally, we provide a summary of our work and discuss possible future research directions in Sect. 7.",Multimodal Video Dataset for Vision and Language Research in Turkish,"The goal of automatic video description generation in natural language, also known as video captioning, is to comprehend the video's visual content and form a natural language sentence illustrating the objects and actions taking place. However, this challenging combination of vision and language has mainly been studied in English. The lack of data and the linguistic characteristics of other languages impede the effectiveness of current approaches for those languages. This paper focuses on Turkish, a morphologically complex and agglutinative language that has different properties than English. To do this, we create the first extensive video captioning dataset for this language by translating the English descriptions of the MSVD (Microsoft Research Video Description Corpus) dataset into Turkish. In addition to allowing research in video captioning in Turkish, the parallel English–Turkish descriptions also enable the investigation of the role of video context in (multimodal) machine translation. Our experiments develop models for both video captioning and multimodal machine translation and examine the effect of various word segmentation approaches and various neural architectures to better address the properties of Turkish. We trust that the MSVD-Turkish dataset and the results reported in this work will result in improved video captioning and multimodal machine translation models for Turkish and other morphologically complex and agglutinative languages.","Recent progress in CV and NLP has triggered the emergence of a new research field, known as ""integrated vision and language"" (iVL), in which video captioning is a prominent problem. All existing work is monolingual and mainly concentrated on English. It is yet unknown if state-of-the-art video captioning methods can be effectively adapted to languages other than English, especially those with low resources. Moreover, language differences between English and other languages, particularly those that are morphologically more complex than English, create additional challenges. To answer these questions, video datasets containing descriptions from languages other than English are needed to further advance iVL research. In this paper, we present a new multilingual dataset, MSVD-Turkish, with around 2k video clips and 80k Turkish video descriptions. The dataset is created by translating the original English video descriptions from MSVD into Turkish. Compared to the original English descriptions, Turkish descriptions have a larger vocabulary size and reflect the highly inflected and highly agglutinative nature of Turkish. We demonstrate the multilingual, multimodal capabilities of the proposed MSVD-Turkish dataset by exploring two distinct iVL tasks, namely video captioning and multimodal machine translation (MT), but with a special focus on Turkish. Our contributions include collecting a new large-scale dataset, MSVD-Turkish, by translating the English descriptions of the videos from the well-known MSVD dataset into Turkish; examining the performance of several (multimodal) MT and video captioning models on the proposed MSVD-Turkish dataset; and exploring different word segmentation approaches in Turkish to address the rich and agglutinative morphology of Turkish.",https://link.springer.com/content/pdf/10.1007/s10590-021-09276-y.pdf
Leveraging auxiliary image descriptions for dense video captioning,"Collecting textual descriptions is an especially costly task for dense video captioning, since each event in the video needs to be annotated separately and a long descriptive paragraph needs to be provided. In this paper, we investigate a way to mitigate this heavy burden and propose to leverage captions of visually similar images as auxiliary context. Our model successfully fetches visually relevant images and combines noun and verb phrases from their captions to generating coherent descriptions. To this end, we use a generator and discriminator design, together with an attention-based fusion technique, to incorporate image captions as context in the video caption generation process. The experiments on the challenging ActivityNet Captions dataset demonstrate that our proposed approach achieves more accurate and more diverse video descriptions compared to the strong baseline using METEOR, BLEU and CIDEr-D metrics and qualitative evaluations.","Video captioning can be described as the process of automat- ically generating natural language sentences to describe the con- tent in a video [46]. In an ideal video captioning setup, objects, ac- tions, scenes and the interactions between people, objects, actions and scenes must be recognized [1]. Following this recognition pro- cess, their time of arrival and temporal order must be learned. In the case of more than one event in the video, the interrelation be- tween events must be considered as well. During preprocessing for multiple events, temporal segmentation methods can be explored [2,3,35]. These steps should lead to generation of grammatically correct, coherent, visually related and human-understandable cap- tions. Although there has been significant interest in this topic with the emergence of new datasets [10,20,52] and tech- niques [20,44,50,53], it remains a very challenging problem. Lack of diversity, redundancies and semantic inconsistencies in gen- erated captions are the main issues. The majority of previous work [24,26,27,44,47,48,53] has focused on generating captions for events in videos that are only based on using visual cues. In Iashin and Rahtu [16] audio, in Shi et al. [39] speech, and in Iashin and Rahtu [17] both audio and speech are utilized along with visual cues as multi-modal information. In this work, we propose a dense video captioning approach that makes use of image captions as auxiliary input alongside video information. In this way, we aim to benefit from the additional di- versity and richness of the image captions in generating more co- herent descriptions for the videos. Our proposed framework is in- spired from the generator and hybrid discriminator design of Park et al. [32], but differs in using an existing image captioning dataset to collect useful information based on similarity of images and events. Our proposal exploits attention mechanism to incorporate this auxiliary information with visual information. Fig. 1 illustrates our overall approach. At training time, our model first fetches similar images to videos by comparing their vi- sual features from a large image-captioning dataset. We then com- pare the corresponding closest k image captions against ground truth event captions, re-order them based on their sentence-level similarity. We extract the respective noun and verb phrases and use an attention mechanism to attentively select the useful parts of these auxiliary caption data. The generator module exploits this new enhanced input and uses it to generate novel captions that are more descriptive of the content in the video. Finally, a hybrid discriminator selects the optimal captions for events. We evaluate our proposed model on ActivityNet Captions benchmark [5], and demonstrate that our proposed framework achieves qualitatively better captioning performance than Park et al. [32]. Our results show that the proposed method utilizes auxiliary image captions effectively as additional context and combines them with visual in- formation of videos. In summary, our main contributions in this work are: a) we propose an auxiliary data enhancement pipeline that en- ables extracting meaningful phrases as auxiliary information to be used in caption generation process (Section 3.1); b) we explore an attention mechanism to incorporate this auxil- iary information with visual information (Section 3.2). c) our model performs comparably to a strong baseline when evaluated using automated metrics. We qualitatively show significant improvements with the generated video captions (Section 4). Our proposed pipeline and attention mechanism can be used in other dense video captioning models to enhance input diversity.",Utilizing extra image descriptions for comprehensive video captioning.,"The cost of collecting textual descriptions for dense video captioning is high, since each event in the video must be labeled separately and a long descriptive paragraph is required. To reduce this burden, we propose using captions of visually similar images as supplementary context. Our model finds visually related images and merges noun and verb phrases from their captions to form coherent descriptions. We apply a generator and discriminator design, combined with an attention-based fusion method, to incorporate image captions into the video caption generation process. Results from experiments on the ActivityNet Captions dataset show our approach produces more accurate and varied video descriptions compared to the strong baseline using METEOR, BLEU and CIDEr-D metrics and qualitative assessments.","Video captioning is the automated generation of natural language sentences to describe the content in a video [46]. Recognition of objects, actions, scenes and their interactions, learning of time of arrival and temporal order, and consideration of interrelation between events if more than one is present must all be done for ideal setup [1]. Temporal segmentation methods can be explored for preprocessing of multiple events [2,3,35]. Desired captions should be grammatically correct, coherent, visually related, and understandable [10,20,52]. Despite increasing interest in the topic and new techniques [20,44,50,53], issues such as lack of diversity, redundancies, and semantic inconsistencies remain. Previous work has mainly focused on visual cues [24,26,27,44,47,48,53], but audio [16], speech [39], and both [17] have also been used. A dense video captioning approach is proposed that makes use of image captions as auxiliary input. Attention mechanism is explored to incorporate this information with visual info. Results show model utilizes auxiliary image captions effectively, combining them with visual information of videos, and produces better captioning performance than baseline [32]. Contributions are: a) proposed auxiliary data enhancement pipeline to extract meaningful phrases (Section 3.1); b) explored attention mechanism to incorporate auxiliary info with visual (Section 3.2); c) model performs comparably to strong baseline, with qualitative improvements in generated captions (Section 4). Pipeline and attention mechanism can be used in other dense video captioning models to enhance input diversity.",https://reader.elsevier.com/reader/sd/pii/S0167865521000647?token=D708B01747CD1199524655309FA7459E09FD9D22C80FEBE8E8E857A84682688D5E96CB4355F2250D46ED2184ED5B181C&originRegion=eu-west-1&originCreation=20221228192637
mustGAN: multi-stream Generative Adversarial Networks for MR Image Synthesis,"Multi-contrast MRI protocols increase the level of morphological information available for diagnosis. Yet, the number and quality of contrasts are limited in practice by various factors including scan time and patient motion. Synthesis of missing or corrupted contrasts from other high-quality ones can alleviate this limitation. When a single target contrast is of interest, common approaches for multi-contrast MRI involve either one-to-one or many-to-one synthesis methods depending on their input. One-to-one meth- ods take as input a single source contrast, and they learn a latent representation sensitive to unique fea- tures of the source. Meanwhile, many-to-one methods receive multiple distinct sources, and they learn a shared latent representation more sensitive to common features across sources. For enhanced image synthesis, we propose a multi-stream approach that aggregates information across multiple source im- ages via a mixture of multiple one-to-one streams and a joint many-to-one stream. The complementary feature maps generated in the one-to-one streams and the shared feature maps generated in the many- to-one stream are combined with a fusion block. The location of the fusion block is adaptively modified to maximize task-specific performance. Quantitative and radiological assessments on T1 ,- T2-, PD-weighted, and FLAIR images clearly demonstrate the superior performance of the proposed method compared to previous state-of-the-art one-to-one and many-to-one methods.","Magnetic resonance imaging (MRI) enables a given anatomy to be imaged under different tissue contrasts by simply manipulating pulse sequences. In turn, images acquired in several distinct con- trasts help better distinguish tissues and increase diagnostic infor- mation. For instance, gray and white matter can be better delin- eated in T1-weighted brain images, whereas fluids and cortical tis- sues can be better delineated in PD-weighted images. Yet, multi- contrast acquisitions often prove impractical due to scan time lim- itations or excessive artifacts related to patient motion (Krupa and Bekiesi  ́nska-Figatowska, 2015; Thukral, 2015). Therefore, within- domain synthesis of missing or corrupted contrasts from other high-quality contrasts is a promising tool to improve the clinical feasibility and utility of multi-contrast MRI (Iglesias et al., 2013). Prior methods proposed for synthesis of a single target con- trast within a multi-contrast MRI protocol can be grouped un- der two main titles depending on their input: one-to-one meth- ods (Abramian and Eklund, 2019; Bowles et al., 2016; Dar et al., 2019; Dewey et al., 2018; 2019; Li et al., 2014; Liu, 2019; Sevetlidis et al., 2016; Sohail et al., 2019; Van Nguyen et al., 2015; Welander et al., 2018; Yang et al., 2018) and many-to-one methods (Chartsias et al., 2017; Dar et al., 2020; Dewey et al., 2018; 2019; Hagiwara et al., 2019; Joyce et al., 2017; Lee et al., 2019; Lei et al., 2020; Li et al., 2019; Mehta and Arbel, 2018; Olut et al., 2018; Sharma and Hamarneh, 2019; Wei et al., 2018b). One-to-one synthesis aims to generate a subject’s image y in a target contrast cT from the same subject’s image x in a source contrast cS. Earlier studies have for- mulated one-to-one synthesis as a sparse dictionary reconstruc- tion problem (Huang et al., 2016; 2017; 2018; Jog et al., 2015b; Roy et al., 2011; 2013a; 2013b; 2016), where patch-based dictio- naries are formed from a set of co-registered atlas image bS of cS and atlas image bT of cT . Each patch in x is expressed as a sparse linear combination of dictionary atoms of bS, and this combina- tion is then used for synthesizing y from bT (Huang et al., 2016; 2017; 2018; Jog et al., 2015b; Roy et al., 2011; 2013a; 2013b; 2016). For improved performance, patch-based non-linear regression us- ing random forests (Jog et al., 2015b) or location-sensitive neural networks (Van Nguyen et al., 2015) has been proposed for source to target mapping. To overcome limitations due to patch-based processing, later studies have proposed deep network models that process the entire source image with convolutional layers (Li et al., 2014; Sevetlidis et al., 2016). One powerful method is based on the encoder-decoder architecture (Sevetlidis et al., 2016), where latent representations of the source image are embedded via an encoder, and the target image is then recovered via a decoder from these representations (Sevetlidis et al., 2016). Recent deep network mod- els have further incorporated an adversarial loss to better capture the high frequency details in the target image (Abramian and Ek- lund, 2019; Dar et al., 2019; Sohail et al., 2019; Welander et al., 2018; Yang et al., 2018; Yu et al., 2018; 2019). An important adver- sarial method is pGAN (Dar et al., 2019), which additionally utilizes pixel-wise and perceptual losses (Johnson et al., 2016) to enhance the synthesis performance. When several source contrasts are available in a multi-contrast MRI protocol, a natural alternative is to perform many-to-one syn- thesis (Chartsias et al., 2017; Dar et al., 2020; Dewey et al., 2018; 2019; Hagiwara et al., 2019; Jog et al., 2014; 2015; 2016; Joyce et al., 2017; Lee et al., 2019; Lei et al., 2020; Li et al., 2019; Mehta and Arbel, 2018; Olut et al., 2018; Sharma and Hamarneh, 2019; Wei et al., 2018b). In this approach, the goal is to generate the sub- ject’s image y in the target contrast cT from a collection of source images X = {xm : m = 1, 2, . . . , K} in varying contrasts CS = {cSm : m = 1, 2, . . . , K}. As in one-to-one synthesis, a common method is to perform non-linear regression using random forests (Jog et al., 2014; 2016). The random-forest method in Jog et al. (2016) fits a non-linear regression model in feature space to estimate intensities of the target contrast given multiple source contrasts (Jog et al., 2016). Deep neural network methods have also been proposed for many-to-one synthesis (Chartsias et al., 2017; Dewey et al., 2018; 2019; Mehta and Arbel, 2018; Joyce et al., 2017). In Chartsias et al. (2017), latent representations of multiple source contrast im- ages are encoded through separate encoder architectures. These la- tent representations are then used to synthesize the target image through a joint decoder architecture (Chartsias et al., 2017). Sim- ilar to one-to-one methods, recent studies have leveraged an ad- versarial loss to improve the quality of many-to-one synthesis (Dar et al., 2020; Hagiwara et al., 2019; Lee et al., 2019; Li et al., 2019; Olut et al., 2018; Sharma and Hamarneh, 2019; Wei et al., 2018b). An important example is MM-GAN (Sharma and Hamarneh, 2019), which learns recovery of missing (target) contrasts from a collec- tion of available source contrasts. MM-GAN receives as input the concatenation of the sources, and treats them as separate informa- tion channels (Sharma and Hamarneh, 2019). In general, one-to-one synthesis methods attempt to recover the target image from the latent representation of a given source image. Since these methods are optimized for a single input chan- nel, they can sensitively learn the unique, detailed features of the given source contrast. While this sensitivity can be preferable when the images of the source and target contrast are highly cor- related, it might limit synthesis performance when the two con- trasts are weakly linked. On the other hand, many-to-one syn- thesis methods aim to recover the target image from a shared latent representation of multiple source images. These methods naturally manifest increased sensitivity for capturing features that are shared across distinct source contrasts, even when these fea- tures are weakly present in individual contrasts. Yet, a shared la- tent representation might also be less sensitive to complemen- tary features that are uniquely present in a specific source contrast. When such unique information is predominantly predic- tive of the target image, many-to-one synthesis might perform suboptimally. Here, we propose a novel method, multi-stream generative ad- versarial network (mustGAN), for enhanced image synthesis in multi-contrast MRI. To alleviate limitations of one-to-one and many-to-one synthesis, mustGAN leverages both shared and com- plementary features of multiple source images via a mixture of multiple one-to-one streams and a joint many-to-one stream. The complementary feature maps generated in the one-to-one streams and the shared feature maps generated in the many-to-one stream are later combined with a fusion block. The optimal position of the fusion block is searched over network layers to maximize task- specific performance. A joint network is then trained to recover the target image from the fused feature maps. Comprehensive quanti- tative assessments and radiological evaluations are performed on multi-contrast MR images (T1-, T2-, PD-weighted, and FLAIR im- ages) from healthy subjects and high/low grade glioma patients. The proposed method yields both quantitatively and qualitatively higher performance in multi-contrast MRI synthesis compared to state-of-the-art one-to-one and many-to-one methods.",Multi-stream GANs for MR Image Creation,"Multi-contrast MRI protocols provide more morphological info for diagnosis, but are restricted by scan time and patient motion. Synthesizing missing or corrupted contrasts from high-quality ones can alleviate this. Common approaches for multi-contrast MRI involve either one-to-one or many-to-one synthesis methods, depending on their input. One-to-one methods take a single source contrast as input and learn a latent representation that is sensitive to the source's unique features, while many-to-one methods receive multiple distinct sources and learn a shared latent representation more sensitive to common features across sources. We propose a multi-stream approach which aggregates information from multiple source images via a mixture of one-to-one streams and a joint many-to-one stream, with a fusion block whose location is adapted to maximize task-specific performance. Quantitative and radiological assessments on T1, T2, PD-weighted, and FLAIR images show the proposed method outperforms previous one-to-one and many-to-one methods.","Magnetic Resonance Imaging (MRI) enables anatomy to be viewed under varying tissue contrasts by manipulating pulse sequences. These images, acquired in distinct contrasts, can better distinguish tissues and provide diagnostic information, e.g. gray and white matter can be delineated in T1-weighted brain images, whereas fluids and cortical tissues can be better depicted in PD-weighted images. However, multi-contrast acquisitions can be impractical due to scan time limitations or patient motion-induced artifacts. Hence, within-domain synthesis of missing or corrupted contrasts from other high-quality contrasts is a promising tool to improve the clinical feasibility and utility of multi-contrast MRI. One-to-one synthesis attempts to generate a subject's target contrast image from the same subject's source contrast image, and has been formulated as a sparse dictionary reconstruction problem. This involves forming patch-based dictionaries from co-registered atlas images, and expressing each patch in the source image as a sparse linear combination of the dictionary atoms. For improved performance, patch-based non-linear regression using random forests or location-sensitive neural networks have been proposed for source to target mapping. Deep network models have been developed to process the entire source image with convolutional layers, such as an encoder-decoder architecture, and an adversarial loss to capture high frequency details in the target image. Many-to-one synthesis, on the other hand, aims to generate the subject's target contrast image from a collection of source images in varying contrasts. Non-linear regression using random forests, and deep neural network methods have been proposed for many-to-one synthesis. Here, we propose a novel method, multi-stream generative adversarial network (mustGAN), for enhanced image synthesis in multi-contrast MRI. mustGAN leverages both shared and complementary features of multiple source images via a mixture of multiple one-to-one streams and a joint many-to-one stream, and a fusion block is used to combine the feature maps. Quantitative assessments and radiological evaluations on multi-contrast MR images from healthy subjects and high/low grade glioma patients show that the proposed method yields higher performance in multi-contrast MRI synthesis compared to state-of-the-art one-to-one and many-to-one methods.",https://reader.elsevier.com/reader/sd/pii/S136184152030308X?token=F765ACA7C1291955F205E9D4518E4EEAAA68D3AAAC7372C4C43F12A429CC33EFE09F93E0CA2CA497DFA63EB6BE1D8B8D&originRegion=eu-west-1&originCreation=20221228192804
Object and Relation Centric Representations for Push Effect Prediction,"Pushing is an essential non-prehensile manipulation skill used for tasks ranging from pre-grasp manipulation to scene rearrangement, reasoning about object relations in the scene, and thus pushing actions have been widely studied in robotics. The effective use of pushing actions often requires an understanding of the dynamics of the manipulated objects and adaptation to the discrepancies between prediction and reality. For this reason, effect prediction and parameter estimation with pushing actions have been heavily investigated in the literature. However, current approaches are limited because they either model systems with a fixed number of objects or use image-based representations whose outputs are not very interpretable and quickly accumulate errors. In this paper, we propose a graph neural network based framework for effect prediction and parameter estimation of pushing actions by modeling object relations based on contacts or articulations. Our framework is validated both in real and simulated environments containing different shaped multi-part objects connected via different types of joints and objects with different masses. Our approach enables the robot to predict and adapt the effect of a pushing action as it observes the scene. Further, we demonstrate 6D effect prediction in the lever-up action in the context of robot-based hard-disk disassembly","Pushing is a fundamental non-prehensile (manipulation without grasping) motion primitive that gives robots great flexibility in manipulating objects [1], [2]. Using push actions, a robot can navigate objects to goal configurations even when objects are not graspable [3]; it can manipulate objects under uncertainty [4], or bring an object to the graspable area [5]. Compared to grasping actions, it is not as restrictive; however, the issue is that the robot does not have direct control over the state of the manipulated objects. This results in greater complexity in planning and control as the dynamics of the manipulated objects are often required to be taken into consideration [1]. Effect prediction of pushing action has many applications [2], [6], including scene rearrangement [7], object segmentation [8], object singulation [9], [10], pre-grasp manipulation [10]–[13]. However, action-effect prediction of pushing actions depends on many factors [14] and requires adaptation when mispredictions occurs. Figure 1 shows an example illustration. The initial prediction of the robot will be objects getting scattered. However, after seeing some of the objects moving together, the robot will understand that their future motion will continue reflecting this dynamic. In many environments, robots work with object clutters containing different shaped and weighted objects with possible articulations between them. A robot should be able to reason about the influence of shape and mass of objects, physical connections like contacts or different types of articulations between objects, propagation of motions between objects, and correction of unknown or partially known objects or object parts in the environment. Current approaches model environments with a fixed number of objects or use image data, an object-independent representation. While there has been great progress on effect prediction using raw sensory data [15]–[18], using them on decision making level has been difficult and required tasks to be generated on pixel level. While there are certain advantages of such approaches, many tasks often require more interpretable representations for the task to be defined. Humans decompose environments into objects and use their interactions for physical reasoning [19]– [21], so there is certainly value in using such representations in effect prediction. We propose using graph neural networks (GNNs) for push effect prediction. Graph neural networks [22] can exploit the graph structure of multi-objects systems by exploiting and using object- and relation-centric representations and they are heavily used in modelling physics [21], [23]–[29]. In this paper, we propose a general-purpose learnable physics engine in which object- and relation-centric representations are learned via a shared propagation network and used for physics prediction and parameter estimation in push manipulation tasks1 . We use articulation based graph representations that use cylinder- and cuboid-shaped objects and their possible interactions via contacts or joints for modeling multipart object systems. We resort to a two-step training scheme where our framework is first trained for effect prediction, then using learned object and relation representations, it is trained for parameter estimation. Our framework can predict low-level trajectories of groups of articulated objects given robot actions and estimate the mass of observed objects and joint relations between them based on their interaction history. Using articulation based representation, novel tools that are not encountered during training can be built by connecting multiple cuboids via fixed joints, and they can be used in planning in tool manipulation tasks. An early version of this work was published in [30]. However, this paper significantly extend the work in several important directions. In [30], for physics prediction and parameter estimation, two independents networks were required. By employing a new weight-sharing mechanism that allows these tasks to share object- and relation-centric representations, the number of learnable parameters is decreased by about thirty percent. Previously, our framework was only able to model cylindrical objects. We extend the input representations of objects and their relations, allowing our network to handle objects with different shapes, predict the mass of objects, and represent complex shaped objects that are built by connecting multiple cuboids and cylinders, which even allow our framework to work with tools that are not previously encountered. In addition, we have shown that our framework can make 6D effect predictions. Furthermore, the training of the network has been improved by the use of scheduled sampling [31] and greater data distribution. These novel contributions decrease the errors for long-horizon prediction tasks, and in Section V-E, our new results have been shown to surpass the ones in [30]. More specifically, the general contributions of our framework can be listed as follows: • We develop a graph neural network based framework for parameter estimation and physics prediction in push manipulation tasks. • We utilize a weight-sharing mechanism to transfer learned representations to be used in new tasks. • We show the feasibility of articulation based graph representations for modeling multi-part objects. • We design a novel 6-D action-effect prediction in lever-up task in the context of hard-disk drive disassembly. • Through simulated and real-world experiments, we verify our framework in joint relation and mass prediction, physics prediction, and tool manipulation and planning tasks.",Predicting Push Effects via Object & Relation-Focused Representations,"The utilization of pushing actions necessitates understanding the dynamics of manipulated objects and adapting to discrepancies between expectations and reality. Thus, effect prediction and parameter estimation with pushing actions have been researched extensively. However, existing approaches are limited due to either modeling systems with a fixed number of objects or utilizing image-based representations with non-interpretable outputs and accumulating errors quickly. This paper proposes a graph neural network based framework for effect prediction and parameter estimation of pushing actions, taking into account object relations based on contacts or articulations. The proposed framework is tested in both real and simulated environments with different shaped multi-part objects connected through various types of joints and objects with varying masses. The robot is enabled to predict and adjust the effect of a pushing action as it observes the scene, as well as 6D effect prediction in the lever-up action in the context of robot-based hard-disk disassembly.","Pushing is a manipulation without grasping that gives robots great flexibility in manipulating objects [1], [2]. Push actions allow navigation of objects to goal configurations even if not graspable [3], manipulation under uncertainty [4], or bringing an object to the graspable area [5]. Compared to grasping, it is less restrictive, but with less direct control over the state of the manipulated objects, complexity in planning and control increases as dynamics of the manipulated objects must be taken into account [1]. Applications include scene rearrangement [7], object segmentation [8], object singulation [9], [10], pre-grasp manipulation [10]–[13]. Prediction of push action effects is complex [14] and requires adaptation when mispredictions occur. An example is shown in Figure 1: initially, objects are predicted to scatter, but after seeing some move together, the robot will understand their future motion will reflect this dynamic. For robots to reason about influence of shape and mass of objects, physical connections like contacts or articulations between them, propagation of motions between them, and correction of unknown or partially known objects or object parts in the environment, graph neural networks (GNNs) can exploit the graph structure of multi-objects systems by exploiting and using object- and relation-centric representations, heavily used in modelling physics [21], [23]–[29]. A two-step training scheme is proposed for effect prediction and parameter estimation, where a shared propagation network is used to learn object- and relation-centric representations. Cylinder- and cuboid-shaped objects and their possible interactions via contacts or joints are modelled for multipart object systems, and the framework can predict low-level trajectories of groups of articulated objects given robot actions and estimate mass of observed objects and joint relations between them based on their interaction history. It can also build novel tools by connecting multiple cuboids via fixed joints for tool manipulation tasks. This paper extends the work in [30] with a weight-sharing mechanism that decreases the number of learnable parameters by about thirty percent, handling objects of different shapes, predicting mass of objects, and representing complex shaped objects built by connecting multiple cuboids and cylinders. It also makes 6D effect predictions and improves the training with scheduled sampling [31] and greater data distribution. The contributions are: • Graph neural network based framework for parameter estimation and physics prediction in push manipulation tasks. • Weight-sharing mechanism to transfer learned representations to new tasks. • Articulation based graph representations for modeling multi-part objects. • Novel 6-D action-effect prediction in lever-up task in hard-disk drive disassembly. • Verification of framework in joint relation and mass prediction, physics prediction, and tool manipulation and planning tasks.",https://arxiv.org/pdf/2102.02100.pdf
Cross-lingual Visual Pre-training for Multimodal Machine Translation,"Pre-trained language models have been shown to improve performance in many natural language tasks substantially. Although the early focus of such models was single language pre-training, recent advances have resulted in cross-lingual and visual pre-training methods. In this paper, we combine these two approaches to learn visually-grounded crosslingual representations. Specifically, we extend the translation language modelling (Lample and Conneau, 2019) with masked region classification and perform pre-training with three-way parallel vision & language corpora. We show that when fine-tuned for multimodal machine translation, these models obtain stateof-the-art performance. We also provide qualitative insights into the usefulness of the learned grounded representations.","Pre-trained language models (Peters et al., 2018; Devlin et al., 2019) have been proven valuable tools for contextual representation extraction. Many studies have shown their effectiveness in discovering linguistic structures (Tenney et al., 2019), which is useful for a wide variety of NLP tasks (Talmor et al., 2019; Kondratyuk and Straka, 2019; Petroni et al., 2019). These positive results led to further exploration of (i) cross-lingual pretraining (Lample and Conneau, 2019; Conneau et al., 2020; Wang et al., 2020) through the use of multiple mono-lingual and parallel resources, and (ii) visual pre-training where large-scale image captioning corpora are used to induce grounded vision & language representations (Lu et al., 2019; Tan and Bansal, 2019; Li et al., 2020a; Su et al., 2020; Li et al., 2020b). The latter is usually achieved by extending the masked language modelling (MLM) objective (Devlin et al., 2019) with auxiliary vision & language tasks such as masked region classification and image sentence matching. In this paper, we present the first attempt to bring together cross-lingual and visual pre-training. Our visual translation language modelling (VTLM) objective combines the translation language modelling (TLM) (Lample and Conneau, 2019) with masked region classification (MRC) (Chen et al., 2020; Su et al., 2020) to learn grounded crosslingual representations. Unlike most of the prior work that use classification or retrieval based downstream evaluation, we focus on the generative task of multimodal machine translation (MMT), where images accompany captions during translation (Sulubacak et al., 2020). Once pre-trained, we transfer the VTLM encoder to a Transformer-based (Vaswani et al., 2017) MMT and fine-tune it for the MMT task. To our knowledge, this is also the first attempt of pre-training & fine-tuning for MMT, where the current state of the art mostly relies on training multimodal sequence-to-sequence systems from scratch (Calixto et al., 2016; Caglayan et al., 2016; Libovicky and Helcl ´ , 2017; Elliott and Kad´ ar´ , 2017; Caglayan et al., 2017; Yin et al., 2020). Our findings highlight the effectiveness of crosslingual visual pre-training: when fine-tuned on the English→German direction of the Multi30k dataset (Elliott et al., 2016), our MMT model surpasses our constrained MMT baseline by about 10 BLEU and 8 METEOR points. The rest of the paper is organised as follows: §2 describes our pre-training and fine-tuning protocol, §3 presents our quantitative and qualitative analyses, and §4 concludes the paper with pointers for future work.",Visual Pre-training Across Languages for Multimodal MT,"Pre-trained language models have been demonstrated to drastically enhance results in many natural language tasks. Initially, focus was on single language pre-training, however recent developments have led to cross-lingual and visual pre-training techniques. This paper combines these two strategies to generate visually-grounded cross-lingual representations. We extended translation language modelling (Lample and Conneau, 2019) with masked region classification and conducted pre-training with three-way parallel vision & language corpora. When fine-tuned for multimodal machine translation, these models achieved state-of-the-art performance. Additionally, we offered qualitative evidence of the utility of the acquired grounded representations.","Pre-trained language models have been proven effective for contextual representation extraction, demonstrating their utility for many NLP tasks. To further explore their capabilities, research has been conducted into cross-lingual pretraining and visual pre-training, where large-scale image captioning corpora are used to induce grounded vision & language representations. This paper presents the first attempt to bring together cross-lingual and visual pre-training, introducing the visual translation language modelling (VTLM) objective. Unlike prior work using classification or retrieval based downstream evaluation, this paper focuses on the generative task of multimodal machine translation (MMT), where images accompany captions during translation. After pre-training, the VTLM encoder is transferred to a Transformer-based MMT and fine-tuned for the MMT task. This is also the first attempt of pre-training & fine-tuning for MMT. The findings show the effectiveness of crosslingual visual pre-training, surpassing a constrained MMT baseline by 10 BLEU and 8 METEOR points. The paper is organised as follows: §2 describes the pre-training and fine-tuning protocol, §3 presents quantitative and qualitative analyses, and §4 concludes with pointers for future work.",https://arxiv.org/pdf/2101.10044.pdf
SLAMP: Stochastic Latent Appearance and Motion Prediction,"Motion is an important cue for video prediction and often utilized by separating video content into static and dynamic components. Most of the previous work utilizing motion is deterministic but there are stochastic methods that can model the inherent uncertainty of the future. Existing stochastic models either do not reason about motion explicitly or make limiting assumptions about the static part. In this paper, we reason about appearance and motion in the video stochastically by predicting the future based on the motion history. Explicit reasoning about motion without history already reaches the performance of current stochastic models. The motion history further improves the results by allowing to predict consistent dynamics several frames into the future. Our model performs comparably to the stateof-the-art models on the generic video prediction datasets, however, significantly outperforms them on two challenging real-world autonomous driving datasets with complex motion and dynamic background.","Videos contain visual information enriched by motion. Motion is a useful cue for reasoning about human activities or interactions between objects in a video. Given a few initial frames of a video, our goal is to predict several frames into the future, as realistically as possible. By looking at a few frames, humans can predict what will happen next. Surprisingly, they can even attribute semantic meanings to random dots and recognize motion patterns [15]. This shows the importance of motion to infer the dynamics of the video and to predict the future frames. Motion cues have been heavily utilized for future frame prediction in computer vision. A common approach is to factorize the video into static and dynamic components [30, 20, 22, 6, 9, 21, 14, 28]. First, most of the previous methods are deterministic and fail to model the uncertainty of the future. Second, motion is typically interpreted as local changes from one frame to the next. However, changes in motion follow certain patterns when observed over some time interval. Consider scenarios where objects move with near-constant velocity, or humans repeating atomic actions in videos. Regularities in motion can be very informative for future frame prediction. In this work, we propose to explicitly model the change in motion, or the motion history, for predicting future frames. Stochastic methods have been proposed to model the inherent uncertainty of the future in videos. Earlier methods encode the dynamics of the video in stochastic latent variables which are decoded to future frames in a deterministic way [4]. We first assume that both appearance and motion are encoded in the stochastic latent variables and decode them separately into appearance and motion predictions in a deterministic way. Inspired by the previous deterministic methods [7, 20, 9], we also estimate a mask relating the two. Both appearance and motion decoders are expected to predict the full frame but they might fail due to occlusions around motion boundaries. Intuitively, we predict a probabilistic mask from the results of the appearance and motion decoders to combine them into a more accurate final prediction. Our model learns to use motion cues in the dynamic parts and relies on appearance in the occluded regions. The proposed stochastic model with deterministic decoders cannot fully utilize the motion history, even when motion is explicitly decoded. In this work, we propose a model to recognize regularities in motion and remember them in the motion history to improve future frame predictions. We factorize stochastic latent variables as static and dynamic components to model the motion history in addition to the appearance history. We learn two separate distributions representing appearance and motion and then decode static and dynamic parts from the respective ones. Our model outperforms all the previous work and performs comparably to the state-of-the-art method, SRVP, [8] without any limiting assumptions on the changes in the static component on the generic video prediction datasets, MNIST, KTH and BAIR. However, our model outperforms all the previous work, including SRVP, on two challenging real-world autonomous driving datasets with dynamic background and complex object motion.",Predicting Stochastic Latent Appearance and Motion with SLAMP,"Motion is an important factor for video prediction, often split into static and dynamic components. Previous work mostly used deterministic approaches, but there are stochastic methods that account for the uncertainty of the future. Existing stochastic models either don't consider motion explicitly or impose restrictions on the static part. We propose a model that takes into account both motion and appearance stochastically, predicting future frames based on motion history. Reasoning about motion without history already reaches the performance of current stochastic models, and motion history further improves results by allowing prediction of consistent dynamics multiple frames ahead. Our model performs comparably to the state-of-the-art models on generic video prediction datasets, but significantly better on two real-world autonomous driving datasets with complex motion and dynamic background.","Videos contain visual information enriched by motion, which is useful for inferring human activities or interactions between objects. Given a few initial frames, our aim is to accurately predict several frames into the future, as humans can do with random dots and motion patterns [15]. Computer vision has utilized motion cues for this purpose, commonly factorizing the video into static and dynamic components [30, 20, 22, 6, 9, 21, 14, 28]. Stochastic methods have been proposed to model the uncertainty of the future, encoding dynamics in latent variables decoded deterministically into frames [4]. We propose a model to explicitly recognize regularities in motion, remembering them in the motion history to improve predictions. We factorize stochastic latent variables into static and dynamic components, learning two separate distributions representing appearance and motion, then decoding static and dynamic parts from them. Our model outperforms all previous work on generic datasets MNIST, KTH and BAIR, and two challenging real-world autonomous driving datasets with dynamic background and complex object motion.",https://openaccess.thecvf.com/content/ICCV2021/papers/Akan_SLAMP_Stochastic_Latent_Appearance_and_Motion_Prediction_ICCV_2021_paper.pdf
Generating visual story graphs with application to photo album summarization,"Making sense of ever-growing amount of visual data available on the web is difficult, especially when considered in an unsupervised manner. As a step towards this goal, this study tackles a relatively less explored topic of generating structured summaries of large photo collections. Our framework relies on the notion of a story graph which captures the main narratives in the data and their relationships based on their visual, textual and spatio-temporal features. Its output is a directed graph with a set of possibly intersecting paths. Our proposed approach identifies coherent visual storylines and exploits sub-modularity to select a subset of these lines which covers the general narrative at most. Our experimental analysis reveals that extracted story graphs allow for obtaining better results when utilized as priors for photo album summarization. Moreover, our user studies show that our approach delivers better performance on next image prediction and coverage tasks than the state-of-the-art.","When we are planning a trip to a place we have never been before, we usually buy a guidebook or a travel app or visit websites such as tripadvisor.com or wikitravel.org to choose which places to visit and what to do in that destination. City guides which were prepared by professional travelers typically include essential information about the attractions, museums or parks in that city. Hence, each traveler, in a way, joins a collaborative act of living and enjoying the city and its culture. This joint act is clearly visible when we look at related travel photo albums shared on the web. Of course, the individual details can vary across trips, but common elements manifest themselves, provid- ing collaborative stories about a city. Same landmark locations and attractions are visited regularly by tourists, and are being photographed again and again. In this study, we propose a novel approach to automatically gen- erate an informative visual summary of a specific city directly from a large set of travel photo albums related about that city. We for- mulate this task as a sub-modular optimization problem in which the structured summary is represented in terms of a story graph, providing information about different characteristics of a city. In general, a story graph allows to illustrate the common relationships between data samples in an informative manner, and has been a topic of interest in the scientific community lately. For instance, story graphs have been used to create summarizes of news articles [1], scientific papers [2],ego-centric videos [3] and the interactions among different characters in a movie or TV series [4]. Given tens of thousands of images of a city, in our work, we aim to identify a few storylines that (1) are coherent, i.e. each tells a co- herent but different story, (2) cover most of the interesting attractions, i.e. they provide collective information regarding important and salient characteristics of the city, and (3) are connected, i.e. they effectively capture the hidden interconnections. Fig. 1 demonstrates an example story graph for the city of Istanbul, reconstructed automatically with our framework by analyzing lots of related travel photo albums. The main contributions of our work are as follows: We develop a collaborative summarization approach which ex- ploits visual and textual data as well as geospatial and timestamp information to automatically extract a visual story graph for a large collection of photo albums. Our formulation enforces max- imum degrees of coherency, coverage and connectivity over the extracted storylines, and as it depends on sub-modularity, it is efficient and scalable. We introduce YFCC100M-CITIES dataset which includes images of six different cities, annotated with GPS, timestamp tags and textual keywords. It contains in total 132,346 images over 1566 photo albums from 323 users for 6 popular travel destinations in the world.We utilize the story graphs generated with our approach as struc- tured abstractions of important concepts, landmarks and events within the photo collections, and demonstrate that they can be employed as a prior in photo album summarization to obtain state-of-the-art results. We further demonstrate the effectiveness of our framework with two user studies on next image prediction and tag coverage tasks. Our experimental results show that our model provides better results than the state-of-the-art.",Creating visual story graphs for photo album summarization,"Tackling a less explored topic of generating structured summaries of large photo collections, this study presents a framework based on the concept of a story graph to make sense of the vast amount of visual data available on the web in an unsupervised manner. This graph captures main narratives and their relationships by using visual, textual and spatio-temporal features, and its output is a directed graph with intersecting paths. Our proposed approach identifies coherent visual storylines and selects a subset which covers the general narrative most, through exploiting sub-modularity. Experimental analysis reveals that the extracted story graphs lead to better results when used as priors for photo album summarization. Furthermore, user studies show that our approach outperforms the state-of-the-art in next image prediction and coverage tasks.","When planning a trip to an unfamiliar place, one often purchases a guidebook, downloads a travel app, or visits websites such as TripAdvisor or Wikitravel to decide which locations to visit and activities to partake in. Professional travelers typically compile city guides containing essential information about attractions, museums, and parks. As a result, travelers engage in a joint effort to explore and experience the city and its culture, which is reflected in the shared travel photo albums on the web. Though individual experiences may differ, common elements are often observed, offering a collective narrative of the city. Popular landmarks and attractions are often visited and photographed repeatedly. This study proposes a novel approach to automatically generate an informative visual summary of a city from a large set of related travel photo albums. It formulates this task as a sub-modular optimization problem where the structured summary is represented as a story graph, providing information about different characteristics of the city. Story graphs have been used to summarize news articles, scientific papers, ego-centric videos, and interactions among characters in movies or TV series. In this work, the goal is to identify a few storylines that are coherent, cover most of the interesting attractions, and are connected. An example story graph for the city of Istanbul is shown in Fig. 1, generated with the framework by analyzing multiple travel photo albums. The main contributions of this work include the development of a collaborative summarization approach that exploits visual and textual data, geospatial and timestamp information to automatically extract a visual story graph for a large collection of photo albums. It also introduces the YFCC100M-CITIES dataset, which includes images of six cities annotated with GPS, timestamp tags, and textual keywords. Additionally, it employs the story graphs generated with the framework as structured abstractions of concepts, landmarks, and events within the photo collections and demonstrates their effectiveness as a prior in photo album summarization, as well as in two user studies on next image prediction and tag coverage tasks. Experimental results show that the model provides better results than the state-of-the-art.",https://reader.elsevier.com/reader/sd/pii/S092359652030182X?token=518A42E24CB886324C2FFA0BF2D9BD1A54EC5F2B0C12763CA24CFA4BC2CE51DFCFD7C2E1B7C5144E0867B22C4405850E&originRegion=eu-west-1&originCreation=20221228202800
MSVD-Turkish: A Comprehensive Multimodal Dataset for Integrated Vision and Language Research in Turkish,"Automatic generation of video descriptions in natural language, also called video captioning, aims to understand the visual content of the video and produce a natural language sentence depicting the objects and actions in the scene. This challenging integrated vision and language problem, however, has been predominantly addressed for English. The lack of data and the linguistic properties of other languages limit the success of existing approaches for such languages. In this paper we target Turkish, a morphologically rich and agglutinative language that has very different properties compared to English. To do so, we create the first large scale video captioning dataset for this language by carefully translating the English descriptions of the videos in the MSVD (Microsoft Research Video Description Corpus) dataset into Turkish. In addition to enabling research in video captioning in Turkish, the parallel English-Turkish descriptions also enables the study of the role of video context in (multimodal) machine translation. In our experiments, we build models for both video captioning and multimodal machine translation and investigate the effect of different word segmentation approaches and different neural architectures to better address the properties of Turkish. We hope that the MSVDTurkish dataset and the results reported in this work will lead to better video captioning and multimodal machine translation models for Turkish and other morphology rich and agglutinative languages.","Recent developments in computer vision (CV) and natural language processing (NLP) have led to a surge of new problems which lie at the intersection of these two fields, creating a new area of research in general called integrated vision and language (iVL). Video captioning is one of the important problems in iVL research, which has gained significant attention in both the CV and NLP communities. It aims at understanding the visual content of a given video clip and contextually generating a natural language description of this clip. Although considerable literature has revolved around this challenging task in recent years, all existing work is monolingual that it has mainly focused on the English language. Hence, whether or not the state-of-the-art video captioning methods can be effectively adapted to languages other than English, especially for low-resource languages, remains an open problem. Moreover, linguistic differences between English and other languages, particularly the ones that are morphologically richer than English, introduce new challenges that need to be addressed. Before these questions can be answered, however, we require video datasets containing descriptions from languages other than English to further enable iVL research. As a first step towards this direction, in this paper, we extend the MSVD (Microsoft Research Video Description Corpus) (Chen and Dolan, 2011) dataset and introduce a new multilingual dataset that we call MSVD-Turkish which contains approximately 2k video clips and a total of 80k Turkish video descriptions. In particular, we collect these Turkish descriptions by manually translating the original English video descriptions from MSVD into Turkish. As compared to the original English descriptions, Turkish descriptions have a larger vocabulary size and more importantly reflects the highly inflected and highly agglutinative nature of Turkish. We demonstrate the multilingual, multimodal capabilities of the proposed MSVD-Turkish dataset, by exploring two distinct iVL tasks shown in Figure 1, namely video captioning and multimodal machine translation, but with a special focus on Turkish. As far as we are aware of, this is the first to investigate generating Turkish descriptions depicting visual content of videos. To this end, we analyse different segmentation strategies for Turkish. Additionally, we explore multimodal machine translation as the second task on MSVD-Turkish where we examine the use of supplementary visual cues within videos to potentially improve the translation quality. Our primary contributions in this paper are: – To foster the research in the multilingual, multimodal language generation, we collect a new large scale dataset called MSVD-Turkish by translating the English descriptions of the videos from the well-known MSVD dataset into Turkish. – We investigate the performance of several (multimodal) machine translation and video captioning models on the proposed MSVD-Turkish dataset. – To address the rich and agglutinative morphology of Turkish, we explore different word segmentation approaches in Turkish. The paper is organised as follows: In Section 2, we briefly review the stateof-the-art in multimodal machine translation and video captioning. In Section 3, we introduce the MSVD-Turkish dataset, examine our data collection strategy and provide some statistics regarding the dataset. We introduce the details regarding the visual and textual representations considered in our machine translation and video captioning models in Section 4, and describe the models themselves in Section 5. In Section 6, we present our experimental results and discuss our findings and finally, we provide a summary of our work and discuss possible future research directions in Section 7. ",Integrated Vision and Language Research in Turkish: MSVD-Turkish Dataset,"The task of automatically generating natural language descriptions for videos, also known as video captioning, requires understanding the visual elements of the video and expressing them in a language. This integrated vision and language problem has mainly been studied in English, however, due to lack of data and the linguistic characteristics of other languages, existing approaches are not effective. In this paper, we focus on Turkish, an agglutinative language with distinct properties than English. To do this, we create the first large-scale video captioning dataset for Turkish by translating the English descriptions of the MSVD (Microsoft Research Video Description Corpus) dataset into Turkish. This dataset can be used to research video captioning in Turkish, as well as the role of video context in (multimodal) machine translation. We experiment with different word segmentation approaches and neural architectures to address the properties of Turkish and build models for both video captioning and multimodal machine translation. The MSVDTurkish dataset and results from this work can be used to improve video captioning and multimodal machine translation models for Turkish and other agglutinative languages.","Recent advancements in computer vision (CV) and natural language processing (NLP) have resulted in a new research field, known as integrated vision and language (iVL). One of the significant problems within this field is video captioning, which has attracted attention from both CV and NLP communities. This task involves understanding the visual elements of a video clip and producing a natural language description for it. Although much literature has been created around this challenging job recently, all existing work is monolingual, with a focus on English. It is yet unknown whether or not the current video captioning methods can be applied to languages other than English, especially those with fewer resources. Moreover, the linguistic differences between English and other languages, particularly those with more morphological complexity than English, introduce additional difficulties. To answer these questions, video datasets containing descriptions in languages other than English are required. To achieve this, in this paper, we extend the MSVD (Microsoft Research Video Description Corpus) (Chen and Dolan, 2011) dataset and create a new multilingual dataset called MSVD-Turkish which has around 2k video clips and 80k Turkish video descriptions. The descriptions were created by translating the English ones from MSVD into Turkish. The Turkish descriptions are larger in vocabulary size and reflect the highly inflected and agglutinative nature of Turkish. We investigate two iVL tasks using MSVD-Turkish: video captioning and multimodal machine translation, with a special emphasis on Turkish. We examine different segmentation strategies for Turkish and explore the use of supplementary visual cues within videos to potentially enhance translation quality. This paper contributes the MSVD-Turkish dataset, the analysis of different segmentation strategies for Turkish, and the evaluation of various machine translation and video captioning models on the dataset.",https://arxiv.org/pdf/2012.07098.pdf
CRAFT: A Benchmark for Causal Reasoning About Forces and inTeractions,"Humans are able to perceive, understand and reason about causal events. Developing models with similar physical and causal understanding capabilities is a long-standing goal of artificial intelligence. As a step towards this direction, we introduce CRAFT1 , a new video question answering dataset that requires causal reasoning about physical forces and object interactions. It contains 58K video and question pairs that are generated from 10K videos from 20 different virtual environments, containing various objects in motion that interact with each other and the scene. Two question categories in CRAFT include previously studied descriptive and counterfactual questions. Additionally, inspired by the Force Dynamics Theory in cognitive linguistics, we introduce a new causal question category that involves understanding the causal interactions between objects through notions like cause, enable, and prevent. Our results show that even though the questions in CRAFT are easy for humans, the tested baseline models, including existing state-of-the-art methods, do not yet deal with the challenges posed in our benchmark.","Causal reasoning is a key cognitive capability that involves making predictions about physical objects and their interactions. Cognitive scientists have mainly studied causal reasoning as simple causes or chains of events (Michotte, 1963; Baillargeon, 1994; Saxe et al., 2005), rather than processing of complex causal scenes, see (Göksun et al., 2013; George et al., 2019). Referring to the interactions of multiple forces, the Force Dynamics Theory emphasizes the processing and reasoning of complex scenes, and how causal language defines the patterns of forces in causal events (Wolff, 2007). In the past decade, though artificial learning systems have shown astonishing progress in natural language and image understanding, there are some tasks in which these systems are still significantly below human performance. One such challenging research area includes reasoning about physical actions of objects in complex causal scenes. In this paper, we explore how language and vision interact with each other in making plausible projections about causal reasoning, and analyze how well the existing neural models understand and reason about physical and causal relationships between dynamic objects in a scene through images and text. We propose a new video question answering dataset, named CRAFT (Causal Reasoning About Forces and inTeractions), which is designed to be complex for artificial models and simple for humans. Our dataset contains synthetically generated videos of 2D scenes with accompanying questions. Its most prominent features are that it contains video clips with complex physical interactions between objects, and questions that test strong reasoning capabilities. Answering our causal questions needs comprehending what is being asked, identifying objects in the scene, tracking their states in relation to other objects, which in turn can be attributed to different semantic categories of causes (cause, enable or prevent) that highlight unique patterns of causal forces in events – in line with the Force Dynamics Theory. In CRAFT, there are also some descriptive and counterfactual questions, the latter requiring understanding what would have happened after an intervention, i.e. a slight change in the scene (Wolff, 2013). Figure 1 shows sample questions from different question types, which are explained in detail in the subsequent sections. ",Benchmarking Causal Reasoning of Forces and Interactions,"Humans possess the capacity to identify, interpret and reason about causes and effects. Developing AI models with comparable physical and causal comprehension abilities is a long-held aspiration. To take a step towards this objective, we introduce CRAFT1 - a novel video question answering dataset that necessitates reasoning regarding physical forces and object interactions. It comprises of 58K video and question pairs generated from 10K videos from 20 virtual settings with various objects in motion that interact with each other and their environment. CRAFT comprises two question categories previously studied, as well as a novel causal question type inspired by the Force Dynamics Theory in cognitive linguistics, involving understanding the causal relationships between objects through concepts such as cause, enable and prevent. Our results demonstrate that even though humans find the questions in CRAFT straightforward, the evaluated baseline models, including existing state-of-the-art methods, are yet to address the difficulties presented in our benchmark.","Cognitive scientists have largely investigated causal reasoning as straightforward causes or sequences of events, instead of processing complex causal scenarios (Michotte, 1963; Baillargeon, 1994; Saxe et al., 2005; Göksun et al., 2013; George et al., 2019). Force Dynamics Theory looks into the processing and understanding of intricate scenes, and how causal language portrays the patterns of forces in causal events (Wolff, 2007). AI has seen tremendous growth in natural language and image recognition over the past decade, but is still lagging behind human performance in some tasks, like making logical deductions about physical actions of objects in complex causal scenes. This paper delves into the interplay of language and vision in predicting causal reasoning, and assessing the current neural models' comprehension and reasoning of physical and causal links between dynamic objects in a scene through images and text. We introduce CRAFT (Causal Reasoning About Forces and inTeractions), a novel video question answering dataset that is complicated for artificial models and easy for humans. It contains synthetic videos of 2D scenes with related questions, featuring complex physical interactions between objects, and questions that require strong reasoning skills. Answering the causal questions necessitates understanding the query, recognizing objects in the scene, and tracking their states relative to other objects, which can be assigned to distinct semantic categories of causes (cause, enable, or prevent) that emphasize unique patterns of causal forces in events – consistent with Force Dynamics Theory. CRAFT also contains descriptive and counterfactual questions, with the latter requiring understanding the results of an intervention, i.e. a minor change in the scene (Wolff, 2013). Figure 1 presents sample questions from different question types, which are detailed in the following sections.",https://arxiv.org/pdf/2012.04293.pdf
Belief Regulated Dual Propagation Nets for Learning Action Effects on Groups of Articulated Objects,"In recent years, graph neural networks have been successfully applied for learning the dynamics of complex and partially observable physical systems. However, their use in the robotics domain is, to date, still limited. In this paper, we introduce Belief Regulated Dual Propagation Networks (BRDPN), a general-purpose learnable physics engine, which enables a robot to predict the effects of its actions in scenes containing groups of articulated multi-part objects. Specifically, our framework extends recently proposed propagation networks (PropNets) and consists of two complementary components, a physics predictor and a belief regulator. While the former predicts the future states of the object(s) manipulated by the robot, the latter constantly corrects the robot’s knowledge regarding the objects and their relations. Our results showed that after training in a simulator, the robot can reliably predict the consequences of its actions in object trajectory level and exploit its own interaction experience to correct its belief about the state of the environment, enabling better predictions in partially observable environments. Furthermore, the trained model was transferred to the real world and verified in predicting trajectories of pushed interacting objects whose joint relations were initially unknown. We compared BRDPN against PropNets, and showed that BRDPN performs consistently well. Moreover, BRDPN can adapt its physic predictions, since the relations can be predicted online. ","In complex robotic systems, predicting the effects is a challenging problem when the number of objects varies, especially in the presence of rich and various interactions among these objects. Previously, we investigated predicting low-level effects on arbitrary shaped single objects given robot actions [1]. On the other hand, when several objects are linked with physical connections, this would also suggest some semantic connections between them, such as the motion of one object can propagate its motion onto another object, which might lead to a chain effect. To be able to model such systems accurately, the representation of data should be able to encode multiple objects and their interactions with each other, and it should be robust to perturbations. Recently, a great amount of effort has put on the prediction of the dynamics via graph networks (e.g. [2], [3], [4], [5], [6], [7], [8]). These works can deal with varying number of objects and learn rich interaction dynamics among these objects. Some of these works have focused on unsupervised learning, while others were aimed at developing learnable physics engines. However, applying them to model robotobject interactions is not very straightforward as the active involvement of the robot was not taken into account and, uncertainty in perception was not explicitly addressed. In this work, we propose Belief Regulated Dual Propagation Network (BRDPN) which takes the actions of the robot and their effects into account in predicting the next states1 . The network continuously regulates its belief about the environment based on its interaction history to correct its future predictions. For belief regulation, extending the recently proposed propagation networks (PropNets) [4] that handle instantaneous effect propagation, we propose a temporal propagation network that takes history of the motion of each object to predict unknown object or relation properties. Our system is verified on a table-top push setup that has cylindrical objects and joint relations between them. Our setup includes varying number of objects that might be connected with rigid, revolute or prismatic joints. The model definitions of these types of relations, including the PropNetn relation, is not provided to the robot. Notice that in our settings the relations between objects cannot be perceived by the robot. From its interaction experience in the simulator, it learns to predict relations between objects given observed object motions, and exploits this information to predict future object trajectories. Furthermore, it was transferred to real world and verified in experiments that included around 100 interactions. Our system was shown to outperform the original PropNets, both in simulation and real-world, when the relations between objects were not reliably provided to the system. Our contribution to the state of the art is two-fold. First, we introduced a deep neural network based method for learning how to exploit the interaction experience of the robot to extract values of otherwise unknown state variables in partially observable environments. Second, we implemented a learning based effect prediction robotic framework that can handle multiple interacting objects that might have different types of connections, and we verified this framework both in simulated and real robot experiments.",Belief-Controlled Dual Propagation Nets for Learning Action Effects on Articulated Object Groups,"Graph neural networks have been employed to comprehend the dynamics of intricate and partially observable physical systems in recent years, yet their use in robotics is still scant. In this paper, we introduce Belief Regulated Dual Propagation Networks (BRDPN), a general-purpose learnable physics engine which allows robots to anticipate the outcomes of their actions in scenes containing multiple connected parts. Our framework develops upon recently proposed Propagation Networks (PropNets) and consists of two distinct components, a physics predictor and a belief regulator. The former forecasts the future states of the objects controlled by the robot, while the latter regularly adjusts the robot's understanding of the objects and their interactions. Our results demonstrated that after training in a simulator, the robot can dependably foresee the results of its actions in object trajectory level and take advantage of its own interaction experience to modify its belief about the state of the environment, allowing for better predictions in partially observable settings. Additionally, the trained model was transferred to the real world and confirmed in predicting trajectories of interacting objects whose joint relations were initially unknown. We compared BRDPN with PropNets, and demonstrated that BRDPN performs reliably well. Moreover, BRDPN is able to adapt its physic predictions, as the relations can be calculated online.","Predicting effects in complex robotic systems with varying numbers of objects and rich interactions is difficult. We previously studied predicting low-level effects on single objects given robot actions [1]. When multiple objects are connected with physical connections, motion of one can propagate onto another, creating a chain effect. To accurately model such systems, data representation must encode objects and interactions and be robust to perturbations. Graph networks (e.g. [2], [3], [4], [5], [6], [7], [8]) have been used for dynamics prediction and can handle varying numbers of objects and learn interactions. Our Belief Regulated Dual Propagation Network (BRDPN) takes robot actions and their effects into account when predicting next states. It regulates belief based on interaction history and predicts unknown object or relation properties from motion history. Our system was tested in a table-top push setup with cylindrical objects and various joint relations between them. It was shown to outperform PropNets in simulations and real-world experiments when relations were not provided. Our method uses a deep neural network to exploit interaction experience to extract values of unknown state variables in partially observable environments. Additionally, we implemented a learning based effect prediction framework that handles multiple interacting objects with different types of connections and verified it in simulated and real robot experiments.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=9196878
Hedging static saliency models to predict dynamic saliency,"In recent years, many computational models for saliency prediction have been introduced. For dynamic scenes, the existing models typically combine different feature maps extracted from spatial and temporal domains either by following generic integration strategies such as averaging or winners take all or using machine learning techniques to set each feature’s importance. Rather than resorting to these fixed feature integration schemes, in this paper, we propose a novel weakly supervised dynamic saliency model called HedgeSal, which is based on a decision-theoretic online learning scheme. Our framework uses two pretrained deep static saliency models as experts to extract individual saliency maps from appearance and motion streams, and then generates the final saliency map by weighted decisions of all these models. As visual characteristics of dynamic scenes constantly vary, the models providing consistently good predictions in the past are automatically assigned higher weights, allowing each expert to adjust itself to the current conditions. We demonstrate the effectiveness of our model on the CRCNS, UCFSports and CITIUS datasets.","Visual saliency estimation, the task of predicting where humans look at images, has been an active research area in the computer vision community [1] over the past 40 years. Especially with the introduction of new benchmark datasets and new methods such as deep learning, we are witnessing increasingly more sophisticated models. Despite this surge of interest, however, saliency prediction has not been solved yet as the existing saliency models are not fully capable of describing all of the phenomena observed in the visual attention studies [2]. Most of the existing approaches for saliency prediction focus primar- ily on static images and thus predict eye fixations without considering dynamic scene characteristics such as apparent motion. For instance, the early static models (e.g. [3,4]) encode local contrast information based on differences of very low-level visual features like intensity, color and orientation. Some more complex models (e.g. [5]) employ features that encode faces and pedestrians in order to include some known top-down factors, yet these features are also based on static object detectors. Another key issue is the so-called feature integration problem. The early models consider very simple fusion strategies such as taking the mean or the product of the individual feature chan- nels, however the progress within the last decade has led to more sophisticated solutions which aim at learning an optimal strategy from training data. While the first group of such approaches uses hand- crafted features and shallow machine learning techniques like SVM [6],AdaBoost [7], the current state-of-the-art models for static saliency prediction are all based on convolutional neural networks and trained in an end-to-end fashion [2]. These deep saliency models, however, require a vast amount of data either in their pre-training or training phases. As compared to its static counterpart, dynamic saliency prediction addresses the problem of estimating where humans fixate their eyes in videos, and is a far more challenging problem. In literature, the number of studies on dynamic saliency is far smaller than that on static saliency. The majority of the dynamic models commonly consider separate appearance (spatial) and motion (temporal) streams, extract features from these streams and finally combine them to obtain a final saliency map (e.g. [8–13]). In this sense, the models mainly differ from each other by the features they use and their feature integration strategies. Regarding feature integration in dynamic saliency, existing models employ either very generic integration strategies like the ones for static saliency such as averaging or winners take all, or consider very ad hoc solutions to combine features from the appearance and motion streams. But, these naive approaches greatly limit the overall performance. As a remedy, recent models try to solve this issue by learning each feature’s contribution to the overall saliency directly from training data. However, for dynamic scenes, this is still not sufficient since these methods still associate a (learned) constant weight with each feature. On the other hand, in regards to human visual system, visual attention mechanisms exhibit completely different, more complex behavior in dynamic scenes than static scenes. For instance, in [14], the authors showed that humans fixate their eyes at different people or objects on videos and static images, or according to the camera motion, the fixations on videos and images are not on the objects exist in the scenes but rather lies on the anticipated directions. Moreover, the central bias which has a strong effect on static images has lesser impacts in dynamic scene fixations. All these observations suggest that to fully deal with the challenges of dynamic scenes and to achieve better prediction accuracies, the weights of the visual features should be defined in a more flexible way and should change over time. That is, it is important to consider integration schemes that can adapt themselves according to changes in the visual content to combine different features in the best possible way. In this paper, we propose a novel weakly supervised dynamic saliency model which is built upon a set pretrained deep static saliency models processing the appearance and motion streams. In short, we use these static models as experts within our framework and combine their results by considering a decision theoretic formulation to infer the master saliency map. Using decision theory helps us to define certain reliability scores to each one of our expert models according to some optimality conditions with respect to the end results and accordingly allows the integration step to be carried out in an adaptive manner. In literature, there are several decision-theoretic solutions exists for defin- ing these optimality conditions such as minimum probability of error. Within our formulation, we specifically follow the decision theoretic online learning scheme known as the Hedge algorithm [15,16], hence, we refer to our proposed approach as HedgeSal throughout our paper. Specifically, we first extract appearance and motion streams of a given video, and then run SALICON [17] and SalNet [18], two deep static saliency models, on individual frames, and generate the final saliency map by the weighted decisions of all these models. Each one of our experts captures different visual characteristics of the scene, the ones which provide consistently good predictions in the previous frames are given higher weights in the current frame, increasing the prediction accuracy. Here, it is important to mentioned that a recent trend in the dynamic saliency literature is to employ deep learning to train saliency models in an end-to-end manner [19–21]. However, all these models are trained in a supervised manner and need a large amount of annotated video data with the groundtruth eye fixations, which is in general very hard to obtain. In summary, our main contributions in this paper are as follows: 1. We propose a novel weakly supervised dynamic saliency model that integrates the results of several deep static saliency models to predict where humans look at videos. 2. We develop an adaptive feature integration scheme which de- pends on a decision theoretic online learning mechanism. 3. We perform an extensive set of experiments on three differ- ent benchmark datasets to demonstrate the effectiveness of the proposed models against the state-of-the-art models. The paper is structured as follows: In Section 2, we give a brief discussion about the existing saliency models in the literature. In Section 3, we introduce our adaptive dynamic saliency model. After that, in Section 4, we present our experimental results together with the details of the benchmark datasets, evaluation metrics used in the experiments. Finally, in Section 5, we provide a summary of our work and discuss possible future research directions.",Predicting dynamic saliency with hedged static models,"In recent years, a novel weakly supervised dynamic saliency model, called HedgeSal, has been proposed. It is based on a decision-theoretic online learning scheme, which uses two pretrained deep static saliency models as experts to extract individual saliency maps from appearance and motion streams, and then generates the final saliency map by weighted decisions of all these models. Rather than employing generic integration strategies such as averaging or winners take all or using machine learning techniques to set each feature’s importance, this framework allows each expert to adjust itself to the current conditions by assigning higher weights to the models providing consistently good predictions in the past. The effectiveness of the model has been demonstrated on the CRCNS, UCFSports and CITIUS datasets.","Research into visual saliency estimation, the task of predicting where humans look at images, has been ongoing in the computer vision community for 40 years. With the introduction of new datasets and methods such as deep learning, increasingly sophisticated models are emerging. However, saliency prediction is yet to be solved, as existing models do not fully explain phenomena observed in visual attention studies. Most approaches focus on static images, and thus predict eye fixations without considering dynamic scene characteristics. Early models encode local contrast information based on low-level visual features, while more complex models use features to account for top-down factors. Feature integration is another issue; early models use simple fusion strategies, but recent approaches use convolutional neural networks and train in an end-to-end fashion. Dynamic saliency prediction is far more challenging than static saliency, with fewer studies in literature. Common models consider separate appearance and motion streams, extract features from them, and combine them to get a saliency map. Feature integration strategies are either generic or ad hoc. Recent models try to learn each feature's contribution from training data, but this is still insufficient. To fully deal with dynamic scenes and improve prediction accuracy, the weights of visual features must be more flexible and change over time. This paper proposes a novel weakly supervised dynamic saliency model which uses pretrained deep static saliency models to process appearance and motion streams, and combines their results with a decision theoretic formulation to infer the master saliency map. Experiments on three benchmark datasets demonstrate the effectiveness of the proposed model.",https://reader.elsevier.com/reader/sd/pii/S0923596518311846?token=7F235B99450FFAEF4F52AED55C1ABB5E2EF8CDE90ADBD089FBEC6F1CEB4FC202B82D242CAF761896AA2E51543EA3423D&originRegion=eu-west-1&originCreation=20221228203929
Deep Learning for Medicine and Remote Sensing: A Brief Review,"In recent years, deep learning methods have come to the forefront in many areas that require remote sensing, from medicine to agriculture, from defense industry to space research; and these methods have achieved tremendous success as compared to traditional methods. Together with substantial growth in available data with high-quality labels and computational resources, these deep neural network architectures and techniques have seen remarkable developments. The major difference between deep learning and classical recognition methods is that deep learning methods consider an end-to-end learning scheme which gives rise to learning features from raw data. Better regularization techniques and robust optimization algorithms introduced with state-of-the-art deep learning models are other factors leading this difference. In this paper, we discuss the remote sensing problems and how deep learning can be used to solve these problems with a special focus on medical and remote sensing applications. In particular, we briefly review outperforming architectures within the deep learning literature and their use cases. ","Machine learning approaches called deep artificial neural networks have led to significant developments in many areas related to artificial intelligence, including computer vision in the past ten years. One of the underlying factors in the success of deep models is the end-to-end learning scheme these models have. Unlike the hand-crafted features extracted from raw data and used by traditional machine learning models in learning, these deep models can learn features from the raw data in a hierarchical manner. However, the distinctness of these features and the performance of deep models depend directly on the capacity of these models as well as on the amount of data on which the models are trained. The companies such as Google, Microsoft, Facebook have used deep learning actively in their applications and services for object recognition, target detection, image segmentation, and classification. Moreover, deep learning toolboxes like Tensorflow from Google, CNTK from Microsoft, Torch from Facebook, Caffe, MXNet, Theano, Deeplearning4j led to a surge of studies on various topics (De Felice, 2017). For example, it has been proven that Convolutional Neural Networks (CNNs) are especially good at extracting medium and high-level features from raw data in a translation-invariant manner (Zhu et al., 2017). In this way, the features learned by CNNs are successfully used in problems such as image recognition, object detection, and semantic segmentation. On the other hand, Recurrent Neural Networks (RNNs) have shown significant success in sequential data analysis such as action recognition, video analysis, and subtitling. In addition to these achievements, the use of deep learning is becoming widespread in remote sensing, medical and defense industries thanks to increased data sizes and computing resources. Medical and remote sensing applications present some new challenges for deep learning. For instance, data in both application areas appear as multi-modal. Data acquired in remote sensing industry using various cameras such as light detection and ranging (LiDAR), radio detection and ranging (RADAR), synthetic aperture radar (SAR), hyperspectral should be collected and processed together. Similarly, it may be necessary to integrate a patient's genetic information and test results (e.g. blood and urine) with information obtained from images such as magnetic resonance imaging (MRI), ultrasound and x-ray. Multi-modality also requires the processing and interpretation of data received at different resolutions, at different times or over a long period of time series. In this paper, firstly we attempt to answer the question about what deep learning is and what kind of success has been achieved. We briefly review state-of-the-art deep learning models and approaches. Then, we discuss supervised and unsupervised deep learning approaches, architectures and frameworks i.e. deep convolutional neural networks and deep generative networks. Also, we provide two sections for deep learning applications in medicine and remote sensing.",Review of Deep Learning for Medicine & Remote Sensing,"In recent years, deep learning methods have been very successful in remote sensing applications such as medicine, agriculture, defense industry and space research, compared to traditional methods. The cause for this is due to the growth in available data with labels, along with computational resources. These deep neural network architectures and techniques have seen remarkable developments, due to the end-to-end learning scheme which allows for learning from raw data. Furthermore, improved regularization techniques and optimization algorithms in the state-of-the-art deep learning models are another contributing factor. In this paper, we discuss how deep learning can be used to solve remote sensing problems, with a focus on medical and remote sensing applications. We also review architectures within the deep learning literature and their respective use cases.","Deep artificial neural networks, a type of machine learning approach, have enabled substantial progress in many areas of artificial intelligence, including computer vision, over the past decade. The success of these deep models is mainly attributed to their end-to-end learning approach, which allows them to learn features from raw data in a hierarchical manner, instead of using pre-crafted features as done in traditional machine learning models. Companies such as Google, Microsoft and Facebook have widely employed deep learning in their applications and services for object recognition, target detection, image segmentation and classification. Additionally, deep learning toolboxes such as Tensorflow, CNTK, Torch, Caffe, MXNet, Theano and Deeplearning4j have spurred research in various fields. Convolutional Neural Networks (CNNs) have been shown to effectively extract medium and high-level features from raw data in a translation-invariant manner, and are thus used in tasks such as image recognition, object detection and semantic segmentation. On the other hand, Recurrent Neural Networks (RNNs) have demonstrated great success in sequential data analysis, such as action recognition, video analysis and subtitling. Moreover, the increased availability of data and computing resources has enabled deep learning to be employed in remote sensing, medical and defense industries. These applications present certain challenges for deep learning, such as the need to process and interpret data from different sources at different resolutions, at different times or over a long period of time series. This paper provides an overview of deep learning, including its state-of-the-art models and approaches, supervised and unsupervised deep learning architectures and frameworks, and applications in medicine and remote sensing.",https://dergipark.org.tr/en/download/article-file/1027010
Manipulating Attributes of Natural Scenes via Hallucination,"In this study, we explore building a two-stage framework for enabling users to directly manipulate high-level attributes of a natural scene. The key to our approach is a deep generative network that can hallucinate images of a scene as if they were taken in a different season (e.g., during winter), weather condition (e.g., on a cloudy day), or at a different time of the day (e.g., at sunset). Once the scene is hallucinated with the given attributes, the corresponding look is then transferred to the input image while preserving the semantic details intact, giving a photo-realistic manipulation result. As the proposed framework hallucinates what the scene will look like, it does not require any reference style image as commonly utilized in most of the appearance or style transfer approaches. Moreover, it allows to simultaneously manipulate a given scene according to a diverse set of transient attributes within a single model, eliminating the need of training multiple networks per each translation task. Our comprehensive set of qualitative and quantitative results demonstrates the effectiveness of our approach against the competing methods.","The visual world we live in constantly changes its appearance depending on time and seasons. For example, at sunset, the sun gets close to the horizon and gives the sky a pleasant red tint; with the advent of warm summer, the green tones of the grass transform into bright tones of yellow; and autumn brings a variety of shades of brown and yellow to the trees. Such visual changes in the nature continues in various forms at almost any moment with the effect of time, weather, and season. Such high-level changes are referred to as transient scene attributes—e.g., cloudy, foggy, night, sunset, winter, summer, to name a few [Laffont et al. 2014]. Recognizing transient attributes of an outdoor image and modifying its content to reflect any changes in these properties was studied in the past; however, current approaches have many constraints that limit their usability and effectiveness in attribute manipulation. In this article, we present a framework that can hallucinate different versions of a natural scene given its semantic layout and its desired real valued transient attributes. Our model can generate many possible output images from scratch, such as the ones in Figure 1, which are made possible by learning from data the semantic meaning of each transient attribute and the corresponding local and global transformations.Image generation is quite a challenging task, since it needs to have realistic-looking outputs. Visual attribute manipulation can be considered a bit harder, as it aims at photorealism as well as results that are semantically consistent with the input image. For example, for predicting the look of a scene at sunset, visual appearances of the sky and the ground undergo changes differently, the sky gets different shades of red while the dominant color of the ground becomes much darker and texture details get lost. Unlike recent image synthesis methods [Chen and Koltun 2017; Isola et al. 2017; Qi et al. 2018; Wang et al. 2018], which explore producing realistic-looking images from semantic layouts, automatically manipulating visual attributes requires modifying the appearance of an input image while preserving object-specific semantic details intact. Some recent style transfer methods achieve this goal to a certain extent but they require a reference style image [Li et al. 2018; Luan et al. 2017]. A simple solution to obtain an automatic style transfer method is to retrieve reference style images with desired attributes from a well-prepared dataset with a rich set of attributes. However, this approach raises new issues that need to be solved such as retrieving images according to desired attributes and semantic layout in an effective way. To overcome these obstacles, we propose to combine neural image synthesis and style transfer approaches to perform visual attribute manipulation. For this purpose, we first devise a conditional image synthesis model that is capable of hallucinating desired attributes on synthetically generated scenes with semantic content similar to the input image and then we resort to a photo style transfer method to transfer the visual look of the hallucinated image to the original input image to produce a resulting image with the desired attributes. A rich variety of generative models including Generative Adversarial Networks (GANs) [Goodfellow et al. 2014; Radford et al. 2016; Vondrick et al. 2016], Variational Autoencoders (VAEs) [Gregor et al. 2015; Kingma and Welling 2014], and autoregressive models [Mansimov et al. 2016; Oord et al. 2016] have been developed to synthesize visually plausible images. Images of higher resolutions, e.g., 256 × 256, 512×512, or 1,024 ×1,024, have also been rendered under improved versions of these frameworks [Berthelot et al. 2017; Chen and Koltun 2017; Gulrajani et al. 2016; Karras et al. 2018, 2019; Reed et al. 2016a, 2016b; Shang et al. 2017; Zhu et al. 2017a]. However, generating diverse, photorealistic, and well-controlled images of complex scenes has not yet been fully solved. For image synthesis, we propose a new conditional GAN-based approach to generate a target image that has the same semantic layout with the input image but reflects the desired transient attributes. As shown in Figure 1, our approach allows users to manipulate the look of an outdoor scene with respect to a set of transient attributes, owing to a learned manifold of natural images. To build the aforementioned model, we argue the necessity of better control over the generator network in GAN. We address this issue by conditioning ample concrete information of scene contents to the default GAN framework, deriving our proposed attribute and semantic layout conditioned GAN model. Spatial layout information tells the network where to draw, resulting in clearly defined object boundaries and transient scene attributes serve to edit visual properties of a given scene so we can hallucinate desired attributes for input image in semantically similar generated image. However, naively importing the side information is insufficient. For one reason, when training the discriminator to distinguish mismatched image-condition pairs, if the condition is randomly sampled, it can easily be too off in describing the image to provide meaningful error derivatives. To address this issue, we propose to selectively sample mismatched layouts for a given real image, inspired by the practice of hard negative mining [Wang and Gupta 2015]. For another reason, given the challenging nature of the scene generation problem, adversarial objective alone can struggle to discover a satisfying output distribution. Existing works in synthesizing complex images apply the technique of “feature matching,” or perceptual loss [Chen and Koltun 2017; Dosovitskiy and Brox 2016]. Here, we also adopt perceptual loss to stabilize and improve adversarial training for more photographic generation but contrasting prior works, our approach employs the layout-invariant features pretrained on segmentation task to ensure consistent layouts between synthesized images and reference images. For photo style transfer, we use a recent deep learning–based approach [Li et al. 2018] that transfers visual appearance between same semantic objects in real photos using semantic layout maps. Our contributions are summarized as follows:  We propose a new two-stage visual attribute manipulation framework for changing high-level attributes of a given outdoor image.  We develop a conditional GAN variant for generating natural scenes faithful to given semantic layouts and transient attributes.  We build up an outdoor scene dataset annotated with layout and transient attribute labels by combining and annotating images from Transient Attributes [Laffont et al. 2014] and ADE20K [Zhou et al. 2017 Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. 2017. Scene parsing through ADE20K dataset. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR’17).].",Altering Elements of Nature via Imagination,"This study proposes a two-stage framework to allow users to manipulate high-level attributes of a natural scene. A deep generative network is used to hallucinate images of the scene in different seasons, weather conditions, and times of day. The corresponding look is then transferred to the input image while preserving semantic details, creating a photo-realistic manipulation result. The framework does not require any reference style image, and enables simultaneous manipulation of a scene according to a diverse set of transient attributes within a single model, eliminating the need for multiple networks per task. Comprehensive qualitative and quantitative results demonstrate the effectiveness of the approach.","The appearance of the visual world we live in is constantly changing depending on time and seasons; for instance, when the sun sets, the sky obtains a pleasing red hue; with the onset of summer, the green grass turns to bright yellow; and autumn brings a mix of brown and yellow shades to the trees. Such changes in nature are referred to as transient scene attributes - e.g. cloudy, foggy, night, sunset, winter, summer, etc. [Laffont et al. 2014]. Developing a framework that can modify an outdoor image's content to reflect any changes in its properties has been studied in the past; however, current approaches have many limitations that restrict their usability and effectiveness. We present a framework which can create various versions of a natural scene from its semantic layout and desired real valued transient attributes. Generating realistic-looking images is a difficult task, as it needs to have outputs that look genuine. Manipulating visual attributes is even harder, as it strives for photorealism as well as results that are consistent with the input image semantically. Recent style transfer methods have achieved this to some extent, but they require a reference style image [Li et al. 2018; Luan et al. 2017]. To solve this, we propose to merge neural image synthesis and style transfer approaches to carry out visual attribute manipulation. For this purpose, we first devise a conditional image synthesis model which can hallucinate desired attributes on synthetically generated scenes with a semantic content similar to the input image. Then, we use a photo style transfer method to transfer the visual look of the hallucinated image to the original input image, to generate a resulting image with the desired attributes. We develop a GAN-based approach to generate a target image which has the same semantic layout with the input image but reflects the desired transient attributes. To ensure the generator network has better control, we condition ample concrete information of scene contents to the default GAN framework, resulting in our proposed attribute and semantic layout conditioned GAN model. To stabilize and improve adversarial training for more photographic generation, we adopt perceptual loss, using layout-invariant features pretrained on segmentation task to guarantee consistent layouts between synthesized images and reference images. For photo style transfer, we use a deep learning–based approach [Li et al. 2018] that transfers visual appearance between same semantic objects in real photos using semantic layout maps. Our contributions are: (1) proposing a new two-stage visual attribute manipulation framework for changing high-level attributes of a given outdoor image; (2) developing a conditional GAN variant for generating natural scenes faithful to given semantic layouts and transient attributes; (3) building an outdoor scene dataset annotated with layout and transient attribute labels by combining and annotating images from Transient Attributes [Laffont et al. 2014] and ADE20K [Zhou et al. 2017 Bolei Zhou, Hang Zhao, Xavier Puig, Sanja Fidler, Adela Barriuso, and Antonio Torralba. 2017. Scene parsing through ADE20K dataset. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR’17).].",https://dl.acm.org/doi/fullHtml/10.1145/3368312
Diverse Neural Photo Album Summarization,"In this paper, we address the problem of learning to summarize personal photo albums. That is, given a photo album, we aim to select a small set of representative images from the album so that the extracted summary captures most of the story that is being told through the images. More specifically, we extend a recently proposed recurrent neural network based framework by employing a more effective way to represent images and, more importantly, adding a diversity term to the main objective. Our diversity term is based on the idea of jointly training a discriminator network to evaluate the diversity of the selected images. This alleviates the issue of selecting near-duplicate or semantically similar images, which is the primary shortcoming of the base approach. The experimental results show that our improved model produces better or comparable summaries, providing a good balance between quality and diversity. ","Fast development and spread of digital cameras brought rapid increase in the amount visual data. Nowadays, a person can take thousands, if not tens of thousands, photos within a single year through a camera or a smart phone. Most of the time, these photos are shared in the social media platforms such as Instagram, Facebook or Flickr, but manually selecting photos from albums and managing them is hugely timeconsuming. Even if these photos are automatically organized within different albums or collections based on date or location, it makes really hard to manage these collections to obtain a visual summary of the memories grabbed in these albums. In that regard, a photo album can be interpreted a set of images containing certain events. The task of photo album summarization is simply defined as selecting a set of representative images from a photo album [1, 3, 9, 10, 13, 17, 18]. Albums generally tend to contain semantically similar or near duplicate images which cover several different events observed within an album. Hence, for summarization, the main challenge lies in understanding the album in a global manner. Figure 1 demonstrates this process on a sample photo album created from a trip to Venice, Italy. As can be seen from the ground truth summary created by a human subject, while generating a summary, humans generally try to cover the whole story of the album with a diverse set of images. Hence, within a summary, there should be no particularly similar or near duplicate images. Of course, these characteristics of a summary requires understanding the temporal relationships among images and image semantics, which is not an easy task. Photo albums can be related to several concepts and may span over different periods of time. For example, if the central theme of the album is traveling, images can be taken over days, a week or longer periods of time. However, if it is about a wedding, generally the time span is in hours or a day. These variations in photo albums makes the summarization task difficult. For instance, traveling London can contains similar human behaviors such that tourists tend to visit same consecutive locations. On the other hand, if the theme of a photo album is wedding, it is really hard to understand the events as different cultures have different wedding ceremonies leading to a variety of different stories in the visual domain. Albums sometimes contain photos which are not related in conceptual terms. For example, consider a photo album created for a snowboarding trip. When people go to snowboarding, they visit not only the snowy mountains but also other places, certain parts of the city as well and take photos there. Hence, a summarization method needs to cover all of these events in the photo albums. Another key challenge is that summarization is generally considered as a subjective task. Hence, the evaluation of summarization methods is generally carried out by comparing the automatically created summaries against a set of summaries produced by human subjects. For example, consider the two photo albums shown in Fig. 2, one about snowboarding, the other about a trip to London. For each album, the first two rows shows the summaries created by different people, and the last row is a summary automatically generated by the Skipping RNN model by Sigurdsson et al. [13]. As can be seen, human generated summaries are more diverse and more inclusive whereas the Skipping RNN model fails to cover the events captured in the albums and its summaries contain quite similar images. In our work, we extend the Skipping RNN model [13] by introducing a new objective function for training that discourages including semantically similar or near duplicate images in the summaries. Our paper is organized as follows: In section II, we first briefly discuss the related work. In section III, we provide the background and introduce our proposed framework. Then, in Section IV, we present and discuss our experimental results.",Summarizing a Variety of Photos in Neural Networks,"Addressing the problem of summarizing personal photo albums, this paper proposes an extension of a recurrent neural network-based framework. A more effective way of representing images is employed, and a diversity term is added to the main objective. This term is based on a discriminator network evaluating the diversity of the chosen images. The improved model produces better or equal summaries, offering a good balance between quality and diversity, while avoiding the issue of selecting near-duplicate or semantically similar images.","Digital cameras have rapidly increased the amount of visual data. One can take thousands of photos a year with a camera or a smartphone, and share them on social media. However, manually selecting and managing photos from albums is time consuming. Even if they are organized into albums or collections, it is difficult to create a visual summary of the memories in the album. Photo album summarization is the selection of representative images from an album, which generally contains semantically similar or near duplicate images of different events. Fig. 1 illustrates this process with a sample photo album from a trip to Venice, Italy. When creating a summary, humans attempt to cover the whole story with a diverse set of images, containing no similar or near duplicate images. This requires understanding temporal relationships and image semantics, which is difficult. Photo albums can span different periods of time, such as days or weeks for travel, or hours or a day for a wedding. Albums may also contain unrelated photos, such as when a snowboarding trip includes visits to other places. Evaluation of summarization methods is done by comparing automatically created summaries to those produced by humans. Fig. 2 shows two photo albums, one about snowboarding and the other about a trip to London, and the summaries created by different people and by the Skipping RNN model. Human summaries are more diverse and inclusive, while the Skipping RNN model fails to cover the events in the albums, and its summaries contain similar images. We extend the Skipping RNN model with a new objective function to discourage semantically similar or near duplicate images in the summaries. Our paper is organized as follows: Section II briefly discusses related work, Section III provides background and introduces our proposed framework, and Section IV presents and discusses our experimental results.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8936084
Procedural Reasoning Networks for Understanding Multimodal Procedures,"This paper addresses the problem of comprehending procedural commonsense knowledge. This is a challenging task as it requires identifying key entities, keeping track of their state changes, and understanding temporal and causal relations. Contrary to most of the previous work, in this study, we do not rely on strong inductive bias and explore the question of how multimodality can be exploited to provide a complementary semantic signal. Towards this end, we introduce a new entity-aware neural comprehension model augmented with external relational memory units. Our model learns to dynamically update entity states in relation to each other while reading the text instructions. Our experimental analysis on the visual reasoning tasks in the recently proposed RecipeQA dataset reveals that our approach improves the accuracy of the previously reported models by a large margin. Moreover, we find that our model learns effective dynamic representations of entities even though we do not use any supervision at the level of entity states.1","A great deal of commonsense knowledge about the world we live is procedural in nature and involves steps that show ways to achieve specific goals. Understanding and reasoning about procedural texts (e.g. cooking recipes, how-to guides, scientific processes) are very hard for machines as it demands modeling the intrinsic dynamics of the procedures (Bosselut et al., 2018; Dalvi et al., 2018; Yagcioglu et al., 2018). That is, one must be aware of the entities present in the text, infer relations among them and even anticipate changes in the states of the entities after each action. For example, consider the cheeseburger recipe presented in Fig. 1. The instruction “salt and pepper each patty and cook for 2 to 3 minutes on the first side” in Step 5 entails mixing three basic ingredients, the ground beef, salt and pepper, together and then applying heat to the mix, which in turn causes chemical changes that alter both the appearance and the taste. From a natural language understanding perspective, the main difficulty arises when a model sees the word patty again at a later stage of the recipe. It still corresponds to the same entity, but its form is totally different. Over the past few years, many new datasets and approaches have been proposed that address this inherently hard problem (Bosselut et al., 2018; Dalvi et al., 2018; Tandon et al., 2018; Du et al., 2019). To mitigate the aforementioned challenges, the existing works rely mostly on heavy supervision and focus on predicting the individual state changes of entities at each step. Although these models can accurately learn to make local predictions, they may lack global consistency (Tandon et al., 2018; Du et al., 2019), not to mention that building such annotated corpora is very labor-intensive. In this work, we take a different direction and explore the problem from a multimodal standpoint. Our basic motivation, as illustrated in Fig. 1, is that accompanying images provide complementary cues about causal effects and state changes. For instance, it is quite easy to distinguish raw meat from cooked one in visual domain. In particular, we take advantage of recently proposed RecipeQA dataset (Yagcioglu et al., 2018), a dataset for multimodal comprehension of cooking recipes, and ask whether it is possible to have a model which employs dynamic representations of entities in answering questions that require multimodal understanding of procedures. To this end, inspired from (Santoro et al., 2018), we propose Procedural Reasoning Networks (PRN) that incorporates entities into the comprehension process and allows to keep track of entities, understand their interactions and accordingly update their states across time. We report that our proposed approach significantly improves upon previously published results on visual reasoning tasks in RecipeQA, which test understanding causal and temporal relations from images and text. We further show that the dynamic entity representations can capture semantics of the state information in the corresponding steps.",Networks for Interpreting Multimodal Processes,"Addressing the difficult task of understanding procedural commonsense knowledge, this paper investigates how multimodality can be utilized to supply a semantic signal. We introduce an entity-aware neural comprehension model with external relational memory units to dynamically update entity states as the text instructions are read. Results from the RecipeQA dataset indicate that our model significantly surpasses the accuracy of prior models. Furthermore, our model learns dynamic entity representations without any state supervision.","Understanding and reasoning about procedural texts (e.g. cooking recipes, how-to guides, scientific processes) are difficult for machines due to the need to model the intrinsic dynamics of the procedures (Bosselut et al., 2018; Dalvi et al., 2018; Yagcioglu et al., 2018). For example, consider a cheeseburger recipe, where the instruction “salt and pepper each patty and cook for 2 to 3 minutes on the first side” entails mixing three basic ingredients and then applying heat to the mix, resulting in changes to the appearance and taste. To address these challenges, various datasets and approaches have been proposed that rely heavily on supervision and focus on predicting the individual state changes of entities at each step (Bosselut et al., 2018; Dalvi et al., 2018; Tandon et al., 2018; Du et al., 2019). In this work, we explore the problem from a multimodal standpoint and propose Procedural Reasoning Networks (PRN) that incorporate entities into the comprehension process, allowing for tracking of entities, understanding of their interactions, and updating of their states across time. We report that our proposed approach significantly improves upon previously published results on visual reasoning tasks in RecipeQA, which test understanding of causal and temporal relations from images and text, and further show that the dynamic entity representations can capture semantics of the state information in the corresponding steps.",https://arxiv.org/pdf/1909.08859.pdf
Image Synthesis in Multi-Contrast MRI With Conditional Generative Adversarial Networks,"Acquiring images of the same anatomy with multiple different contrasts increases the diversity of diagnostic information available in an MR exam. Yet, the scan time limitations may prohibit the acquisition of certain contrasts, and some contrasts may be corrupted by noise and artifacts. In such cases, the ability to synthesize unacquired or corrupted contrasts can improve diagnostic utility. For multi-contrast synthesis, the current methods learn a nonlinear intensity transformation between the source and target images, either via nonlinear regression or deterministic neural networks. These methods can, in turn, suffer from the loss of structural details in synthesized images. Here, in this paper, we propose a new approach for multi-contrast MRI synthesis based on conditional generative adversarial networks. The proposed approach preserves intermediate-to-high frequency details via an adversarial loss, and it offers enhanced synthesis performance via pixel-wise and perceptual losses for registered multi-contrast images and a cycle-consistency loss for unregistered images. Information from neighboring cross-sections are utilized to further improve synthesis quality. Demonstrations on T1 - and T2- weighted images from healthy subjects and patients clearly indicate the superior performance of the proposed approach compared to the previous state-of-the-art methods. Our synthesis approach can help improve the quality and versatility of the multi-contrast MRI exams without the need for prolonged or repeated examinations.","MAGNETIC resonance imaging (MRI) is pervasively used in clinical applications due to the diversity of contrasts it can capture in soft tissues. Tailored MRI pulse sequences enable the generation of distinct contrasts while imaging the same anatomy. For instance, T1-weighted brain images clearly delineate gray and white matter tissues, whereas T2-weighted images delineate fluid from cortical tissue. In turn, multi-contrast images acquired in the same subject increase the diagnostic information available in clinical and research studies. However, it may not be possible to collect a full array of contrasts given considerations related to the cost of prolonged exams and uncooperative patients, particularly in pediatric and elderly populations [1]. In such cases, acquisition of contrasts with relatively shorter scan times might be preferred. Even then a subset of the acquired contrasts can be corrupted by excessive noise or artifacts that prohibit subsequent diagnostic use [2]. Moreover, cohort studies often show significant heterogeneity in terms of imaging protocol and the specific contrasts that they acquire [3]. Thus, the ability to synthesize missing or corrupted contrasts from other successfully acquired contrasts has potential value for enhancing multi-contrast MRI by increasing availability of diagnostically-relevant images, and improving analysis tasks such as registration and segmentation [4]. Cross-domain synthesis of medical images has recently been gaining popularity in medical imaging. Given a subject’s image x in X (source domain), the aim is to accurately estimate the respective image of the same subject y in Y (target domain). Two main synthesis approaches are registration-based [5]–[7] and intensity-transformation-based methods [8]–[24]. Registration-based methods start by generating an atlas based on a co-registered set of images, x1 and y1, respectively acquired in X and Y [5]. These methods further make the assumption that within-domain images from separate subjects are related to each other through a geometric warp. For synthesizing y2 from x2, the warp that transforms x1 to x2 is estimated, and this warp is then applied on y1. Since they only rely on geometric transformations, registration-based methods that rely on a single atlas can suffer from across-subject differences in underlying morphology [23]. For example, inconsistent pathology across a test subject and the atlas can cause failure. Multi-atlas registration in conjunction with intensity fusion can alleviate this limitation, and has been successfully used in synthesizing CT from MR images [6], [7]. Nevertheless, within-domain registration accuracy might still be limited even in normal subjects [23]. An alternative is to use intensity-based methods that do not rely on a strict geometric relationship among different subjects’ anatomies [8]–[24]. One powerful approach for multi-contrast MRI is based on the compressed sensing framework, where each patch in the source image x2 is expressed as a sparse linear combination of patches in the atlas image x1 [10], [22]. The learned sparse combinations are then applied to estimate patches in y2 from patches in y1. To improve matching of patches across domains, generative models were also proposed that use multi-scale patches and tissue segmentation labels [16], [18]. Instead of focusing on linear models, recent studies aimed to learn more general non-linear mappings that express individual voxels in y1 in terms of patches in x1, and then predict y2 from x2 based on these mappings. Nonlinear mappings are learned on training data via techniques such as nonlinear regression [8], [9], [23] or location-sensitive neural networks [19]. An important example is Replica that performs random forest regression on multiresolution image patches [23]. Replica demonstrates great promise in multi-contrast MR image synthesis. However, dictionary construction at different spatial scales is independent, and the predictions from separate random forest trees are averaged during synthesis. These may lead to loss of detailed structural information and suboptimal synthesis performance. Recently an end-to-end framework for MRI image synthesis has been proposed, Multimodal, based on deep neural networks [21]. Multimodal trains a neural network that receives as input images in multiple source contrasts and predicts the image in the target contrast. This method performs multiresolution dictionary construction and image synthesis in a unified framework, and it was demonstrated to yield higher synthesis quality compared to non-network-based approaches even when only a subset of the source contrasts is available. That said, Multimodal assumes the availability of spatiallyregistered multi-contrast images. In addition, Multimodal uses mean absolute error loss functions that can perform poorly in capturing errors towards higher spatial frequencies [25]–[27]. Here we propose a novel approach for image synthesis in multi-contrast MRI based on generative adversarial network (GAN) architectures. Adversarial loss functions have recently been demonstrated for various medical imaging applications with reliable capture of high-frequency texture information [28]–[48]. In the domain of cross-modality image synthesis, important applications include CT to PET synthesis [29], [40], MR to CT synthesis [28], [33], [38], [42], [48], CT to MR synthesis [36], and retinal vessel map to image synthesis [35], [41]. Inspired by this success, here we introduce conditional GAN models for synthesizing images of distinct contrasts from a single modality, with demonstrations on multi-contrast brain MRI in normal subjects and glioma patients. For improved accuracy, the proposed method also leverages correlated information across neighboring crosssections within a volume. Two implementations are provided for use when multi-contrast images are spatially registered (pGAN) and when they are unregistered (cGAN). For the first scenario, we train pGAN with pixel-wise loss and perceptual loss between the synthesized and true images (Fig. 1) [25], [49]. For the second scenario, we train cGAN after replacing the pixel-wise loss with a cycle loss that enforces the ability to reconstruct back the source image from the synthesized target image (Fig. 2) [50]. Extensive evaluations are presented on multi-contrast MRI images (T1- and T2-weighted) from healthy normals and glioma patients. The proposed approach yields visually and quantitatively enhanced accuracy in multi-contrast MRI synthesis compared to state-of-the-art methods (Replica and Multimodal) [21], [23].",Generative Adversarial Networks for Synthesizing Multi-Contrast MRI Images,"Obtaining images of the same anatomy with numerous different contrasts increases the variety of diagnostic data available in an MR exam. However, scan time restrictions may hinder the acquisition of certain contrasts, and some contrasts may be impaired by noise and artifacts. In such scenarios, the capacity to synthesize unacquired or corrupted contrasts can enhance diagnostic utility. For multi-contrast synthesis, current methods learn a nonlinear intensity transformation between source and target images, either via nonlinear regression or deterministic neural networks. These methods may, however, lead to the loss of structural details in synthesized images. This paper presents a new approach for multi-contrast MRI synthesis based on conditional generative adversarial networks. The proposed approach preserves intermediate-to-high frequency details via an adversarial loss, and it provides improved synthesis performance via pixel-wise and perceptual losses for registered multi-contrast images and a cycle-consistency loss for unregistered images. Utilizing information from neighboring cross-sections can further enhance synthesis quality. Demonstrations on T1 - and T2- weighted images from healthy subjects and patients demonstrate the superior performance of the proposed approach compared to the previous state-of-the-art methods. Our synthesis approach can help improve the quality and versatility of the multi-contrast MRI exams without the need for prolonged or repeated examinations.","MRI is widely used in clinical settings due to its ability to capture various contrasts in soft tissues. T1-weighted images delineate gray and white matter, while T2-weighted images show fluid from cortical tissue. However, in certain cases, such as with pediatric and elderly patients, obtaining a full array of contrasts may not be possible due to cost and cooperation factors. Additionally, heterogeneity in imaging protocols and acquired contrasts in cohort studies can further complicate matters. Synthesizing missing or corrupted contrasts from other successfully acquired ones can thus be valuable for improving multi-contrast MRI, increasing diagnostic images and aiding registration and segmentation. Cross-domain synthesis of medical images has been gaining popularity, with the aim of accurately estimating a subject's image in the target domain from their image in the source domain. Registration-based methods are based on the assumption that within-domain images from different subjects are related to each other through a geometric warp, while intensity-transformation-based methods are based on learning non-linear mappings that express individual voxels in the target domain in terms of patches in the source domain. GAN architectures have recently been utilized in various medical imaging applications, and here we introduce conditional GAN models for synthesizing images of distinct contrasts from a single modality, with demonstrations on multi-contrast brain MRI in normal subjects and glioma patients. The proposed approach yields improved accuracy in multi-contrast MRI synthesis compared to existing methods (Replica and Multimodal). Magnetic Resonance Imaging (MRI) is commonly used in clinical applications due to its capability to capture diverse contrasts in soft tissues. Tailored MRI pulse sequences enable the generation of distinct contrasts when imaging the same anatomy, such as T1-weighted brain images that clearly delineate gray and white matter tissues, and T2-weighted images that distinguish fluid from cortical tissue. Acquiring a complete set of contrasts might not be possible in certain cases, such as with pediatric and elderly populations, due to cost of prolonged exams and uncooperative patients. Moreover, cohort studies often exhibit significant heterogeneity in imaging protocol and the specific contrasts that they acquire. Thus, synthesizing missing or corrupted contrasts from other successfully acquired ones has potential value for enhancing multi-contrast MRI by increasing availability of diagnostically-relevant images, and improving analysis tasks such as registration and segmentation. Cross-domain synthesis of medical images has recently been gaining popularity in medical imaging, with the goal of accurately estimating a subject's image in the target domain from their image in the source domain. Two main synthesis approaches are registration-based and intensity-transformation-based methods. Registration-based methods use a single atlas, but can suffer from across-subject differences in underlying morphology. Intensity-based methods, on the other hand, do not rely on a strict geometric relationship among different subjects' anatomies. An example is Replica, which performs random forest regression on multi-resolution image patches, and demonstrates great promise in multi-contrast MR image synthesis. Multimodal is a more recent approach based on deep neural networks, which trains a neural network that receives images in multiple source contrasts and predicts the image in the target contrast. Here we propose a novel approach for image synthesis in multi-contrast MRI based on generative adversarial network (GAN) architectures. Adversarial loss functions have recently been demonstrated for various medical imaging applications, and here we introduce conditional GAN models for synthesizing images of distinct contrasts from a single modality, with demonstrations on multi-contrast brain MRI in normal subjects and glioma patients. For improved accuracy, the proposed method also leverages correlated information across neighboring cross-sections within a volume. Two implementations are provided for use when multi-contrast images are spatially registered (pGAN) and when they are unregistered (cGAN). For the first scenario, we train pGAN with pixel-wise loss and perceptual loss between the synthesized and true images. For the second scenario, we train cGAN after replacing the pixel-wise loss with a cycle loss that enforces the ability to reconstruct back the source image from the synthesized target image. Extensive evaluations are presented on multi-contrast MRI images (T1- and T2-weighted) from healthy normals and glioma patients. The proposed approach yields visually and quantitatively enhanced accuracy in multi-contrast MRI synthesis compared to state-of-the-art methods (Replica and Multimodal).",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8653423
A Challenge Dataset for Multimodal Comprehension of Cooking Recipes,"Understanding and reasoning about cooking recipes is a fruitful research direction towards enabling machines to interpret procedural text. In this work, we introduce RecipeQA, a dataset for multimodal comprehension of cooking recipes. It comprises of approximately 20K instructional recipes with multiple modalities such as titles, descriptions and aligned set of images. With over 36K automatically generated question-answer pairs, we design a set of comprehension and reasoning tasks that require joint understanding of images and text, capturing the temporal flow of events and making sense of procedural knowledge. Our preliminary results indicate that RecipeQA will serve as a challenging test bed and an ideal benchmark for evaluating machine comprehension systems.","There is a rich literature in natural language processing (NLP) and information retrieval on question answering (QA) (Hirschman and Gaizauskas, 2001), but recently deep learning has sparked interest in a special kind of QA, commonly referred to as reading comprehension (RC) (Vanderwende, 2007). The aim in RC research is to build intelligent systems with the abilities to read and understand natural language text and answer questions related to it (Burges, 2013). Such tests are appealing as they require joint understanding of the question and the related passage (i.e. context), and moreover, they can analyze many different types of skills in a rather objective way (Sugawara et al., 2017). Despite the progress made in recent years, there is still a significant performance gap between humans and deep neural models in RC, and researchers are pushing forward our understanding of the limitations and capabilities of these approaches by introducing new datasets. Existing tasks for RC mainly differ in two major respects: the questionanswer formats, e.g. cloze (fill-in-the-blank), span selection or multiple choice, and the text sources they use, such as news articles (Hermann et al., 2015; Trischler et al., 2017), fictional stories (Hill et al., 2016), Wikipedia articles (Kocisk ˇ y et al. ´ , 2018; Hewlett et al., 2016; Rajpurkar et al., 2016) or other web sources (Joshi et al., 2017). A popular topic in computer vision closely related to RC is Visual Question Answering (VQA) in which context takes the form of an image in the comprehension task, where recent datasets have also been compiled, such as (Antol et al., 2015; Yu et al., 2015; Johnson et al., 2017; Goyal et al., 2017), to name a few. More recently, research in QA has been extended to focus on the multimodal aspects of the problem where different modalities are being explored. Tapaswi et al. (2016) introduced MovieQA where they concentrate on evaluating automatic story comprehension from both video and text. In COMICS, Iyyer et al. (2017) turned to comic books to test understanding of closure, transitions in the narrative from one panel to the next. In AI2D (Kembhavi et al., 2016) and FigureQA (Kahou et al., 2018), the authors addressed comprehension of scientific diagrams and graphical plots. Last but not least, Kembhavi et al. (2017) has proposed another comprehensive and challenging dataset named TQA, which comprised of middle school science lessons of diagrams and texts. In this study, we focus on multimodal machine comprehension of cooking recipes with images and text. To this end, we introduce a new QA dataset called RecipeQA that consists of recipe instructions and related questions (see Fig. 1 for an example text cloze style question). There are a handful of reasons why understanding and reasoning about recipes is interesting. Recipes are written with a specific goal in mind, that is to teach others how to prepare a particular food. Hence, they contain immensely rich information about the real world. Recipes consist of instructions, wherein one needs to follow each instruction to successfully complete the recipe. As a classical example in introductory programming classes, each recipe might be seen as a particular way of solving a task and in that regard can also be considered as an algorithm. We believe that recipe comprehension is an elusive challenge and might be seen as important milestone in the long-standing goal of artificial intelligence and machine reasoning (Norvig, 1987; Bottou, 2014). Among previous efforts towards multimodal machine comprehension (Tapaswi et al., 2016; Kembhavi et al., 2016; Iyyer et al., 2017; Kembhavi et al., 2017; Kahou et al., 2018), our study is closer to what Kembhavi et al. (2017) envisioned in TQA. Our task primarily differs in utilizing substantially larger number of images – the average number of images per recipe in RecipeQA is 12 whereas TQA has only 3 images per question on average. Moreover, in our case, each image is aligned with the text of a particular step in the corresponding recipe. Another important difference is that TQA contains mostly diagrams or textbook images whereas RecipeQA consists of natural images taken by users in unconstrained environments. Some of the important characteristics of RecipeQA are as follows: • There are arbitrary numbers of steps in recipes and images in steps, respectively. • There are different question styles, each requiring a specific comprehension skill. • There exists high lexical and syntactic divergence between contexts, questions and answers. • Answers require understanding procedural language, in particular keeping track of entities and/or actions and their state changes. • Answers may need information coming from multiple steps (i.e. multiple images and multiple paragraphs). • Answers inherently involve multimodal understanding of image(s) and text. To sum up, we believe RecipeQA is a challenging benchmark dataset which will serve as a test bed for evaluating multimodal comprehension systems. In this paper, we present several statistical analyses on RecipeQA and also obtain baseline performances for a number of multimodal comprehension tasks that we introduce for cooking recipes.",Multi-Modal Recipe Comprehension Challenge Dataset,"We present RecipeQA, a dataset for machines to comprehend cooking recipes, comprising of around 20K instructional recipes with titles, descriptions, images and 36K generated Q&A pairs. Tasks set up require joint comprehension of images and text, accounting for temporal flow and knowledge of procedures. Early results suggest RecipeQA will be a demanding test bed and benchmark to assess machine comprehension systems.","There is a considerable body of literature in NLP and IR on QA (Hirschman and Gaizauskas, 2001), but recently deep learning has triggered an interest in a particular kind of QA, called reading comprehension (RC) (Vanderwende, 2007). The aim in RC is to create systems with the capability to comprehend and answer questions regarding natural language text (Burges, 2013). These tests are attractive as they require joint understanding of the question and related passage (Sugawara et al., 2017). Despite recent progress, there is still a significant gap between humans and deep neural models in RC. New datasets have been created to investigate the limitations and capabilities of these approaches. RC tasks mainly differ in two aspects: the question-answer format, e.g. cloze, span selection or multiple choice, and the text sources, such as news articles (Hermann et al., 2015; Trischler et al., 2017), fictional stories (Hill et al., 2016), Wikipedia articles (Kocisk ˇ y et al. ´ , 2018; Hewlett et al., 2016; Rajpurkar et al., 2016) or other web sources (Joshi et al., 2017). A similar field in computer vision, Visual Question Answering (VQA), has also had new datasets compiled, such as (Antol et al., 2015; Yu et al., 2015; Johnson et al., 2017; Goyal et al., 2017). Recent research in QA has shifted to focus on multimodal aspects of the problem. Tapaswi et al. (2016) introduced MovieQA, where they studied automatic story comprehension from both video and text. Iyyer et al. (2017) utilized comic books to evaluate understanding of closure and transitions in the narrative in COMICS. AI2D (Kembhavi et al., 2016) and FigureQA (Kahou et al., 2018) focused on comprehending scientific diagrams and graphical plots. Kembhavi et al. (2017) proposed the TQA dataset, which contained middle school science lessons of diagrams and texts. In this study, we concentrate on multimodal machine comprehension of cooking recipes with images and text. For this purpose, we introduce a new QA dataset, RecipeQA, which consists of recipe instructions and related questions (see Fig. 1 for an example text cloze style question). There are various reasons why comprehending and reasoning about recipes is interesting. Recipes are written with the goal of teaching others how to prepare a particular food, thus containing immense information about the real world. As a classic example in introductory programming classes, each recipe can be seen as a way of solving a task, and thus an algorithm. We believe that recipe comprehension is a challenging task and could be seen as an important milestone in the long-standing goal of artificial intelligence and machine reasoning (Norvig, 1987; Bottou, 2014). Compared to previous efforts towards multimodal machine comprehension (Tapaswi et al., 2016; Kembhavi et al., 2016; Iyyer et al., 2017; Kembhavi et al., 2017; Kahou et al., 2018), our task is more similar to what Kembhavi et al. (2017) envisioned in TQA. Our task differs in utilizing a larger number of images – the average number of images per recipe in RecipeQA is 12, while TQA has only 3 images per question on average. Additionally, each image in RecipeQA is aligned with the text of a particular step in the recipe. Other differences include that TQA mostly contains diagrams or textbook images, whereas RecipeQA consists of natural images taken in unconstrained environments. Some characteristics of RecipeQA are: • Variable numbers of steps in recipes and images in steps. • Different question styles, each requiring a specific comprehension skill. • High lexical and syntactic divergence between contexts, questions and answers. • Answers require understanding procedural language, particularly keeping track of entities and/or actions and their state changes. • Answers may need information from multiple steps (i.e. multiple images and multiple paragraphs). • Answers inherently involve multimodal understanding of image(s) and text. To conclude, we believe RecipeQA is a difficult benchmark dataset which will serve as a platform for evaluating multimodal comprehension systems. In this paper, we present statistical analyses on RecipeQA and obtain baseline performances for several multimodal comprehension tasks that we introduce for cooking recipes.",https://arxiv.org/pdf/1809.00812.pdf
Language Guided Fashion Image Manipulation with Feature-wise Transformations,"Developing techniques for editing an outfit image through natural sentences and accordingly generating new outfits has promising applications for art, fashion and design. However, it is considered as a certainly challenging task since image manipulation should be carried out only on the relevant parts of the image while keeping the remaining sections untouched. Moreover, this manipulation process should generate an image that is as realistic as possible. In this work, we propose FiLMedGAN, which leverages feature-wise linear modulation (FiLM) to relate and transform visual features with natural language representations without using extra spatial information. Our experiments demonstrate that this approach, when combined with skip connections and total variation regularization, produces more plausible results than the baseline work, and has a better localization capability when generating new outfits consistent with the target description.","Language based image editing (LBIE) [4] is a recently proposed umbrella term which describes the task of transforming a source image based on natural language descriptions. A specific case of LBIE aims at modifying an outfit in an image using textual descriptions as target transformations [44], which has potential applications in art, fashion, shopping and design. However, this is a rather challenging problem mainly due to two reasons. A successful model should be able to (i) reflect the changes to the input image while preserving structural coherence (e.g. body shape, pose, person identity), and (ii) understand and resolve the local changes in images according to only the relevant parts of textual description. While the former is about the image generation process, the latter is related to understanding the relations between the source image and the language description and it requires to disentangle semantics from both visual and textual modalities. In this respect, it shares some similarities with other integrated vision and language problems such as visual question answering (VQA). The main motivation of this paper comes from a recent conditioning mechanism known as Feature-wise Linear Modulation (FiLM), which has been initially proposed for solving complicated VQA tasks [26] and has been proven very useful. In this work, we propose a new conditional Generative Adversarial Network (GAN), which we name FiLMedGAN, which incorporates FiLM based feature transformations to better guide the manipulation process based on natural language descriptions. To increase the overall quality of the resulting images, our network architecture also employs skip connections [29] and we additionally use total variation regularization [30] during training. We demonstrate that our proposed approach can synthesize and modify plausible outfit images without a need to utilize extra spatial information like segmentation maps or body joints and pose guidance as commonly considered in the previous work",Feature-wise Transformations to Modify Fashion Images with Language Assistance,"Generating new outfits from an image via natural language commands is a difficult task, as it necessitates modification of only certain parts of the image while leaving the rest intact, and creating an image that looks realistic. We introduce FiLMedGAN, which couples feature-wise linear modulation (FiLM) to link visual features with language descriptions, and does not require additional spatial information. Our experiments prove that, when combined with skip connections and total variation regularization, FiLMedGAN produces more convincing results than the baseline work, and has a superior ability to localize new outfits in accordance with the specified description.","LBIE, a task of transforming a source image according to natural language descriptions, has potential applications in art, fashion, shopping and design. This is challenging due to (i) preserving structural coherence and (ii) understanding and resolving local changes in the image with only relevant parts of the description. Thus, this problem is similar to VQA. To tackle it, we propose FiLMedGAN, a GAN incorporating Feature-wise Linear Modulation (FiLM) based feature transformations to guide manipulation. Skip connections and total variation regularization are also employed to improve the image quality. Our approach synthesizes and modifies plausible outfit images without requiring extra spatial info like segmentation maps or body joints/pose guidance.",https://arxiv.org/pdf/1808.04000.pdf
Joint Exploitation of Features and Optical Flow for Real-Time Moving Object Detection on Drones,"Moving object detection is an imperative task in computer vision, where it is primarily used for surveillance applications. With the increasing availability of low-altitude aerial vehicles, new challenges for moving object detection have surfaced, both for academia and industry. In this paper, we propose a new approach that can detect moving objects efficiently and handle parallax cases. By introducing sparse flow based parallax handling and downscale processing, we push the boundaries of real-time performance with 16 FPS on limited embedded resources (a five-fold improvement over existing baselines), while managing to perform comparably or even improve the state-of-the-art in two different datasets. We also present a roadmap for extending our approach to exploit multi-modal data in order to mitigate the need for parameter tuning.","Ranging from high-altitude Unmanned Aerial Vehicles (UAV) capable of flying at 65,000 feet 5 to low-altitude miniature drones, long-endurance variants to micro air vehicles weighing just a few grams 6 , UAV industry has gone through a meteoric rise. Owing to their ever increasing availability in civilian and military sectors alike, UAV variants have been disruptive in the last decade and consequently found use in several applications, such as disaster relief, precision agriculture, cinematography, cargo delivery, industrial inspection, mapping, military surveillance and air support [1]. Following the industrial attention, academic community also contributed to the transformation of UAVs in various aspects, such as aerodynamics, avionics and various sensory data acquired by said platforms. Slightly different than remote sensing domain, drone-mounted imagery has paved the way for new research in computer vision (CV). There has been a large quantity of studies reported in object detection [2–6], action detection [7], visual object tracking [8– 10], object counting [11] and road extraction [12]. In recent years, new datasets [7, 13–17], challenges and dedicated workshops [18, 19] have surfaced to bridge the gap between drone-specific vision problems and their generic versions. From a practical perspective, low-altitude drones introduce several new problems for CV algorithms. Proneness to sudden platform movements and exposure to environmental conditions arguably affect low-altitude drones in a more pronounced manner compared to their high-altitude counterparts. Moreover, fastchanging operating altitudes and camera viewpoints result into the generation of data with a large diversity, which inherently furthers the complexity of virtually any vision problem. Their small-sized nature also impose severe limits on the availability of computational resources installed on-board, which calls for non-trivial engineering solutions [20, 21]. Moving object detection (MOD), primarily used for surveillance purposes, is a long-standing problem in CV and has been the subject of many studies [22–24]. Due to the presence of platform motion in drone vision, it becomes a notorious problem, where platform motion can easily be confused with moving regions/objects. Several solutions addressing platform motion issue have been reported [25, 26]. Moreover, low-altitude drone cases also suffer from severe motion parallax which causes objects closer to camera move faster than objects further away. Solutions provided for motion parallax issue is considered computationally expensive [27–29, 17], which makes the solutions even harder especially when on-board processing with (near) real-time performance is a hard constraint. In this paper, we propose a new approach for moving object detection, primarily optimized for embedded resources for on-board functionality. We make two main contributions; first, we show that performing a large portion of our pipeline in lower resolutions significantly improve the runtime performance while keeping our accuracy high. Second, we design the matching part of the parallax handling scheme using a simple sparse-flow based technique which avoids the bottlenecks such as failing to extract features from candidate objects or inferior feature matching. Its sparse nature also contributes to further speed-ups, pushing further to real-time performance on embedded platforms. The paper is organized as follows. In Section 2, related work in the literature is reviewed. The proposed approach is explained thoroughly in Section 3. Experimental results and their analysis are reported in Section 4. We conclude our work by drawing insights and making future recommendations in Section 5.",Utilizing Features & Optical Flow to Detect Moving Objects on Drones in Real-Time,"Moving object detection is a significant task in computer vision, particularly for surveillance purposes. The rise of low-altitude aerial vehicles has presented new difficulties for moving object detection for both industry and academia. This paper suggests a novel technique to detect moving objects competently and handle parallax cases. Sparse flow based parallax handling and downscale processing are used to reach real-time performance at 16 FPS on limited embedded resources (five times better than existing baselines), while still performing similarly or even better than the current state-of-the-art on two different datasets. Additionally, a plan is proposed to expand the approach to make use of multi-modal data in order to reduce the requirement for parameter tuning.","Ranging from high-altitude UAVs capable of flying at 65,000 feet to low-altitude miniature drones and micro air vehicles weighing only a few grams, the UAV industry has experienced rapid growth. This has been seen in both civilian and military sectors, with UAVs being used for disaster relief, precision agriculture, cinematography, cargo delivery, industrial inspection, mapping, military surveillance and air support. The academic community has also contributed to the development of UAVs, particularly in aerodynamics, avionics and sensory data. Drone-mounted imagery has opened up new research opportunities in computer vision (CV). There have been numerous studies in object detection, action detection, visual object tracking, object counting and road extraction. In addition, new datasets, challenges and workshops have emerged to bridge the gap between drone-specific vision problems and their generic versions. Low-altitude drones present unique problems for CV algorithms, due to their proneness to sudden platform movements and exposure to environmental conditions, as well as fast-changing operating altitudes and camera viewpoints. This leads to data with a large diversity, and a limitation of computational resources installed on-board. Moving object detection (MOD) is a long-standing problem in CV, and is particularly difficult in drone vision due to the presence of platform motion. Several solutions have been reported to address this issue, as well as motion parallax, which causes objects closer to the camera to move faster than those further away. In this paper, we propose a new approach for moving object detection that is optimized for embedded resources for on-board functionality. We show that performing a large portion of the pipeline in lower resolutions can significantly improve runtime performance while maintaining accuracy, and we design a sparse-flow based technique for the matching part of the parallax handling scheme, which avoids feature extraction and feature matching bottlenecks. After reviewing related work in the literature, we present our proposed approach, experimental results and analysis, and conclude by drawing insights and making future recommendations.",https://openaccess.thecvf.com/content_ECCVW_2018/papers/11130/Lezki_Joint_Exploitation_of_Features_and_Optical_Flow_for_Real-Time_Moving_ECCVW_2018_paper.pdf
Spatio-Temporal Saliency Networks for Dynamic Saliency Prediction,"Computational saliency models for still images have gained significant popularity in recent years. Saliency prediction from videos, on the other hand, has received relatively little interest from the community. Motivated by this, in this paper, we study the use of deep learning for dynamic saliency prediction and propose the so-called spatio-temporal saliency networks. The key to our models is the architecture of two-stream networks where we investigate different fusion mechanisms to integrate spatial and temporal information. We evaluate our models on the dynamic images and eye movements and University of Central Florida-Sports datasets and present highly competitive results against the existing state-of-the-art models. We also carry out some experiments on a number of still images from the MIT300 dataset by exploiting the optical flow maps predicted from these images. Our results show that considering inherent motion information in this way can be helpful for static saliency estimation.","AS A key part of the human visual system, visual attention mechanisms filter irrelevant visual stimuli in order to focus more on the important parts. Computational models of attention try to mimic this process through the use of machines and algorithms. These models have gained increasing attention lately. The reason behind this growing interest lies in their use in different computer vision and multimedia problems including but not limited to image retrieval [1], visual quality assessment [2], [3] video resizing/summarization [4], [5], action recognition [6], event detection [7] either as a tool for visual feature extraction or as a mechanism for selecting features. These models are also important for generic applications such as advertisement and web design [8] as attention plays a key role in both user interfaces and human-machine interaction.  In the literature, the computational attention models developed so far generally aim to predict where humans fixate their eyes in images [9]. Specifically, they produce the so-called saliency maps from the visual data where a high saliency score at an image location indicates that the point is more likely to be fixated. These models are largely inspired by the hierarchical processing of human visual system [10] and the theoretical studies like Feature Integration Theory [11] or Guided Search Model [12]. They consider low-level image features (color, orientation, contrast, motion, etc.) and/or high-level features (pedestrians, faces, text) while predicting saliency maps. In this process, while low-level features are employed to examine how different an image point from its surroundings, high-level features are employed as it is experimentally shown that humans have a tendency to fixate on certain object classes more than others. Another current trend in the existing literature is to detect salient objects [13]–[16] from the images. These models specifically aim at identifying the most prominent objects in an image that attract attention under free-viewing conditions and then segmenting them out from the background. These computational models for visual attention can be further grouped into two according to their inputs as static and dynamic saliency models. While static models work on images, dynamic models take video sequences as input. Saliency prediction from videos leads to great challenges when it is compared to carrying out the same task in still images. The reason is that the dynamic saliency frameworks require taking into account both spatial and temporal characteristics of the video sequences. Static saliency models use features like color, intensity and orientation, however dynamic models need to focus more on the moving objects or image parts as it is shown that humans there is a tendency for humans to look at them while viewing. Hence, the preliminary models proposed for dynamic saliency prediction extend the existing saliency models suggested for still images so that they consider extra motion features [17]–[20]. However, more recent works approach the same task from a different point of view and propose novel solutions [21]–[23]. ",Networks to Predict Dynamic Saliency over Time and Space,"The utilization of deep learning for dynamic saliency prediction has recently been neglected by the community, despite computational saliency models for still images being increasingly popular. We therefore introduce the spatio-temporal saliency networks, which utilize a two-stream network architecture and investigate different fusion mechanisms to combine spatial and temporal information. These models are evaluated on dynamic images and eye movements, as well as the University of Central Florida-Sports dataset, yielding highly competitive results compared to the current state-of-the-art models. Experiments on the MIT300 dataset with optical flow maps predicted from these images demonstrate that considering motion information can improve static saliency estimation.","Visual attention mechanisms, an integral part of the human visual system, filter out irrelevant visual stimuli to focus more on the important parts. Computational models of attention attempt to replicate this process through the use of machines and algorithms. These models have become increasingly popular due to their use in various computer vision and multimedia problems such as image retrieval, visual quality assessment, video resizing/summarization, action recognition, and event detection, either as a tool for visual feature extraction or as a mechanism for selecting features. Additionally, they are essential for generic applications such as advertisement and web design, as attention plays a key role in both user interfaces and human-machine interaction. The existing computational attention models are generally designed to predict where humans fixate their eyes in images, producing the so-called saliency maps in which a high saliency score at an image location indicates that the point is more likely to be fixated. These models draw inspiration from the hierarchical processing of the human visual system and theoretical studies such as Feature Integration Theory and Guided Search Model. Low-level image features (color, orientation, contrast, motion, etc.) and/or high-level features (pedestrians, faces, text) are considered in the saliency prediction process, with low-level features used to examine how different an image point is from its surroundings, and high-level features employed due to humans having a tendency to fixate on certain object classes more than others. Furthermore, current research is also focused on detecting salient objects from images and segmenting them from the background. Computational models for visual attention can be divided into two categories depending on their inputs: static and dynamic saliency models. Static models work on images, while dynamic models take video sequences as input. Dynamic saliency prediction is far more challenging than static saliency prediction due to the need to consider both spatial and temporal characteristics of the video sequences, as well as the fact that humans tend to look at moving objects or image parts when viewing. Preliminary models for dynamic saliency prediction extend existing saliency models for still images to include extra motion features. However, more recent works are approaching the same task from a different perspective and proposing novel solutions.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=8119879
Data-driven image captioning via salient region discovery," In the past few years, automatically generating descriptions for images has attracted a lot of attention in computer vision and natural language processing research. Among the existing approaches, data-driven methods have been proven to be highly effective. These methods compare the given image against a large set of training images to determine a set of relevant images, then generate a description using the associated captions. In this study, the authors propose to integrate an objectbased semantic image representation into a deep features-based retrieval framework to select the relevant images. Moreover, they present a novel phrase selection paradigm and a sentence generation model which depends on a joint analysis of salient regions in the input and retrieved images within a clustering framework. The authors demonstrate the effectiveness of their proposed approach on Flickr8K and Flickr30K benchmark datasets and show that their model gives highly competitive results compared with the state-of-the-art models"," In the past few years, automatically generating descriptions for images has attracted a lot of attention in computer vision and natural language processing research. Among the existing approaches, data-driven methods have been proven to be highly effective. These methods compare the given image against a large set of training images to determine a set of relevant images, then generate a description using the associated captions. In this study, the authors propose to integrate an objectbased semantic image representation into a deep features-based retrieval framework to select the relevant images. Moreover, they present a novel phrase selection paradigm and a sentence generation model which depends on a joint analysis of salient regions in the input and retrieved images within a clustering framework. The authors demonstrate the effectiveness of their proposed approach on Flickr8K and Flickr30K benchmark datasets and show that their model gives highly competitive results compared with the state-of-the-art models some inaccuracies may occur. Some studies have eliminated this drawback by employing visual features along with the captions [6, 7]. They replace the detector-based analysis step with a simpler yet effective text-based alternative where the captions of the retrieved set of images are examined directly. Another critical issue is that sometimes the caption of a single selected image falls short of describing the input image with all its complexities. The problem is that all the aforementioned models analyse the images and the captions in a global manner. On the other hand, a local image analysis through image regions enables to obtain a better interpretation of a query image. To put it in a more clear way, consider Fig. 2. Here, for the query image on the left, assume that three images that are locally similar to the query are retrieved. Although none of the retrieved captions can describe the query image on its own, it is possible to generate a good caption for the query image from the captions of these images by combining the phrases relevant to the query image. These relevant parts are shown in red, purple and green colours, respectively, for each retrieved image. Our first contribution is a novel mechanism to select more relevant images from a large pool by combining deep features with an object-specific image representation. Specifically, we first compare a given image against a large set of images using the state-of-the-art deep features that encode scene and object characteristics. We further improve this initial retrieval by reranking the images considering a novel visual representation, called Bag-of-Object-Proposals (BoOP), which encodes images in terms of objects. This improves the accuracy of the relevancy of retrieved images, and from here, we can generate more accurate descriptions. Our second contribution is an effective local image analysis method which is used to explore the similarities among local image regions. Motivated by the observation that people mention different salient image regions when describing an image, we utilise a stateof-the-art visual saliency model to select image regions that are likely to attract attention. In particular, using a single caption from the retrieved captions might fall short in describing every detail of the input (Fig. 2). We use similarities among salient image regions to collect phrases from the relevant set of images, and develop a phrase-based sentence generation approach. This allows us to consider local image characteristics within a data-driven strategy, and leads to the generation of more accurate descriptions, as we illustrate on Flickr8K [8] and Flickr30K [9] datasets. The paper is organised as follows. We first provide a review of the related work in Section 2. Next, we present our approach in Section 3, where we discuss the proposed image retrieval and reranking scheme (Section 3) and our image captioning model which first discovers salient image regions (Section 3.1) and then employ them to collect a set of relevant phrases that are used to generate a novel description (Section 3.2). Section 4 presents our experimental results on benchmark datasets and discusses the proposed framework.",Generating image captions using data-based salient region detection,"Recently, the task of automatically generating descriptions for images has gained much interest in computer vision and natural language processing. Data-driven methods, which compare the given image to a set of training images to identify relevant images and generate descriptions based on the associated captions, have been shown to be successful. This study introduces an object-based semantic image representation into a deep features-based retrieval framework to select the relevant images. Additionally, a novel phrase selection paradigm and sentence generation model is proposed which performs a joint analysis of salient regions in the input and retrieved images within a clustering framework. The effectiveness of this approach was tested on the Flickr8K and Flickr30K benchmark datasets, and was found to give highly competitive results compared to the state-of-the-art models.","In recent years, generating descriptions for images has become a prominent research topic in computer vision and natural language processing. Data-driven methods have been shown to be highly successful. These techniques compare the given image to a set of training images to identify relevant ones, then generate a caption using the associated captions. This study proposes to integrate an object-based semantic image representation into a deep features-based retrieval framework to select the relevant images. Additionally, a novel phrase selection paradigm and sentence generation model is presented which depends on a joint analysis of salient regions in the input and retrieved images within a clustering framework. The authors demonstrate the effectiveness of their proposed approach on Flickr8K and Flickr30K benchmark datasets and show that their model gives highly competitive results compared with the state-of-the-art models. To address some inaccuracies, some studies have utilized visual features along with the captions. Instead of a detector-based analysis step, a simpler yet effective text-based alternative is used where the captions of the retrieved set of images are examined directly. Another issue is that sometimes the caption of a single selected image fails to describe the input image fully. To solve this, a local image analysis through image regions is employed to obtain a better interpretation of the query image. To illustrate this, consider Fig. 2. Here, for the query image on the left, three images that are locally similar to the query are retrieved. Though none of the retrieved captions can describe the query image on its own, it is possible to generate a good caption for the query image from the captions of these images by combining the phrases relevant to the query image. Our first contribution is a novel mechanism to select more relevant images from a large pool by combining deep features with an object-specific image representation. We compare a given image against a large set of images using the state-of-the-art deep features that encode scene and object characteristics, then rerank the images considering a novel visual representation, called Bag-of-Object-Proposals (BoOP). This improves the accuracy of the relevancy of retrieved images, and from here, we can generate more accurate descriptions. Our second contribution is an effective local image analysis method which is used to explore the similarities among local image regions. Utilizing a state-of-the-art visual saliency model, we select image regions that are likely to attract attention. This allows us to consider local image characteristics within a data-driven strategy, and leads to the generation of more accurate descriptions, as demonstrated on Flickr8K and Flickr30K datasets. The paper is organised as follows. We first provide a review of the related work in Section 2. Next, we present our approach in Section 3, where we discuss the proposed image retrieval and reranking scheme (Section 3) and our image captioning model which first discovers salient image regions (Section 3.1) and then employ them to collect a set of relevant phrases that are used to generate a novel description (Section 3.2). Section 4 presents our experimental results on benchmark datasets and discusses the proposed framework.",https://ietresearch.onlinelibrary.wiley.com/doi/pdfdirect/10.1049/iet-cvi.2016.0286
Alpha Matting With KL-Divergence-Based Sparse Sampling,"In this paper, we present a new sampling-based alpha matting approach for the accurate estimation of foreground and background layers of an image. Previous samplingbased methods typically rely on certain heuristics in collecting representative samples from known regions, and thus their performance deteriorates if the underlying assumptions are not satisfied. To alleviate this, we take an entirely new approach and formulate sampling as a sparse subset selection problem where we propose to pick a small set of candidate samples that best explains the unknown pixels. Moreover, we describe a new dissimilarity measure for comparing two samples which is based on KL-divergence between the distributions of features extracted in the vicinity of the samples. The proposed framework is general and could be easily extended to video matting by additionally taking temporal information into account in the sampling process. Evaluation on standard benchmark data sets for image and video matting demonstrates that our approach provides more accurate results compared with the state-of-the-art methods.","ACCURATE estimation of foreground and background layers of an image or video frames plays an important role for many image and video editing applications. In the computer vision literature, this problem is known as alpha matting, and mathematically, it refers to the problem of decomposing a given image or video frame I into two layers, the foreground F and the background B, defined in accordance with the following linear image composition equation. I = αp Fp + (1 − αp)Bp (1) where αp represents the unknown alpha matte which defines the true opacity of each pixel p and whose values lies in [0, 1] with αp = 1 denoting a foreground pixel and αp = 0 indicating a background pixel. This is a highly ill-posed problem since for each pixel we have only three inputs but seven unknowns (α and the RGB values of Fp and Bp). The general approach to resolve this issue for image matting is to consider a kind of prior knowledge about the foreground and background in form of user scribbles or a trimap to simplify the problem and use the spatial and photometric relations between these known pixels and the unknown ones. As for the video matting, estimating the alpha mattes of each frame is a more challenging task than single image matting since it requires both temporally coherent and spatially accurate maps. Image matting methods can be mainly categorized into two groups: propagation-based methods [5]–[11] and samplingbased methods [2]–[4], [12]–[16]. The first group defines an affinity matrix representing the similarity between pixels and propagate the alpha values of known pixels to the unknown ones. These approaches mostly differ from each other in their propagation strategies or affinity definitions. The latter group, on the other hand, collects color samples from known foreground and background regions to represent the corresponding color distributions and determine the alpha value of an unknown pixel according to its closeness to these distributions. Early examples of sampling-based matting methods [12], [13] fit parametric models to color distributions of foreground and background regions. Difficulties arise, however, when an image contains highly textured areas. Thus, virtually all recent sampling-based approaches [2]–[4], [14]–[16] consider a nonparametric setting and employ a particular selection criteria to collect a subset of known F and B samples. Then, for each unknown pixel p, they search for the best (Fp, Bp) pair within the representative samples, and once the best pair is found, the final alpha matte is computed as αˆ p = (Ip − Bp).(Fp − Bp) Fp − Bp2 . (2) The recent sampling-based approaches mentioned above also apply local smoothing as a post-processing step to further improve the quality of the estimated alpha matte. Apart from the two main types of approaches, there are also some hybrid methods which consider a combination of propagation and sampling based formulations [17], or some supervised machine learning based methods which learn proper matting functions from a training set of examples [18]. Very recently, Cho et al. [19] proposed a deep learning based solution as well. For video matting, several researchers extend the existing image matting methods so that they can extract temporally coherent alpha mattes by using either user-generated or predefined trimaps along the video frames. Some of these approaches [20]–[22] automatically generate trimaps by using user interaction to segment foreground object and morphological dilation operation and then apply image matting methods to compute alpha matte. The methods [20], [23], [24] which do not directly use temporal information suffer from the temporal inconsistency, on the other hand, the ones [21], [25]–[30] utilize the temporal information present more temporally coherent alpha matte results. These methods differ from each other in terms of how they incorporate temporal information to compute alpha matte along the video sequences. For a more comprehensive up-to-date survey of image and video matting methods, we refer the reader to [31] and [32]. The proposed matting approach belongs to the group of sampling-based methods which will be reviewed in the next subsection. Relying on a non-parametric formulation, these methods typically exploit different strategies to gather the representative foreground and background samples. Our observation is that all these strategies lack a strong theoretical basis, i.e. they require certain assumptions to hold to capture the true foreground and background colors, and moreover, they fail to adequately utilize the relationship between known and unknown regions. In contrast, our approach offers a more principled way to sampling by casting it as a sparse subset selection problem [33], [34], in which the resulting samples refers to a small subset of known foreground and background pixels that best explains the unknown pixels. In particular, sampling is formulated as a row-sparsity regularized trace minimization problem which solely depends on pairwise dissimilarities between known and unknown pixels, and for that, we propose a new KL-divergence based contextual measure as an efficient alternative to chromatic and spatial distances. Besides we extend this sampling approach to video matting by incorporating temporal information together with temporal matting Laplacian to provide temporal coherency. Finally we demonstrate proposed sampling strategy is quite feasible for sparse user input as scribble. ",Sparse Sampling Using KL-Divergence for Alpha Matting,"We propose a sampling-based alpha matting approach to accurately estimate foreground and background layers of an image. Existing sampling-based methods depend on heuristics to gather samples from known regions, and thus, are not reliable if the assumptions are not met. To resolve this, we address sampling as a sparse subset selection issue and suggest a small set of candidate samples to explain unknown pixels. Furthermore, we introduce a dissimilarity measure based on KL-divergence between distributions of features around the samples. Our framework is versatile and can be extended to video matting by taking temporal information into account in the sampling process. Evaluation on standard benchmarks for image and video matting shows that our approach provides more precise results compared to existing methods.","Accurate estimation of foreground and background layers of an image or video frames is a significant part for many image and video editing applications. This issue, known as alpha matting, is mathematically expressed as decomposing a given image or video frame I into two layers, F and B, following the equation I = αp Fp + (1 − αp)Bp (1), where αp is the unknown alpha matte that lies in [0, 1] with αp = 1 for a foreground pixel and αp = 0 for a background pixel. This is a highly ill-posed problem since for each pixel there are only three inputs but seven unknowns. Propagation-based and sampling-based methods are the two main approaches to solve this problem, with the latter gathering color samples from known foreground and background regions to determine the alpha value of an unknown pixel according to its closeness to these distributions. Recent sampling-based methods are nonparametric and employ a selection criteria to collect a subset of known F and B samples. Then, for each unknown pixel p, they search for the best (Fp, Bp) pair within the representative samples, and once the best pair is found, the final alpha matte is computed. Post-processing local smoothing is usually applied to further improve the quality of the estimated alpha matte. There are also hybrid methods, as well as supervised machine learning based methods. For video matting, approaches extend existing image matting methods by using either user-generated or predefined trimaps along the video frames. Some of these incorporate temporal information to compute alpha matte along the video sequences, while others do not. Our approach offers a more principled way to sampling by casting it as a sparse subset selection problem, in which the resulting samples refer to a small subset of known foreground and background pixels that best explains the unknown pixels. We propose a new KL-divergence based contextual measure as an efficient alternative to chromatic and spatial distances, and extend this sampling approach to video matting by incorporating temporal information together with temporal matting Laplacian to provide temporal coherency. Our proposed sampling strategy is feasible for sparse user input as scribble.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7955081
A comparative study for feature integration strategies in dynamic saliency estimation,"With the growing interest in computational models of visual attention, saliency prediction has become an important research topic in computer vision. Over the past years, many different successful saliency models have been proposed especially for image saliency prediction. However, these models generally do not consider the dynamic nature of the scenes, and hence, they work better on static images. To date, there has been relatively little work on dynamic saliency that deals with predicting where humans look at videos. In addition, previous studies showed that how the feature integration is carried out is very crucial for more accurate results. Yet, many dynamic saliency models follow a similar simple design and extract separate spatial and temporal saliency maps which are then integrated together to obtain the final saliency map. In this paper, we present a comparative study for different feature integration strategies in dynamic saliency estimation. We employ a number of low and high-level visual features such as static saliency, motion, faces, humans and text, some of which have not been previously used in dynamic saliency estimation. In order to explore the strength of feature integration strategies, we investigate four learning-based (SVM, Gradient Boosting, NNLS, Random Forest) and two transformation- based (Mean, Max) fusion methods, resulting in six new dynamic saliency models. Our experimental analysis on two different dynamic saliency benchmark datasets reveal that our models achieve better performance than the individual features. In addition, our learning-based models outperform the state-of-the-art dynamic saliency models.","Visual attention is a key mechanism of the human visual system, and is responsible from filtering out the irrelevant parts of the visual data to focus more on the relevant parts. Many researchers from very different fields have tried to mimic the human attention mechanism through the use of computers by developing computational attention models. Especially, the recent years have witnessed significant progress in this field, and a large number of saliency models have been proposed, which are now being used in many different machine vision applica- tions such as object detection [1], image classification [2] and summarization [3]. In existing literature, the models developed so far largely aim to predict where humans fixate their eyes in images. Inspired by the theoretical works such as Feature Integration Theory [4] or Guided Search Model [5], these models consider either low-level image features or high-level features, or a combination of both. For instance, Itti et al. [6] proposed to use center surround differences of bottom–up features such as color, intensity and orientation. The GBVS model [7], which formulates the prediction task as a graph problem employs similar features. Some models such as AIM [8] consider a patch-based representation and formulates saliency from an information theoretic point of view. Moreover, other works exploit high-level features, such as using faces in the images as in [9]. The effectiveness of these models are directly related to the features and the feature integration strategies used in the prediction step. Learning-based saliency models such as [10] and [11], solve the integration problem from a machine learning perspective by formulating the problem as a classification problem and by learning the optimal weights used in the feature integration. While the recently proposed static saliency models [12,13] have reported very impressive results, their main drawback is that they don't consider any temporal information so they don't work well for videos where the content is changing rapidly. Although the main focus in the literature right now is on the static saliency prediction task, we know that we humans have an active vision system. Hence, evaluations on the benchmark datasets consisting of static images is a bit misleading since they can not fully reflect the effectiveness of static saliency models when dealing with dynamic nature of natural scenes. As a consequence, researchers have also developed dynamic sal- iency models which can cope with the dynamic changes in the scenes [6,14–16]. Most of these models employ static features such as color and intensity along with dynamic features, representing mainly the motion in the scenes, since dynamic saliency is strongly related with the change in both spatial and temporal features. However, this makes dynamic saliency a much more challenging problem than static saliency, further complicating the feature integration step of the saliency estimation pipeline. For example, in dynamic scenes, attention can be gathered on a simple moving object. It doesn't have to be very different from the surrounding objects by means of the Gestalt principles. Moving in different direction than others is enough to make it more attractive to attention mechanisms. Many features (color, motion, faces, text, etc.) are known to be effective in predicting visual saliency. Each feature has its own individual saliency map, however, as discussed, they do not need to contribute equally to the final saliency map. In this paper, we propose a comparative study of features and feature integration strategies for dynamic saliency to address the aforementioned issues. Our contributions are as follows: (1) We conduct an analysis of individual effectiveness of several low and high-level visual features for dynamic saliency. These features include two low-level features, static saliency and motion contrast, and three semantic features, faces, humans and text. (2) We investigate different feature integration strategies for dynamic saliency estimation, which consist of learning-based methods containing early and late fusion models, and transformation based methods containing max and mean combination schemes. (3) We provide a thorough analysis of these features and feature integration schemes on two different benchmark datasets, namely CRCNS-ORIG [17] and UCF-Sports [18]. (4) Our experimental results reveal that our models in general outperform the individual features, and in particular, our learning-based models give better results than the state-of-the-art dynamic saliency models. The rest of this paper is as follows: In Section 2, we provide a brief summary of the existing dynamic saliency models. After that, in Section 3, we explain the feature integration strategies that we propose to use in dynamic saliency estimation. In Section 4, we give the details of our experimental setup containing benchmark datasets, visual features and evaluation metrics. In Section 5, we present our experimental analysis about the effectiveness of the individual features and the proposed feature integration models, and compare them against the state-of-the- art dynamic saliency models. Finally, in the last section, we provide a summary of our study and discuss possible future research directions.",Comparing strategies for incorporating features in dynamic saliency estimation,"Interest in computational models of visual attention has increased, leading to the importance of saliency prediction in computer vision. Over recent years, many successful saliency models for image saliency prediction have been proposed, yet they do not take into account dynamic scenes. Little research has been done on dynamic saliency with regards to predicting where humans look in videos. It has been shown that how feature integration is carried out is critical for more accurate results, yet many dynamic saliency models employ a simple design which extracts separate spatial and temporal saliency maps which are integrated together to form the final saliency map. This paper presents a comparative study of different feature integration strategies in dynamic saliency estimation. Low and high-level visual features such as static saliency, motion, faces, humans and text (some of which have not been used in dynamic saliency estimation before) are employed. To explore the strength of feature integration strategies, six new dynamic saliency models are created, based on four learning-based (SVM, Gradient Boosting, NNLS, Random Forest) and two transformation-based (Mean, Max) fusion methods. Experimental analysis on two different dynamic saliency benchmark datasets shows that our models outperform individual features, and that our learning-based models are better than the state-of-the-art dynamic saliency models.","Visual attention is a crucial part of the human visual system, filtering out irrelevant data to focus on relevant elements. Researchers from various fields have attempted to mimic this through computers, creating computational attention models. This has seen great progress in recent years, with many saliency models being used in machine vision applications like object detection, image classification and summarization. In the literature, these models largely predict where humans fixate their eyes in images. Feature Integration Theory and Guided Search Model have inspired models that use low-level or high-level features, or both. For example, Itti et al. employ center surround differences of bottom–up features such as color, intensity and orientation, while GBVS model formulates the prediction task as a graph problem using similar features. AIM considers a patch-based representation and formulates saliency from an information theoretic point of view. Additionally, other works exploit high-level features like faces. The effectiveness of these models is related to the features and feature integration strategies used in the prediction step. Learning-based saliency models, such as those proposed in [10] and [11], solve the integration problem from a machine learning perspective by formulating the problem as a classification problem and learning the optimal weights used in feature integration. While the recently proposed static saliency models have reported impressive results, they don't consider temporal information, making them unsuitable for videos with rapidly changing content. As a result, dynamic saliency models have been developed to cope with dynamic changes in the scenes. These models mostly use static features such as color and intensity along with dynamic features, representing mainly motion. This makes dynamic saliency a much more challenging problem than static saliency, further complicating the feature integration step. In this paper, a comparative study of features and feature integration strategies for dynamic saliency is proposed to address these issues. Analysis of individual effectiveness of several low and high-level visual features is conducted, including two low-level features, static saliency and motion contrast, and three semantic features, faces, humans and text. Different feature integration strategies for dynamic saliency estimation are investigated, consisting of learning-based methods containing early and late fusion models, and transformation based methods containing max and mean combination schemes. A thorough analysis of these features and feature integration schemes is provided on two different benchmark datasets, namely CRCNS-ORIG and UCF-Sports. Results show that proposed models generally outperform individual features, and in particular, learning-based models give better results than the state-of-the-art dynamic saliency models.",https://reader.elsevier.com/reader/sd/pii/S0923596516301667?token=9A398B4AD5D85F5FB949E6D05865C4AE45B99DE7C6C8648638EA5D505AC6447B1BCC334CA076C59B7ACA7155DD2A1273&originRegion=eu-west-1&originCreation=20221228211902
Feature-Based Efficient Moving Object Detection for Low-Altitude Aerial Platforms,"Moving Object Detection is one of the integral tasks for aerial reconnaissance and surveillance applications. Despite the problem’s rising potential due to increasing availability of unmanned aerial vehicles, moving object detection suffers from a lack of widely-accepted, correctly labelled dataset that would facilitate a robust evaluation of the techniques published by the community. Towards this end, we compile a new dataset by manually annotating several sequences from VIVID and UAV123 datasets for moving object detection. We also propose a feature-based, efficient pipeline that is optimized for near real-time performance on GPU-based embedded SoMs (system on module). We evaluate our pipeline on this extended dataset for low altitude moving object detection. Ground-truth annotations are made publicly available to the community to foster further research in moving object detection field. ","In line with the recent trend of Unmanned Aerial Vehicle (UAV) usage in civilian and military sectors, miniature UAVs became more and more affordable and thus accessible for everyone. In addition to its vast military deployment in various armed forces in the world, civilian applications of UAVs also emerged swiftly; industrial inspection, agriculture, mapping, transport, cinematography and numerous indoor applications are examples for UAV usage in various fields 1 . The primary issue of UAV-based computer vision (CV) applications is the platform itself; it is not stable, it tends to have sudden movements, it is exposed to weather conditions, it generates non-homogeneous data (scale, angle, rotation, depth, etc.) and most importantly, it is inherently limited in computational resources. All these difficulties and constraints further the complexity of conventional vision problems, such as object tracking, object detection, object classification and prominently, moving object detection. Moving object detection is a well-studied problem in controlled environments where ego-motion is not present [31]. Introduction of unconstrained ego-motion, however, transforms moving object detection into a much harder problem. Ego-motion estimation and compensation, which exploits image alignment techniques, is performed to address moving object detection in such scenarios [1]. In Wide Area Motion Imagery (WAMI), these techniques could suffice due to comparably low effect of motion parallax [25]. However, for low-altitude UAV scenarios, motion parallax has a far more detrimental effect [32]. Our contributions are primarily as follows: First, we compile a dataset comprised of sequences from VIVID [6] and UAV123 [18] datasets. We carefully pick sequences with various scenarios; motion parallax, altitude variation, viewpoint variation, presence of multiple objects, varying object sizes and speeds are considered. We handannotate the sequences specifically for moving object detection. Second, we present our analysis on this dataset using an efficient, feature-based pipeline which we optimize for near real-time performance on embedded GPU-based SoMs. The annotations are also made publicly available on https://github.com/LAMODDATASET/LAMOD. The remainder of this paper is organized as follows: related work is reviewed in Section 2. The proposed moving object detection framework is explained in-detail in Section 3. Experimental results as well as details on our extended dataset are given in Section 4. Our conclusions and future works are outlined in Section 5.",Detecting Moving Objects Efficiently from Low-Altitude Aerial Platforms Using Features,"Moving Object Detection is essential for aerial reconnaissance and surveillance. However, a lack of labelled datasets hinders the evaluation of techniques proposed by the community. We compile a new dataset, manually annotating sequences from VIVID and UAV123 datasets. We also present a feature-based, efficient pipeline optimized for near real-time performance on GPU-based embedded SoMs. Our pipeline is evaluated on the extended dataset for low altitude moving object detection. We make the ground-truth annotations publicly available to further research in the field.","Miniature UAVs have become increasingly accessible and are being used for a variety of civilian and military applications. Despite the challenges of motion parallax, UAV-based computer vision applications are being used for tasks such as object tracking, object detection, object classification, and moving object detection. To address these issues, we have compiled a dataset from the VIVID and UAV123 datasets that includes sequences with various scenarios such as motion parallax, altitude variation, viewpoint variation, presence of multiple objects, varying object sizes and speeds. We then present our analysis on this dataset using an efficient, feature-based pipeline optimized for near real-time performance on embedded GPU-based SoMs. The annotations for this dataset are also made publicly available. This paper is organized to review related work, explain the proposed moving object detection framework, present experimental results and details on the extended dataset, and conclude with future works.",https://openaccess.thecvf.com/content_ICCV_2017_workshops/papers/w30/Logoglu_Feature-Based_Efficient_Moving_ICCV_2017_paper.pdf
Re-evaluating Automatic Metrics for Image Captioning,"The task of generating natural language descriptions from images has received a lot of attention in recent years. Consequently, it is becoming increasingly important to evaluate such image captioning approaches in an automatic manner. In this paper, we provide an in-depth evaluation of the existing image captioning metrics through a series of carefully designed experiments. Moreover, we explore the utilization of the recently proposed Word Mover’s Distance (WMD) document metric for the purpose of image captioning. Our findings outline the differences and/or similarities between metrics and their relative robustness by means of extensive correlation, accuracy and distraction based evaluations. Our results also demonstrate that WMD provides strong advantages over other metrics.","There has been a growing interest in research on integrating vision and language in natural language processing and computer vision communities. As one of the key problems in this emerging area, image captioning aims at generating natural descriptions of a given image (Bernardi et al., 2016). This is a challenging problem since it requires the ability to not only understand the visual content, but also to generate a linguistic description of that content. In this regard, it can be framed as a machine translation task where the source language denotes the visual domain and the target language is a specific language such as English. The recently proposed deep image captioning studies follow this interpretation and model the process via an encoder-decoder architecture (Vinyals et al., 2015; Xu et al., 2015; Karpathy and FeiFei, 2015; Jia et al., 2015). These approaches have attained considerable success in the recent benchmarks such as FLICKR8K (Hodosh et al., 2013), FLICKR30K (Young et al., 2014) and MS COCO (Lin et al., 2014) as compared to the earlier techniques which explicitly detect objects and generate descriptions by using surface realization techniques (Kulkarni et al., 2013; Li et al., 2011; Elliott and Keller, 2013). With the size of the benchmark datasets becoming larger and larger, evaluating image captioning models has become increasingly important. Human-based evaluations become obsolete as they are costly to acquire and, more importantly, not repeatable. Automatic evaluation metrics are employed as an alternative to human evaluation in both developing new models and comparing them against the state-of-the-art. These metrics compute a score that indicates the similarity/dissimilarity between an automatically generated caption and a number of human-written reference (gold standard) descriptions. Some of these automatic metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), and TER (Snover et al., 2006) have originated from the readily available metrics for machine translation and/or text summarization. On the contrary, the more recent metrics such as CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016) are specifically developed for image caption evaluation task. Evaluation with automatic metrics has some challenges as well. As previously analyzed in (Elliott and Keller, 2014), the existing automatic evaluation measures have proven to be inadequate in successfully mimicking the human judgements for evaluating the image descriptions. The latest evaluation results of 2015 MS COCO Challenge on image captioning has also revealed some interesting findings in line with this observation (Vinyals et al., 2016). In the challenge, the recent deep models outperform the human upper bound according to automatic measures, yet they could not beat the humans when the subjective human judgements are considered. These demonstrate that we need to better understand the drawbacks of existing automatic evaluation metrics. This motivates us to present an in-depth analysis of the current metrics employed in image description evaluation. We first review BLEU, ROUGE, METEOR, CIDEr and SPICE metrics, and discuss their main drawbacks. In this context, we additionally describe WMD metric which has been recently proposed as a distance measure between text documents in (Kusner et al., 2015). We then investigate the performance of these automatic metrics through different experiments. We analyze how well these metrics mimic human assessments by estimating their correlations with the collected human judgements. Different from the previous related work (Elliott and Keller, 2014; Vedantam et al., 2015; Anderson et al., 2016), we perform a more accurate analysis by additionally reporting the results of Williams significance test. This further allows us to figure out the differences and/or similarities between a pair of metrics, whether any two metrics complement each other or provide similar results. We then test the ability of these metrics to distinguish certain pairs of captions from one another in reference to a ground truth caption. Next, we carry out an analysis on robustness of these metrics by analyzing how well they cope with the distractions in the descriptions (Hodosh and Hockenmaier, 2016). ",Assessing Image Captioning Automated Metrics,"Recently, there has been a great deal of interest in the task of generating natural language descriptions from images. Therefore, it is becoming essential to automatically assess such image captioning approaches. This paper provides a thorough assessment of existing image captioning metrics through experiments designed carefully. Additionally, we investigate the utilization of the recently developed Word Mover's Distance (WMD) document metric for image captioning. Our findings demonstrate the differences and/or similarities between metrics, their relative robustness via extensive correlation, accuracy and distraction-based evaluations. Furthermore, our results demonstrate that WMD offers considerable benefits compared to other metrics.","Interest in integrating vision and language in NLP and CV is increasing. Image captioning, a key problem in this area, involves understanding visual content and generating a linguistic description of it. Deep image captioning models use an encoder-decoder architecture (Vinyals et al., 2015; Xu et al., 2015; Karpathy and FeiFei, 2015; Jia et al., 2015) and have attained success in FLICKR8K (Hodosh et al., 2013), FLICKR30K (Young et al., 2014) and MS COCO (Lin et al., 2014) compared to earlier techniques. Human-based evaluation is expensive and not repeatable, so automatic metrics like BLEU (Papineni et al., 2002), ROUGE (Lin, 2004), METEOR (Banerjee and Lavie, 2005), TER (Snover et al., 2006), CIDEr (Vedantam et al., 2015) and SPICE (Anderson et al., 2016) are used to develop and compare models. However, these metrics are inadequate in mimicking human judgements, as demonstrated in the 2015 MS COCO Challenge on image captioning (Vinyals et al., 2016). We therefore present an in-depth analysis of the current automatic metrics, discussing their drawbacks and estimating their correlations with human judgements using Williams significance test. We also analyze their ability to distinguish certain pairs of captions from one another, and their robustness in the presence of distractions.",https://arxiv.org/pdf/1612.07600.pdf
A region covariances-based visual attention model for RGB-D images,"Existing computational models of visual attention generally employ simple image features such as color, intensity or orientation to generate a saliency map which highlights the image parts that attract human attention. Interestingly, most of these models do not process any depth information and operate only on standard two-dimensional RGB images. On the other hand, depth processing through stereo vision is a key characteristics of the human visual system. In line with this observation, in this study, we propose to extend two state-of-the-art static saliency models that depend on region covariances to process additional depth information available in RGB-D images. We evaluate our proposed models on NUS-3D benchmark dataset by taking into account different evaluation metrics. Our results reveal that using the additional depth information improves the saliency prediction in a statistically significant manner, giving more accurate saliency maps.  ","Amount of visual information captured through the eyes is so vast that the brain develops certain mechanisms to process them. Visual attention, as an umbrella term, denotes these mechanisms which are responsible from selecting the most relevant parts of the visual data while discarding the rest. This selection procedure is carried out by fixating the eyes to certain locations of the visual data. Visual attention is an umbrella term since it has been shown that it includes both bottom-up and top-down mechanisms. Bottom-up attention is mostly involuntary and processes in a purely data-driven manner. In short, it is in charge of extracting the image parts which have very different characteristics than their surroundings in terms of different visual features. On the other hand, top-down attention is task-specific and processes the data in a goal-dependent manner, and thus semantic and topdown knowledge play a key role in top-down attention. In computer vision literature, researchers have developed several computational approaches for visual attention to predict where human look at images [4,5,8,10-12,14,18,20,24,28,29,32] (see Figure 1). Most of these visual attention models follow the same bottom-up architecture. By processing the raw visual data, they first extract certain visual features such as intensity, color and orientation, and then compute individual saliency maps for each one of these features. As the last step, they combine these individual maps (after applying certain normalization strategies) to output a final saliency map whose maxima indicate the image locations that attract human attention. For a detailed review of these models, the reader can refer to [2]. It is important to note that the recent trend in saliency prediction is to use deep learning, but to perform an end-to-end learning they need very large image datasets with eye fixation data (e.g., [21,26]). Although, in recent years, we have witnessed a huge increase in the number of visual saliency models, most of these models operate on RGB images and do not use any depth information. On the other hand, the human visual system has a stereo vision capability which enables to capture and process the depth information. In that respect, it can be argued that these attention models do not fully mimic the human visual system. Motivated with this observation and the recent advances in the 3D-capable acquisition equipments such as RealSense and Microsoft Kinect, some researchers have developed a number of depth-aware visual saliency models [3,16,17,19,22,23,25,27,31]. In this paper, we propose two new visual saliency models that additionally process depth information and operate on RGB-D images. A RGB-D image is an image which consists of four channels with the first three channels forming a standard RGB image, and the last channel denoting a depth channel aligned with the RGB component. In particular, we extend the previously suggested CovSal image saliency models [8] to operate on RGBD images. These models originally operate on RGB images and predict saliency by estimating the center-surround differences based on first and second order feature statistics. In our work, we employ these models to estimate two different saliency maps, one from the RGB image and the second from the depth image. We then combine these RGB and depth saliency maps and output a single saliency map for a RGB-D image. As the second contribution of the paper, we evaluate the effect of using depth information through these models in saliency prediction on the NUS-3D benchmark dataset [22] by using several different evaluation metrics. Our results demonstrate that using depth information in saliency estimation improves the prediction accuracy. The paper organization is as follows: In Section 2, we provide a brief overview of the existing depth-aware saliency models. In Section 3, we review the CovSal models and present the proposed extensions to these models to process depth information through RGB-D images. In Section 4, we evaluate the proposed models and present our qualitative and quantitative results. Finally, in Section 5, we provide a summary and discuss our findings. ",Visual Attention Model for RGB-D Images Using Region Covariances,"Computational models of visual attention generally utilize basic image components like color, intensity, and orientation to create a saliency map which shows the image parts that attract human attention. Most of these models don't take into account depth info, instead just analyzing standard 2D RGB images. This study proposes to modify two state-of-the-art static saliency models that depend on region covariances to include depth info from RGB-D images. Evaluation on NUS-3D benchmark dataset via different metrics demonstrates that using the additional depth info enhances saliency prediction in a statistically significant way, producing more precise saliency maps.","Visual data is so vast that the brain creates mechanisms to process it, known as visual attention. It does this by fixating the eyes to certain parts of the data, which is done both involuntarily and goal-dependently. Computational models for visual attention have been created to predict where humans look at images. These models mostly use bottom-up architecture, extracting features such as intensity, color, and orientation, then combining them to output a saliency map. With the recent trend of deep learning, large datasets with eye fixation data are needed. Many of these models only operate on RGB images, not taking into account the human capability of stereo vision and depth information. To address this, some researchers have developed depth-aware saliency models. This paper proposes two such models, extending CovSal models to operate on RGB-D images, and evaluates them using the NUS-3D benchmark dataset. Results show that using depth information improves accuracy.",https://dergipark.org.tr/en/download/article-file/279301
Learning to Generate Images of Outdoor Scenes from Attributes and Semantic Layouts,"Automatic image synthesis research has been rapidly growing with deep networks getting more and more expressive. In the last couple of years, we have observed images of digits, indoor scenes, birds, chairs, etc. being automatically generated. The expressive power of image generators have also been enhanced by introducing several forms of conditioning variables such as object names, sentences, bounding box and key-point locations. In this work, we propose a novel deep conditional generative adversarial network architecture that takes its strength from the semantic layout and scene attributes integrated as conditioning variables. We show that our architecture is able to generate realistic outdoor scene images under different conditions, e.g. daynight, sunny-foggy, with clear object boundaries. ","Automatically synthesizing realistic images has been an emerging research area in deep learning. Imagining an entire scene in the presence of discriminative properties of the scene such as “sunny beach with mountains on the back” is an ability that humans possess. As the most expressive artificial neural networks would presumably have human-like properties including imagination, automatic image generation research is a step towards this goal. Moreover, it is of practical interest as generated images would ideally augment data for various other tasks, e.g. image classification. Generating photo-realistic images of various object types has not yet been solved, however many successful attempts have been made. Generative Adversarial Nets (GANs) [7] generated digits [7], faces [20], chairs [6], room interiFigure 1: Our conditional generative adversarial network synthesizes realistic outdoor images from semantic layouts and transient scene attributes (Images generated automatically using a layout seen during training). ors [20] and videos [26]. On the other hand, Variational Autoencoders (VAEs) [11] are combined with visual attention [8] and have been extended to generating images based on textual descriptions [18]. Moreover, Pixel RNN [25] has been proposed as an alternative model for the same. Deep neural networks take their strength from the availability of large image collections which stabilizes the learning. However, in some domains with limited number of images various complementary sources of information has been proposed to stabilize the learning. Recently, for the domain of fine-grained image generation, GAN conditioned on detailed sentences synthesizes realistic bird images [22] where visual training data was limited. Moreover, integrating textual GAN with bounding box and keypoint conditionals [21] allows drawing bird at the desired location. On the other hand, conditioining VAEs on discriminative object properties, i.e. attributes, has generated faces [30] with different hair color, beard or glasses, at different ages. Apart from stabilizing the learning, conditioning variables also provide diversity to the generated images. Hence,we argue that the descriptive power of a generator network can be increased by conditioning it with respect to the object type, visual properties and location information. Object type conditioning teaches the network what to draw, visual properties specifies the visual details of the object and finally the location encodes where that object should be drawn. We propose a new GAN model architecture to generate realistic outdoor scenes, e.g. sea, mountain, urban scenes, conditioned on transient attributes, e.g. sunny, foggy, and on semantic layouts to determine the exact boundaries of where the object should be drawn. Our aim is to automatically generate outdoor scenes with various scene properties as shown in Figure 1. This problem has previously been tackled by designing hand-crafted procedures [13], however we propose to learn such transformations automatically through training deep convolutional networks. Towards this goal, we employ the recent ADE20K dataset [31] that contains outdoor scenes with dense semantic layout annotations. To complement semantic layouts, we exploit a dataset of outdoor webcam sequences [13] that provides per-scene attribute annotations. We complement the missing spatial layouts of [13] with coarse semantic annotations of each scene and the missing attributes of [31] with attribute predictions. We will make these supplementary annotations and our code publicly available. Our contributions are summarized as follows. We propose a new conditioned GAN model that learns the content, i.e. transient attributes, to be drawn inside a scene layout. We show that our model generates realistic images of scenes with objects drawn within their own segments as well as transforming the scene by, for instance, imagining how a day scene would look like in the night. ",Creating Outdoor Images from Attributes and Semantic Structures,"Research into automatic image synthesis has been advancing rapidly, with deep networks becoming increasingly expressive. In the past few years, we have seen images of digits, indoor scenes, birds, chairs etc. automatically generated, and their expressive power boosted by incorporating various conditioning variables, e.g. object names, sentences, bounding boxes and key-point locations. Here, we introduce a novel deep conditional generative adversarial network architecture that harnesses semantic layout and scene attributes as conditioning variables. We demonstrate that this architecture is able to produce realistic outdoor scene images in different conditions, such as day/night, sunny/foggy, with distinct object outlines.","Automatically creating realistic images has been a focus of deep learning. Humans possess the capability to imagine an entire scene with distinguishing properties, such as ""sunny beach with mountains in the background"". Artificial neural networks with this type of capacity could provide useful applications and image generation research is a step in that direction. Generating realistic images of various objects has yet to be solved, however, many successful attempts have been made. GANs (Generative Adversarial Nets) have produced digits, faces, chairs, room interiors, and videos. VAEs (Variational Autoencoders) combined with visual attention have been used to generate images from textual descriptions. Pixel RNN has also been proposed as an alternative model. To stabilize learning in domains with limited images, complementary sources of information have been proposed. For example, GANs conditioned on detailed sentences have generated realistic bird images when visual training data was limited. VAEs conditioned on object properties have created faces with different hair color, beard, or glasses, at different ages. We suggest that the descriptive power of a generator network can be increased by conditioning it with respect to the object type, visual properties, and location information. We present a new GAN model architecture to produce realistic outdoor scenes, such as sea, mountain, and urban scenes, conditioned on transient attributes (e.g. sunny, foggy) and semantic layouts to determine where the object should be drawn. This problem has been tackled through hand-crafted procedures, however, we propose to learn these transformations automatically through training deep convolutional networks. We employ the ADE20K dataset that contains outdoor scenes with dense semantic layout annotations and the outdoor webcam sequences dataset that provides per-scene attribute annotations. We provide supplementary annotations and our code to the public. Our model learns the content to be drawn inside a scene layout and generates realistic images of scenes with objects drawn within their own segments, as well as transforming the scene (e.g. imagining how a day scene would look like in the night).",https://arxiv.org/pdf/1612.00215.pdf
Leveraging Captions in the Wild to Improve Object Detection,"In this study, we explore whether the captions in the wild can boost the performance of object detection in images. Captions that accompany images usually provide significant information about the visual content of the image, making them an important resource for image understanding. However, captions in the wild are likely to include numerous types of noises which can hurt visual estimation. In this paper, we propose data-driven methods to deal with the noisy captions and utilize them to improve object detection. We show how a pre-trained state-of-theart object detector can take advantage of noisy captions. Our experiments demonstrate that captions provide promising cues about the visual content of the images and can aid in improving object detection.","Visual data on the Internet is generally coupled with descriptive text such as tags, keywords or captions. While tags and keywords are typically composed of single words, or phrases, and generally depict the main entities in an image (e.g. objects, places, etc.), a caption is a complete sentence which is intended to describe the image in a holistic manner. It can reveal information about not just the existing objects or the corresponding event but also the relationships between the objects/scene elements, their attributes or the actions in a scene (Figure 1). In this respect, captions provide a much richer source of information in order to understand the image content. This has recently motivated researchers to automate the task of describing images in natural languages using captions (Raffaella et al., 2016). However, most of these studies employ carefully collected image descriptions which are obtained by services like Amazon’s Mechanical Turk (Rashtchian et al., 2010a; Hodosh et al., 2013; Young et al., 2014a; Lin et al., 2014). Little has been done on utilizing captions in the wild, i.e. the captions that accompany images readily available on the Web. Although captions are rich, there are some challenges that limit their use in computer vision, and related language tasks. First, a caption may not be a visual depiction of the scene, but rather a sort of comment not directly related to the visual content of the image (Figure 1). The users might avoid explaining the obvious, but talk about more indirect aspects, abstract concepts and/or feelings. Third, the caption may be poorly written, which makes it difficult to understand the meaning of the text associated with the image. On the other hand, there is also a major advantage in having image-caption pairs on the Web; billions of them are freely available online. Collectively considering image-caption pairs associated with a certain query image may allow to eliminate noisy information. Researchers have used this idea to collect a large scale images-captions dataset consisting of clean, descriptive texts paired with images (Chen et al., 2015). When noisy captions are eliminated, the rest can serve as an excellent source of information for what is available in the visual world. In this paper, we investigate whether we can leverage captions in the wild to improve object detection. Object detection has seen some significant advances in recent years thanks to convolutional neural networks (LeCun et al., 2015). But in some cases, even state-of-the-art object detectors may fail to accurately locate objects or may produce false positives (see Figure 2). For such situations, we propose to utilize captions as an alternative source of information to determine what is present in the image. Due to the reasons stated above, however, leveraging captions directly may result in errors. Therefore, we suggest to use datadriven methods which can eliminate the noise in the captions and inform about which objects are available in the image. For our purpose, we first consider a constrained scenario where we assume access to test image captions and run detectors for objects mentioned in the caption, as previously motivated by (Ordonez et al., 2015). Then, we proceed to explore a more general setting where we observe captions only at training stage and infer possible objects within the test image using similar training images and their captions. In finding similar images/captions, we propose to use three different approaches, based on nearest neighbors, 2-view Canonical Correlation Analysis (CCA) and 3-view CCA. When the visual input is combined with caption information, these approaches not only help us to eliminate the noise in the captions, but also to infer about possible objects not even mentioned in the caption of a test image (see Figure 2). Our experimental results show that utilizing noisy captions of visually similar images in the proposed ways can indeed help in improving the performance of the object detection. ",Improving Object Detection by Utilizing Captions in the Wild,"This study examines if captions in the wild can enhance object detection in images. Such captions usually give important details about the image's visual content, thus being a vital resource for image comprehension. But, captions in the wild could include various types of noise which can affect visual estimation. To counter this, data-driven methods were proposed to manage noisy captions and make use of them to improve object detection. The paper shows how a pre-trained, state-of-the-art object detector can benefit from noisy captions. Experiments illustrate that captions offer promising clues about the visual content of the images and can help in boosting object detection.","Visual data on the Internet is usually accompanied by descriptive text such as tags, keywords or captions. Tags and keywords are often single words or phrases, and generally show the main entities in an image (e.g. objects, places, etc.), while a caption is a full sentence intended to explain the image as a whole. It can disclose information not only about existing objects or the relevant event, but also the connections between the objects/scene elements, their attributes or the actions in a scene (Figure 1). This has recently motivated scientists to create automated image description in natural languages using captions (Raffaella et al., 2016). Nevertheless, most of these studies make use of carefully gathered image descriptions which are procured from services like Amazon's Mechanical Turk (Rashtchian et al., 2010a; Hodosh et al., 2013; Young et al., 2014a; Lin et al., 2014). Little has been done on utilizing captions in the wild, i.e. the captions that accompany images readily available on the Web. Even though captions are rich, there are some difficulties that restrict their use in computer vision, and related language tasks. For instance, a caption may not be a visual representation of the scene, but rather a sort of comment not directly linked to the visual content of the image (Figure 1). Users may avoid explaining the obvious, but talk about more indirect aspects, abstract concepts and/or feelings. Additionally, the caption may be poorly written, which makes it hard to comprehend the meaning of the text connected to the image. However, there is also a major benefit in having image-caption pairs on the Web; billions of them are freely accessible online. Collectively looking at image-caption pairs related to a certain query image may enable to get rid of noisy information. Scientists have employed this idea to assemble a large scale images-captions dataset consisting of clean, descriptive texts paired with images (Chen et al., 2015). When noisy captions are removed, the rest can serve as an excellent source of information for what is present in the visual world. In this paper, we examine whether we can take advantage of captions in the wild to improve object detection. Object detection has seen some remarkable advances in recent years due to convolutional neural networks (LeCun et al., 2015). However, in some cases, even state-of-the-art object detectors may fail to accurately locate objects or may produce false positives (see Figure 2). For such situations, we propose to utilize captions as an alternative source of information to figure out what is present in the image. Due to the reasons mentioned above, however, utilizing captions directly may lead to errors. Therefore, we suggest to use data-driven methods which can eliminate the noise in the captions and inform about which objects are available in the image. For our purpose, we initially consider a restricted scenario where we assume access to test image captions and run detectors for objects mentioned in the caption, as previously suggested by (Ordonez et al., 2015). Then, we move on to explore a more general setting where we observe captions only at training stage and infer possible objects within the test image using similar training images and their captions. In finding similar images/captions, we propose to use three different approaches, based on nearest neighbors, 2-view Canonical Correlation Analysis (CCA) and 3-view CCA. When the visual input is combined with caption information, these approaches not only help us to eliminate the noise in the captions, but also to infer about possible objects not even mentioned in the caption of a test image (see Figure 2). Our experimental results show that using noisy captions of visually similar images in the proposed ways can indeed help in improving the performance of the object detection.",https://aclanthology.org/W16-3204.pdf
Two-Stream Convolutional Networks for Dynamic Saliency Prediction,"In recent years, visual saliency estimation in images has attracted much attention in the computer vision community. However, predicting saliency in videos has received relatively little attention. Inspired by the recent success of deep convolutional neural networks based static saliency models, in this work, we study two different two-stream convolutional networks for dynamic saliency prediction. To improve the generalization capability of our models, we also introduce a novel, empirically grounded data augmentation technique for this task. We test our models on DIEM dataset and report superior results against the existing models. Moreover, we perform transfer learning experiments on SALICON, a recently proposed static saliency dataset, by finetuning our models on the optical flows estimated from static images. Our experiments show that taking motion into account in this way can be helpful for static saliency estimation. ","Visual saliency models have gained significant popularity in recent years. The reason behind this growing interest lies in the effective use of these models in various computer vision problems such as segmentation, object detection, video summarization and compression where extracted saliency maps are employed either as a visual feature or as a feature selection mechanism. Broadly speaking, saliency models can be split into two categories in terms of whether they try to predict human eye fixations [2] or detect salient objects [1]. The models can be further divided into static and dynamic saliency models according to which type of input they process. While static models take still images as input, dynamic models work on video sequences. Predicting saliency in videos poses great challenges for researchers as compared to performing the same task in still images. First and foremost, the dynamic models need to consider both the spatial and the temporal characteristics of the scene when computing saliency maps. While static saliency models employ visual features such as intensity, color and orientation, for the dynamic saliency one need to focus more on the motion features since humans have a tendency to fixate their eyes on the objects in motion. In that regard, the early examples of the dynamic saliency models extend the static saliency models so that they take into account additional motion features [8, 7, 5, 30]. In addition, there are a limited number of dynamic saliency models which approach the saliency prediction in videos from a novel point of view [10, 23, 29]. There is a recent interest in applying deep learning to saliency prediction in still images [19, 25, 18, 32, 37]. These models all employ deep neural networks, and give the state-of-the-art results in most of the benchmark datasets. In this paper, our contributions are three-fold. First, inspired by the success of these models, we investigate convolutional neural networks for dynamic saliency prediction. We study the use of two-stream convolutional neural network architectures which integrate the spatial stream with the temporal stream. These network models predict a saliency map for a given video frame by simultaneously exploiting the appearance and motion information via filters learned in an end-toend fashion. In particular, we propose two different models which consider late or early fusion strategies. To our knowledge, we are the first to apply a two-stream deep model for dynamic saliency. Second, we propose a data augmentation technique for this task to improve the generalization of the convolutional networks. Experiments on DIEM dataset [24] validate the effectiveness of our models and our data augmentation strategy. Third, we demonstrate transfer learning could be used to predict static saliency by exploitin optical flow information extracted from still images [33]. Experiments on SALICON dataset [13] show that taking motion into account can improve the prediction accuracy",Dynamic Saliency Prediction with Two-Stream ConvNets,"Recent research in the computer vision community has focused on estimating visual saliency in images. However, predicting saliency in videos has not been widely explored. This work examines two two-stream convolutional networks for dynamic saliency prediction, utilizing a novel data augmentation technique to enhance the models' generalization capability. Tests on the DIEM dataset reveal the models' superiority over existing models. Additionally, transfer learning experiments on the static saliency dataset SALICON, with the models fine-tuned on optical flows estimated from static images, demonstrate that motion consideration is advantageous for static saliency estimation.","Visual saliency models have gained popularity due to their effective use in computer vision problems, such as segmentation, object detection, video summarization and compression. These models can be divided into two categories: those that try to predict human eye fixations and those that detect salient objects. They can also be split into static and dynamic saliency models, which process still images and video sequences, respectively. Predicting saliency in videos is difficult due to the need to consider spatial and temporal characteristics. Static models employ visual features such as intensity, color and orientation, while dynamic models focus on motion features as humans are more likely to fixate on objects in motion. Early dynamic models extended static models to take motion features into account, and some models approach saliency prediction from a novel perspective. Deep learning has been applied to static saliency prediction with successful results. In this paper, convolutional neural networks are investigated for dynamic saliency prediction. Two-stream architectures that integrate the spatial and temporal streams are used to predict saliency maps by learning filters in an end-to-end fashion. Data augmentation is proposed to improve the generalization of the networks. Transfer learning is used to predict static saliency by exploiting optical flow information extracted from still images. Experiments on various datasets validate the effectiveness of the models and data augmentation strategy.",https://vision.cs.hacettepe.edu.tr/publication/fulltext/619e066db82f6cb47fd3c46c268cc39e.pdf
Deformable part-based tracking by coupled global and local correlation filters,"Correlation filters have recently attracted attention in visual tracking due to their efficiency and high per- formance. However, their application to long-term tracking is somewhat limited since these trackers are not equipped with mechanisms to cope with challenging cases like partial occlusion, deformation or scale changes. In this paper, we propose a deformable part-based correlation filter tracking approach which depends on coupled interactions between a global filter and several part filters. Specifically, local filters provide an initial estimate, which is then used by the global filter as a reference to determine the final result. Then, the global filter provides a feedback to the part filters regarding their updates and the related deformation parameters. In this way, our proposed collaborative model handles not only partial occlusion but also scale changes. Experiments on two large public benchmark datasets demonstrate that our approach gives significantly better results compared with the state-of-the-art trackers.","The main goal of object tracking is to determine the location of a given object, usually specified with a bounding box in the first frame, in the subsequent video frames. Tracking has a wide range of applications including but not limited to visual surveillance, human computer interaction, robotics, automatic navigation and action recognition. Although recent studies have reported promis- ing results, tracking is still considered as a difficult problem due to the challenges such as occlusion, illumination variation, changes in scale, object deformations and in or out of plane rotations. Object tracking methods can be divided into two broad cate- gories as generative and discriminative. The first group of works learns a generic model from the given initial conditions and then defines tracking as a search problem where the image region that is closest to the object of interest is determined within a neighbor- hood. On the other hand, the second group, which is generally referred to as tracking-by-detection, formulates tracking as a bin- ary classification problem [1–3]. They mainly employ HoG [4], Haar-like features [1] and region covariances [5] as the object representations and use SVM [2], Multiple Instance Learning [1],Random Forests [6], and Ada-boost [7] as the visual classifiers. Recently, correlation filters [8–10,4,11,12] give very promising results while offering very low processing time. These trackers simply try to maximize the correlation between a trained object template with the current frame in an online fashion, and since the processing is usually done in the Fourier domain, they are very fast. In this paper, we propose a robust generic object based tracking algorithm. In particular, we suggest to combine recently proposed tracking-by-detection schema of correlation filters with deform- able part-based model within a unified framework of coupled glo- bal and local filters. In our tracker, which we call deformable part- based correlation filter tracking (DPCF), the appearance model is implemented by a correlation filter based on histogram-of- gradient (HoG) features and color, while the deformable model is based on the relative arrangement of parts. The object is repre- sented with several local filters, which have a deformable struc- ture, and an additional global appearance filter that has a coupled interaction with the part-based schema. Specifically, local filters help us to track a target object by focusing on specific object parts and provide a rough solution for the target’s position. Global filter, however, is learned by considering the whole object region and employs the approximate solution from the local filters to determine the final solution. Moreover, it provides feedbacks to the local filters regarding the utilized deformation model and the new parameters. We make four main contributions to the long-term online visual tracking problem: (i) We propose a deformable part-based tracking framework based on correlation filters. (ii) We present a collabora- tive algorithm for tracking-by-detection with coupled local and global filters. (iii) We introduce a simple yet natural model for han- dling scale changes, and a robust update scheme that addresses occlusion, scaling and fast motion issues while keeping the pro- cessing in real time. (iv) We improve the state-of-the-art tracking results in comparison to the recent successful trackers on two large scale public benchmark datasets [13]. The rest of the paper is organized as follows. In Section 2, we review recent tracking algorithms. Section 3, we give a detailed description of correlation based tracking and the proposed coupled global and local model. In Section 4, we provide a thorough exper- imental analysis by comparing our tracker against the state-of-the- art trackers. In Section 5, we conclude with some discussions and directions for the future research.",Tracking Parts with Global and Local Correlation Filters,"Correlation filters, due to their efficiency and high performance, have recently gained attention in visual tracking. However, they are not suitable for long-term tracking, as they lack mechanisms to handle challenging cases such as partial occlusion, deformation or scale changes. To address this, we present a deformable part-based correlation filter tracking approach which relies on interactions between a global filter and several part filters. Local filters first provide an initial estimate which is used by the global filter to compute the final result. The global filter then provides feedback to the part filters on their updates and deformation parameters. This collaborative model is able to cope with both partial occlusion and scale changes. Experiments on two large public benchmark datasets show that our approach achieves significantly better results compared to the state-of-the-art trackers.","The main aim of object tracking is to locate a given object, usually specified with a bounding box in the first frame, in the subsequent video frames. This has various applications such as visual surveillance, human-computer interaction, robotics, automatic navigation and action recognition. Despite promising outcomes, it remains a challenging task due to issues such as occlusion, illumination alteration, changes in scale, object deformations and in or out of plane rotations. Tracking methods can be classified into two groups, generative and discriminative. The first group learns a generic model from the initial conditions and then locates the closest image region to the object of interest within a neighbourhood. The second group, known as tracking-by-detection, considers tracking as a binary classification problem, using HoG, Haar-like features and region covariances as object representations and SVM, Multiple Instance Learning, Random Forests and Ada-boost as visual classifiers. Recently, correlation filters have achieved very promising results while providing low processing time, by attempting to maximize the correlation between a trained object template and the current frame. This paper presents a robust generic object based tracking algorithm which combines the tracking-by-detection schema of correlation filters with a deformable part-based model in a unified framework of coupled global and local filters. The proposed tracker, called deformable part-based correlation filter tracking (DPCF), uses a correlation filter based on histogram-of-gradient (HoG) features and colour for the appearance model and a relative arrangement of parts for the deformable model. The object is represented with several local filters which have a deformable structure and an additional global appearance filter that interacts with the part-based schema. Local filters help to track a target object by focusing on specific object parts and provide a rough solution for the target's position. Global filter is learned by considering the whole object region and uses the approximate solution from the local filters to determine the final solution, also providing feedbacks regarding the utilized deformation model and new parameters. Four main contributions are made to the long-term online visual tracking problem: (i) a deformable part-based tracking framework based on correlation filters; (ii) a collaborative algorithm for tracking-by-detection with coupled local and global filters; (iii) a simple yet effective model for handling scale changes, and a robust update scheme to address occlusion, scaling and fast motion while keeping the processing in real time; and (iv) improved state-of-the-art tracking results in comparison to recent successful trackers on two large scale public benchmark datasets.",https://reader.elsevier.com/reader/sd/pii/S1047320316300517?token=E68C95A69BFA45D1382E6CEBA7BF9FF9A81688FB1B1D885977142809883614E6BD90F0D912E4E15A8E35F4673F0C920A&originRegion=eu-west-1&originCreation=20221228213153
An Objective Deghosting Quality Metric for HDR Images,"Reconstructing high dynamic range (HDR) images of a complex scene involving moving objects and dynamic backgrounds is prone to artifacts. A large number of methods have been proposed that attempt to alleviate these artifacts, known as HDR deghosting algorithms. Currently, the quality of these algorithms are judged by subjective evaluations, which are tedious to conduct and get quickly outdated as new algorithms are proposed on a rapid basis. In this paper, we propose an objective metric which aims to simplify this process. Our metric takes a stack of input exposures and the deghosting result and produces a set of artifact maps for different types of artifacts. These artifact maps can be combined to yield a single quality score. We performed a subjective experiment involving 52 subjects and 16 different scenes to validate the agreement of our quality scores with subjective judgements and observed a concordance of almost 80%. Our metric also enables a novel application that we call as hybrid deghosting, in which the output of different deghosting algorithms are combined to obtain a superior deghosting result.","Due to its low-cost and availability, the most commonly used HDR image capture method remains to be the multiple exposures technique (MET), which involves combining a set of exposures of a scene into a single HDR image [DM97]. The main requirements of this technique are that the camera and the captured scene remain static throughout the capture process. Otherwise, the lack of correspondence between exposures result in what is known as ghosting artifacts. While stabilizing a camera can be achieved by using a tripod, ensuring a static scene is much more difficult as most real-world scenes contain dynamic objects. Many deghosting algorithms have been proposed to address this problem ranging from simple alignment methods to sophisticated computer vision algorithms. To this date, more than 50 deghosting algorithms have been proposed [TAEE15]. As in all fields, the proliferation of these algorithms gave rise to subjective experiments that aim to evaluate their performance [HT13, HTM13, HTM14, TAEE15]. However, subjective comparisons of HDR deghosting algorithms is problematic for several reasons. First, ideally the comparison medium must be an HDR display [SHS∗04], as otherwise some artifacts may be lost or new artifacts may be generated during tone mapping. Secondly, the comparison task is challenging as participants need to compare a stack of LDR images with one or more deghosted images. Finally, the findings of subjective experiments become outdated as new algorithms are being proposed on a rapid basis. In order to overcome these problems, there is a clear need to define objective metrics to compare HDR deghosting algorithms, which is the primary goal of this paper. A number of quality assessment metrics have been proposed for HDR images [RWPD10, Chapter 10] [SKMS06, KJF07, RFWB07, AMMS08, MKRH11]. However, none of these metrics are suitable for evaluating deghosting artifacts. The objective metric proposed in this work is the result of analyzing the outputs of several HDR deghosting algorithms to identify the most prevalent artifacts that are present. Some of these artifacts and the corresponding distortion maps produced by our algorithm are shown in Figure 1. To validate the compliance of our metric with subjective judgements of real observers, we conducted a subjective experiment that involves 16 scenes of varying characteristics, 10 deghosting algorithms, and 52 participants. We found that there is a high degree of correlation between the subjective and objective results. The proposed metric has several applications such as automatic comparison of deghosting algorithms, automatic image quality inspection, understanding the strengths and weaknesses of existing algorithms, optimizing parameter selection, providing feedback for developing better HDR deghosting algorithms, and hybrid deghosting in which multiple deghosting results are combined to obtain a superior one.",Quality Measurement for HDR Image Deghosting,"The reconstruction of HDR images of a complex scene with moving objects and dynamic backgrounds can cause artifacts. Numerous methods, known as HDR deghosting algorithms, have been developed to reduce these artifacts. Subjective evaluations are used to assess the quality of these algorithms, but this process is time-consuming and not up-to-date with newer algorithms. We present an objective metric that simplifies this process by creating artifact maps from input exposures and the deghosting result, which can be combined to form a single quality score. To validate our score's agreement with subjective judgements, we conducted a subjective experiment with 52 subjects and 16 scenes and obtained a concordance of nearly 80%. Our metric also enables a novel application, hybrid deghosting, which combines the output of multiple deghosting algorithms to create a better deghosting result.","The most popular HDR image capture technique is Multiple Exposures Technique (MET), where a set of exposures of a scene are merged into one HDR image. This requires a static camera and scene, otherwise ghosting artifacts occur. To address this, numerous deghosting algorithms have been proposed, leading to subjective experiments to evaluate their performance. However, these experiments are difficult, as they involve comparing LDR images with deghosted ones and need an HDR display. Therefore, objective metrics are needed to compare HDR deghosting algorithms, which is the focus of this paper. After analyzing several HDR deghosting algorithms, artifacts and distortion maps were identified. A subjective experiment was conducted with 16 scenes, 10 algorithms and 52 participants, and high correlation between the subjective and objective results was found. This metric has various uses, including automatic comparison of algorithms, image quality inspection, understanding existing algorithms, parameter selection, feedback for better algorithms, and hybrid deghosting.",https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12818
"Automatic Description Generation from Images: A Surveyof Models, Datasets, and Evaluation Measures","Automatic description generation from natural images is a challenging problem thathas recently received a large amount of interest from the computer vision and natural lan-guage processing communities.  In this survey,  we classify the existing approaches basedon how they conceptualize this problem, viz., models that cast description as either gen-eration  problem  or  as  a  retrieval  problem  over  a  visual  or  multimodal  representationalspace.  We provide a detailed review of existing models, highlighting their advantages anddisadvantages.  Moreover, we give an overview of the benchmark image datasets and theevaluation measures that have been developed to assess the quality of machine-generatedimage descriptions.  Finally we extrapolate future directions in the area of automatic imagedescription generation.","Over the past two decades, the fields of natural language processing (NLP) and computervision (CV) have seen great advances in their respective goals of analyzing and generatingtext, and of understanding images and videos.  While both fields share a similar set of meth-ods rooted in artificial intelligence and machine learning, they have historically developedseparately, and their scientific communities have typically interacted very little.Recent  years,  however,  have  seen  an  upsurge  of  interest  in  problems  that  require  acombination of linguistic and visual information.  A lot of everyday tasks are of this nature,e.g.,  interpreting  a  photo  in  the  context  of  a  newspaper  article,  following  instructions  inconjunction with a diagram or a map, understanding slides while listening to a lecture.  In addition to this, the web provides a vast amount of data that combines linguistic and visualinformation:  tagged photographs, illustrations in newspaper articles, videos with subtitles,and multimodal feeds on social media.  To tackle combined language and vision tasks and toexploit the large amounts of multimodal data, the CV and NLP communities have movedcloser together, for example by organizing workshops on language and vision that have beenheld regularly at both CV and NLP conferences over the past few years.In this new language-vision community,automatic image descriptionhas emerged as akey task.  This task involves taking an image, analyzing its visual content, and generatinga textual description (typically a sentence) that verbalizes the most salient aspects of theimage.  This is challenging from a CV point of view, as the description could in principletalk about any visual aspect of the image:  it can mention objects and their attributes, itcan  talk  about  features  of  the  scene  (e.g.,  indoor/outdoor),  or  verbalize  how  the  peopleand objects in the scene interact.  More challenging still, the description could even refer toobjects that are not depicted (e.g., it can talk about people waiting for a train, even whenthe train is not visible because it has not arrived yet) and provide background knowledgethat cannot be derived directly from the image (e.g., the person depicted is the Mona Lisa).In  short,  a  good  image  description  requires  full  image  understanding,  and  therefore  thedescription task is an excellent test bed for computer vision systems, one that is much morecomprehensive than standard CV evaluations that typically test, for instance, the accuracyof object detectors or scene classifiers over a limited set of classes.Image understanding is necessary, but not sufficient for producing a good description.Imagine  we  apply  an  array  of  state-of-the-art  detectors  to  the  image  to  localize  objects(e.g., Felzenszwalb,  Girshick,  McAllester, & Ramanan,  2010;  Girshick,  Donahue,  Darrell,& Malik, 2014), determine attributes (e.g., Lampert, Nickisch, & Harmeling, 2009;  Berg,Berg,  &  Shih,  2010;  Parikh  &  Grauman,  2011),  compute  scene  properties  (e.g.,  Oliva  &Torralba,  2001;  Lazebnik,  Schmid,  &  Ponce,  2006),  and  recognize  human-object  interac-tions (e.g., Prest,  Schmid, & Ferrari,  2012;  Yao & Fei-Fei,  2010).  The result would be along, unstructured list of labels (detector outputs), which would be unusable as an imagedescription.  A good image description,  in contrast,  has to be comprehensive but concise(talk about all and only the important things in the image), and has to be formally correct,i.e., consists of grammatically well-formed sentences.From an NLP point of view, generating such a description is a natural language gener-ation  (NLG)  problem.   The  task  of  NLG  is  to  turn  a  non-linguistic  representation  intohuman-readable  text.   Classically,  the  non-linguistic  representation  is  a  logical  form,  adatabase  query,  or  a  set  of  numbers.   In  image  description,  the  input  is  an  image  rep-resentation (e.g.,  the detector outputs listed in the previous paragraph),  which the NLGmodel has to turn into sentences.  Generating text involves a series of steps, traditionallyreferred to as the NLP pipeline (Reiter & Dale,  2006):  we need to decide which aspectsof the input to talk about (content selection), then we need to organize the content (textplanning) and verbalize it (surface realization).  Surface realization in turn requires choos-ing  the  right  words  (lexicalization),  using  pronouns  if  appropriate  (referential  expressiongeneration), and grouping related information together (aggregation).In other words, automatic image description requires not only full image understanding,but also sophisticated natural language generation. This is what makes it such an interesting task  that  has  been  embraced  by  both  the  CV  and  the  NLP  communities.1Note  thatthe  description  task  can  become  even  more  challenging  when  we  take  into  account  thatgood descriptions are often user-specific.  For instance, an art critic will require a differentdescription than a librarian or a journalist, even for the same photograph.  We will brieflytouch upon this issue when we talk about the difference between descriptions and captionsin Section 3 and discuss future directions in Section 4.Given that automatic image description is such an interesting task, and it is driven by theexistence of mature CV and NLP methods and the availability of relevant datasets, a largeimage description literature has appeared over the last five years.  The aim of this surveyarticle is to give a comprehensive overview of this literature, covering models, datasets, andevaluation metrics.We  sort  the  existing  literature  into  three  categories  based  on  the  image  descriptionmodels used.  The first group of models follows the classical pipeline we outlined above: theyfirst detect or predict the image content in terms of objects, attributes, scene types, andactions, based on a set of visual features.  Then, these models use this content informationto drive a natural language generation system that outputs an image description.  We willterm these approachesdirect generation models.The second group of models cast the problem as a retrieval problem.  That is, to create adescription for a novel image, these models search for images in a database that are similar tothe novel image.  Then they build a description for the novel image based on the descriptionsof  the  set  of  similar  images  that  was  retrieved.   The  novel  image  is  described  by  simplyreusing the description of the most similar retrieved image (transfer), or by synthesizing anovel description based on the description of a set of similar images.  Retrieval-based modelscan be further subdivided based on what type of approach they use to represent imagesand compute similarity.  The first subgroup of models uses avisual spaceto retrieve images,while the second subgroup uses amultimodal spacethat represents images and text jointly.For an overview of the models that will be reviewed in this survey, and which category theyfall into, see Table 1.Generating natural language descriptions from videos presents unique challenges overand  above  image-based  description,  as  it  additionally  requires  analyzing  the  objects  andtheir attributes and actions in the temporal dimension.  Models that aim to solve descrip-tion  generation  from  videos  have  been  proposed  in  the  literature  (e.g.,  Khan,  Zhang,  &Gotoh, 2011; Guadarrama, Krishnamoorthy, Malkarnenkar, Venugopalan, Mooney, Darrell,& Saenko,  2013;  Krishnamoorthy,  Malkarnenkar,  Mooney,  Saenko, & Guadarrama,  2013;Rohrbach, Qiu, Titov, Thater, Pinkal, & Schiele, 2013; Thomason, Venugopalan, Guadar-rama, Saenko, & Mooney, 2014; Rohrbach, Rohrback, Tandon, & Schiele, 2015; Yao, Torabi,Cho, Ballas, Pal, Larochelle, & Courville, 2015; Zhu, Kiros, Zemel, Salakhutdinov, Urtasun,Torralba, & Fidler, 2015).  However, most existing work on description generation has usedstatic images, and this is what we will focus on in this survey.2In this survey article, we first group automatic image description models into the threecategories  outlined  above  and  provide  a  comprehensive  overview  of  the  models  in  each category in Section 2.  We then examine the available multimodal image datasets used fortraining and testing description generation models in Section 3.  Furthermore,  we reviewevaluation measures that have been used to gauge the quality of generated descriptions inSection  3.  Finally,  in Section  4,  we  discuss future  research  directions,  including  possiblenew tasks related to image description, such as visual question answering. ","Generating Descriptions from Images: Overview of Models, Data & Evaluation","Generating descriptions of natural images is a difficult task that has recently sparked a lot of interest among computer vision and natural language processing researchers. In this review, we categorize existing approaches based on how they view the problem, i.e. models that see description generation as a problem of either generation or retrieval in a visual or multimodal representation space. We analyze the benefits and drawbacks of current models, provide an overview of benchmark image datasets and evaluation measures for assessing the quality of machine-generated image descriptions, and discuss potential future directions for automatic image description generation.","In the past two decades, Natural Language Processing (NLP) and Computer Vision (CV) have seen significant advances in analyzing and generating text, and understanding images and videos. Although sharing similar methods based on Artificial Intelligence and Machine Learning, the scientific communities of these two fields have usually had little interaction. However, in recent years, there has been increased interest in problems requiring a combination of linguistic and visual information. Everyday tasks such as interpreting a photo in a newspaper article or understanding slides while listening to a lecture require this. The web also provides a vast amount of data that combines linguistic and visual information, such as tagged photographs, illustrations in newspaper articles, videos with subtitles, and multimodal feeds on social media. To address combined language and vision tasks and exploit the large amounts of multimodal data, the CV and NLP communities have grown closer together, evidenced by regular workshops held at both CV and NLP conferences in recent years. One of the key tasks to emerge from this new language-vision community is automatic image description, which involves taking an image, analyzing its visual content, and generating a textual description that verbalizes the salient aspects of the image. This is difficult from a CV perspective, as the description could theoretically talk about any visual aspect of the image - objects and their attributes, features of the scene, or how people and objects interact. Furthermore, the description may refer to objects not depicted and provide background knowledge not directly derived from the image. In other words, it requires full image understanding and is an excellent test bed for computer vision systems. Image understanding alone is not enough for producing a good description, though. A list of labels generated by applying state-of-the-art detectors to the image would be unusable. In contrast, a good image description should be comprehensive yet concise, and composed of grammatically well-formed sentences. From an NLP point of view, this is a natural language generation (NLG) problem, where the input is an image representation and the output is human-readable text. This involves content selection, text planning, and surface realization, which includes choosing the right words, using pronouns, and grouping related information. Automatic image description is interesting because it is driven by mature CV and NLP methods and relevant datasets, and a large literature has emerged over the last five years. This survey article aims to provide a comprehensive overview of this literature, covering models, datasets, and evaluation metrics. It is divided into three categories based on the image description models used: direct",https://jair.org/index.php/jair/article/view/10985/26134
Predicting memorability of images using attention-driven spatial pooling and image semantics,"In daily life, humans demonstrate an amazing ability to remember images they see on magazines, commercials, TV, web pages, etc. but automatic prediction of intrinsic memorability of images using computer vision and machine learning techniques has only been investigated very recently. Our goal in this article is to explore the role of visual attention and image semantics in understanding image memorability. In particular, we present an attention-driven spatial pooling strategy and show that considering image features from the salient parts of images improves the results of the previous models. We also investigate different semantic properties of images by carrying out an analysis of a diverse set of recently proposed semantic features which encode meta-level object categories, scene attributes, and invoked feelings. We show that these features which are automatically extracted from images provide memorability predictions as nearly accurate as those derived from human anno- tations. Moreover, our combined model yields results superior to those of state-of-the art fully automatic models.","We humans have an astonishing ability to rapidly perceive and un- derstand complex visual scenes. When exploring parts of a city that we have never visited before, glancing at the pages of a magazine or a newspaper, watching a film on television, or the like, we are constantly bombarded with a vast amount of visual information, yet we are able to process this information and identify certain aspects of the scenes almost effortlessly [1,2]. We also have an exceptional visual memory [3,4] that we can remember particular characteristics of a scene with ease even if we look at it only a few seconds [5]. Here, what is being remembered is considered nothing like an identical representation of the scene itself but the gist of it [6,7]. Although there is no general agree- ment in the literature about the contents of this “gist”, the most common definitions include statistical properties of the scene such as the distributions of basic features like color and orientation, the struc- tural information about the scene layout like the spatial envelope of Torralba and Oliva [8], and the image semantics such as existing objects and their spatial relationships. Interestingly, we can recall some images surprisingly well while some are lost in our minds. Put simply, not all images are equally mem- orable. Isola et al. [9] were the first to carry out a computational study about this phenomenon, the so-called intrinsic memorability of images. They devised a Visual Memory Game experiment and utilized Amazon's Mechanical Turk service to quantify the memorability of 2222 natural images (see Fig. 1). In the course of these experiments, a total of 665 par- ticipants were shown a sequence of images, each of which was displayed for 1 s with a short gap in between image presentations. These subjects were then asked to provide a feedback any time whenever he/she thinks an identical image is displayed. By this setup, a memorability score for each image is calculated by the rate at which the subjects detect a re- peated presentation of it. The authors showed that the memorability of an image is pretty consistent across subjects and under a wide range of contexts, which indicates that image memorability is in fact an intrinsic property of images. In addition, the authors explored the use of different visual features and interestingly showed that the intrin- sic memorability of an image can indeed be estimated reasonably well by a machine. Since that seminal work, there has been only a few works that explore this difficult and interesting problem [10–14]. Our first goal in this study is to explore the role of visual attention in understanding image memorability. We humans use attentional mechanisms to efficiently perform higher level cognitive tasks by focus- ing on small and relevant bits of the visual stimuli. Our intuition is that we are perhaps more likely to remember or forget an image depending on which parts of the image we focus more. To give an example, Fig. 2 illustrates the function of visual attention in selecting important features from images. Suppose that we are exposed to these three natural images, each having different visual contents, i.e. different objects, scene characteristics. Our visual system focuses on certain regions that attract our attention as modeled here by a bottom-up sa- liency model. In this work we propose a visual attention-driven spatial memorability models make use of any attentional mechanisms for fea- ture selection, and only [11,13] use saliency maps but as additional image features. Apart from the global dense image features, some previous studies on image memorability [9–12] have also investigated the use of high- level semantic information about images. They consider objects- related features [9,12], presence of certain object and scene categories [9–11], and their attributes [10], which are all based on manual annota- tions produced by humans. Fig. 3 illustrates some sample images from the MIT memorability dataset along with the semantic features that are manually collected from the human subjects [10]. As illustrated here, an image can be semantically represented in terms of objects, scene information and related attributes. In addition to our attention-driven feature selection strategy, our second focus in this study is to investigate the use of a diverse set of recently proposed semantic features which encode meta-level object categories [20], scene attributes [21], and invoked feelings [22] for predicting image memorability. Compared to the features consid- ered in the former studies [9,10,12], these semantic features can be di- rectly extracted from the images, eliminating the need for manual annotations. Using these features thus decreases the complexity of the prediction process and makes the prediction model to work in a fully automatic manner. Moreover, compared to prior work, these features encode semantic properties of images from a perspective or scale that has not been investigated before. The Meta-class descriptor [20] encodes image semantics based on a hierarchical structure of object categories (concepts) by capturing the relationships among them. The SUN Scene Attributes [21] represents an image by means of responses of a comprehensive list of attribute classifiers that relates to different scene characteristics such as affordances, materials and surface proper- ties. The SentiBank features [22] are the responses of a set of classifiers trained to detect adjective–noun pairs (attributes—objects), and used to associate certain sentiments with images. In order to validate our approach, we performed a series of experi- ments on the MIT memorability dataset. To show the effectiveness of the attention-driven pooling strategy, we used the dense global features employed in [9], namely SIFT [23], HOG [24], SSIM [25] and we analyzed the gain when the features pooled over the salient regions are concatenated to the feature vectors obtained with spatial pyramid pooling [26]. Moreover, regarding our second goal, we performed experiments with the high-level semantic features [20–22] and tested their performances on predicting image memorability. Lastly, we compared our combined model, which uses both semantic features and dense global features pooled over salient regions and spatial pyramids, to the state-of-the-art models in the literature. Our main contributions are: (1) an attention-driven pooling approach to put special emphasis on the interesting parts of the images in the computations, (2) a systematic analysis of a diverse set of semantic features on predicting image memorability, and (3) ex- periments demonstrating that the combination of these ideas provides significant improvement over the existing fully-automatic models.",Using attention-driven spatial pooling and image semantics to forecast image recall,"Humans display remarkable capability to recall images they observe in magazines, commercials, TV, web pages, etc. However, utilization of computer vision and machine learning to automatically forecast intrinsic memorability of images has only recently been studied. Here, we analyze the role of visual attention and image semantics in comprehending image memorability. We propose an attention-driven spatial pooling approach and find that incorporating image features from salient parts of images enhances the results of earlier models. We also examine diverse semantic properties of images through an analysis of semantic features which encode meta-level object categories, scene attributes, and invoked feelings. Results show that these features, extracted automatically from images, provide memorability predictions as accurate as those derived from human annotations. Furthermore, our combined model produces results that surpass state-of-the art fully automatic models.","Humans possess an impressive capability to rapidly perceive and comprehend complex visual scenes. When experiencing parts of a city never before visited, perusing a magazine or newspaper, or viewing a film on television, a great amount of visual data is presented to us, yet we are able to process and recognize particular elements almost immediately [1,2]. Additionally, our visual memory is exceptional [3,4], and we can recall particular characteristics of a scene quickly, even after only a few seconds [5]. It is not an exact representation of the scene that is remembered, but the overall gist [6,7]. Although there is no agreement in literature as to the contents of this ""gist"", commonly accepted definitions comprise statistical properties of the scene such as the distributions of basic features like color and orientation, the structural information about the scene layout like the spatial envelope of Torralba and Oliva [8], and the image semantics such as existing objects and their spatial relationships. Notably, some images are memorized remarkably well, while others are forgotten easily; Isola et al. [9] was the first to explore this phenomenon, referred to as intrinsic memorability of images, computationally. They implemented a Visual Memory Game experiment and utilized Amazon's Mechanical Turk service to evaluate the memorability of 2222 natural images (Fig. 1). 665 participants were shown a sequence of images, each displayed for one second with a short gap between presentations. If a subject thought an identical image was displayed, feedback was provided. This setup allowed for a memorability score for each image to be calculated by the rate of detection of repeated presentations. The authors demonstrated that memorability is consistent across subjects and contexts, signifying it is an intrinsic property of images. Moreover, the use of different visual features was explored, with results suggesting memorability can be estimated by a machine. After this seminal work, few studies have explored this problem [10-14]. The role of visual attention in understanding image memorability is the first objective of this study. Humans use attentional mechanisms to perform higher level cognitive tasks by focusing on small, relevant pieces of the visual stimuli. It is likely that an image is remembered or forgotten based on which parts of the image are focused on more. An example of this is provided in Fig. 2, where a bottom-up saliency model selects important features from three different images. Additionally, the use of high-level semantic information about images is being investigated, including objects-related features [9,12], presence of certain object and scene categories [9-11], and their attributes [10], which are based on manual annotations produced by humans (Fig. 3). To achieve our goals, an attention-driven feature selection strategy and a set of recently proposed semantic features encoding meta-level object categories [20], scene attributes [21], and invoked feelings [22] are used to predict image memorability. Experiments on the MIT memorability dataset validate our approach. To show the effectiveness of the attention-driven pooling strategy, SIFT [23], HOG [24], SSIM [25], and spatial pyramid pooling [26] are used. Furthermore, experiments with the high-level semantic features [20-22] are performed and their performances on predicting image memorability are tested. Lastly, our combined model, which uses both semantic features and dense global features pooled over salient regions and spatial pyramids, is compared to the state-of-the-art models in the literature. Our main contributions are: (1) an attention-driven pooling approach to emphasize the interesting parts of the images in the computations, (2) a systematic analysis of a diverse set of semantic features on predicting image memorability, and (3) experiments demonstrating that the combination of these ideas provides significant improvement over the existing fully-automatic models.",https://reader.elsevier.com/reader/sd/pii/S0262885615000955?token=5C3D2E33C959CA9F333D0C9547E7839F7926550111988560FE8473A63BBDD1CABDB5036B17E554B3DF8CA346CC7D18ED&originRegion=eu-west-1&originCreation=20221228213652
A Distributed Representation Based Query Expansion Approach for Image Captioning,"In this paper, we propose a novel query expansion approach for improving transferbased automatic image captioning. The core idea of our method is to translate the given visual query into a distributional semantics based form, which is generated by the average of the sentence vectors extracted from the captions of images visually similar to the input image. Using three image captioning benchmark datasets, we show that our approach provides more accurate results compared to the state-of-theart data-driven methods in terms of both automatic metrics and subjective evaluation.","Automatic image captioning is a fast growing area of research which lies at the intersection of computer vision and natural language processing and refers to the problem of generating natural language descriptions from images. In the literature, there are a variety of image captioning models that can be categorized into three main groups as summarized below. The first line of approaches attempts to generate novel captions directly from images (Farhadi et al., 2010; Kulkarni et al., 2011; Mitchell et al., 2012). Specifically, they borrow techniques from computer vision such as object detectors and scene/attribute classifiers, exploit their outputs to extract the visual content of the input image and then generate the caption through surface realization. More recently, a particular set of generative approaches have emerged over the last few years, which depends on deep neural networks (Chen and Zitnick., 2015; Karpathy and Fei-Fei, 2015; Xu et al., 2015; Vinyals et al., 2015). In general, these studies combine convolutional neural networks (CNNs) with recurrent neural networks (RNNs) to generate a description for a given image. The studies in the second group aim at learning joint representations of images and captions (Hodosh et al., 2013; Socher et al., 2014; Karpathy et al., 2014). They employ certain machine learning techniques to form a common embedding space for the visual and textual data, and perform crossmodal (image-sentence) retrieval in that intermediate space to accordingly score and rank the pool of captions to find the most proper caption for a given image. The last group of works, on the other hand, follows a data-driven approach and treats image captioning as a caption transfer problem (Ordonez et al., 2011; Kuznetsova et al., 2012; Patterson et al., 2014; Mason and Charniak, 2014). For a given image, these methods first search for visually similar images and then use the captions of the retrieved images to provide a description, which makes them much easier to implement compared to the other two classes of approaches. The success of these data-driven approaches depends directly on the amount of data available and the quality of the retrieval set. Clearly, the image features and the corresponding similarity measures used in retrieval play a significant role here but, as investigated in (Berg et al., 2012), what makes this particularly difficult is that while describing an image humans do not explicitly mention every detail. That is, some parts of an image are more salient than the others. Hence, one also needs to bridge the semantic gap between what is there in the image and what people say when describing it. As a step towards achieving this goal, in this paper, we introduce a novel automatic query expansion approach for image captioning to retrieve semantically more relevant captions. As illustrated in Fig. 1, we swap modalities at our query expansion step and synthesize a new query, based on distributional representations (Baroni and Lenci, 2010; Turney and Pantel, 2010; Mikolov et al., 2013; Pennington et al., 2014) of the captions of the images visually similar to the input image. Through comprehensive experiments over three benchmark datasets, we show that our model improves upon existing methods and produces captions more appropriate to the query image.",Image Captioning with Distributed Representation Query Expansion,"A new query expansion strategy is proposed to enhance transfer-based image captioning. This method translates the visual query into a distributional semantics form, created by the mean of sentence vectors obtained from captions of images similar to the input image. Through 3 benchmark datasets, it is demonstrated that this technique surpasses current data-driven methods in terms of both automated metrics and subjective evaluation.","Automatic image captioning is a research area which combines computer vision and natural language processing. It involves generating natural language descriptions from images. Different image captioning models can be grouped into three main classes. The first type of approaches generate novel captions directly from images, using computer vision techniques such as object detectors and scene/attribute classifiers. A newer set of generative approaches combine convolutional neural networks (CNNs) and recurrent neural networks (RNNs) to generate a description. The second type of works learn joint representations of images and captions. Lastly, the data-driven approach treats image captioning as a caption transfer problem. To bridge the semantic gap between what is in the image and what people say when describing it, a novel automatic query expansion approach for image captioning is introduced. This model improves upon existing methods and produces more appropriate captions for the query image.",https://aclanthology.org/P15-2018.pdf
The State of the Art in HDR Deghosting: A Survey and Evaluation,"Obtaining a high quality high dynamic range (HDR) image in the presence of camera and object movement has been a long-standing challenge. Many methods, known as HDR deghosting algorithms, have been developed over the past ten years to undertake this challenge. Each of these algorithms approaches the deghosting problem from a different perspective, providing solutions with different degrees of complexity, solutions that range from rudimentary heuristics to advanced computer vision techniques. The proposed solutions generally differ in two ways: (1) how to detect ghost regions and (2) what to do to eliminate ghosts. Some algorithms choose to completely discard moving objects giving rise to HDR images which only contain the static regions. Some other algorithms try to find the best image to use for each dynamic region. Yet others try to register moving objects from different images in the spirit of maximizing dynamic range in dynamic regions. Furthermore, each algorithm may introduce different types of artifacts as they aim to eliminate ghosts. These artifacts may come in the form of noise, broken objects, under- and over-exposed regions, and residual ghosting. Given the high volume of studies conducted in this field over the recent years, a comprehensive survey of the state of the art is required. Thus, the first goal of this paper is to provide this survey. Secondly, the large number of algorithms brings about the need to classify them. Thus the second goal of this paper is to propose a taxonomy of deghosting algorithms which can be used to group existing and future algorithms into meaningful classes. Thirdly, the existence of a large number of algorithms brings about the need to evaluate their effectiveness, as each new algorithm claims to outperform its precedents. Therefore, the last goal of this paper is to share the results of a subjective experiment which aims to evaluate various state-of-the-art deghosting algorithms.","The real world encompasses a wide range of luminance values that exceeds the capabilities of most image capture devices. However, in general it is desirable to capture, store, process, and display this wide range of luminance values. The field of HDR imaging is primarily developed to address this problem, that is to bridge the gap between what is available in the real-world in terms of light levels and what we can do to represent it using digital equipment [RWPD10]. The first stage of the HDR imaging pipeline is acquisition. There have been many studies in HDR image and video acquisition, which can be grouped under three categories. The first category consists of the methods that use specialized hardware to directly capture HDR data. The second category consists of the techniques based on reconstructing an HDR image from a set of low dynamic range (LDR) images of the scene with different exposure settings, techniques that are collectively called as multiple exposure methods. The third category consists of the techniques which aim to expand the dynamic range of a normally LDR image – be it through pseudo-multi-exposure or inverse tone mapping [BADC11]. In general, the techniques in the first and third categories produce inherently ghost-free HDR images as they operate on data captured at a single time instance. The techniques in the second category, however, must deal with moving objects as the image capture process takes a longer time due to necessity of capturing multiple exposures. This is due to the fact that the ensuing HDR image reconstruction process simply computes a weighted average of all exposures, resulting in different objects being blended together in case of object movement. The artifacts that occur as a result of such blending are collectively termed as ghosts or ghosting artifacts (see Figure 1). We can formalize this notion as follows (see Table 1 for the terminology used in this survey). Let L(p) represent an LDR image pixel p which is obtained when the corresponding sensor location is exposed to an irradiance E(p) for ∆t units of time (see Table 1): L(p) = f(E(p)·∆t), (1) where f represents the camera response function (CRF) which depends on several factors such as the white balance and gamma correction setting, analog-to-digital conversion parameters, physical characteristics of the sensor, camera manufacturer preferences, etc. If the function f is known, it is possible to recover the correct sensor irradiance from the image pixel intensity using the following relation: E(p) = f −1(L(p)) ∆t . (2) Most of the time, f is not known but can be recovered using various techniques [BK93,MP94,DM97,RBS99,MN99, GN03,OAG13]. Alternatively, the images can be captured in RAW formats which are typically linear (thus f(x) = mx for an easily recoverable slope value, m). Once f is recovered, the HDR value H(p) can be computed as: H(p) = N ∑ n=1 α(Ln(p)) f −1 (Ln(p)) ∆tn N ∑ n=1 α(Ln(p)) , (3) where α is a weighting function which depends on the pixel intensity level. Although one can use a simple triangular weighting function that gives high weights to the center of the intensity range while penalizing the extremes as proposed by Debevec and Malik [DM97], recent research has shown that other parameters such as the camera noise must be taken into account to determine an optimal weighting function [GAW∗10]. The critical assumption of Equation 3 is that all input images L1,...,LN measure the same scene radiance value for each pixel position p: f −1(Ln(p)) ∆tn = f −1(Lm(p)) ∆tm ∀n,m, p. (4) If this assumption, known asreciprocity, does not hold, H(p) will be equal to the weighted sum of different sensor irradiance values, resulting in semi-transparent object appearances known as ghosts. The reciprocity assumption may break down for saturated pixels – a problem that is to be dealt with by using a good α function. The requirement of a pixel measuring the same irradiance in all input exposures necessitates that the camera and the scene remain static throughout the capture process. Because this requirement would severely limit the range of scenes that can be captured using the multiple exposures technique, various solutions have been proposed for dealing with both camera and object movement. It should noted however that, of these two problems, the object movement is much more severe as the former can be avoided by using a tripod or registering the individual exposures prior to HDR image reconstruction. Although this survey discusses both types of methods, the emphasis is especially placed on deghosting algorithms that deal with dynamic objects",Evaluating HDR Deghosting: A Survey,"The difficulty of obtaining a high quality, high dynamic range (HDR) image when camera and object movement is present has been a long-standing issue. To tackle this challenge, numerous HDR deghosting algorithms have been created over the past decade. Each of these algorithms looks at the deghosting problem from a different point of view, providing solutions with various levels of complexity, from simple heuristics to advanced computer vision techniques. The solutions vary in two ways: (1) how to detect ghost regions and (2) what to do to get rid of ghosts. Some algorithms opt to remove moving objects, producing HDR images that only feature static regions. Others try to identify the best image to use for each dynamic region, while others try to register moving objects from various images in order to maximize dynamic range in dynamic regions. Additionally, each algorithm may create various types of artifacts as they work to eliminate ghosts, such as noise, broken objects, under- and over-exposed regions, and residual ghosting. Due to the high number of studies conducted in this field in recent years, an extensive survey of the state of the art is necessary. Therefore, the first goal of this paper is to provide such a survey. The second goal is to propose a taxonomy of deghosting algorithms which can be used to organize existing and future algorithms into meaningful classes. Lastly, the existence of a large number of algorithms leads to the need to assess their effectiveness, as each new algorithm claims to be better than its predecessors. Therefore, the last goal of this paper is to present the results of a subjective experiment that aims to evaluate different state-of-the-art deghosting algorithms.","The real world has a wide range of luminance values that exceed the capabilities of most image capture devices. HDR imaging is developed to bridge this gap between what is available in the real world and what can be digitally represented. The first stage of HDR imaging is acquisition. There are three categories for this: specialized hardware, reconstructing HDR from LDR images with different exposure settings (multi-exposure), and expanding the dynamic range of an LDR image. The first and third categories produce ghost-free HDR images since data is captured at a single time instance. The second category must deal with moving objects as image capture takes longer due to multiple exposures. This results in blending of different objects, creating artifacts called ghosts. Reciprocity must be maintained, meaning the same irradiance must be measured in all exposures, necessitating camera and scene to remain static. Solutions have been proposed for both camera and object movement, with emphasis on deghosting algorithms for dynamic objects.",https://onlinelibrary.wiley.com/doi/pdf/10.1111/cgf.12593
City Scale Image Geolocalization via Dense Scene Alignment,"Predicting where a photo was taken is quite important and yet a challenging task for computer vision algorithms. Our motivation is to solve this difficult problem in a cityscale setting by employing a data-driven approach. In order to pursue this goal, we developed a fast and robust scene matching method that follows a coarse-to-fine strategy. In particular, we combine scene retrieval via global features and dense scene alignment and use a large set of geo-tagged images of downtown San Francisco in our evaluation. The experimental results show that the proposed approach, despite its simplicity, is surprisingly effective and achieves comparable results with the state-of-the-art.","In this paper, we focus on solving the city scale scene geolocalization problem. Image geolocalization and its variants are among the most popular problems investigated by the computer vision community in recent years. With the increase in the amount of visual data and the publicly available datasets, it is now possible to address this problem from a data-driven perspective by matching a given scene with a dataset of geo-tagged images and predicting the location of the query image via the matched scenes. The underlying challenge here is to come up with a technique for detecting the exact location of a scene that is robust in the presence of occlusions, illumination, seasonal and structural changes, and yet efficient in dealing with large data (Figure 1). To this end, we propose an accurate and computationally efficient method that both benefits from global image representations and dense scene matching techniques for fast image retrieval in a city scale setting. The proposed method follows a simple yet effective coarse-to-fine approach. Using the global and local cues exist in the query image, i.e. shapes, colors, skylines, landscapes, buildings, etc. allows us to make a fairly accurate geolocation estimation through considering the similarity between the query scene and the dataset images at different levels, and accordingly using the geolocation information of the matched scenes. In the literature, there are two common approaches to collect geo-tagged image data. One way is to construct a dataset by performing queries in public photo sharing websites such as Flickr as in [3, 8, 9]. In regard to city-scale geolocalization, the downside of this approach is that the collected data is usually disorganized, unevenly distributed and undersampled to represent a city, and might have noisy GPS tags. Other and more structured way is to collect the data by using surveying vehicles such as the Google Street View cars. This approach offers a good representation for this problem in terms of dense and evenly distributed data. However, such data is usually not publicly available on the web. To the best of our knowledge, the only large-scale datasets publicly available are the ones by Chen et al. [4] (1.06M images) for downtown San Francisco (Figure 2), and by Google [1] (102K images) for downtown and neighboring areas of Pittsburg, PA, Orlando, FL and partially Manhattan, NY. In this work we evaluated our results using the dataset by Chen et al. [4] due to its large coverage (Figure 7). Contributions. The main contribution of this study is to approach the city-scale geolocation problem from a perspective that rely on a coarse-to-fine strategy so that the suggested approach scales up well to very large datasets. More specifically, we propose to refine the predictions based on GIST [14] and Tiny Image [17] global image descriptors with a dense matching technique called deformable spatial pyramid (DSP) matching [10]. While the former step reduces the search space to a large extent, the latter scene alignment step greatly increases the recall rate. Moreover, for each of these steps, we introduce a simple outlier removal procedure to further improve the results. The remaining parts of this paper is as follows. In Section 2 we briefly review the related work. Section 3 gives the detailed description of the problem. Section 4 presents our proposed approach. In Section 5, we evaluate our method and present the experimental results. In Section 6, we conclude the paper with a summary, some discussions on the proposed method and possible directions for future work.",Geolocating Images on a City Level Using Dense Scene Matching,"Solving the difficult task of predicting a photo's location is a challenge for computer vision algorithms. To tackle this problem on a city-scale, we employed a data-driven approach and developed a fast and robust scene matching method with a coarse-to-fine strategy. We tested it on a large set of geo-tagged images from downtown San Francisco, and the results demonstrate that our approach is surprisingly effective, even with its simplicity, and yields comparable performance to the current state-of-the-art.","This paper focuses on solving the city-scale scene geolocalization problem. Recently, computer vision researchers have been investigating image geolocalization and its variants. Now, this problem can be addressed from a data-driven perspective by matching a query image with a dataset of geo-tagged images and predicting the location of the query image. The challenge is to detect the exact location of a scene while being robust to occlusions, illumination, seasonal and structural changes, and efficient when dealing with large data. We propose an accurate and computationally efficient method that uses global image representations and dense scene matching techniques for fast image retrieval in a city scale setting. Our approach follows a coarse-to-fine strategy to refine predictions based on GIST and Tiny Image global image descriptors with a dense matching technique called deformable spatial pyramid (DSP) matching. We also introduce an outlier removal procedure to improve results. In the literature, two common approaches to collect geo-tagged image data exist: one is to construct a dataset by performing queries in public photo sharing websites such as Flickr, and the other is to collect the data using surveying vehicles such as the Google Street View cars. The only large-scale datasets publicly available are the ones by Chen et al. (1.06M images) for downtown San Francisco, and by Google (102K images) for downtown and neighboring areas of Pittsburg, PA, Orlando, FL and partially Manhattan, NY. We evaluate our results using the dataset by Chen et al. due to its large coverage. Our main contribution is to address the city-scale geolocation problem from a perspective that relies on a coarse-to-fine strategy to scale up to very large datasets. We review related work in Section 2, describe the problem in Section 3, present our proposed approach in Section 4, evaluate our method and present experimental results in Section 5, and conclude the paper with a summary, some discussions, and possible directions for future work in Section 6.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=7045956
Image Matting with KL-Divergence Based Sparse Sampling,"Previous sampling-based image matting methods typically rely on certain heuristics in collecting representative samples from known regions, and thus their performance deteriorates if the underlying assumptions are not satisfied. To alleviate this, in this paper we take an entirely new approach and formulate sampling as a sparse subset selection problem where we propose to pick a small set of candidate samples that best explains the unknown pixels. Moreover, we describe a new distance measure for comparing two samples which is based on KL-divergence between the distributions of features extracted in the vicinity of the samples. Using a standard benchmark dataset for image matting, we demonstrate that our approach provides more accurate results compared with the state-of-the-art methods. ","Accurately estimating foreground and background layers of an image plays an important role for many image and video editing applications. In the computer vision literature, this problem is known as image matting or alpha matting, and mathematically, refers to the problem of decomposing a given image I into two layers, the foreground F and the background B, which is defined in accordance with the following linear image composition equation: I = ↵F + (1 − ↵)B (1) where ↵ represents the unknown alpha matte which defines the true opacity of each pixel and whose values lies in [0, 1] with ↵ = 1 denoting a foreground pixel and ↵ = 0 indicating a background pixel. This is a highly ill-posed problem since for each pixel we have only three inputs but seven unknowns (↵ and the RGB values of F and B). The general approach to resolve this issue is to consider a kind of prior knowledge about the foreground and background in form of user scribbles or a trimap to simplify the problem and use the spatial and photometric relations between these known pixels and the unknown ones.Image matting methods can be mainly categorized into two groups: propagation-based methods [23, 10, 16, 15, 3, 22, 11] and sampling-based methods [6, 27, 9, 12, 20, 21, 25, 13]. The first group defines an affinity matrix representing the similarity between pixels and propagate the alpha values of known pixels to the unknown ones. These approaches mostly differ from each other in their propagation strategies or affinity definitions. The latter group, on the other hand, collects color samples from known foreground and background regions to represent the corresponding color distributions and determine the alpha value of an unknown pixel according to its closeness to these distributions. Early examples of sampling-based matting methods [6, 27] fit parametric models to color distributions of foreground and background regions. Difficulties arise, however, when an image contains highly textured areas. Thus, virtually all recent sampling-based approaches [9, 12, 20, 21, 25, 13] consider a non-parametric setting and employ a particular selection criteria to collect a subset of known F and B samples.Then, for each unknown pixel, they search for the best (F, B) pair within the representative samples, and once the best pair is found, the final alpha matte is computed as ↵ˆ = (I − B).(F − B) kF − Bk 2 . (2) The recent sampling-based approaches mentioned above also apply local smoothing as a post-processing step to further improve the quality of the estimated alpha matte. Apart from the two main types of approaches, there are also some hybrid methods which consider a combination of propagation and sampling based formulations [4], or some supervised machine learning based methods which learn proper matting functions from a training set of examples [29]. For a more comprehensive up-to-date survey of image matting methods, we refer the reader to [30, 26]. The matting approach we present in this paper belongs to the group of sampling-based methods that rely on a nonparametric formulation. As will be discussed in more detail in the next section, these methods typically exploit different strategies to gather the representative foreground and back ground samples. Our observation is that all these strategies lack a strong theoretical basis, i.e. they require certain assumptions to hold to capture the true foreground and background colors, and moreover, they fail to adequately utilize the relationship between known and unknown regions. In contrast, our approach offers a more principled way to sampling by casting it as a sparse subset selection problem [7, 8], in which the resulting samples refers to a small subset of known foreground and background pixels that best explains the unknown pixels. In particular, sampling is formulated as a row-sparsity regularized trace minimization problem which solely depends on pairwise dissimilarities between known and unknown pixels, and for that, we propose a new KL-divergence based contextual measure as an efficient alternative to chromatic and spatial distances. ",Sparse Sampling with KL-Divergence for Image Matting,"Sampling-based image matting techniques usually depend on particular heuristics to obtain samples from known areas, leading to poor results when underlying assumptions are not met. To address this, we introduce a new method where sample selection is viewed as a sparse subset selection problem, selecting a small set of samples that best explain unknown pixels. Additionally, we introduce a new distance measure to compare two samples, based on KL-divergence between the distributions of features from the vicinity of the samples. Results from a standard benchmark dataset for image matting show our approach gives more accurate outcomes compared to current state-of-the-art methods.","Estimating foreground and background layers of an image is important for editing applications. This is a difficult problem since only three inputs are available for each pixel, but seven unknowns. Two groups of approaches exist: propagation-based and sampling-based. Propagation-based methods create an affinity matrix and spread alpha values from known pixels to unknown ones. Sampling-based methods collect color samples from known foreground and background regions, and determine the alpha value of an unknown pixel according to its closeness to these distributions. Recent sampling-based approaches use nonparametric settings, and select a subset of known F and B samples. Hybrid approaches also exist, which consider a combination of propagation and sampling based formulations. We present a sampling-based approach that casts sampling as a sparse subset selection problem, and uses a KL-divergence based contextual measure as an efficient alternative to chromatic and spatial distances.",https://openaccess.thecvf.com/content_iccv_2015/papers/Karacan_Image_Matting_With_ICCV_2015_paper.pdf
Top down saliency estimation via superpixel-based discriminative dictionaries,"Predicting where humans look in images has gained significant popularity in recent years. In this work, we present a novel method for learning top-down visual saliency, which is well-suited to locate objects of interest in complex scenes. During training, we jointly learn a superpixel based class-specific dictionary and a Conditional Random Field (CRF). While using such a discriminative dictionary helps to distinguish target objects from the background, performing the computations at the superpixel level allows us to improve accuracy of object localizations. Experimental results on the Graz-02 and PASCAL VOC 2007 datasets show that the proposed approach is able to achieve stateof-the-art results and provides much better saliency maps.","Predicting salient parts of an image that attract attention has recently gained a huge interest in computer vision. This is particularly because such a prediction allows to filter out irrelevant information within an image and to focus on the bits that are really important. In that regard, visual saliency has been used to improve the performance of different computer vision tasks, including image retargeting [1], video compression [18], video summarization [21], object detection [25], object recognition [31, 37], object tracking [7], scene classification [33]. Most works on visual saliency concentrate on predicting human eye fixations (see [5] for a recent survey). However, there is an increasing number of studies that aim at the alternative task of detecting salient objects [3, 6, 20, 23, 24, 27, 38, 39]. These recent methods can be categorized into two groups as bottom-up (e.g. [23, 27, 38]) and top-down (e.g. [3, 20, 24, 39]) approaches according to how they define salient objects. Bottom-up models mostly rely on low-level cues such as intensity, color, texture, etc. and try to localize objects which show distinct characteristics from their surroundings. On the other hand, topdown approaches are task-oriented and look for a target object from a specific category. Hence, they employ appearance characteristics of the object of interest.In this paper, we propose a novel method for top-down saliency estimation to generate category-specific saliency maps. Our approach is inspired in part by the recent dictionarybased top-down saliency approaches [20, 39] and the new superpixel-based bottom-up salient object detection methods [23, 27, 38]. Specifically, both [39] and [20] learn sparse coding based discriminative dictionaries within a CRF framework to model appearance properties of object classes. This results in a better representation of target objects and thus better saliency maps. However, the shortcoming of those approaches is that they are patch-based, i.e they estimate patchwise saliency scores and in turn provide very coarse localizations of the target objects. Towards improving object localizations and obtaining better saliency maps, we suggest a more effective way to represent objects and a more accurate inference mechanism. As shown in Fig. 1, the proposed method provides considerably better top-down saliency maps. We describe our contributions and their relation to closely related previous work in detail in the next two sections – but to summarize, they are as follows: (1) We propose to learn discriminative dictionaries at a superpixel-based level, instead of a patch-based one. We suggest to use the so-called sigma points descriptor [17] to represent superpixels in terms of first and second-order feature statistics. These allow us to encode the visual characteristics of the targets objects in a compact way. (2) Generic bottom-up information about objects has shown to play an important role in object detection (e.g. [4, 28, 36]). We also propose to include such generic objectness cues into our framework to further boost the performance. This gives much better localizations of the salient objects in the images than the previous related models. We evaluate our model on the Graz-02 and PASCAL VOC 2007 benchmark datasets and demonstrate substantial improvements in the estimated saliency maps upon current state-of-the-art methods.",Estimating saliency from top down using superpixel-based discriminative dictionaries.,"In recent years, predicting where humans look in images has become increasingly popular. We present a new approach to top-down visual saliency that is effective for locating objects of interest in complex scenes. Training entails the joint learning of a class-specific superpixel dictionary and a Conditional Random Field (CRF). The discriminative dictionary enables distinguishing between target objects and background, while the superpixel level calculations enhance accuracy of object localizations. Experiments on Graz-02 and PASCAL VOC 2007 datasets demonstrate that the proposed method outperforms existing approaches and produces superior saliency maps.","Recently, there has been a great deal of interest in predicting salient parts of an image that attract attention in computer vision. This is because it allows filtering out irrelevant information and concentrating on the important bits. Visual saliency has been used to enhance the performance of different computer vision tasks, including image retargeting, video compression, video summarization, object detection, object recognition, object tracking, and scene classification. Most research on visual saliency focuses on predicting human eye fixations. However, there is an increasing number of studies that aim to detect salient objects through bottom-up (e.g. intensity, color, texture) and top-down (e.g. appearance characteristics) approaches. This paper proposes a novel method for top-down saliency estimation to generate category-specific saliency maps, which is inspired by the recent dictionary-based top-down saliency approaches and the new superpixel-based bottom-up salient object detection methods. The method suggests to learn discriminative dictionaries at a superpixel-based level and to use the sigma points descriptor to represent superpixels in terms of first and second-order feature statistics. Furthermore, generic objectness cues are included into the framework to further improve the performance. The proposed model is evaluated on the Graz-02 and PASCAL VOC 2007 benchmark datasets and is shown to substantially increase the estimated saliency maps compared to current state-of-the-art methods.",http://www.bmva.org/bmvc/2014/files/paper051.pdf
Structure-Preserving Image Smoothing via Region Covariances,"Recent years have witnessed the emergence of new image smoothing techniques which have provided new insights and raised new questions about the nature of this well-studied problem. Specifically, these models separate a given image into its structure and texture layers by utilizing non-gradient based definitions for edges or special measures that distinguish edges from oscillations. In this study, we propose an alternative yet simple image smoothing approach which depends on covariance matrices of simple image features, aka the region covariances. The use of second order statistics as a patch descriptor allows us to implicitly capture local structure and texture information and makes our approach particularly effective for structure extraction from texture. Our experimental results have shown that the proposed approach leads to better image decompositions as compared to the state-of-the-art methods and preserves prominent edges and shading well. Moreover, we also demonstrate the applicability of our approach on some image editing and manipulation tasks such as image abstraction, texture and detail enhancement, image composition, inverse halftoning and seam carving.","Natural images provide rich visual information about the world we live in, and typically contain various objects organized in a meaningful configuration. For instance, consider the image given in Figure 1, which shows a historical site consisting of highly textured figures on a rocky surface. While our visual system is very successful at extracting the prominent structures beneath the image without getting distracted by the texture, enabling a machine to perform the same task, i.e. decomposing the image into its structure and texture components, poses great challenges. From a computational point of view, image decomposition can be expressed as an estimation problem in which a given image is separated into two components that respectively correspond to coarse and fine scale image details. Historically, Gaussian filter is the oldest and the most commonly used smoothing operator [Witkin 1984; Burt and Adelson 1983]. It provides a linear scale-space representation of an image where the input image is smoothed at a constant rate in all directions. Nonlinear scale-space operators extend linear operators by creating a scale space representation of images that consists of gradually simplified images where some image features such as edges are preserved [Perona and Malik 1990; Rudin et al. 1992; Tomasi and Manduchi 1998; Durand and Dorsey 2002; Buades et al. 2005; Farbman et al. 2008; Xu et al. 2011]. Each of these operators integrates a priori edge information in the smoothing process in a different way, with the aim of extracting or removing certain image details. Edge-preserving smoothing approaches, namely Anisotropic diffusion filter [Perona and Malik 1990], Total Variation model [Rudin et al. 1992], Bilateral filter [Tomasi and Manduchi 1998; Durand and Dorsey 2002], NL-means filter [Buades et al. 2005], WLS filter [Farbman et al. 2008], L0 smoothing [Xu et al. 2011] commonly employ differences in the brightness values or gradient magnitudes as the main cues for edge indicator at an image pixel, and make use of this information to guide the smoothing process. These local contrast-based definition of edges, however, might fail to capture high-frequency components that are related to fine image details or textures. Therefore these approaches can not fully separate textured regions from the main structures as they consider them as part of the structure to be retained during computations. In this paper, we present a novel structure-preserving image smoothing approach which jointly eliminates texture. In the literature, only a few studies tackle this challenging problem of structure extraction from texture [Meyer 2001; Subr et al. 2009; Farbman et al. 2010; Buades et al. 2010; Xu et al. 2012]. In contrast to these previous models, our approach performs a patch-based analysis which depends on second order feature statistics. Specifically, we consider the region covariance matrices [Tuzel et al. 2006] of simple image features such as intensity, color and orientation to estimate the similarity between two image patches within a simple adaptive filtering framework. As demonstrated in Figure 1, the proposed model can effectively eliminate texture without distorting structure. Extracting structure from texture greatly improves the results of many image editing tools. Throughout the paper, we will also demonstrate several image editing and manipulation applications including image abstraction, texture and detail enhancement, image composition, inverse halftoning and seam carving. ",Image Smoothing Preserving Structure through Region Covariances,"Recent years have seen the introduction of new image smoothing techniques which have deepened understanding and raised questions about this long-studied problem. These models separate a given image into its structure and texture layers using non-gradient based definitions for edges and distinguishing features for oscillations. In this study, we propose a straightforward image smoothing approach based on covariance matrices of simple image features, the region covariances. Utilizing second order statistics as a patch descriptor allows us to capture local structure and texture information, making it especially effective for structure extraction from texture. Results demonstrate that our approach surpasses state-of-the-art methods in terms of image decomposition, preserving prominent edges and shading. Additionally, it is applicable to various image editing and manipulation tasks such as image abstraction, texture and detail enhancement, image composition, inverse halftoning and seam carving.","Images of the world we live in are abundant with objects organized in a significant manner. Figure 1 displays an example of this, a historical site composed of intricate figures on a rough surface. While humans can accurately determine the primary elements of an image without getting distracted by texture, it is difficult for machines to accomplish the same task. From a computational standpoint, breaking down an image into its structure and texture components can be regarded as a calculation problem. Gaussian filter is the earliest and most used smoothing operator [Witkin 1984; Burt and Adelson 1983], creating a linear scale-space representation of an image by equalizing the input image in all directions. Nonlinear scale-space operators have also been developed [Perona and Malik 1990; Rudin et al. 1992; Tomasi and Manduchi 1998; Durand and Dorsey 2002; Buades et al. 2005; Farbman et al. 2008; Xu et al. 2011], which incorporate edge information in the smoothing process. Edge-preserving smoothing approaches such as Anisotropic diffusion filter [Perona and Malik 1990], Total Variation model [Rudin et al. 1992], Bilateral filter [Tomasi and Manduchi 1998; Durand and Dorsey 2002], NL-means filter [Buades et al. 2005], WLS filter [Farbman et al. 2008], L0 smoothing [Xu et al. 2011] use brightness values or gradient magnitudes as the main indicators for edge detection at an image pixel, and utilize this data to control the smoothing process. However, these local contrast-based definitions of edges might not be able to detect high-frequency components related to small details or textures. Thus, these methods can not completely distinguish textured regions from the main structures as they consider them part of the structure to be conserved during calculations. This paper introduces a new structure-protecting image smoothing approach that eliminates texture. There are only a few studies concerning this hard task of structure extraction from texture [Meyer 2001; Subr et al. 2009; Farbman et al. 2010; Buades et al. 2010; Xu et al. 2012]. Different from these previous models, our approach implements a patch-based analysis based on second order feature statistics. Specifically, we consider the region covariance matrices [Tuzel et al. 2006] of basic image features such as intensity, color and orientation to estimate the similarity between two image patches within a basic adaptive filtering framework. As seen in Figure 1, the proposed model can successfully get rid of texture without altering structure. By taking out texture, the results of many image editing tools are greatly improved. Throughout the paper, we will also demonstrate multiple image editing and manipulation applications, such as image abstraction, texture and detail enhancement, image composition, inverse halftoning and seam carving.",https://dl.acm.org/doi/pdf/10.1145/2508363.2508403
Visual saliency estimation by integrating features using multiple kernel learning,"In the last few decades, significant achievements have been attained in predicting where humans look at images through different computational models. However, how to determine contributions of different visual features to overall saliency still remains an open problem. To overcome this issue, a recent class of models formulates saliency estimation as a supervised learning problem and accordingly apply machine learning techniques. In this paper, we also address this challenging problem and propose to use multiple kernel learning (MKL) to combine information coming from different feature dimensions and to perform integration at an intermediate level. Besides, we suggest to use responses of a recently proposed filterbank of object detectors, known as ObjectBank, as additional semantic high-level features. Here we show that our MKL-based framework together with the proposed object-specific features provide state-of-the-art performance as compared to SVM or AdaBoost-based saliency models. ","Enabling computers to understand the external world around them has attracted the interest of many researchers in different disciplines. A primary source of inspiration for solving this hard problem is our visual system, and some researcher have come up with elegant solutions which imitate certain mechanisms present in the human visual system. In that respect, computational models of visual attention have gained popularity for the last decades. As being one of the most important characteristics of human vision, attentional mechanisms manage limited processing resources by selecting the most interesting or salient parts of a scene while ignoring other stimuli. Accordingly, they help carry out certain visual tasks such as object recognition and scene understanding with ease. Motivated by the theories of preattentive vision, namely the Feature Integration Theory [1] or the Guided Search Model [2], a large number of computational methods have been proposed in the past decade with the aim of predicting where people are likely to look in images. These studies introduce different saliency models which are grounded in different computational frameworks. Besides helping understanding the human visual system, they also have many applications such as scene classification [3], object recognition [4], and object tracking [5]. In computer vision literature, the interest in visual salience methods dates back to the late 80’s [6,7,8,9,10,11]. As a basic implementation of the approach of Koch and Ullman [6], the Itti-Koch model [12] is considered one of the earliest modern saliency models. It attempts to saliency estimation by computing center-surround differences for a set of features by means of Difference of Gaussians and then linearly combines them into a final saliency map, mimicking the workings of retinal receptive fields. Another popular model is Harel et al.’s the graph-based visual saliency (GBVS) model [13]. It also computes some feature maps at multiple scales but represents them as fully connected graphs, and then estimates the resulting saliency map from these graphs using a graph-theoretic approach. Other alternatives include frequency domain-based approaches [14,15] and probabilistic formulations [16,17,18,19]. Please refer to [20] for a more detailed review. Most of the existing models consider the bottom-up strategy suggested in the Itti’s model [12] in which center-surround differences of various features at multiple scales are computed for each feature dimension and a final saliency map is obtained by combining these individual feature maps in a linear manner after a normalization step. This approach suggests that every visual feature is equally important for saliency estimation so that the integration step can be carried out in a straightforward way. However, how different feature dimensions contribute to the overall saliency is still an open question. In recent years, a number of computational models [21,22,23,24,25] try to overcome the feature integration problem by posing saliency estimation as a supervised learning problem where optimal values are found for different feature maps or a combination of them. In this study, we address the feature integration problem in visual saliency estimation. The proposed model employs a supervised approach to classify image pixels as salient or non-salient. Our main contributions are two-folds. First, we propose to use multiple kernel learning (MKL) paradigm [26] to learn a classifier which combines information coming from multiple sources at an intermediate step and in effect integrates features in a more effective way. More specifically, we employ certain variants of MKL algorithms which provide combination schemes that are more complex than ordinary linear summation and achieve better prediction performances compared to previous learning-based formulations. Second, motivated by an increasing number of studies which suggest that humans generally attend to objects in a scene [27,28,29,30] (See Fig. 1), we suggest to use a recently suggested large filter bank of object detectors called ObjectBank [31] as high-level features for saliency prediction. Here, it is important to note that object-level feature maps have been previously used by some previous models [32,22,33,24,25] but they are limited to only a very small set of hand-picked object classes like faces, pedestrians, or cars",Estimating Visual Saliency Through Feature Combination with Multiple Kernel Learning,"In recent years, predicting human visual attention in images has been achieved through computational models. However, how to measure the influence of various visual features on overall saliency is still unsolved. To tackle this, a recent type of models treats saliency estimation as a supervised learning task and applies machine learning methods. In this article, we use multiple kernel learning (MKL) to integrate data from multiple feature dimensions and conduct integration at an intermediate level. Moreover, we use responses of a recently developed ObjectBank filterbank of object detectors as extra semantic high-level features. Our MKL-based framework combined with the proposed object-specific features show superior performance compared to SVM or AdaBoost-based saliency models.","Investigating how computers can understand the external world has sparked the interest of many researchers from various fields. Visual system is a source of motivation for tackling this difficult problem and some have proposed efficient solutions which imitate certain components of the human visual system. Visual attention, being a vital part of human vision, manages processing resources by choosing the most significant parts of a scene while disregarding other stimuli, thus helping in carrying out tasks such as object recognition and scene understanding with ease. Inspired by the Feature Integration Theory and the Guided Search Model, many computational models have been developed in the last decade to predict where people are likely to look in images. These studies introduce saliency models based on different computational frameworks and have many applications like scene classification, object recognition, and object tracking. The first modern saliency model, the Itti-Koch model, was introduced in the late 80’s and was based on the approach of Koch and Ullman. It computes center-surround differences for a set of features by utilizing Difference of Gaussians and then combines them into a single saliency map, imitating the functioning of retinal receptive fields. The graph-based visual saliency (GBVS) model is another popular model which also computes feature maps at multiple scales but represents them as fully connected graphs, and then estimates the resulting saliency map from these graphs using a graph-theoretic approach. Other alternatives include frequency domain-based approaches and probabilistic formulations. Most of the existing models consider the bottom-up strategy suggested in the Itti’s model in which center-surround differences of various features at multiple scales are computed for each feature dimension and a final saliency map is obtained by combining these individual feature maps in a linear manner after a normalization step. Nevertheless, how different feature dimensions contribute to the overall saliency is still an open question. Recently, a number of computational models have tried to overcome the feature integration problem by making saliency estimation a supervised learning problem where optimal values are found for different feature maps or a combination of them. This study addresses the feature integration problem in visual saliency estimation. The proposed model employs a supervised approach to classify image pixels as salient or non-salient. It utilizes the multiple kernel learning (MKL) paradigm to learn a classifier which combines information coming from multiple sources at an intermediate step and in effect integrates features in a more effective way. Additionally, it utilizes a recently suggested large filter bank of object detectors called ObjectBank as high-level features for saliency prediction, motivated by the increasing number of studies which suggest that humans generally attend to objects in a scene.",https://arxiv.org/pdf/1307.5693.pdf
Visual saliency estimation by nonlinearly integrating features using region covariances,"Abstract  To detect visually salient elements of complex natural scenes, computational bottom-up saliency models commonly examine several feature channels such as color and orientation in parallel. They compute a separate feature map for each channel and then linearly combine these maps to produce a master saliency map. However, only a few studies have investigated how different feature dimensions contribute to the overall visual saliency. We address this integration issue and propose to use covariance matrices of simple image features (known as region covariance descriptors in the computer vision community; Tuzel, Porikli, & Meer, 2006) as meta-features for saliency estimation. As low-dimensional representations of image patches, region covariances capture local image structures better than standard linear filters, but more importantly, they naturally provide nonlinear integration of different features by modeling their correlations. We also show that first-order statistics of features could be easily incorporated to the proposed approach to improve the performance. Our experimental evaluation on several benchmark data sets demonstrate that the proposed approach outperforms the state-of-art models on various tasks including prediction of human eye fixations, salient object detection, and image-retargeting.","A natural scene typically contains many objects of various structures at different scales. This complexity poses a great challenge for our visual system since, with its limited capacity, it has to analyze a vast amount of visual information at any given time. To cope with this information overload, the human visual system has developed attentional mechanisms to select the most relevant (salient) parts of a scene that stand out relative to the other parts. What captures our attention depends on factors relevant to bottom-up or top-down selection processes, or a combination of those. While the bottom-up visual attention is mainly driven by intrinsic low-level properties of a scene, the top-down attention involves high-level visual tasks such as searching for a specific object.  Recent years have seen an increase in the number of computational approaches to visual saliency estimation. Starting from the seminal work by Itti, Koch, and Niebur (1998) most of the proposed saliency models consider a bottom-up strategy in which a saliency map is extracted in a purely data-driven manner by considering center-surround differences (e.g., Gao & Vasconcelos, 2007; Harel, Koch, & Perona, 2007; Seo & Milanfar, 2009). There are also some studies that carry out such computations in the frequency domain (Achanta, Hemami, Estrada, & Susstrunk, 2009; Hou & Zhang, 2007) or make use of natural image statistics (Bruce & Tsotsos, 2006; Zhang, Tong, Marks, Shan, & Cottrell, 2008). Another important line of models integrates low-level cues with some task-specific top-down knowledge such as face and object detectors (Cerf, Harel, Einhaeuser, & Koch, 2007; Goferman, Zelnik-Manor, & Tal, 2010; Judd, Ehinger, Durand, & Torralba, 2009), and global scene context (Torralba, Oliva, Castelhano, & Henderson, 2006) to improve their predictions. Lastly, some recent studies pose saliency estimation as a supervised learning problem (Judd et al., 2009; Liu, Jian Sun, & Shum, 2007; Zhao & Koch, 2011, 2012).  These computational models of visual saliency not only provide important insights into the underlying mechanisms of the human visual system but also have been shown to improve the performances of many computer vision applications such as scene classification (Siagian & Itti, 2007), object recognition (Gao, Han, & Vasconcelos, 2009; Rutishauser, Walther, Koch, & Perona, 2004), object tracking (Butko, Lingyun, Cottrell, & Movellan, 2008), video compression (Wang, Lu, & Bovik, 2003), and image retargeting (Achanta & Susstrunk, 2009; Avidan & Shamir, 2007; Cheng, Zhang, Mitra, Huang, & Hu, 2011; Goferman et al., 2010; Wang, Tai, Sorkine, & Lee, 2008).  In most of the existing bottom-up models, we observe the following basic structure: (a) extract some basic visual features such as color and orientation, (b) investigate feature channels in parallel and extract a feature map for each dimension, and (c) integrate these maps to produce a master saliency map. Here, the most troublesome step is the last step, which is typically carried out by taking the weighted average (linear summation). In this regard, some recent saliency approaches try to overcome the feature integration problem by finding optimal values for the weights in the linear summation of feature maps in a supervised manner (Judd et al., 2009; Liu et al., 2007; Zhao & Koch, 2011). More recently, Zhao and Koch (2012) proposed combining feature maps in a nonlinear way by using AdaBoost learning method.  However, it is important to note that a limited number of studies have addressed how different feature dimensions contribute to the overall saliency (Callaghan, 1989, 1990; Eckstein, Thomas, Palmer, & Shimozaki, 2000; Rosenholtz, 1999, 2001; Rosenholtz, Nagy, & Bell, 2004). For instance, it has been argued that for certain tasks some visual features may become visually more salient than others. In detecting region boundaries, Callaghan (1989, 1990) showed that the human visual system favors color over shape and form. In a related discussion, Eckstein et al. (2000) suggested that the difficulty in searching for conjunctions (Treisman & Gelade, 1980) can be explained by examining the target and the distractors in a high-dimensional visual feature space and by looking into the feature dimensions along which the distractor and the target differ. Similarly, Rosenholtz (1999, 2001) and Rosenholtz et al. (2004) suggested that the covariance of the distractors in a higher dimensional feature space may provide an explanation for the difficulty of searching a target in motion or in different color. ",Estimating visual attention by combining features through nonlinear region covariances.,"Computational bottom-up saliency models usually process several feature channels, e.g. color and orientation, in parallel to detect salient elements of complex natural scenes. Maps for each feature are combined linearly to create a master saliency map; however, few studies have investigated the contribution of different feature dimensions to overall visual saliency. To address this integration issue, we propose using region covariance descriptors (Tuzel, Porikli, & Meer, 2006) as meta-features for saliency estimation. These low-dimensional representations capture local image structures better than linear filters and nonlinearly integrate different features by modeling their correlations. We also show that incorporating first-order statistics of features can further improve performance. Our experiments on several benchmark data sets show our approach outperforms state-of-the-art models for predicting human eye fixations, salient object detection, and image-retargeting.","Complexity of a natural scene, with its multiple objects of various structures and sizes, presents a challenge to our visual system, due to its limited capacity. To manage this information overload, the human visual system has developed attentional mechanisms to select the most relevant parts of the scene. This selection is based on both bottom-up and top-down processes, which involve low-level properties of a scene and high-level tasks such as searching for a specific object. A number of computational approaches to visual saliency estimation have been proposed in recent years, ranging from center-surround differences to natural image statistics and frequency domain. Some models integrate low-level cues with task-specific top-down knowledge and global scene context to improve their predictions. Lastly, saliency estimation has been posed as a supervised learning problem. These models not only provide insights into the mechanisms of the human visual system but also improve the performances of many computer vision applications. Most existing bottom-up models extract basic visual features such as color and orientation, investigate feature channels in parallel and extract a feature map for each dimension, and integrate these maps to produce a master saliency map. Some approaches attempt to overcome the feature integration problem by finding optimal values for the weights in the linear summation of feature maps in a supervised manner or by using AdaBoost learning method. It has been suggested that for certain tasks, some visual features may become visually more salient than others. For instance, it has been shown that the human visual system favors color over shape and form in detecting region boundaries.",https://jov.arvojournals.org/article.aspx?articleid=2121371
Visual Attention-driven Spatial Pooling for Image Memorability,"In daily life, humans demonstrate astounding ability to remember images they see on magazines, commercials, TV, the web and so on, but automatic prediction of intrinsic memorability of images using computer vision and machine learning techniques was not investigated until a few years ago. However, despite these recent advances, none of the available approaches makes use of any attentional mechanism, a fundamental aspect of human vision, which selects relevant image regions for higher-level processing. Our goal in this paper is to explore the role of visual attention in understanding memorability of images. In particular, we present an attention-driven spatial pooling strategy for image memorability and show that the regions estimated by bottom-up and object-level saliency maps are more effective in predicting memorability than considering a fixed spatial pyramid structure as in the previous studies.","We humans have an astonishing ability to rapidly perceive and understand complex visual scenes. When exploring parts of a city that we have never visited before, glancing at the pages of a magazine or a newspaper, watching a film on television or in a movie theatre, or the like, we are constantly bombarded with a vast amount of visual information, yet we are able to process this information and identify certain aspects of the scenes almost effortlessly [25, 27]. Humans also have an exceptional visual memory [29, 3] that we can remember particular characteristics of the scenes with ease even if we look at them only a few seconds [30]. Here, what is being remembered is considered nothing like an identical representation of the scene itself but the gist of it [33, 34]. Although there is no general agreement in the literature about the contents of this “gist”, the most common definitions include statistical properties of the scene such as the distributions of basic features like color and orientation, the structural information about the scene layout like the spatial envelope of Torralba and Oliva [24], and the semantic knowledge about the existing objects and their spatial relationships. It is intuitive that not all images are equally memorable. We can recall some images surprisingly well whereas some are lost in our minds. In [15], Isola et al. carried out the first computational study about understanding the memorability of images using computer vision and machine learning techniques. They quantified the memorability of 2222 photographs (See Figure 1) by performing experiments on Amazon’s Mechanical Turk service to collect the rate at which the workers detect a repeat presentation of the images. Then, they investigated the contributions of several factors to memorability such as simple object statistics, scene semantics and global image features, both of which can be related to the aforementioned definitions of the ‘gist’ of a scene. The authors also showed that the memorability of an image can be estimated reasonably well by a machine. In a follow-up work [14], it was demonstrated that extending the previous framework to incorporate a set of humanunderstandable visual attributes of scenes such as attractiveness, peacefulness, etc. further improves the predictions. In a more recent study, Khosla et al. [20] proposed an algorithm to estimate memorability of local image regions and obtain memorability maps of images. They showed that these local features, when combined with global features, also increase the performance of memorability estimates. As humans, we use attentional mechanisms to filter the flow of sensory information and select only a small portion of the visual stimuli in complex visual scenes for further processing to perform higher level cognitive tasks in an efficient way. Despite the recent advances in understanding image memorability from a computational viewpoint, the available models do not make use of any such attentional mechanism. In this study, we wanted to explore the role of visual attention in understanding intrinsic memorability of images. Specifically, we proposed a visual attention-driven spatial pooling strategy and analyzed its contribution to predicting image memorability in detail. Our approach made use of two complementary feature pooling schemes which are both related to visual attention. First, we investigated selecting features only from the most salient regions of the images determined according to a recently proposed bottom-up visual saliency model [8]. Our second scheme, on the other hand, considers a top-down definition of visual attention and employs an object-centric spatial pooling scheme. Pooling strategies similar to ours have been recently suggested for image and scene classification [26, 9]. Our experimental results demonstrated that memorability predictions can be improved by integrating attentional mechanisms. These results are also in line with a body of research in cognitive sciences which argues that attention plays an important role in understanding natural scenes and enhancing visual memory [34, 12, 11, 4, 13]. Here we should note that the authors of [20] utilized a visual saliency model in their model but they used saliency values as complementary features not as a part of feature pooling. The system diagram of the proposed pooling approach is given in Figure 2. First, dense visual features such as SIFT and HOG are extracted from the input image. These features are then encoded into higher dimensions through vector quantization using a bag of features approach. In the meantime, bottom-up and object-level saliency maps are estimated from the image and then thresholded to obtain both the salient regions and those which possibly containing foreground objects. Next, to form histogram-based visual descriptors the encoded vectors are pooled together over the extracted attention-driven spatial layouts. Finally, these descriptors are concatenated together to generate the final image-level representation for memorability prediction.",Image Memorability Driven by Visual Attention Spatial Pooling,"Humans possess an impressive ability to recall images they observe in everyday life, yet the potential of automatic memorability prediction of images through computer vision and machine learning had not been explored until a few years ago. Despite the advancements, none of the existing methods use an attentional mechanism, a significant part of human vision, to select important image areas for more advanced processing. In this paper, we research the role of visual attention in understanding image memorability. Specifically, we present an attention-driven spatial pooling approach for image memorability and prove that the regions identified by bottom-up and object-level saliency maps are more efficient in predicting memorability than utilizing a fixed spatial pyramid structure, as in the previous studies.","Humans possess remarkable capability to swiftly discern and interpret intricate visual scenes. Exploring unfamiliar cities, leafing through magazines or newspapers, watching films on TV or in a cinema, or the like, we are continuously exposed to a copious amount of visual information, yet are able to process it and determine certain features almost instinctively [25, 27]. Additionally, we have outstanding visual memory [29, 3], enabling us to recall scene characteristics with ease even if viewed for a few seconds [30]. This is not an exact reproduction of the scene, but rather its ""gist"" [33, 34]. The components of this ""gist"" vary, but generally involve statistical elements such as color and orientation distributions, structural information of the scene, like Torralba and Oliva's spatial envelope [24], and semantic knowledge about objects and their spatial relationships. Not all images are equally memorable - some we remember well, while others quickly fade. Isola et al. [15] conducted the initial computational analysis of image memorability, utilizing computer vision and machine learning. They quantified the memorability of 2222 photos (see Figure 1) by testing on Amazon's Mechanical Turk, to ascertain the rate of repeat image detection. They then researched the influence of various factors on memorability, such as basic object statistics, scene semantics, and global image features, all of which are related to the ""gist"" definition. Furthermore, they showed that memorability of an image can be reasonably predicted by a machine. In [14], this was expanded to incorporate human-understandable visual attributes of scenes, such as attractiveness, peacefulness, etc., further improving predictions. Khosla et al. [20] proposed an algorithm to gauge memorability of local image regions, and gain memorability maps of images. When merged with global features, these local features augment memorability estimations. Humans use attentional mechanisms to filter sensory info and select a small part of visual stimuli from complex scenes for further processing, to perform higher level cognitive tasks efficiently. Despite recent progress in understanding image memorability from a computational perspective, available models don't make use of any attentional mechanism. We explored the role of visual attention in comprehending intrinsic memorability of images, proposing a visual attention-driven spatial pooling strategy, and analyzing its contribution to predicting image memorability in detail. Our approach made use of two feature pooling schemes related to visual attention - one based on a bottom-up visual saliency model [8], and the other employing an object-centric spatial pooling scheme. Our experiments demonstrated that memorability predictions can be improved by integrating attentional mechanisms, in line with cognitive sciences research arguing attention's importance in understanding natural scenes and enhancing visual memory [34, 12, 11, 4, 13]. Our system diagram is given in Figure 2. We first extract dense visual features such as SIFT and HOG from the image, and encode them into higher dimensions through vector quantization using a bag of features approach. We then estimate bottom-up and object-level saliency maps, and threshold them to identify salient regions and those likely containing foreground objects. To form histogram-based visual descriptors, we pool encoded vectors over the extracted attention-driven spatial layouts, and concatenate the descriptors to generate the final image-level representation for memorability prediction.",https://www.cv-foundation.org/openaccess/content_cvpr_workshops_2013/W23/papers/Celikkale_Visual_Attention-Driven_Spatial_2013_CVPR_paper.pdf
Fragments based tracking with adaptive cue integration,"In this paper, we address the issue of part-based tracking by proposing a new fragments-based tracker. The proposed tracker enhances the recently suggested FragTrack algorithm to employ an adaptive cue integration scheme. This is done by embedding the original tracker into a particle filter framework, asso- ciating a reliability value to each fragment that describes a different part of the target object and dynam- ically adjusting these reliabilities at each frame with respect to the current context. Particularly, the vote of each fragment contributes to the joint tracking result according to its reliability, and this allows us to achieve a better accuracy in handling partial occlusions and pose changes while preserving and even improving the efficiency of the original tracker. In order to demonstrate the performance and the effec- tiveness of the proposed algorithm we present qualitative and quantitative results on a number of chal- lenging video sequences","Tracking an object in an accurate way is essential for applica- tions like activity analysis, man–machine interaction and visual surveillance. However, for many real-world problems, the ambigu- ities inherent to the visual data and the tracking process make it difficult to develop accurate, robust and efficient trackers. During the tracking process, usually, the target object becomes occluded by the other objects in the scene, its pose or appearance undergoes some changes, or the lighting conditions vary. A common solution to these issues is to use complementary observations/cues from different sources. Within such a strategy, each cue provides a likelihood or a matching score for the possible positions of the object, and the tracker determines the final output by the product of individual likelihoods or the summation of the matching scores. This highly improves the tracking performance. In the literature, such tracking frameworks are named multi-cue trackers. Multi-cue trackers require to use visual cues that are orthogonal to each other as much as possible so that if one cue fails, the other cue or cues can compensate its deficiency. There are mainly two ways to obtain such orthogonal cues [1]. One possibility is to con- sider visual cues that express different features of the target object.An alternative solution is to consider a single visual feature and to use it to describe different sections of the target object within a single framework. We provide a sample illustration of approaches to multi-cue tracking in Fig. 1. The trackers in the first line of works determine the target posi- tion at each frame by combining cues that are based on several dif- ferent visual features [2–4] (Fig. 1a). For example, in one of early studies [2], Birchfield suggested to track heads using intensity gra- dients and color histograms together. The critical point with this approach is that during tracking the cues do not always provide reliable information about the target object. Giving equal impor- tance to all the features in determining the combined result gener- ally leads to false outcomes. Triesch and von der Malsburg [5] proposed a dynamic framework called the Democratic Integration to adaptively integrate different cues, addressing this problem. In their framework, each cue has an adaptive reliability value associ- ated with it, and each cue contributes to the joint result according to its reliability. A number of studies, e.g. [1,6,7], follow such a strategy by performing an adaptive integration of cues to boost the accuracy of the tracking process. Considering a single feature and using it to describe different sections of the target object via an explicit part-based model is an- other strategy (Fig. 1b-i). For instance, in human trackers a human can often be subdivided into parts corresponding to head, limbs and torso [1,8]. Splitting an object into parts introduces a kind of supplementary shape information regarding the target object by providing the relative spatial arrangements of different object sections. This offers an important advantage over the classical region-based trackers where the content of the region of interest is modeled with a single histogram with the loss of spatial information. This kind of part-based trackers mostly aims at track- ing articulated/non-rigid objects (e.g., [8,9]), and generally requires the model of the target object to be known or given a priori. Although one can easily devise such models for domain-specific trackers (e.g. for faces or humans) and thus make use of strong prior information, it is not convenient to use this approach for gen- eric object tracking. The recently proposed robust fragments-based tracker called FragTrack [10] mainly tackled this issue. In that study, the target object is again represented with multiple image fragments or patches each describing a different object section, but these patches are extracted arbitrarily without considering any reference object model (Fig. 1b-ii). This makes the suggested part-based tracking algorithm applicable to any object without changing the representation. Particularly, at each frame of the se- quence, different object sections vote on possible target locations and scales, and the tracker combines these votes using a robust statistics scheme to obtain an outcome by performing an exhaus- tive search on this combined vote map. Another line of works that also tackles model-free tracking in- cludes boosting-based tracking frameworks [11–14], and it has drawn quite a bit of attention lately. Like in the FragTrack, in these works the tracking is cast as a sequential detection problem but the detection is performed via on-line learning of an object-specific classifier using boosting. The common approach is to represent the target object by a set of Haar-like features and to define the strong classifier which serves as the object detector as an adaptive combination of several weak classifiers that are selected from the most discriminative features in the current set. The aforemen- tioned multi-cue tracking strategies are summarized in Table 1. In this paper, we also address the issue of part-based object tracking. Our proposed model-free tracker combines the Frag- Track’s arbitrary-fragments based object representation [10] and the concept of adaptive multi-cue integration [5] (Fig. 2). Our main contributions can be summarized as follows: Our algorithm employs an adaptive cue integration scheme [5]. A reliability value is associated to each fragment that describes a different part of the target object and it is dynamically adjusted at each frame with respect to the current context. The vote of each fragment contributes to the joint tracking result according to its reliability, and the ones having low values have little effect on the outcome. This allows us to achieve a better tracking accuracy in handling partial occlusions and pose changes. Note that FragTrack does not assign reliabilities to fragments and does not adaptively integrate the corresponding cues.  Our adaptive cue integration scheme allows us to make infer- ences about the current status of the target object as well, through the fragments and the reliabilities associated with them. The dynamic reliability maps present the most informa- tive fragments at each frame according to the current context, and for example might provide simultaneous information on the visible and occluded object sections.Tracking is realized by means of a particle filter-based frame- work [15–17], which enables the adaptive cue integration scheme to be derived, and which additionally provides an increased computational efficiency as exhaustive search on a combined vote map is no more required. It also allows us to eas- ily include the scale information into the object state and gives a more natural way to estimate it.  We present experimental results on challenging tracking sequences, which show that the adaptive formulation proposed in this paper results in a more accurate, efficient and robust tracking than the FragTrack and it competes and even outper- forms the recently proposed boosting-based trackers [11,12,14] that use online appearance learning mechanisms. The remainder of the paper is organized as follows: We begin with a brief summary of the FragTrack algorithm in Section 2. It is followed, in Section 3, by the description of the particle filter. In Section 4, we introduce our fragments-based tracking algorithm with an adaptive integration scheme, which is the main contribu- tion of this paper. Following this, in Section 5, we present our experimental results and discuss different aspects of the proposed framework",Tracking with Adaptive Cue Combination Using Fragments,"In this paper, we propose a new fragments-based tracker that addresses the issue of part-based tracking. This tracker, called FragTrack, is enhanced with an adaptive cue integration scheme embedded into a particle filter framework. Each fragment, which describes a different part of the target object, is associated with a reliability value that is dynamically adjusted in each frame with respect to the current context. Consequently, each fragment's vote contributes to the joint tracking result based on its reliability, thus improving accuracy in handling partial occlusions and pose changes while maintaining the efficiency of the original tracker. To illustrate the performance and effectiveness of the proposed algorithm, qualitative and quantitative results are presented on various challenging video sequences.","Accurate tracking of an object is essential for applications such as activity analysis, man–machine interaction, and visual surveillance. However, ambiguities within the visual data and the tracking process make it difficult to create precise, reliable, and effective trackers. During tracking, the target object can be occluded by other objects, its pose or appearance may change, or the lighting conditions may vary. A common solution to these issues is to use complementary observations/cues from different sources. This strategy involves each cue providing a likelihood or a matching score for the possible positions of the object, and the tracker determining the final output by the product or summation of individual likelihoods/matching scores, improving the tracking performance. Multi-cue trackers need to use visual cues that are as orthogonal to each other as possible, so if one cue fails, the other can compensate. There are two ways to achieve this: (1) considering visual cues that express different features of the target object, or (2) using a single visual feature to describe different sections of the target object. An example of these approaches is provided in Figure 1. The first line of works combines cues based on multiple visual features (Fig. 1a). Triesch and von der Malsburg proposed a dynamic framework called the Democratic Integration to adaptively integrate different cues, each with an adaptive reliability value associated with it, contributing to the joint result according to its reliability. Other studies followed this strategy. The second line of works uses a single feature to describe different sections of the target object via an explicit part-based model (Fig. 1b-i). FragTrack tackled this issue by representing the target object with multiple image fragments or patches, extracted arbitrarily without considering any reference object model (Fig. 1b-ii). Boosting-based trackers are another line of works that also deals with model-free tracking (Fig. 1b-iii). In this paper, we propose a model-free tracker which combines the FragTrack's arbitrary-fragments based object representation and the concept of adaptive multi-cue integration (Fig. 2). Our adaptive cue integration scheme allows us to make inferences about the current status of the target object, through the fragments and the reliabilities associated with them. We use a particle filter-based framework which enables the adaptive cue integration scheme to be derived, and which provides an increased computational efficiency. We present experimental results on challenging tracking sequences, showing that our adaptive formulation results in more accurate, efficient, and robust tracking than FragTrack, and it competes and even outperforms boosting-based trackers.",https://reader.elsevier.com/reader/sd/pii/S1077314212000525?token=F6E5BB5D9A3167B38D450CC29210DBB5C2E9C4B80E77BB7DB7953F883FF8A736298791FB3AD007998D3D563BCCEF170E&originRegion=eu-west-1&originCreation=20221229134332
Visual tracking by fusing multiple cues with context-sensitive reliabilities,"Many researchers argue that fusing multiple cues increases the reliability and robustness of visual tracking. However, how the multi-cue integration is realized during tracking is still an open issue. In this work, we present a novel data fusion approach for multi-cue tracking using particle filter. Our method differs from previous approaches in a number of ways. First, we carry out the integration of cues both in making predictions about the target object and in verifying them through observations. Our second and more significant contribution is that both stages of integration directly depend on the dynamically changing reliabilities of visual cues. These two aspects of our method allow the tracker to easily adapt itself to the changes in the context, and accordingly improve the tracking accuracy by resolving the ambiguities","Visual tracking is a widely studied topic in computer vision for a wide range of application areas. These include visual surveillance, activity analysis, man–machine interaction, augmented reality, etc. Here we consider the task of locating an object of interest on each frame of a given video sequence. This object of interest can be an actual object in the scene, e.g. a person, or a specific image region of prime importance, e.g. a face. For real-world applications, it is generally accepted that tracking based on a single visual feature would be likely to fail due to the complex nature of the data and the tracking process. Thus, it has been argued in many works that considering multi-modal data leads to an improvement in tracking. It increases the robustness by letting complementary observations from different sources work together. These sources are either the visual features extracted from the same image sequence, such as color and motion cues, or the visual cues coming from different physical sensors, such as from a CCD or from an infrared camera. However, how the information extracted from these sources is combined in tracking is still an open problem.",Fusing Cues with Context-Aware Reliabilities for Visual Tracking,"Many researchers believe combining multiple cues boosts reliability and robustness of visual tracking; yet, how this integration is realized during tracking is still unknown. In this work, we propose a new data fusion approach for multi-cue tracking using particle filter. It differs from previous approaches in two ways. First, integration of cues is used to make predictions and verify observations. Second, and more importantly, both integration stages depend on dynamically changing reliabilities of visual cues. These two aspects of our method allow the tracker to adjust to context changes, thus improving tracking accuracy by resolving ambiguities.","Tracking an object of interest on each frame of a video sequence is a widely studied topic in computer vision, applied to areas such as visual surveillance, activity analysis, man–machine interaction, and augmented reality. As the data and tracking process are complex, it is widely accepted that tracking based on a single visual feature is likely to fail. Thus, multi-modal data has been suggested to improve robustness by allowing complementary observations from various sources to work together. These sources can be the visual features extracted from the same image sequence, such as color and motion cues, or visual cues from different physical sensors, such as from a CCD or an infrared camera. Nevertheless, combining information from these sources for tracking is still an open problem.",https://reader.elsevier.com/reader/sd/pii/S0031320311004596?token=191153CF5A14296FBB2C6941035B392755F652F62008BD20EB312E72B1E966D10CDF7063AB6437851DB2A5DA643E4669&originRegion=eu-west-1&originCreation=20221229134459
SEGMENTATION USING THE EDGE STRENGTH FUNCTION AS A SHAPE PRIOR WITHIN A LOCAL DEFORMATION MODEL,"This paper presents a new image segmentation framework which employs a shape prior in the form of an edge strength function to introduce a higher-level influence on the segmentation process. We formulate segmentation as the minimization of three coupled functionals, respectively, defining three processes: prior-guided segmentation, shape feature extraction and local deformation estimation. Particularly, the shape feature extraction process is in charge of estimating an edge strength function from the evolving object region. The local deformation estimation process uses this function to determine a meaningful correspondence between a given prior and the evolving object region, and the deformation map estimated in return supervises the segmentation by enforcing the evolving object boundary towards the prior shape.","The goal of segmentation is partitioning an image into coherent regions that are likely to correspond to objects which are imaged. Finding region boundaries accurately becomes particularly challenging when the corrupting influences due to missing regions, partial occlusions and noise appear in images. Recent works, including [5, 6, 7, 8, 11, 13, 16], resolve these ambiguities by integrating lowlevel image features with high-level shape information. With the exception of [7], these works represent prior shape globally. In [7], Hong et al. present an alternative formulation which takes into account a local deformation model to constrain the shape of the evolving contour. The model captures a different appearance of the object of interest with ease by accordingly warping the reference shape. This, in a certain extent, provides an advantage over the global models. In this paper, we employ the local deformation model of Hong et al. [7] in a new framework for prior-guided image segmentation. Specifically, our work differs from [7] in two aspects. The first difference is the way we formulate the segmentation energies. It is expressed as the minimization of three coupled functionals defining three processes that respectively account for prior-guided segmentation, shape feature extraction and local deformation estimation. The second difference is the way we represent the shape itself. As the experimental results demonstrate, our framework not only extracts the object of interest from an image but also simultaneously registers the prior shape on the image data, allowing further semantic analysis to be performed on the extracted object. ",Edge Strength Function as Shape Prior in Local Deformation Model for Segmentation,"A new image segmentation framework is presented which incorporates an edge strength function as a shape prior to introduce higher-level control over the segmentation process. The segmentation is formulated as the minimization of three functionals, each representing a distinct process: prior-guided segmentation, shape feature extraction, and local deformation estimation. The shape feature extraction process estimates an edge strength function from the object region while the local deformation estimation process utilizes the function to find a correspondence between the prior and the object region, the deformation map then guiding the segmentation by guiding the object boundary to the prior shape.","The goal of segmentation is to divide an image into parts that likely represent objects. It can be difficult to find boundaries when there are missing regions, obstructions, or noise. Recent works, including [5, 6, 7, 8, 11, 13, 16], tackle this problem by combining low-level image features with high-level shape info. Except for [7], these works use global shape models. Hong et al. [7] use a local deformation model which warps the reference shape to represent the object's different appearances. This paper proposes a new prior-guided segmentation framework which is different from [7] in two ways: 1) it minimizes three coupled functionals to account for prior-guided segmentation, shape feature extraction, and local deformation estimation; 2) it represents the shape differently. Experiments show that our framework not only extracts the object but also registers the prior shape on the image data, allowing further semantic analysis.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=5414504
Mumford-Shah Regularizer with Contextual Feedback,"We present a simple and robust feature preserving image regularization by letting local region measures modulate the diffusivity. The purpose of this modulation is to disambiguate low level cues in early vision. We interpret the Ambrosio-Tortorelli approximation of the Mumford-Shah model as a system with modulatory feedback and utilize this interpretation to integrate high level information into the regularization process. The method does not require any prior model or learning; the high level information is extracted from local regions and fed back to the regularization step. An important characteristic of the method is that both negative and positive feedback can be simultaneously used without creating oscillations. Experiments performed with both gray and color natural images demonstrate the potential of the method under difficult noise types, non-uniform contrast, existence of multi-scale patterns and textures.","The prevalent view in computer vision since Marr [52] is that the early vision is a data-driven and bottom-up process. A sequence of processes transforms low-level cues into larger perceptual units as the raw image data is processed in a feed-forward fashion. Over the years, a variety of techniques has been developed using this paradigm. Most of these techniques cannot easily cope with ambiguities since low-level cues are not always reliable and any misinterpretation in the early stages significantly affects the later stages. For example, consider edge detection from a noisy image. When a low-pass denoising filter is applied to the image, the filter not only removes the noise but also blurs intensity discontinuities making the edges difficult to detect. If the cut-off frequency of the low-pass filter is set to a large value to reduce the blurring, then one cannot eliminate noise. The noise which is passed to the second level in the hierarchy is enhanced more than the signal itself [38] by the edge detection process. About twenty years ago, the dilemma in the one way information flow between smoothing and edge detection processes, prompted eminent researchers including Mumford and Shah to propose methods that integrate these two processes [14, 40, 55, 58]. The formulation of Mumford and Shah [55] is based on a functional minimization via which a piecewise smooth approximation of a given image and an edge set are recovered simultaneously. The goal is to decompose an image into cartoon and noise components. The Mumford-Shah (MS) model is: EMS(u,Γ ) = β  R (u − g)2dx + α  R\Γ |∇u| 2dx + length(Γ ) (1) where R ⊂ _x0004_2 is a connected, bounded, open subset representing the image domain, – g is an image defined on R, – Γ ⊂ R is the edge set segmenting R, – u is the piecewise smooth approximation of g, – α, β are the scale space parameters of the model. The first term in EMS(u,Γ ) is the data fidelity term which forces the solution u to be close to the original image g. The other two terms are the regularization terms that force u to be piecewise smooth with simple edge set [54]. The unknown edge set Γ of a lower dimension makes the minimization difficult. A convenient approximation is suggested by Ambrosio and Tortorelli [4] following the Γ convergence framework [15]. The basic idea is to introduce a smooth edge indicator function v which is more convenient than using the characteristic function χΓ as the edge indicator. The function v depends on a parameter ρ, and as ρ → 0, v → 1 − χΓ . That is, v(x) ≈ 0 if x ∈ Γ and v(x) ≈ 1 otherwise. The cardinality of the edge set Γ can be approximated by 1 2  R  ρ|∇v| 2 + (1 − v)2 ρ _x0004_ dx. The new functional with the Ambrosio and Tortorelli (AT) approximation becomes EAT (u,v) =  R  β(u − g)2 + α(v2|∇u| 2) + 1 2  ρ|∇v| 2 + (1 − v)2 ρ _x0004__x0004_dx. (2) The most appealing property of the AT approximation is that the condition for minima is easily obtained in the form of a pair of coupled PDEs: ∂u ∂t =∇· (v2∇u) − β α (u − g); ∂u ∂n _x0005_ _x0005_ _x0005_ _x0005_ ∂R = 0, (3) ∂v ∂t = ∇2v − 2α|∇u| 2v ρ − (v − 1) ρ2 ; ∂v ∂n _x0005_ _x0005_ _x0005_ _x0005_ ∂R = 0 (4) where ∂R denotes the boundary of R and n denotes the outer unit normal vector to ∂R. Equations (3) and (4) can be simultaneously solved for u and v using standard numerical discretization techniques such as Finite Differences. When these equations are discretized using a modified explicit scheme (Appendix A), the iterations converge in the sense that the rate of change is smaller than a threshold. In each iteration, only one variable is updated while the other variable is kept fixed. Keeping v fixed, (3) minimizes a convex quadratic functional given by  R (αv2|∇u| 2 + β(u − g)2)dx. (5) While the bias term in (5) forces u to be close to the original image g, the first term acts as an edge preserving regularizer by smoothing the image with a smoothing radius proportional to the values of v2 and α β . If there is an edge (v ≈ 0), no smoothing (diffusion) is carried out. As v → 0, the smoothness constraint in the piecewise smooth model is switched off. It is possible to interpret v2 as an analog form of the line process introduced by Geman and Geman [40]. As shown by Bar, Kiryati and Sochen [7] and Teboul et al. [77], the AT approximation of the MS functional defines an extended line process regularization where the regularizer has an additional constraint introduced by the term ρ|∇v| 2. This term forces some spatial organization by requiring that the edges to be smooth. However, the reconstruction results are still affected by a heavy noise or a texture (Fig. 1). Over the years, a variety of modifications to the AT model were proposed. Shah [71] considered replacing the quadratic cost functions in both the data fidelity and the regularizer with L1-functions. Erdem, Erdem and Tari [35] considered incorporating prior shape information into the AT by introducing additional terms that, at the same time, make the minimization difficult. Esedoglu and Shen [37] considered incorporating higher order geometric terms in the length functional in order to improve the regularity of the edge sets found by the MS. An extreme modification to the AT has been proposed by Tari, Shah and Pien [76], by using large ρ values and interpreting v as a smoothed distance function from which a skeletal analysis is performed. Recently, Aslan and Tari [5] considered letting ρ → ∞. Whereas the method in [5] can only handle silhouettes, the one in [76] can handle grayscale images. In this paper, we propose a simple yet effective modification to the AT that converts it to a regularizer with much better feature preserving capabilities without resorting to additional energies, prior models and difficult to minimize cost functions. The new model can cope with difficult noise types such as impulse noise and preserve structures even in highly textured images. Interesting non-linear and non-local behaviors arise even though the computations are carried out in a locally linear fashion. Key to our approach is the link between regularization and diffusion filters [13, 54, 56, 67, 70]. After casting the AT approximation as a biased diffusion filter, we use local measures to steer the diffusion. The local measures are computed from a collection of local neighborhoods that are not necessarily centered on the pixel to be regularized. These measures are referred as contextual feedback measures or simply feedback measures throughout the paper.",Mumford-Shah Reg. w/ Contextual Reflection,"We propose a robust, feature-preserving image regularization that modulates diffusivity using local region measures. This disambiguates low-level cues in early vision. The Ambrosio-Tortorelli approximation of the Mumford-Shah model is interpreted as a system with modulatory feedback, which allows us to integrate high-level information into the regularization process. The method does not need prior models or learning; local regions provide the high-level information which is fed back to the regularization. Our method has the advantage of using both negative and positive feedback simultaneously, without oscillations. Experiments on gray and color natural images demonstrate the potential of our method for dealing with difficult noise types, non-uniform contrast, multi-scale patterns, and textures.","Since Marr [52], it has been widely accepted that early vision is a data-driven and bottom-up process, which involves transforming low-level cues into larger perceptual units as raw image data is processed in a feed-forward manner. Many techniques have been developed using this approach, yet they cannot deal with ambiguities due to unreliable low-level cues, leading to misinterpretation in the earlier stages that affects the later ones. For example, when an image is denoised by a low-pass filter, the edges become blurred and hard to detect, while if the filter's cut-off frequency is increased to reduce blurring, noise remains. In response to this one-way information flow between smoothing and edge detection processes, Mumford and Shah proposed a method to integrate these two processes [14, 40, 55, 58]. Their formulation, based on a functional minimization, finds a piecewise smooth approximation of the image and its edge set simultaneously, aiming to decompose the image into cartoon and noise components. This is done through the Mumford-Shah (MS) model, which consists of three terms: a data fidelity term to force the solution to be close to the original image, and two regularization terms to make the solution piecewise smooth with a simple edge set. The Ambrosio and Tortorelli (AT) approximation makes the minimization easier by introducing a smooth edge indicator function v, which approximates the characteristic function χΓ . This allows the cardinality of the edge set Γ to be approximated by a new functional, and the condition for minima is obtained in the form of a pair of coupled PDEs. These equations can be solved numerically via discretization techniques, resulting in iterations that converge in the sense that the rate of change is smaller than a threshold. As shown by Bar, Kiryati and Sochen [7] and Teboul et al. [77], the AT approximation of the MS functional defines an extended line process regularization with an additional constraint. However, the reconstruction results are still affected by noise or texture. Subsequently, a variety of modifications to the AT model were proposed, such as replacing the quadratic cost functions with L1-functions, incorporating prior shape information, incorporating higher order geometric terms, and interpreting v as a smoothed distance function. Most recently, Aslan and Tari [5] considered letting ρ → ∞, though this can only handle silhouettes. Here, we propose a modification to the AT that makes it a regularizer with better feature preserving capabilities, without resorting to additional energies, prior models and difficult to minimize cost functions. The new model can cope with difficult noise types such as impulse noise and preserve structures even in highly textured images. The approach is based on the link between regularization and diffusion filters, wherein local measures are used to steer the diffusion, computed from a collection of local neighborhoods not necessarily centered on the pixel to be regularized.",https://link.springer.com/content/pdf/10.1007/s10851-008-0109-y.pdf
Disconnected Skeleton: Shape at Its Absolute Scale,"We present a new skeletal representation along with a matching framework to address the deformable shape recognition problem. The disconnectedness arises as a result of excessive regularization that we use to describe a shape at an attainably coarse scale. Our motivation is to rely on the stable properties of the shape instead of inaccurately measured secondary details. The new representation does not suffer from the common instability problems of traditional connected skeletons and the matching process gives quite successful results on a diverse database of 2D shapes. An important difference of our approach from the conventional use of the skeleton is that we replace the local coordinate frame with a global euclidean frame supported by additional mechanisms to handle articulations and local boundary deformations. As a result, we can produce descriptions that are sensitive to any combination of changes in scale, position, orientation, and articulation, as well as invariant ones.","LOCAL symmetry axis-based representations, commonly referred to as shape skeletons, have been used in generic shape representation since the pioneering work by Blum [10] on the study of form via axis morphology. The strength of axis-based representations lies in expressing the links among shape primitives and providing a shape centered coordinate frame. Blum’s skeleton can be explained using three alternative constructions. The original one is to initiate fire fronts at time t ¼ 0 along all of the points of the shape boundary and to let these fronts propagate as wave fronts toward the center of the shape at uniform speed (Fig. 1a). The locus of shock points, where these wave fronts would intersect and extinguish, defines the skeleton. An important property of the skeleton is its ability to reconstruct the shape boundary by propagating the wave fronts backward. The second construction replaces the dynamic view of propagating fronts with a static view by embedding time-dependent wave fronts as the level curves of a surface whose value at each point is the minimum distance of the point to the boundary (Fig. 1b). Skeleton points are the ones that are equidistant from at least two boundary points. They coincide with the shock points. They are the sharp points of the level curves of the distance function. It is possible to detect these points by projecting the ridges of the distance function on the shape plane. The third construction is via maximal circles that are inscribable inside the shape and touch the shape boundary at more than one point. In this construction, circle radius plays the role of time of arrival in the symmetry axis function. Considering the envelope of maximal circles, one can reconstruct the shape boundary (Fig. 1c). Independently of Blum’s morphological analysis, mathematical morphology has been developed as a set theoretic approach to shape [36], [46]. Following Lantuejoul’s work [28], which showed that the skeleton can be constructed by set transformations, a significant amount of skeletonization work developed within the mathematical morphology community. Basic set transformations that are applied to shapes with the help of structuring elements are called erosions and dilations (Fig. 1d). Note that, when the structuring element is chosen as a disk of diameter d, the eroded shape boundary becomes equivalent to the wave front at t ¼ d. Eroding the shape first and dilating it later with the diskstructuring element removes the sharp corners that correspond to the shock points of the wave model. Although all of these constructions lead to the same representation, they inspired many others having different properties and being computed with different algorithms, for example, [12], [13], [20], [23], [26], [27], [31], [32], [35], [39], [44], [52], [58], [60], [61]. There is a connection between the positive curvature maxima of the shape boundary and the skeleton in the sense that each curvature maximum gives rise to a skeletal branch [32]. This connection is often used to extract the skeleton. When the skeleton is extended to include branches that arise from curvature minima, a richer set, commonly referred to as local symmetry axes, is obtained [12]. Note that the term symmetry as used in this context is different from conventional symmetry and is a property of points generated by the shape rather than being a property of shapes themselves [10], [56]. A major reason for having alternative skeleton constructions is that the skeleton is an unstable representation in the sense that a small change in the shape may cause a significant change in its description. Difficulties associated with a robust implementation are the major source of a significant body of research dedicated to regularization and richer definitions of local symmetry. Even though there are interesting ideas in recent works [20], [52], [60], pruning of the axes has been mostly used to simplify or regularize the skeleton. In general, pruning methods define a saliency measure for the axis points and discard those points whose significance is below a threshold. Axis length, propagation velocity, maximal thickness, and the ratio of the axis to the boundary it unfolds are the most typical significance measures; however, they do not necessarily reflect the perceptual prominence of parts [51]. Interestingly, as also observed in [5], [49], [59], [60], while the accurate computation of the skeleton branches corresponding to noise and secondary details is difficult, the ones corresponding to ribbon-like parts of the shape with slowly varying width can be accurately determined with any method. This observation motivates us to propose a new skeletal representation by computing the local symmetries only at the locations where they can be accurately determined. Unlike conventional skeletons, our skeleton is disconnected. The disconnectedness is not a result of pruning according to some measure, as in Shah [49], but it is a result of excessive regularization added to local symmetry extraction. In that respect, our work is closely related to multiscale skeleton ideas despite the fact that we have a single shape dependent scale which is as coarse as possible yet permits a morphological analysis. We refer to this scale as the absolute scale. The rest of the introduction is divided into two parts. In the first part, multiscale skeleton ideas from the literature are reviewed. In the second part, an overview of our approach and its connections to the previous multiscale ideas is given. ",Structure of Minimal Proportions,"We propose a new skeletal representation and a matching framework to address deformable shape recognition. We regularize the shape at a coarse scale to focus on its stable properties instead of inaccurately measured secondary details. Our representation is immune to instability problems of connected skeletons, and the matching process yields successful results on a diverse 2D shape database. Instead of local coordinates, our approach utilizes a global Euclidean frame with additional mechanisms for articulations and boundary deformations. Consequently, we can generate descriptions that are sensitive to any combination of changes in scale, position, orientation, and articulation, as well as invariant ones.","Shape skeletons, also known as local symmetry axis-based representations, have been used in generic shape representation since Blum's pioneering work [10] on form study via axis morphology. These representations have the advantage of expressing shape primitives links and providing a shape centered coordinate frame. Blum's skeleton can be explained using three constructions: 1) fire fronts initiation and propagation to the shape center at uniform speed; 2) embedding wave fronts as the level curves of a distance function surface; and 3) maximal circles inscribable inside the shape. Mathematical morphology has also been developed as a set theoretic approach to shape [36], [46], which shows that the skeleton can be constructed by set transformations. Pruning methods are typically used to simplify or regularize the skeleton by defining a saliency measure for axis points. Our new skeletal representation is based on local symmetries computation only at locations where they can be accurately determined, leading to a disconnected skeleton. The introduction is divided into two parts: a review of multiscale skeleton ideas from literature, and an overview of our approach and its connections to previous multiscale ideas.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=4415271
Mumford-Shah Regularizer with Spatial Coherence,"As recently discussed by Bar, Kiryati, and Sochen in [3], the Ambrosio-Tortorelli approximation of the Mumford-Shah functional defines an extended line process regularization where the regularizer has an additional constraint introduced by the term ρ|∇v| . This term mildly forces some spatial organization by demanding that the edges are smooth. However, it does not force spatial coherence such as edge direction compatibility or edge connectivity, as in the traditional edge detectors such as Canny. Using the connection between regularization and diffusion filters, we incorporate further spatial structure into the regularization process of the Mumford-Shah model. The new model combines smoothing, edge detection and edge linking steps of the traditional approach to boundary detection. Importance of spatial coherence is best observed if the image noise is salt and pepper like. Proposed approach is able to deal with difficult noise cases without using non-smooth cost functions such as  in the data fidelity or regularizer. ","Mumford and Shah [13] formulated image segmentation process as a functional minimization via which a piecewise smooth approximation of a given image and an edge set are to be recovered simultaneously.  The first term in EMS(u, Γ) is the data fidelity term, which forces the solution u to be as close as to the original image g. The other two terms are the regularization terms, which give preference to piecewise smooth images with simple edge sets. The unknown edge set Γ makes the minimization mathematically difficult. A convenient approximation is suggested by Ambrosio and Tortorelli [2] following the Γ convergence framework [7]. The basic idea is to introduce a smooth edge indicator function v which is more convenient than the original edge indicator represented by the characteristic function 1−χΓ . The function v depends on a parameter ρ, and as ρ → 0, v → 1−χΓ . That is, v(x) ≈ 0 if x ∈ Γ and v(x) ≈ 1 otherwise. Moreover, the cardinality of the edge set Γ can be approximated by Notice that as v → 0, the smoothness constraint in the piecewise smooth model is switched off. It is possible to interpret v 2 as an analog form of the line process introduced by Geman and Geman [12]. As shown by Bar et al. [3] and Teboul et al. [18], the Ambrosio-Tortorelli approximation of the Mumford Shah functional defines an extended line process regularization where the regularizer has an additional constraint introduced by the term ρ|∇v| . This term mildly forces some spatial organization by demanding that the edges are smooth. However, it does not force spatial coherence such as edge direction compatibility or edge connectivity. On the other hand, in the traditional approach, segmentation is defined as a sequential bottom-up process composed of the following three steps: – smoothing, – edge detection, – edge linking. The purpose of the last step is to force global consistency to locally detected edges in order to come up with a coherent edge set. Interestingly, this last step is what Mumford-Shah model or its Ambrosio-Tortorelli approximation lacks. The importance of spatial coherence can be best observed when the image contains impulse noise (Fig 1). Some works use Mumford-Shah regularizer or its modification [17, 1] for restoration in the presence of impulse noise. In [3, 4], Bar et al. present a very promising approach. However, the success of their method stems mostly from the use of robust data fidelity by replacing L 2 norm with L. cost, which leads to singular diffusivity. Numerical difficulties are the cons of singular diffusivities [9]. The cost function choice in [18] also leads to directional smoothing. As explored by Weickert [19], directional smoothing can offer significant feature preserving  capabilities. However, the models get complicated and numerics is not as simple as in the case of isotropic diffusion. In this work, we propose a modification to the Ambrosio-Tortorelli approximation of the Mumford-Shah functional, turning it into an edge-preserving regularization with spatial coherence. Key to our approach is the link between edge preserving regularization and diffusion filters [15, 16, 6]. Proposed model is a set of coupled linear diffusion equations. Hence, it is easy to implement. We experimentally demonstrate denoising and edge preserving abilities of the proposed method. It can handle impulse noise and fill boundary gaps. Moreover, it can produce sharper results. Smoothed images obtained by the proposed method are qualitatively comparable to that are obtained by singular diffusion equations [9]. In the next section, we review the Ambrosio Tortorelli approximation and analyze it’s behavior relevant to our developments given in § 3. We present and discuss experimental results in § 4. Finally, § 5 is the summary and the conclusion.",Mumford-Shah with Spatial Coherence Regularization,"Bar, Kiryati, and Sochen [3] discussed the Ambrosio-Tortorelli approximation of the Mumford-Shah functional, which introduces an extra constraint to the regularizer in the form of ρ|∇v|, thus mildly encouraging spatial organization. However, it does not enforce coherence such as edge direction compatibility or edge connectivity as seen in traditional edge detectors such as Canny. To incorporate further spatial structure into the regularization process of the Mumford-Shah model, we established a connection between regularization and diffusion filters. The new model combines smoothing, edge detection and edge linking steps of the traditional approach for boundary detection, particularly when the image noise is salt and pepper like. This approach is able to handle difficult noise cases without using non-smooth cost functions in the data fidelity or regularizer.","Mumford and Shah [13] formulated image segmentation as a functional minimization to simultaneously recover a piecewise smooth approximation of a given image and an edge set. The first term in EMS(u, Γ) forces the solution u to be close to the original image g, while the other two terms give preference to piecewise smooth images with simple edge sets. Ambrosio and Tortorelli [2] proposed a convenient approximation of the unknown edge set Γ, introducing a smooth edge indicator function v which depends on a parameter ρ. As ρ → 0, v → 1−χΓ, meaning v(x) ≈ 0 if x ∈ Γ and v(x) ≈ 1 otherwise, with the cardinality of the edge set Γ approximated by . When v → 0, the smoothness constraint in the piecewise smooth model is switched off, similar to the line process introduced by Geman and Geman [12]. Bar et al. [3] and Teboul et al. [18] showed that the Ambrosio-Tortorelli approximation of the Mumford Shah functional defines an extended line process regularization with an additional constraint imposed by the term ρ|∇v|. This enforces some spatial organization but does not ensure edge direction compatibility or edge connectivity. To this end, the traditional approach uses a sequential bottom-up process composed of smoothing, edge detection, and edge linking. In this work, we propose a modification to the Ambrosio-Tortorelli approximation of the Mumford-Shah functional, turning it into an edge-preserving regularization with spatial coherence. This is linked to diffusion filters [15, 16, 6], and is easy to implement. We experimentally demonstrate denoising and edge preserving abilities of the proposed method, which can handle impulse noise and fill boundary gaps, producing sharper results than those obtained by singular diffusivities [9]. We review the Ambrosio Tortorelli approximation and analyze its behavior relevant to our developments in the next section, and present and discuss experimental results in § 4 before concluding in § 5.",https://web.cs.hacettepe.edu.tr/~erkut/publications/MSSpCoherence.pdf
Articulation Prior in an Axial Representation,"Local symmetry axis based schemes have been used for generic shape recognition as they lead to articulation insensitive representations. Despite their strengths, purely syntactic level of axial representations precludes the possibility of distinguishing a likely articulation from an unlikely one. In order to overcome this weakness, syntax should be combined with pragmatics and/or semantics. As a solution we propose a novel articulation space which enables inferences on the likelihood of possible articulations. Articulation priors can be constructed directly from examples (pragmatics) or set externally (semantics). We incorporate articulation priors to a skeletal matching scheme to arrive at an enriched axial representation which is sensitive to unlikely articulations but insensitive to likely ones","Generic shape recognition demands representations which are robust with respect to local deformations and articulations as well as transformations which arise due to viewpoint changes. Different representations and matching schemes which are invariant or insensitive to articulations and bendings were presented in the literature [1–9]. The schemes based on local symmetry axis have been particularly considered due to their ability to capture the shape structure. Despite their strengths, presemantic and purely syntactic representations used in these techniques fail to distinguish a likely articulation from an unlikely one. Such a situation is demonstrated in Figure 1: Instances of two shapes share a similar change in local symmetry structure, but based on our past knowledge, the shape given in Figure 1(d) should not be matched with the one given in Figure 1(c) and not be considered as a fork. Therefore, a mechanism to represent possible variations on the symmetry branches and infer them from data becomes essential. In this work, we propose articulation space as a new representation space in which similar articulations and bendings yield closer coordinates. A point in the articulation space is deprived of shape information and global deformation such as affine transformation and represents only a local posture assumed by a section of the shape. We demonstrate that it is possible to build articulation priors and make inferences on this space. These inferences can be combined with spatial layout models such as skeletal matching to arrive at a shape recognition scheme which is insensitive to likely articulations and sensitive to unlikely ones, thus articulation space provides a mechanism to combine syntax with pragmatics. An important feature of our method is that the part information is utilized without explicit computation of parts or joints. Even though we start with the coarsest representation with one level of part hierarchy representing just the limbs of a main body, existence of ignored joints such as an elbow leads to special patterns in the articulation space. Main inspiration for our work comes from two sources. First one is the landmark based shape analysis of Kendall [10] and Bookstein [11], which has been adopted by Perona et. al. [12] to design a recognition scheme by considering the relative spatial arrangement of shape sections. Our goal is completely different from that of Perona et.al. [12]. They use Kendall’s idea to filter out global transformations in order to capture shape. On the other hand we filter out the shape information in addition to global transformations to capture the articulations. Second inspiration comes from a recent local symmetry formulation of Aslan et.al. [8] which computes, in a naive sense, the coarsest representation of a shape with one level of hierarchy. Main appeal of this representation is that it allows very robust and easy reference point extraction and makes the presented ideas practically possible. Previously in [4] deformations on two primitive shapes (the circle and the worm) are considered. A linear shape space is constructed from examples with the help of principle component analysis. Principal component analysis is also used in [13] to learn a linear shape space which captures the global deformations of the medial axis. Articulation priors are considered particularly in applied problems involving motion and tracking [14]. Pose configurations are represented as data determined manifolds embedded in high dimensional measurement spaces [14, 15]. The paper is organized as follows: In Section 2 we review local symmetry representation presented in [8] and discuss how candidate deformable shape sections are identified from it. In Section 3 we explain representation of articulations in a special coordinate system which is used to form the articulation space. In Section 4 we explore the structure of articulation space. In Section 5 we give some illustrative examples demonstrating some inferences on this space. Finally, we conclude with the summary and future work. ",Axial Representation's Precedence,"Axial representations, while providing articulation-insensitive representations for generic shape recognition, are unable to distinguish between likely and unlikely articulations. To address this issue, we propose a novel articulation space which allows inferences on the probability of possible articulations. Articulation priors can be either derived from examples or set externally, and are incorporated into a skeletal matching scheme to create an enriched axial representation that is sensitive to unlikely articulations yet insensitive to likely ones.","Generic shape recognition requires representations robust to local deformations and transformations arising from viewpoint changes. Literature [1-9] presented invariant/insensitive matching schemes based on local symmetry axis, able to capture shape structure. Yet, these presemantic/syntactic representations fail to differentiate between likely and unlikely articulations, e.g. in Figure 1(d) and 1(c). Therefore, a mechanism to represent possible variations on the symmetry branches and infer them from data is essential. We propose articulation space as a new representation space, from which similar articulations/bendings yield closer coordinates. It is deprived of shape information and global deformation, representing only a local posture. We demonstrate it is possible to build articulation priors and make inferences. These can be combined with spatial layout models such as skeletal matching to arrive at a shape recognition scheme, sensitive to unlikely articulations. Our method utilizes part info without explicit computation of parts/joints, and inspiration comes from Kendall/Bookstein's landmark based shape analysis [10, 11], adopted by Perona et.al. [12], and Aslan et.al.'s local symmetry formulation [8]. Deformations on two primitive shapes were considered in [4], while PCA was used in [13] to learn a linear shape space capturing global deformations of the medial axis. Articulation priors are considered in applied problems involving motion/tracking [14]. Pose configurations are represented as data determined manifolds embedded in high dimensional measurement spaces [14, 15]. The paper is organized as follows: Section 2 reviews local symmetry rep. presented in [8] and discusses candidate deformable shape sections. Section 3 explains representation of articulations in a special coordinate system for articulation space. Section 4 explores its structure, Section 5 provides illustrative examples, and Section 6 concludes with summary/future work.",https://web.cs.hacettepe.edu.tr/~erkut/publications/ArtPrior.pdf
Edge Strength Functions as Shape Priors in Image Segmentation,"Many applications of computer vision requires segmenting out of an object of interest from a given image. Motivated by unlevel-sets formulation of Raviv, Kiryati and Sochen [8] and statistical formulation of Leventon, Grimson and Faugeras [6], we present a new image segmentation method which accounts for prior shape information. Our method depends on Ambrosio-Tortorelli approximation of Mumford-Shah functional. The prior shape is represented by a by-product of this functional, a smooth edge indicator function, known as the “edge strength function”, which provides a distance-like surface for the shape boundary. Our method can handle arbitrary deformations due to shape variability as well as plane Euclidean transformations. The method is also robust with respect to noise and missing parts. Furthermore, this formulation does not require simple closed curves as in a typical level set formulation.","In many vision applications, one searches an object of interest whose pose may vary and whose shape may exhibit variability. Deliniating the object boundary correctly and estimating the pose becomes particularly challenging when corrupting influences due to missing regions and noise appear. As a remedy, use of prior shape models are considered. During the last five years, quite interesting works addressing shape prior integration directly into segmentation process appeared [4–6, 8, 9, 11]. These methods differ in terms of – How they represent shape prior; – Boundary detection rule which forms the backbone; – Extension of the boundary detection rule to allow the influence of the prior; – Computation. In Cremers et.al. [5], a variational framework is used for the integration of shape statistics and segmentation. Shape boundaries are represented explicitly as spline curves. Aligned shape variability is captured by a Gaussian distribution model whose mean and covariance matrix are computed from a group of splines. A shape energy that maximizes the shape probability is combined with MumfordShah [7] segmentation energy and minimized by applying gradient descent. In Leventon et.al. [6], a shape boundary is embedded as the zero level curve of a level set function (distance function). Shape prior is represented via coefficients of the principal components computed from a group of distance functions whose zero-levels correspond to the various appearances of a shape of interest. A two step procedure is employed. First, the level set function is evolved such that its zero level curve converges to the shape boundary. Second, pose and shape variables are computed via MAP estimation on the Gaussian probability space. In Tsai et.al. [11], a shape based curve evolution technique is proposed. The implicit shape representation proposed by Leventon et.al. [6] is embedded into region-based active contour models as in [3]. Again a two step procedure is employed. However, pose and shape variables are computed by applying gradient descent. In Chen et. al. [4], prior shape is represented by the average of aligned contours. A shape term which measures the similarity between evolving and prior contours is added into a variational active contours framework. Hence, evolution of the active contour is controlled by a force which depends on both image gradients and prior shape. In Rousson et.al. [9], a probabilistic approach to generate shape priors using level set representations which can also handle local shape variability is proposed. From a set of training samples, represented as level sets, a probability density function is constructed by maximum likelihood estimation. A shape energy is defined and incorporated into a level set based segmentation method depending on the proposed model. In Raviv et.al. [8], shape variability is ignored. Quite elegant formulation is obtained by utilizing a generalized cone whose cross sections are the various appearances of a given object under pose changes. This cone also functions as a level set function which evolves according to a constraint derived from the prior. The cost function is an extension of Chan-Vese approximation [3] of MumfordShah functional with a shape prior term. Inspired by Raviv et.al. [8] and Leventon et.al. [6], we present a new method for shape prior incorporation into segmentation process. Backbone of our method is Ambrosio-Tortorelli [1] approximation (AT) of Mumford-Shah functional. We employ a by-product of this functional, a smooth edge indicator function which is known as the “edge strength function” as a distance-like surface which embeds the shape boundary. Shape similarity term which is a normalized difference between “deformed” shape prior and the evolving edge strength function is added to Ambrosio-Tortorelli functional. The edge strength function has some nice properties which makes it a quite versatile tool for different vision tasks. Despite its shortcomings as a segmentation tool, it has been proven to be quite useful in capturing essential shape characteristics [10] and recently applied to object recognition very successfully [2]. An interesting property of the edge strength function is that it encodes the local symmetry information [10]. This makes possible the integration of boundary and local symmetry information and design shape energies which will force morphological equivalence. As an example, it is possible to change the shape en- ergy by simply adding a weight that is proportional to local symmetry strength. Furthermore, the embedding provided by the edge strength function does not require simple closed curves as in a typical level set formulation. The paper is organized as follows. Section 2 is on Ambrosio-Tortorelli functional. Representation of a set of prior shapes is explained in Section 3. In Section 4, the combined energy and its minimization is discussed. Experimental results are presented in Section 5. Finally, Section 6 is the summary. ",Shape Priors in Image Segmentation Utilizing Edge Strength,"Motivated by the unlevel-sets formulation of Raviv, Kiryati and Sochen [8] and the statistical formulation of Leventon, Grimson and Faugeras [6], a new image segmentation method has been proposed which accounts for prior shape information. This method relies on the Ambrosio-Tortorelli approximation of the Mumford-Shah functional and uses the “edge strength function”, a smooth edge indicator function, as a distance-like surface for the shape boundary. This method is capable of handling arbitrary deformations due to shape variability as well as plane Euclidean transformations, while being robust with respect to noise and missing parts. Additionally, this formulation does not require simple closed curves as in a typical level set formulation.","In vision applications, the challenge of finding an object of interest with varying pose and shape is addressed by using prior shape models. In the past five years, various methods have been proposed to integrate shape priors into segmentation process, varying in representation, boundary detection rule, extension of the rule to allow for prior influence, and computation. Cremers et.al. [5] utilize a variational framework with spline curves to represent shape boundaries, combined with a shape energy that maximizes shape probability and MumfordShah [7] segmentation energy. Leventon et.al. [6] embed shape boundary as the zero level curve of a level set function, and use coefficients of the principal components to represent prior shape. Tsai et.al. [11] propose a shape based curve evolution technique, incorporating a level set function into region-based active contour models. Chen et.al. [4] add a shape term to a variational active contours framework, which measures similarity between evolving and prior contours. Rousson et.al. [9] construct a probability density function from a set of training samples, represented as level sets, to generate shape priors. Raviv et.al. [8] use a generalized cone to represent shape variability and extend Chan-Vese approximation [3] of MumfordShah functional with a shape prior term. Our method is inspired by Raviv et.al. [8] and Leventon et.al. [6], employing Ambrosio-Tortorelli [1] approximation (AT) of Mumford-Shah functional, with a shape similarity term being the normalized difference between ""deformed"" shape prior and the evolving edge strength function. The edge strength function encodes local symmetry information [10], allowing for the integration of boundary and local symmetry information and design of shape energies that will force morphological equivalence. Experiments are presented in Section 5 and the paper is summarized in Section 6.",https://aykuterdem.github.io/papers/eet05.pdf
Vision-based continuous Graffit-like text entry system,"It is now possible to design real-time, low-cost computer vision systems even in personal computers due to the recent advances in electronics and the computer industry. Due to this reason, it is feasible to develop computer-vision-based human-computer interaction systems. A vision-based continuous Graffiti™-like text entry system is presented. The user sketches characters in a Graffiti™-like alphabet in a continuous manner on a flat surface using a laser pointer. The beam of the laser pointer is tracked on the image sequences captured by a camera, and the corresponding written word is recognized from the extracted trace of the laser beam.","We address the problem of entering ASCII text into a wearable computer or a mobile communication device. Mobile communication and computing devices currently have tiny keyboards that are not easy to use. Furthermore, such keyboards occupy a large part of the screen in tablet computers and touch screen systems. Computer vision may provide alternative, flexible, and versatile ways for humans to communicate with computers. In this approach, the key idea is to monitor the actions of the user by a camera and interpret them in real time. For example, character recognition techniques developed in document analysis1–3 can be used to recognize handwriting or sketching. In a previous study by Ozer et al.,1 a vision-based system for recognizing isolated characters is developed, where users draw with a pointer or a stylus on a flat surface or the forearm of a person. The user’s actions are captured by a head-mounted camera. To achieve very high recognition rates, characters are restricted to a single-stroke alphabet, like the Graffiti™ alphabet. The Graffiti™ alphabet was first developed by Xerox Corp. and nowadays its variants are used in many handheld computers. We develop a vision-based continuous Graffiti™-like text entry system as an extension of Ref. 1. In this system, instead of drawing isolated characters, the user sketches the Graffiti™ alphabet in a continuous manner on his or her left arm or on a flat surface using a pointer, stylus, or a finger. In this approach, the alphabet is also based on the Graffiti™ alphabet. However, some letters of the Graffiti™ alphabet have to be modified to increase recognition accuracy. By restricting the alphabet to Graffiti™-like characters, very high recognition rates can be achieved. The proposed continuous Graffiti™ recognition system can be incorporated into a presentation system as well. In many large auditoriums, the computer containing the presentation material is not on the stage. It is usually very difficult for the speaker to jump to previous or future slides or to extract another document from the computer. The user can mark some keywords or slides before the presentation. During the presentation, he or she can write the keyword on the screen using the laser pointer, and then the system brings the premarked slide or the requested document to the screen. The organization of the work is as follows: In Sec. 2, the basics of the overall text entry system are presented. The details of tracking and recognition phases are described in Secs. 3 and 4, respectively. The experimental results are given in Sec. 5. The work concludes with Sec. 6, in which the presented study is discussed and future work is stated.",Continuous text input system based on vision similar to Graffiti,"Recent advancements in electronics and the computer industry have enabled the creation of low-cost, real-time computer vision systems on personal computers. This has made it possible to build human-computer interaction systems based on computer vision. An example of this is a vision-based continuous Graffiti™-like text entry system. This system allows the user to sketch characters in a Graffiti™-like alphabet using a laser pointer on a flat surface. The camera captures image sequences of the laser beam and the corresponding written word is then recognized from the trace of the laser beam.","We tackle the issue of entering ASCII text into wearable computers and mobile communication devices, which have small, difficult-to-use keyboards that take up much of the screen in tablet computers and touch screen systems. Computer vision may offer alternative, flexible, and versatile ways for humans to interact with computers, monitoring user actions by a camera and interpreting them in real time. Ozer et al.1 developed a vision-based system for recognizing individual characters, where users draw with a pointer or stylus on a flat surface or the forearm of a person, with the actions captured by a head-mounted camera. To ensure high recognition rates, characters are restricted to a single-stroke alphabet, such as the Graffiti™ alphabet. This study expands on Ref. 1 by developing a vision-based continuous Graffiti™-like text entry system, where the user sketches the Graffiti™ alphabet in a continuous manner on his or her left arm or on a flat surface using a pointer, stylus, or a finger. The alphabet is based on the Graffiti™ alphabet but some letters are modified to increase recognition accuracy. The proposed system can also be incorporated into a presentation system, allowing the user to mark keywords or slides before the presentation, then writing the keyword on the screen using the laser pointer, with the premarked slide or requested document brought to the screen. Section 2 outlines the basics of the overall text entry system, with details of tracking and recognition phases described in Sections 3 and 4, respectively. Experimental results are provided in Section 5, followed by discussion and future work in Section 6.",http://repository.bilkent.edu.tr/bitstream/handle/11693/24309/Vision-based%20continuous%20Graffiti%E2%84%A2-like%20text%20entry%20system.pdf?sequence=1
COMPUTER VISlON BASED UNISTROKE KEYBOARD SYSTEM AND MOUSE FOR THE HANDlCAPPED,"In this paper, a unistroke keyboard based on computer vision is described for the handicapped. The keyboard can be made of paper or fabric containing an image of a keyboard, which has an upside down U-shape. It can even be displayed on a computer screen. Each character is represented by a non-overlapping rectangular region on the keyboard image and the user enters a character by illuminating a character region with a laser pointer. The keyboard image is monitored by a camera and illuminated key locations are recognized. During the text entry process the user neither have to tum the laser light off nor raise the laser light from the keyboard. A disabled person who bas difficulty using hisiher hands may attach the laser pointer to an eyeglass and easily enter text by moving hisher head to point the laser beam on a character location. In addition, a mouse-like device can be developed based on the same principle. The user can move the cursor by moving the laser light on the computer screen which is monitored by a camera. ","In this paper, a unistroke keyboard and a mouse-like system based on computer vision is described for the handicapped. The system is developed for people with Quadriplegia, Cerebral Palsy, Multiple Sclerosis, Muscular Dystrophy, ALS, Carpal Tunnel Syndrome and any other disability where the user has little or no control of their hands to use a standard mouse or a keyboard. Speech recognition based systems may partially solve the text entry problem in some languages but cannot provide a solution for the mouse. In many agglunative languages including Turkish spoken by more than 200 million people there are no large vocabulary speech recognition systems. Computer vision may provide altemative, flexible and versatile ways for humans to communicate with computers. In this approach the key idea is to monitor the actions of the user by a camera and interpret them in realtime. For example, character recognition techniques developed in document analysis [4,6,12] can he used to recognize hand-writing or sketching. In [4] we developed a vision based system for recognizing isolated characters drawn by a stylus or a laser pointer on a flat surface or the forearm of a person. The user's actions are captured by a head mounted camera. To achieve very high recogniton rates characters are restricted to a single stroke alphabet as in Graffiti in [4]. The beam of the laser pointer is detected in each image of the video and characters are recognized from the trace of the laser beam. The concept of computer vision based regular QWERTY type keyboards is independently proposed by us [I31 and Zhang et.al.[7]. In this system, a character is entered to the computer if its location on the keyboard image is covered by a finger. In this approach the keyboard is a passive device. Therefore it can be made out of paper, fabric or foldable plastic having an image of a QWERTY or other type of regular size keyboard. It can even be displayed on a computer screen. The current version of this keyboard system cannot handle 10-finger typing. , Unistroke keyboards provide a good trade off between 10- finger typing and continuous handwriting recognition. In this paper, we present a computer vision based unistroke keyboard, which is based on a soft keyboard system called Cimn (the CIRculaR INput device) developed in [14]. The Cimn was designed for tablet computers and the user draws one stroke per word on a touch-sensitive screen. The key locations are placed circularly and to enter a word the user traces out a path determined by the characters forming the word using a stylus. Whenever the stylus enters a key location the corresponding character is recognized by the tablet computer. In this paper, we place our key locations on a U-like curve or an upside down Ulike curve or instead of a circle as shown in Figure 1. The advantage of this layout design is that the keyboard image can be displayed on the computer screen and whenever the beam of the laser pointer leaves the keyboard area it is treated as a mouse and the cursor is moved towards the direction of the beam. Similarly, the keyboard becomes active when the beam of the laser pointer enters the keyboard area. This keyboard system is designed for disabled people who have difficulty using their hands and ordinary keyboards and mice. The keyboard image is displayed on the lower part of the screen and a USB web camera is placed in front of the monitor to capture the movement of the laser beam as shown in Figure 2. As pointed out above the user can attach a laser pointer to an eyeglass and control the beam of the laser pointer by moving his or her head. In this way, an inexpensive system is realized for entering text to a computer because there is no need to have a special purpose hardware to realize this keyboard system which is implemented in software. Whereas in commercial head-mouse and keyboard systems, an optical sensor is used to track a small reflective target dot worn on one’s forehead or eyeglasses [SI. This special purpose sensor increases the cost of the system. On the other hand the cost of an ordinary red laser pointer and a USB web camera is about 50 US dollars. In the next section, the video processing algorithm to realize this keyboard and pointing system is described. In Section 3, experimental results and conclusions are presented. ",System to Aid Handicapped with Computer Vision-Based Keyboard and Mouse,"A unistroke keyboard is proposed for the handicapped which can be made of paper or fabric in an upside down U-shape. Each character is represented by a non-overlapping rectangular region and is entered by illuminating the region with a laser pointer. The user doesn't have to turn off or raise the laser light while entering text. A disabled person may attach the laser pointer to an eyeglass to enter text by moving their head. Furthermore, a mouse-like device based on the same principle can be developed where the user can move the cursor by moving the laser light on the computer screen which is monitored by a camera.","A unistroke keyboard and mouse-like system based on computer vision is presented for those with limited hand control due to disability, such as Quadriplegia, Cerebral Palsy, Multiple Sclerosis, Muscular Dystrophy, ALS, and Carpal Tunnel Syndrome. Speech recognition cannot be used in some languages, including Turkish, which is spoken by 200 million people. Computer vision offers an alternative way to communicate with computers, with the user's actions monitored by a camera. Character recognition techniques can be used to recognize hand-writing or sketching. A vision-based system has been developed to recognize isolated characters drawn by a stylus or laser pointer on a flat surface or forearm. A head-mounted camera captures the user's actions. To achieve high recognition rates, characters are restricted to a single stroke alphabet. A regular QWERTY type keyboard based on computer vision has been proposed, with a character entered when its location on the keyboard image is covered by a finger. Unistroke keyboards provide a balance between 10-finger typing and continuous handwriting recognition. This paper presents a computer vision based unistroke keyboard, based on a soft keyboard system called Cimn. Key locations are placed circularly or on a U-like curve, with the keyboard image displayed on the computer screen. A USB web camera captures the movement of the laser beam, with the user able to attach a laser pointer to an eyeglass and control the beam by moving their head. An inexpensive system is realized, with no need for special purpose hardware. Experimental results and conclusions are presented in Section 3.",https://ieeexplore.ieee.org/stamp/stamp.jsp?arnumber=1221729
HUCVL at MediaEval 2016: Predicting Interesting Key Frames with Deep Models,"In MediaEval 2016, we focus on the image interestingness subtask which involves predicting interesting key frames of a video in the form of a movie trailer. We specifically propose three different deep models for this subtask. The first two models are based on fine-tuning two pretrained models, namely AlexNet and MemNet, where we cast the interestingness prediction as a regression problem. Our third deep model, on the other hand, depends on a triplet network which is comprised of three instances of the same feedforward network with shared weights, and trained according to a triplet ranking loss. Our experiments demonstrate that all these models provide relatively similar and promising results on the image interestingness subtask.","Understanding and predicting interestingness of images or video shots have been proposed as a recent problem in computer vision literature [7, 6, 2], which finds many applications such as video summarization [3] or automatic generation of animated gifs [4]. The MediaEval 2016 Predicting Media Interestingness Task is introduced as a new task which consists of two subtasks on image and video levels, respectively. In our work, we concentrate only on the image subtask, which involves identifying interesting keyframes of a given video of a movie trailer, and where we process each frame independently. Details about this subtask including the related dataset and the experimental setting can be found in the overview paper [1].",Deep Models Predicting Interesting Key Frames at MediaEval 2016,"For MediaEval 2016, we investigate the image interestingness subtask, which requires predicting interesting key frames of a video in the form of a movie trailer. We present three deep models for this task. The first two are based on fine-tuning AlexNet and MemNet, with the prediction being a regression problem. The third model is a triplet network, consisting of three feedforward networks with shared weights, and trained with a triplet ranking loss. Results from our experiments show that all these models provide comparable and satisfactory performance on the image interestingness subtask.","Computer vision literature [7, 6, 2] proposes a problem of predicting interestingness of images or video shots, with applications such as video summarization [3] and automatic generation of animated gifs [4]. MediaEval 2016 introduces a new task with two subtasks, one for images and one for videos. Our focus is on the image subtask, which requires identification of interesting keyframes from a movie trailer's video. Details of this subtask, dataset, and experimental setting can be found in [1].",http://slim-sig.irisa.fr/me16proc/MediaEval_2016_paper_18.pdf
