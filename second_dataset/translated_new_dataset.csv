Text1,Text2,Similarity
Text1,Текст2,Similarity
Текст1,Text2,Similarity
Текст1,Текст2,Similarity
"This paper describes our two-stage system1 for the Euphemism Detection shared task hosted by the 3rd Workshop on Figurative Language Processing in conjunction with EMNLP 2022. Euphemisms tone down expressions about sensitive or unpleasant issues like addiction and death. The ambiguous nature of euphemistic words or expressions makes it challenging to detect their actual meaning within a context. In the first stage, we seek to mitigate this ambiguity by incorporating literal descriptions into input text prompts to our baseline model. It turns out that this kind of direct supervision yields remarkable performance improvement. In the second stage, we integrate visual supervision into our system using visual imageries, two sets of images generated by a text-to-image model by taking terms and descriptions as input. Our experiments demonstrate that visual supervision also gives a statistically significant performance boost. Our system achieved the second place with an F1 score of 87.2%, only about 0.9% worse than the best submission.","Our two-stage system for the Euphemism Detection shared task hosted by the 3rd Workshop on Figurative Language Processing in conjunction with EMNLP 2022 is presented in this paper. It is difficult to identify the true meaning of euphemistic words or phrases due to their ambiguous nature when it comes to sensitive or unpleasant topics such as addiction and death. To reduce this ambiguity, we added literal descriptions to the input text prompts of our baseline model in the first stage, which resulted in significant performance improvement. In the second stage, visual supervision was integrated by using two sets of images generated by a text-to-image model that takes terms and descriptions as input. Our experiments showed that visual supervision gives a statistically significant performance boost. We achieved the second place with an F1 score of 87.2%, only about 0.9% worse than the best submission.",1
"This paper describes our two-stage system1 for the Euphemism Detection shared task hosted by the 3rd Workshop on Figurative Language Processing in conjunction with EMNLP 2022. Euphemisms tone down expressions about sensitive or unpleasant issues like addiction and death. The ambiguous nature of euphemistic words or expressions makes it challenging to detect their actual meaning within a context. In the first stage, we seek to mitigate this ambiguity by incorporating literal descriptions into input text prompts to our baseline model. It turns out that this kind of direct supervision yields remarkable performance improvement. In the second stage, we integrate visual supervision into our system using visual imageries, two sets of images generated by a text-to-image model by taking terms and descriptions as input. Our experiments demonstrate that visual supervision also gives a statistically significant performance boost. Our system achieved the second place with an F1 score of 87.2%, only about 0.9% worse than the best submission.","У цьому документі представлено нашу двоетапну систему для спільного завдання «Виявлення евфемізмів», організованого 3-м семінаром з обробки образної мови спільно з EMNLP 2022. Важко визначити справжнє значення евфемістичних слів або фраз через їх неоднозначну природу, коли мова йде про чутливі або неприємні теми, такі як залежність і смерть. Щоб зменшити цю неоднозначність, ми додали буквальні описи до вхідних текстових підказок нашої базової моделі на першому етапі, що призвело до значного покращення продуктивності. На другому етапі візуальний нагляд був інтегрований за допомогою двох наборів зображень, згенерованих моделлю перетворення тексту в зображення, яка приймає терміни та описи як вхідні дані. Наші експерименти показали, що візуальне спостереження дає статистично значущий приріст продуктивності. Ми зайняли друге місце з результатом Формули-1 87,2%, що лише на 0,9% гірше, ніж найкраще подання.",1
"У цьому документі описано нашу двоетапну систему1 для спільного завдання «Виявлення евфемізмів», організованого 3-м семінаром з обробки образної мови спільно з EMNLP 2022. Евфемізми пом’якшують вирази про чутливі або неприємні питання, такі як залежність і смерть. Неоднозначний характер евфемістичних слів або виразів ускладнює виявлення їх справжнього значення в контексті. На першому етапі ми намагаємося пом’якшити цю неоднозначність, включивши літеральні описи до вхідних текстових підказок нашої базової моделі. Виявляється, такий вид прямого контролю дає значне покращення продуктивності. На другому етапі ми інтегруємо візуальний нагляд у нашу систему за допомогою візуальних зображень, двох наборів зображень, створених за допомогою моделі тексту в зображення, використовуючи терміни та описи як вхідні дані. Наші експерименти демонструють, що візуальний контроль також дає статистично значуще підвищення продуктивності. Наша система посіла друге місце з результатом F1 87,2%, що лише на 0,9% гірше, ніж найкраще подання.","Our two-stage system for the Euphemism Detection shared task hosted by the 3rd Workshop on Figurative Language Processing in conjunction with EMNLP 2022 is presented in this paper. It is difficult to identify the true meaning of euphemistic words or phrases due to their ambiguous nature when it comes to sensitive or unpleasant topics such as addiction and death. To reduce this ambiguity, we added literal descriptions to the input text prompts of our baseline model in the first stage, which resulted in significant performance improvement. In the second stage, visual supervision was integrated by using two sets of images generated by a text-to-image model that takes terms and descriptions as input. Our experiments showed that visual supervision gives a statistically significant performance boost. We achieved the second place with an F1 score of 87.2%, only about 0.9% worse than the best submission.",1
"У цьому документі описано нашу двоетапну систему1 для спільного завдання «Виявлення евфемізмів», організованого 3-м семінаром з обробки образної мови спільно з EMNLP 2022. Евфемізми пом’якшують вирази про чутливі або неприємні питання, такі як залежність і смерть. Неоднозначний характер евфемістичних слів або виразів ускладнює виявлення їх справжнього значення в контексті. На першому етапі ми намагаємося пом’якшити цю неоднозначність, включивши літеральні описи до вхідних текстових підказок нашої базової моделі. Виявляється, такий вид прямого контролю дає значне покращення продуктивності. На другому етапі ми інтегруємо візуальний нагляд у нашу систему за допомогою візуальних зображень, двох наборів зображень, створених за допомогою моделі тексту в зображення, використовуючи терміни та описи як вхідні дані. Наші експерименти демонструють, що візуальний контроль також дає статистично значуще підвищення продуктивності. Наша система посіла друге місце з результатом F1 87,2%, що лише на 0,9% гірше, ніж найкраще подання.","У цьому документі представлено нашу двоетапну систему для спільного завдання «Виявлення евфемізмів», організованого 3-м семінаром з обробки образної мови спільно з EMNLP 2022. Важко визначити справжнє значення евфемістичних слів або фраз через їх неоднозначну природу, коли йдеться про чутливі чи неприємні теми, такі як залежність і смерть. Щоб зменшити цю неоднозначність, ми додали буквальні описи до вхідних текстових підказок нашої базової моделі на першому етапі, що призвело до значного покращення продуктивності. На другому етапі візуальний нагляд був інтегрований за допомогою двох наборів зображень, згенерованих моделлю тексту в зображення, яка приймає терміни та описи як вхідні дані. Наші експерименти показали, що візуальне спостереження дає статистично значущий приріст продуктивності. Ми зайняли друге місце з результатом Формули-1 87,2%, що лише на 0,9% гірше, ніж найкраще подання.",1
"Giving machines the ability to imagine possible new objects or scenes from linguistic descriptions and produce their realistic renderings is arguably one of the most challenging problems in computer vision. Recent advances in deep generative models have led to new approaches that give promising results towards this goal. In this paper, we introduce a new method called DiCoMoGAN for manipulating videos with natural language, aiming to perform local and semantic edits on a video clip to alter the appearances of an object of interest. Our GAN architecture allows for better utilization of multiple observations by disentangling content and motion to enable controllable semantic edits. To this end, we introduce two tightly coupled networks: (i) a representation network for constructing a concise understanding of motion dynamics and temporally invariant content, and (ii) a translation network that exploits the extracted latent content representation to actuate the manipulation according to the target description. Our qualitative and quantitative evaluations demonstrate that DiCoMoGAN significantly outperforms existing frame-based methods, producing temporally coherent and semantically more meaningful results.","Developing machines that can generate realistic renderings of new objects or scenes from linguistic descriptions is one of the most difficult tasks in computer vision. Recently, deep generative models have been utilized to produce promising results. In this paper, we propose a new method, DiCoMoGAN, for manipulating videos with natural language, to modify the look of a particular object. Our GAN architecture permits better utilization of multiple observations by separating content and motion to allow for controllable semantic edits. To accomplish this, two networks are used: (i) a representation network for creating a succinct understanding of motion dynamics and content that is temporally invariant, and (ii) a translation network that uses the latent content representation to effect the manipulation according to the target description. Our qualitative and quantitative evaluations show that DiCoMoGAN significantly surpasses existing frame-based methods, producing results that are temporally coherent and semantically more meaningful.",1
"Giving machines the ability to imagine possible new objects or scenes from linguistic descriptions and produce their realistic renderings is arguably one of the most challenging problems in computer vision. Recent advances in deep generative models have led to new approaches that give promising results towards this goal. In this paper, we introduce a new method called DiCoMoGAN for manipulating videos with natural language, aiming to perform local and semantic edits on a video clip to alter the appearances of an object of interest. Our GAN architecture allows for better utilization of multiple observations by disentangling content and motion to enable controllable semantic edits. To this end, we introduce two tightly coupled networks: (i) a representation network for constructing a concise understanding of motion dynamics and temporally invariant content, and (ii) a translation network that exploits the extracted latent content representation to actuate the manipulation according to the target description. Our qualitative and quantitative evaluations demonstrate that DiCoMoGAN significantly outperforms existing frame-based methods, producing temporally coherent and semantically more meaningful results.","Розробка машин, які можуть генерувати реалістичні візуалізації нових об’єктів або сцен на основі лінгвістичних описів, є одним із найскладніших завдань комп’ютерного зору. Останнім часом глибокі генеративні моделі були використані для отримання багатообіцяючих результатів. У цій статті ми пропонуємо новий метод, DiCoMoGAN, для обробки відео за допомогою природної мови, щоб змінити зовнішній вигляд певного об’єкта. Наша архітектура GAN дозволяє краще використовувати кілька спостережень, відокремлюючи вміст і рух, щоб забезпечити кероване семантичне редагування. Щоб досягти цього, використовуються дві мережі: (i) мережа представлення для створення короткого розуміння динаміки руху та вмісту, незмінного за часом, і (ii) мережа перекладу, яка використовує представлення латентного вмісту для здійснення маніпуляцій відповідно до цільовий опис. Наші якісні та кількісні оцінки показують, що DiCoMoGAN значно перевершує існуючі методи на основі кадрів, створюючи результати, які є часово узгодженими та семантично більш значущими.",1
"Надання машинам здатності уявляти можливі нові об’єкти чи сцени на основі лінгвістичних описів і виробляти їх реалістичне відтворення є, мабуть, однією з найскладніших проблем комп’ютерного зору. Останні досягнення в глибоких генеративних моделях привели до нових підходів, які дають багатообіцяючі результати для досягнення цієї мети. У цій статті ми представляємо новий метод під назвою DiCoMoGAN для обробки відео за допомогою природної мови, спрямований на виконання локальних і семантичних редагувань відеокліпу, щоб змінити зовнішній вигляд цікавого об’єкта. Наша архітектура GAN дозволяє краще використовувати кілька спостережень, розмежовуючи вміст і рух, щоб уможливити кероване семантичне редагування. З цією метою ми представляємо дві тісно пов’язані мережі: (i) репрезентативну мережу для побудови стислого розуміння динаміки руху та тимчасово незмінного вмісту, і (ii) мережу перекладу, яка використовує виділене латентне представлення вмісту для активації маніпуляції відповідно до цільовий опис. Наші якісні та кількісні оцінки демонструють, що DiCoMoGAN значно перевершує існуючі методи на основі фреймів, створюючи узгоджені в часі та семантично більш значущі результати.","Developing machines that can generate realistic renderings of new objects or scenes from linguistic descriptions is one of the most difficult tasks in computer vision. Recently, deep generative models have been utilized to produce promising results. In this paper, we propose a new method, DiCoMoGAN, for manipulating videos with natural language, to modify the look of a particular object. Our GAN architecture permits better utilization of multiple observations by separating content and motion to allow for controllable semantic edits. To accomplish this, two networks are used: (i) a representation network for creating a succinct understanding of motion dynamics and content that is temporally invariant, and (ii) a translation network that uses the latent content representation to effect the manipulation according to the target description. Our qualitative and quantitative evaluations show that DiCoMoGAN significantly surpasses existing frame-based methods, producing results that are temporally coherent and semantically more meaningful.",1
"Надання машинам здатності уявляти можливі нові об’єкти чи сцени на основі лінгвістичних описів і виробляти їх реалістичне відтворення є, мабуть, однією з найскладніших проблем комп’ютерного зору. Останні досягнення в глибоких генеративних моделях привели до нових підходів, які дають багатообіцяючі результати для досягнення цієї мети. У цій статті ми представляємо новий метод під назвою DiCoMoGAN для обробки відео за допомогою природної мови, спрямований на виконання локальних і семантичних редагувань відеокліпу, щоб змінити зовнішній вигляд цікавого об’єкта. Наша архітектура GAN дозволяє краще використовувати кілька спостережень, розмежовуючи вміст і рух, щоб уможливити кероване семантичне редагування. З цією метою ми представляємо дві тісно пов’язані мережі: (i) репрезентативну мережу для побудови стислого розуміння динаміки руху та тимчасово незмінного вмісту, і (ii) мережу перекладу, яка використовує виділене латентне представлення вмісту для активації маніпуляції відповідно до цільовий опис. Наші якісні та кількісні оцінки демонструють, що DiCoMoGAN значно перевершує існуючі методи на основі фреймів, створюючи узгоджені в часі та семантично більш значущі результати.","Розробка машин, які можуть генерувати реалістичні візуалізації нових об’єктів або сцен на основі лінгвістичних описів, є одним із найскладніших завдань комп’ютерного зору. Останнім часом глибокі генеративні моделі були використані для отримання багатообіцяючих результатів. У цій статті ми пропонуємо новий метод, DiCoMoGAN, для обробки відео за допомогою природної мови, щоб змінити зовнішній вигляд певного об’єкта. Наша архітектура GAN дозволяє краще використовувати кілька спостережень, відокремлюючи вміст і рух, щоб забезпечити кероване семантичне редагування. Щоб досягти цього, використовуються дві мережі: (i) мережа представлення для створення короткого розуміння динаміки руху та вмісту, незмінного за часом, і (ii) мережа перекладу, яка використовує представлення латентного вмісту для здійснення маніпуляцій відповідно до цільовий опис. Наші якісні та кількісні оцінки показують, що DiCoMoGAN значно перевершує існуючі методи на основі кадрів, створюючи результати, які є часово узгодженими та семантично більш значущими.",1
"Flow-based generative super-resolution (SR) models learn to produce a diverse set of feasible SR solutions, called the SR space. Diversity of SR solutions increases with the temperature (τ ) of latent variables, which introduces random variations of texture among sample solutions, resulting in visual artifacts and low fidelity. In this paper, we present a simple but effective image ensembling/fusion approach to obtain a single SR image eliminating random artifacts and improving fidelity without significantly compromising perceptual quality. We achieve this by benefiting from a diverse set of feasible photorealistic solutions in the SR space spanned by flow models. We propose different image ensembling and fusion strategies which offer multiple paths to move sample solutions in the SR space to more desired destinations in the perception-distortion plane in a controllable manner depending on the fidelity vs. perceptual quality requirements of the task at hand. Experimental results demonstrate that our image ensembling/fusion strategy achieves more promising perception-distortion tradeoff compared to sample SR images produced by flow models and adversarially trained models in terms of both quantitative metrics and visual quality.","Flow-based generative super-resolution (SR) models learn to generate a wide range of SR solutions, referred to as the SR space. As the temperature (τ ) of latent variables increases, the diversity of SR solutions also increases, resulting in visible artifacts and low accuracy. In this paper, we present a straightforward yet effective image ensembling/fusion approach to obtain a single SR image that eliminates random artifacts and boosts fidelity without significantly reducing perceptual quality. We take advantage of the wide range of realistic solutions in the SR space generated by flow models. We propose several image ensembling and fusion strategies that provide multiple paths to move sample solutions in the SR space to the desired locations in the perception-distortion plane depending on the task's fidelity vs. perceptual quality requirements. Experimental results demonstrate that our image ensembling/fusion strategy offers a better perception-distortion tradeoff compared to sample SR images produced by flow models and adversarially trained models in terms of both quantitative metrics and visual quality.",1
"Flow-based generative super-resolution (SR) models learn to produce a diverse set of feasible SR solutions, called the SR space. Diversity of SR solutions increases with the temperature (τ ) of latent variables, which introduces random variations of texture among sample solutions, resulting in visual artifacts and low fidelity. In this paper, we present a simple but effective image ensembling/fusion approach to obtain a single SR image eliminating random artifacts and improving fidelity without significantly compromising perceptual quality. We achieve this by benefiting from a diverse set of feasible photorealistic solutions in the SR space spanned by flow models. We propose different image ensembling and fusion strategies which offer multiple paths to move sample solutions in the SR space to more desired destinations in the perception-distortion plane in a controllable manner depending on the fidelity vs. perceptual quality requirements of the task at hand. Experimental results demonstrate that our image ensembling/fusion strategy achieves more promising perception-distortion tradeoff compared to sample SR images produced by flow models and adversarially trained models in terms of both quantitative metrics and visual quality.","Моделі generative super-resolution (SR) на основі потоку вчаться генерувати широкий спектр рішень SR, які називаються простором SR. Зі збільшенням температури (τ ) латентних змінних також збільшується різноманітність рішень SR, що призводить до видимих артефактів і низької точності. У цій статті ми представляємо простий, але ефективний підхід до ансамблювання/злиття зображень для отримання єдиного зображення SR, яке усуває випадкові артефакти та підвищує точність без значного зниження якості сприйняття. Ми використовуємо широкий спектр реалістичних рішень у просторі SR, створених моделями потоку. Ми пропонуємо кілька стратегій поєднання зображень і злиття, які забезпечують кілька шляхів для переміщення зразків рішень у просторі SR до бажаних місць у площині сприйняття-спотворення залежно від вимог до точності завдання та вимог до якості сприйняття. Експериментальні результати демонструють, що наша стратегія ансамблювання/злиття зображень пропонує кращий компроміс між сприйняттям і спотворенням порівняно з зразками SR-зображень, створених потоковими моделями та навченими моделями з точки зору як кількісних показників, так і візуальної якості.",1
"Моделі generative super-resolution (SR) на основі потоку вчаться створювати різноманітний набір можливих рішень SR, які називають простором SR. Різноманітність розчинів SR збільшується з температурою (τ) прихованих змінних, що вводить випадкові варіації текстури серед розчинів зразків, що призводить до візуальних артефактів і низької точності. У цій статті ми представляємо простий, але ефективний підхід до ансамблювання/злиття зображень для отримання єдиного зображення SR, усуваючи випадкові артефакти та покращуючи точність без значного погіршення якості сприйняття. Ми досягаємо цього, використовуючи переваги різноманітного набору можливих фотореалістичних рішень у просторі SR, охопленому моделями потоку. Ми пропонуємо різні стратегії поєднання зображень і злиття, які пропонують кілька шляхів для переміщення зразків рішень у просторі SR до більш бажаних пунктів призначення в площині сприйняття-спотворення контрольованим способом залежно від вимог щодо точності та якості сприйняття для поточного завдання. Експериментальні результати демонструють, що наша стратегія ансамблювання/злиття зображень досягає більш перспективного компромісу між сприйняттям і спотворенням порівняно з зразками SR-зображень, створених потоковими моделями та конкурентно навченими моделями як з точки зору кількісних показників, так і якості зображення.","Flow-based generative super-resolution (SR) models learn to generate a wide range of SR solutions, referred to as the SR space. As the temperature (τ ) of latent variables increases, the diversity of SR solutions also increases, resulting in visible artifacts and low accuracy. In this paper, we present a straightforward yet effective image ensembling/fusion approach to obtain a single SR image that eliminates random artifacts and boosts fidelity without significantly reducing perceptual quality. We take advantage of the wide range of realistic solutions in the SR space generated by flow models. We propose several image ensembling and fusion strategies that provide multiple paths to move sample solutions in the SR space to the desired locations in the perception-distortion plane depending on the task's fidelity vs. perceptual quality requirements. Experimental results demonstrate that our image ensembling/fusion strategy offers a better perception-distortion tradeoff compared to sample SR images produced by flow models and adversarially trained models in terms of both quantitative metrics and visual quality.",1
"Моделі generative super-resolution (SR) на основі потоку навчаються виробляти різноманітний набір можливих рішень SR, які називають простором SR. Різноманітність розчинів SR збільшується з температурою (τ) прихованих змінних, що вводить випадкові варіації текстури серед розчинів зразків, що призводить до візуальних артефактів і низької точності. У цій статті ми представляємо простий, але ефективний підхід до ансамблювання/злиття зображень для отримання єдиного зображення SR, усуваючи випадкові артефакти та покращуючи точність без значного погіршення якості сприйняття. Ми досягаємо цього, використовуючи переваги різноманітного набору можливих фотореалістичних рішень у просторі SR, охопленому моделями потоку. Ми пропонуємо різні стратегії поєднання зображень і злиття, які пропонують кілька шляхів для переміщення зразків рішень у просторі SR до більш бажаних пунктів призначення в площині сприйняття-спотворення контрольованим способом залежно від вимог щодо точності та якості сприйняття для поточного завдання. Експериментальні результати демонструють, що наша стратегія ансамблювання/злиття зображень досягає більш перспективного компромісу між сприйняттям і спотворенням порівняно з зразками SR-зображень, створених потоковими моделями та конкурентно навченими моделями як з точки зору кількісних показників, так і якості зображення.","Моделі generative super-resolution (SR) на основі потоку вчаться генерувати широкий спектр рішень SR, які називаються простором SR. Зі збільшенням температури (τ ) латентних змінних також збільшується різноманітність рішень SR, що призводить до видимих артефактів і низької точності. У цій статті ми представляємо простий, але ефективний підхід до ансамблювання/злиття зображень для отримання єдиного зображення SR, яке усуває випадкові артефакти та підвищує точність без значного зниження якості сприйняття. Ми використовуємо широкий спектр реалістичних рішень у просторі SR, створених моделями потоку. Ми пропонуємо кілька стратегій поєднання зображень і злиття, які забезпечують кілька шляхів для переміщення зразків рішень у просторі SR до бажаних місць у площині сприйняття-спотворення залежно від вимог до точності завдання та вимог до якості сприйняття. Експериментальні результати демонструють, що наша стратегія ансамблювання/злиття зображень пропонує кращий компроміс між сприйняттям і спотворенням порівняно з зразками SR-зображень, створених потоковими моделями та навченими моделями з точки зору як кількісних показників, так і візуальної якості.",1
"Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 444 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI’s GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit “breakthrough” behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting. ","Language models show improvement and new capabilities with size increase. These are not well understood, so to be prepared for future developments and to avoid negative effects, we must know the current and near-future abilities and limitations of these models. To do this, BIG-bench was created with 204 tasks from 444 authors at 132 institutions, covering linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and more. It focuses on tasks that are believed to be too difficult for existing language models. We tested OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, with sizes ranging from millions to hundreds of billions of parameters. Furthermore, human experts completed all tasks to set a strong baseline. Results showed that performance and calibration increase with size, but are still low in absolute terms (and compared to the experts). Performance was similar across model types, but sparsity was beneficial. Tasks that improve gradually usually have large knowledge or memorization components, while tasks that show dramatic improvement at a certain size usually involve multiple steps or components, or have fragile metrics. Social bias usually rises with scale in ambiguous contexts, but can be lessened with prompting.",1
"Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 444 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI’s GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit “breakthrough” behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting. ","Мовні моделі демонструють покращення та нові можливості зі збільшенням розміру. Вони недостатньо вивчені, тому, щоб бути готовими до майбутніх розробок і уникнути негативних наслідків, ми повинні знати поточні та найближчі майбутні можливості та обмеження цих моделей. Для цього було створено BIG-bench із 204 завданнями від 444 авторів із 132 закладів, які стосуються лінгвістики, розвитку дітей, математики, здорового глузду, біології, фізики, соціальних упереджень, розробки програмного забезпечення тощо. Він зосереджений на завданнях, які вважаються надто складними для існуючих мовних моделей. Ми протестували моделі GPT OpenAI, внутрішні архітектури щільних трансформаторів Google і розріджені трансформатори у стилі Switch на BIG-стенді з розмірами від мільйонів до сотень мільярдів параметрів. Крім того, експерти-людини виконали всі завдання, щоб встановити міцну базову лінію. Результати показали, що продуктивність і калібрування зростають із збільшенням розміру, але вони все ще низькі в абсолютному вираженні (та порівняно з експертами). Продуктивність була однаковою для всіх типів моделей, але розрідженість була корисною. Завдання, які покращуються поступово, зазвичай мають значні компоненти знань або запам’ятовування, тоді як завдання, які демонструють значне покращення певного розміру, зазвичай включають кілька кроків чи компонентів або мають крихкі показники. Соціальна упередженість зазвичай зростає з масштабом у неоднозначних контекстах, але її можна зменшити за допомогою підказок.",1
"Мовні моделі демонструють як кількісне вдосконалення, так і нові якісні можливості зі збільшенням масштабу. Незважаючи на їхній потенційно трансформаційний вплив, ці нові можливості ще недостатньо охарактеризовані. Для того, щоб інформувати про майбутні дослідження, підготуватися до руйнівних можливостей нових моделей і пом’якшити соціально шкідливі наслідки, життєво важливо, щоб ми розуміли теперішні та найближчі можливості та обмеження мовних моделей. Щоб вирішити цю проблему, ми представляємо тест Beyond the Imitation Game (BIG-bench). BIG-стенд наразі складається з 204 завдань, доданих 444 авторами зі 132 установ. Теми завдань різноманітні, вони охоплюють проблеми з лінгвістики, розвитку дитини, математики, здорового глузду, біології, фізики, соціальних упереджень, розробки програмного забезпечення тощо. BIG-bench зосереджується на завданнях, які, як вважають, виходять за межі можливостей поточних мовних моделей. Ми оцінюємо поведінку моделей GPT OpenAI, внутрішніх архітектур щільних трансформаторів Google і розріджених трансформаторів у стилі Switch на BIG-стенді за розмірами моделей, що охоплюють від мільйонів до сотень мільярдів параметрів. Крім того, команда оцінювачів-людей виконувала всі завдання, щоб забезпечити надійну базову лінію. Висновки включають: продуктивність моделі та калібрування покращуються з масштабом, але є поганими в абсолютному вираженні (і порівняно з продуктивністю оцінювача); продуктивність надзвичайно однакова для всіх класів моделей, хоча й має переваги від розрідженості; завдання, які вдосконалюються поступово і передбачувано, зазвичай включають великий компонент знань або запам’ятовування, тоді як завдання, які демонструють «проривну» поведінку в критичному масштабі, часто включають кілька кроків або компонентів або крихкі показники; соціальне упередження зазвичай зростає з масштабом в умовах з неоднозначним контекстом, але це можна покращити за допомогою підказок.","Language models show improvement and new capabilities with size increase. These are not well understood, so to be prepared for future developments and to avoid negative effects, we must know the current and near-future abilities and limitations of these models. To do this, BIG-bench was created with 204 tasks from 444 authors at 132 institutions, covering linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and more. It focuses on tasks that are believed to be too difficult for existing language models. We tested OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, with sizes ranging from millions to hundreds of billions of parameters. Furthermore, human experts completed all tasks to set a strong baseline. Results showed that performance and calibration increase with size, but are still low in absolute terms (and compared to the experts). Performance was similar across model types, but sparsity was beneficial. Tasks that improve gradually usually have large knowledge or memorization components, while tasks that show dramatic improvement at a certain size usually involve multiple steps or components, or have fragile metrics. Social bias usually rises with scale in ambiguous contexts, but can be lessened with prompting.",1
"Мовні моделі демонструють як кількісне вдосконалення, так і нові якісні можливості зі збільшенням масштабу. Незважаючи на їхній потенційно трансформаційний вплив, ці нові можливості ще недостатньо охарактеризовані. Для того, щоб інформувати про майбутні дослідження, підготуватися до руйнівних можливостей нових моделей і пом’якшити соціально шкідливі наслідки, життєво важливо, щоб ми розуміли теперішні та найближчі можливості та обмеження мовних моделей. Щоб вирішити цю проблему, ми представляємо тест Beyond the Imitation Game (BIG-bench). BIG-стенд наразі складається з 204 завдань, доданих 444 авторами зі 132 установ. Теми завдань різноманітні, вони охоплюють проблеми з лінгвістики, розвитку дитини, математики, здорового глузду, біології, фізики, соціальних упереджень, розробки програмного забезпечення тощо. BIG-bench зосереджується на завданнях, які, як вважають, виходять за межі можливостей поточних мовних моделей. Ми оцінюємо поведінку моделей GPT OpenAI, внутрішніх архітектур щільних трансформаторів Google і розріджених трансформаторів у стилі Switch на BIG-стенді за розмірами моделей, що охоплюють від мільйонів до сотень мільярдів параметрів. Крім того, команда оцінювачів-людей виконувала всі завдання, щоб забезпечити надійну базову лінію. Висновки включають: продуктивність моделі та калібрування покращуються з масштабом, але є поганими в абсолютному вираженні (і порівняно з продуктивністю оцінювача); продуктивність надзвичайно однакова для всіх класів моделей, хоча й має переваги від розрідженості; завдання, які вдосконалюються поступово і передбачувано, зазвичай включають великий компонент знань або запам’ятовування, тоді як завдання, які демонструють «проривну» поведінку в критичному масштабі, часто включають кілька кроків або компонентів або крихкі показники; соціальне упередження зазвичай зростає з масштабом в умовах з неоднозначним контекстом, але це можна покращити за допомогою підказок.","Мовні моделі демонструють покращення та нові можливості зі збільшенням розміру. Вони недостатньо вивчені, тому, щоб бути готовими до майбутніх розробок і уникнути негативних наслідків, ми повинні знати поточні та найближчі майбутні можливості та обмеження цих моделей. Для цього було створено BIG-bench із 204 завданнями від 444 авторів із 132 закладів, які стосуються лінгвістики, розвитку дітей, математики, здорового глузду, біології, фізики, соціальних упереджень, розробки програмного забезпечення тощо. Він зосереджений на завданнях, які вважаються надто складними для існуючих мовних моделей. Ми протестували моделі GPT OpenAI, внутрішні архітектури щільних трансформаторів Google і розріджені трансформатори у стилі Switch на BIG-стенді з розмірами від мільйонів до сотень мільярдів параметрів. Крім того, експерти-людини виконали всі завдання, щоб встановити міцну базову лінію. Результати показали, що продуктивність і калібрування зростають із збільшенням розміру, але вони все ще низькі в абсолютному вираженні (та порівняно з експертами). Продуктивність була однаковою для всіх типів моделей, але розрідженість була корисною. Завдання, які покращуються поступово, зазвичай мають значні компоненти знань або запам’ятовування, тоді як завдання, які демонструють значне покращення певного розміру, зазвичай включають кілька кроків чи компонентів або мають крихкі показники. Соціальна упередженість зазвичай зростає з масштабом у неоднозначних контекстах, але її можна зменшити за допомогою підказок.",1
"Magnetic resonance imaging (MRI) is used in many diagnostic applications as it has a high soft-tissue contrast and is a non-invasive medical imaging method. MR signal levels differs according to the parameters T1, T2 and PD that change with respect to the chemical structure of the tissues. However, long scan times might limit acquiring images from multiple contrasts or if the multi-contrasts images are acquired, the contrasts are noisy. To overcome this limitation of MRI, multi-contrast synthesis can be utilized. In this paper, we propose a deep learning method based on Channel-Exchanging-Network (CEN) for multi-contrast image synthesis. Demonstrations are provided on IXI dataset. The proposed model based on CEN is compared against alternative methods based on CNNs and GANs. Our results show that the proposed model achieves superior performance to the competing methods. ","MRI has high soft-tissue contrast and is a non-invasive method, making it ideal for diagnostic purposes. Its signal levels are affected by T1, T2 and PD, which vary depending on the tissue's chemical structure. However, long scan times can make it difficult to acquire multiple contrasts. To address this limitation, multi-contrast synthesis using deep learning based on Channel-Exchanging-Network (CEN) is proposed in this paper. Demonstrations on IXI dataset show that CEN outperforms CNNs and GANs in multi-contrast image synthesis.",1
"Magnetic resonance imaging (MRI) is used in many diagnostic applications as it has a high soft-tissue contrast and is a non-invasive medical imaging method. MR signal levels differs according to the parameters T1, T2 and PD that change with respect to the chemical structure of the tissues. However, long scan times might limit acquiring images from multiple contrasts or if the multi-contrasts images are acquired, the contrasts are noisy. To overcome this limitation of MRI, multi-contrast synthesis can be utilized. In this paper, we propose a deep learning method based on Channel-Exchanging-Network (CEN) for multi-contrast image synthesis. Demonstrations are provided on IXI dataset. The proposed model based on CEN is compared against alternative methods based on CNNs and GANs. Our results show that the proposed model achieves superior performance to the competing methods. ","МРТ має високу контрастність м’яких тканин і є неінвазивним методом, що робить його ідеальним для діагностичних цілей. На рівні його сигналу впливають T1, T2 і PD, які змінюються залежно від хімічної структури тканини. Однак тривалий час сканування може ускладнити отримання кількох контрастів. Щоб усунути це обмеження, у цьому документі пропонується багатоконтрастний синтез із використанням глибокого навчання на основі мережі обміну каналами (CEN). Демонстрації на наборі даних IXI показують, що CEN перевершує CNN і GAN у багатоконтрастному синтезі зображень.",1
"Магнітно-резонансна томографія (МРТ) використовується в багатьох діагностичних програмах, оскільки вона має високий контраст м’яких тканин і є неінвазивним медичним методом візуалізації. Рівень сигналу МР відрізняється відповідно до параметрів T1, T2 і PD, які змінюються залежно від хімічної структури тканин. Однак тривалий час сканування може обмежити отримання зображень із кількох контрастів, або якщо отримані багатоконтрастні зображення, контрасти мають шум. Щоб подолати це обмеження МРТ, можна використовувати мультиконтрастний синтез. У цій статті ми пропонуємо метод глибокого навчання на основі мережі обміну каналами (CEN) для синтезу мультиконтрастного зображення. Демонстрації надаються на базі даних IXI. Запропонована модель на основі CEN порівнюється з альтернативними методами на основі CNN і GAN. Наші результати показують, що запропонована модель досягає кращої продуктивності порівняно з конкурентними методами.","MRI has high soft-tissue contrast and is a non-invasive method, making it ideal for diagnostic purposes. Its signal levels are affected by T1, T2 and PD, which vary depending on the tissue's chemical structure. However, long scan times can make it difficult to acquire multiple contrasts. To address this limitation, multi-contrast synthesis using deep learning based on Channel-Exchanging-Network (CEN) is proposed in this paper. Demonstrations on IXI dataset show that CEN outperforms CNNs and GANs in multi-contrast image synthesis.",1
"Магнітно-резонансна томографія (МРТ) використовується в багатьох діагностичних програмах, оскільки вона має високий контраст м’яких тканин і є неінвазивним медичним методом візуалізації. Рівень сигналу МР відрізняється відповідно до параметрів T1, T2 і PD, які змінюються залежно від хімічної структури тканин. Однак тривалий час сканування може обмежити отримання зображень із кількох контрастів, або якщо отримані багатоконтрастні зображення, контрасти мають шум. Щоб подолати це обмеження МРТ, можна використовувати мультиконтрастний синтез. У цій статті ми пропонуємо метод глибокого навчання на основі мережі обміну каналами (CEN) для синтезу багатоконтрастного зображення. Демонстрації надаються на базі даних IXI. Запропонована модель на основі CEN порівнюється з альтернативними методами на основі CNN і GAN. Наші результати показують, що запропонована модель досягає кращої продуктивності порівняно з конкурентними методами.","МРТ має високу контрастність м’яких тканин і є неінвазивним методом, що робить його ідеальним для діагностичних цілей. На рівні його сигналу впливають T1, T2 і PD, які змінюються залежно від хімічної структури тканини. Однак тривалий час сканування може ускладнити отримання кількох контрастів. Щоб усунути це обмеження, у цьому документі пропонується багатоконтрастний синтез із використанням глибокого навчання на основі мережі обміну каналами (CEN). Демонстрації на наборі даних IXI показують, що CEN перевершує CNN і GAN у багатоконтрастному синтезі зображень.",1
"The immense amount of videos being uploaded to video sharing platforms makes it impossible for a person to watch all the videos understand what happens in them. Hence, machine learning techniques are now deployed to index videos by recognizing key objects, actions and scenes or places. Summarization is another alternative as it offers to extract only important parts while covering the gist of the video content. Ideally, the user may prefer to analyze a certain action or scene by searching a query term within the video. Current summarization methods generally do not take queries into account or require exhaustive data labeling. In this work, we present a weakly supervised query-focused video summarization method. Our proposed approach makes use of semantic attributes as an indicator of query relevance and semantic attention maps to locate related regions in the frames and utilizes both within a submodular maximization framework. We conducted experiments on the recently introduced RAD dataset and obtained highly competitive results. Moreover, to better evaluate the performance of our approach on longer videos, we collected a new dataset, which consists of 10 videos from YouTube and annotated with shot-level multiple attributes. Our dataset enables much diverse set of queries that can be used to summarize a video from different perspectives with more degrees of freedom.","It is infeasible for a person to watch all the videos on video sharing platforms and comprehend their content. Therefore, machine learning techniques are employed to index videos by identifying essential objects, actions, places and scenes. Summarization is another choice that enables extraction of relevant parts while maintaining the essence of the video. The user may prefer to search a specific action or scene with a query term. Existing summarization methods generally do not take queries into account or require exhaustive data labelling. This paper proposes a weakly supervised query-focused video summarization technique that makes use of semantic attributes as a marker of query relevance and semantic attention maps to locate related regions in the frames. This approach is employed in a submodular maximization framework. Experiments on the RAD dataset produced promising results. Furthermore, a new dataset was created which consists of 10 YouTube videos with shot-level multiple attributes. This dataset enables a more diverse set of queries to summarize videos from various perspectives with more freedom.",1
"The immense amount of videos being uploaded to video sharing platforms makes it impossible for a person to watch all the videos understand what happens in them. Hence, machine learning techniques are now deployed to index videos by recognizing key objects, actions and scenes or places. Summarization is another alternative as it offers to extract only important parts while covering the gist of the video content. Ideally, the user may prefer to analyze a certain action or scene by searching a query term within the video. Current summarization methods generally do not take queries into account or require exhaustive data labeling. In this work, we present a weakly supervised query-focused video summarization method. Our proposed approach makes use of semantic attributes as an indicator of query relevance and semantic attention maps to locate related regions in the frames and utilizes both within a submodular maximization framework. We conducted experiments on the recently introduced RAD dataset and obtained highly competitive results. Moreover, to better evaluate the performance of our approach on longer videos, we collected a new dataset, which consists of 10 videos from YouTube and annotated with shot-level multiple attributes. Our dataset enables much diverse set of queries that can be used to summarize a video from different perspectives with more degrees of freedom.","Людині неможливо переглянути всі відео на платформах обміну відео і зрозуміти їх зміст. Тому методи машинного навчання використовуються для індексування відео шляхом визначення основних об’єктів, дій, місць і сцен. Резюмування — це ще один варіант, який дозволяє виділити відповідні частини, зберігаючи при цьому суть відео. Користувач може віддати перевагу пошуку певної дії або сцени за терміном запиту. Існуючі методи підсумовування зазвичай не враховують запити або вимагають вичерпного маркування даних. У цьому документі пропонується техніка підсумовування відео, орієнтована на запит із слабким контролем, яка використовує семантичні атрибути як маркер релевантності запиту та семантичні карти уваги для визначення місцезнаходження пов’язаних регіонів у кадрах. Цей підхід використовується в структурі субмодульної максимізації. Експерименти з набором даних RAD дали обнадійливі результати. Крім того, було створено новий набір даних, який складається з 10 відео YouTube з кількома атрибутами на рівні кадру. Цей набір даних дає змогу використовувати більш різноманітний набір запитів для узагальнення відео з різних точок зору з більшою свободою.",1
"Величезна кількість відео, які завантажуються на платформи для обміну відео, не дозволяє людині переглянути всі відео, зрозуміти, що в них відбувається. Таким чином, тепер застосовуються методи машинного навчання для індексування відео шляхом розпізнавання ключових об’єктів, дій і сцен або місць. Резюмування є ще однією альтернативою, оскільки воно пропонує виділити лише важливі частини, охоплюючи суть відеоконтенту. В ідеалі користувач може віддати перевагу аналізу певної дії чи сцени шляхом пошуку за запитом у відео. Поточні методи підсумовування зазвичай не враховують запити або вимагають вичерпного маркування даних. У цій роботі ми представляємо метод підсумовування відео, орієнтований на запити зі слабким контролем. Запропонований нами підхід використовує семантичні атрибути як індикатор релевантності запиту та семантичні карти уваги для визначення місцезнаходження пов’язаних регіонів у фреймах і використовує обидва в межах субмодульної структури максимізації. Ми провели експерименти з нещодавно представленим набором даних RAD і отримали дуже конкурентоспроможні результати. Крім того, щоб краще оцінити ефективність нашого підходу на довших відео, ми зібрали новий набір даних, який складається з 10 відео з YouTube і анотованих декількома атрибутами на рівні кадру. Наш набір даних дозволяє створювати різноманітні запити, які можна використовувати для узагальнення відео з різних точок зору з більшим ступенем свободи.","It is infeasible for a person to watch all the videos on video sharing platforms and comprehend their content. Therefore, machine learning techniques are employed to index videos by identifying essential objects, actions, places and scenes. Summarization is another choice that enables extraction of relevant parts while maintaining the essence of the video. The user may prefer to search a specific action or scene with a query term. Existing summarization methods generally do not take queries into account or require exhaustive data labelling. This paper proposes a weakly supervised query-focused video summarization technique that makes use of semantic attributes as a marker of query relevance and semantic attention maps to locate related regions in the frames. This approach is employed in a submodular maximization framework. Experiments on the RAD dataset produced promising results. Furthermore, a new dataset was created which consists of 10 YouTube videos with shot-level multiple attributes. This dataset enables a more diverse set of queries to summarize videos from various perspectives with more freedom.",1
"Величезна кількість відео, які завантажуються на платформи для обміну відео, не дозволяє людині переглянути всі відео, зрозуміти, що в них відбувається. Таким чином, тепер застосовуються методи машинного навчання для індексування відео шляхом розпізнавання ключових об’єктів, дій і сцен або місць. Резюмування є ще однією альтернативою, оскільки воно пропонує виділити лише важливі частини, охоплюючи суть відеоконтенту. В ідеалі користувач може віддати перевагу аналізу певної дії чи сцени шляхом пошуку за запитом у відео. Поточні методи підсумовування зазвичай не враховують запити або вимагають вичерпного маркування даних. У цій роботі ми представляємо метод підсумовування відео, орієнтований на запити зі слабким контролем. Запропонований нами підхід використовує семантичні атрибути як індикатор релевантності запиту та семантичні карти уваги для визначення місцезнаходження пов’язаних регіонів у фреймах і використовує обидва в межах субмодульної структури максимізації. Ми провели експерименти з нещодавно представленим набором даних RAD і отримали дуже конкурентоспроможні результати. Крім того, щоб краще оцінити ефективність нашого підходу на довших відео, ми зібрали новий набір даних, який складається з 10 відео з YouTube і анотованих декількома атрибутами на рівні кадру. Наш набір даних дозволяє створювати різноманітні запити, які можна використовувати для узагальнення відео з різних точок зору з більшим ступенем свободи.","Людині неможливо переглянути всі відео на платформах обміну відео і зрозуміти їх зміст. Тому методи машинного навчання використовуються для індексування відео шляхом визначення основних об’єктів, дій, місць і сцен. Резюмування — це ще один варіант, який дозволяє виділити відповідні частини, зберігаючи при цьому суть відео. Користувач може віддати перевагу пошуку певної дії або сцени за терміном запиту. Існуючі методи підсумовування зазвичай не враховують запити або вимагають вичерпного маркування даних. У цьому документі пропонується техніка підсумовування відео, орієнтована на запит із слабким контролем, яка використовує семантичні атрибути як маркер релевантності запиту та семантичні карти уваги для визначення місцезнаходження пов’язаних регіонів у кадрах. Цей підхід використовується в структурі субмодульної максимізації. Експерименти з набором даних RAD дали обнадійливі результати. Крім того, було створено новий набір даних, який складається з 10 відео YouTube з кількома атрибутами на рівні кадру. Цей набір даних дає змогу використовувати більш різноманітний набір запитів для узагальнення відео з різних точок зору з більшою свободою.",1
"Developing artificial learning systems that can understand and generate natural language has been one of the long-standing goals of artificial intelligence. Recent decades have witnessed an impressive progress on both of these problems, giving rise to a new family of approaches. Especially, the advances in deep learning over the past couple of years have led to neural approaches to natural language generation (NLG). These methods combine generative language learning techniques with neural-networks based frameworks. With a wide range of applications in natural language processing, neural NLG (NNLG) is a new and fast growing field of research. In this state-of-the-art report, we investigate the recent developments and applications of NNLG in its full extent from a multidimensional view, covering critical perspectives such as multimodality, multilinguality, controllability and learning strategies. We summarize the fundamental building blocks of NNLG approaches from these aspects and provide detailed reviews of commonly used preprocessing steps and basic neural architectures. This report also focuses on the seminal applications of these NNLG models such as machine translation, description generation, automatic speech recognition, abstractive summarization, text simplification, question answering and generation, and dialogue generation. Finally, we conclude with a thorough discussion of the described frameworks by pointing out some open research directions. ","Advances in deep learning have enabled artificial intelligence to achieve the long-standing goal of understanding and generating natural language. This has led to the emergence of a new family of approaches known as neural natural language generation (NNLG). With applications in natural language processing, NNLG is a rapidly growing field of research. This report provides a multidimensional overview of NNLG, including preprocessing steps, basic neural architectures, multimodality, multilinguality, controllability, and learning strategies. It also covers seminal applications of NNLG such as machine translation, description generation, automatic speech recognition, abstractive summarization, text simplification, question answering and generation, and dialogue generation. Lastly, the report concludes with a discussion of open research directions.",1
"Developing artificial learning systems that can understand and generate natural language has been one of the long-standing goals of artificial intelligence. Recent decades have witnessed an impressive progress on both of these problems, giving rise to a new family of approaches. Especially, the advances in deep learning over the past couple of years have led to neural approaches to natural language generation (NLG). These methods combine generative language learning techniques with neural-networks based frameworks. With a wide range of applications in natural language processing, neural NLG (NNLG) is a new and fast growing field of research. In this state-of-the-art report, we investigate the recent developments and applications of NNLG in its full extent from a multidimensional view, covering critical perspectives such as multimodality, multilinguality, controllability and learning strategies. We summarize the fundamental building blocks of NNLG approaches from these aspects and provide detailed reviews of commonly used preprocessing steps and basic neural architectures. This report also focuses on the seminal applications of these NNLG models such as machine translation, description generation, automatic speech recognition, abstractive summarization, text simplification, question answering and generation, and dialogue generation. Finally, we conclude with a thorough discussion of the described frameworks by pointing out some open research directions. ","Досягнення глибокого навчання дозволили штучному інтелекту досягти давньої мети розуміння та створення природної мови. Це призвело до появи нового сімейства підходів, відомих як нейронна генерація природної мови (NNLG). З додатками в обробці природної мови NNLG є галуззю досліджень, яка швидко розвивається. Цей звіт містить багатовимірний огляд NNLG, включаючи етапи попередньої обробки, базові нейронні архітектури, мультимодальність, багатомовність, керованість і стратегії навчання. Він також охоплює основні застосування NNLG, такі як машинний переклад, генерація описів, автоматичне розпізнавання мовлення, абстрактне резюмування, спрощення тексту, відповіді на запитання та генерація, а також генерація діалогів. Нарешті, доповідь завершується обговоренням відкритих напрямків дослідження.",1
"Розробка систем штучного навчання, які можуть розуміти та генерувати природну мову, була однією з давніх цілей штучного інтелекту. Останні десятиліття стали свідками вражаючого прогресу в обох цих проблемах, що породило нове сімейство підходів. Зокрема, прогрес у глибокому навчанні за останні пару років призвів до нейронних підходів до створення природної мови (NLG). Ці методи поєднують методи генеративного вивчення мови з нейронними мережами. Завдяки широкому спектру застосувань у обробці природної мови, нейронна NLG (NNLG) є новою галуззю досліджень, яка швидко розвивається. У цьому сучасному звіті ми досліджуємо останні розробки та застосування NNLG у повному обсязі з багатовимірного погляду, охоплюючи важливі перспективи, такі як мультимодальність, багатомовність, керованість і стратегії навчання. Ми підсумовуємо фундаментальні будівельні блоки підходів NNLG з цих аспектів і надаємо детальні огляди поширених етапів попередньої обробки та основних нейронних архітектур. У цьому звіті також зосереджено увагу на основних застосуваннях цих моделей NNLG, таких як машинний переклад, генерація описів, автоматичне розпізнавання мовлення, абстрактне резюмування, спрощення тексту, відповіді на запитання та генерація, а також генерація діалогів. Нарешті, ми закінчуємо детальним обговоренням описаних структур, вказуючи на деякі відкриті напрямки дослідження.","Advances in deep learning have enabled artificial intelligence to achieve the long-standing goal of understanding and generating natural language. This has led to the emergence of a new family of approaches known as neural natural language generation (NNLG). With applications in natural language processing, NNLG is a rapidly growing field of research. This report provides a multidimensional overview of NNLG, including preprocessing steps, basic neural architectures, multimodality, multilinguality, controllability, and learning strategies. It also covers seminal applications of NNLG such as machine translation, description generation, automatic speech recognition, abstractive summarization, text simplification, question answering and generation, and dialogue generation. Lastly, the report concludes with a discussion of open research directions.",1
"Розробка систем штучного навчання, які можуть розуміти та генерувати природну мову, була однією з давніх цілей штучного інтелекту. Останні десятиліття стали свідками вражаючого прогресу в обох цих проблемах, що породило нове сімейство підходів. Зокрема, прогрес у глибокому навчанні за останні пару років призвів до нейронних підходів до створення природної мови (NLG). Ці методи поєднують методи генеративного вивчення мови з нейронними мережами. Завдяки широкому спектру застосувань у обробці природної мови, нейронна NLG (NNLG) є новою галуззю досліджень, яка швидко розвивається. У цьому сучасному звіті ми досліджуємо останні розробки та застосування NNLG у повному обсязі з багатовимірного погляду, охоплюючи важливі перспективи, такі як мультимодальність, багатомовність, керованість і стратегії навчання. Ми підсумовуємо фундаментальні будівельні блоки підходів NNLG з цих аспектів і надаємо детальні огляди поширених етапів попередньої обробки та основних нейронних архітектур. У цьому звіті також зосереджено увагу на основних застосуваннях цих моделей NNLG, таких як машинний переклад, генерація описів, автоматичне розпізнавання мовлення, абстрактне резюмування, спрощення тексту, відповіді на запитання та генерація, а також генерація діалогів. Нарешті, ми закінчуємо детальним обговоренням описаних структур, вказуючи на деякі відкриті напрямки дослідження.","Досягнення глибокого навчання дозволили штучному інтелекту досягти давньої мети розуміння та створення природної мови. Це призвело до появи нового сімейства підходів, відомих як нейронна генерація природної мови (NNLG). З додатками в обробці природної мови NNLG є галуззю досліджень, яка швидко розвивається. Цей звіт містить багатовимірний огляд NNLG, включаючи етапи попередньої обробки, базові нейронні архітектури, мультимодальність, багатомовність, керованість і стратегії навчання. Він також охоплює основні застосування NNLG, такі як машинний переклад, генерація описів, автоматичне розпізнавання мовлення, абстрактне резюмування, спрощення тексту, відповіді на запитання та генерація, а також генерація діалогів. Нарешті, доповідь завершується обговоренням відкритих напрямків дослідження.",1
"While stochastic video prediction models enable future prediction under uncertainty, they mostly fail to model the complex dynamics of real-world scenes. For example, they cannot provide reliable predictions for scenes with a moving camera and independently moving foreground objects in driving scenarios. The existing methods fail to fully capture the dynamics of the structured world by only focusing on changes in pixels. In this paper, we assume that there is an underlying process creating observations in a video and propose to factorize it into static and dynamic components. We model the static part based on the scene structure and the ego-motion of the vehicle, and the dynamic part based on the remaining motion of the dynamic objects. By learning separate distributions of changes in foreground and background, we can decompose the scene into static and dynamic parts and separately model the change in each. Our experiments demonstrate that disentangling structure and motion helps stochastic video prediction, leading to better future predictions in complex driving scenarios on two real-world driving datasets, KITTI and Cityscapes. ","Stochastic video prediction models are inadequate for modeling the intricacies of real-world scenes, such as those with moving cameras and independently moving foreground objects in driving settings. To address this, we propose factoring the process of generating observations in a video into static and dynamic components. The static part is based on the scene structure and the vehicle's ego-motion, while the dynamic part is dependent on the motion of the dynamic objects. Our technique of disentangling structure and motion helps stochastic video prediction, thereby enhancing future predictions in complex driving scenarios on KITTI and Cityscapes datasets.",1
"While stochastic video prediction models enable future prediction under uncertainty, they mostly fail to model the complex dynamics of real-world scenes. For example, they cannot provide reliable predictions for scenes with a moving camera and independently moving foreground objects in driving scenarios. The existing methods fail to fully capture the dynamics of the structured world by only focusing on changes in pixels. In this paper, we assume that there is an underlying process creating observations in a video and propose to factorize it into static and dynamic components. We model the static part based on the scene structure and the ego-motion of the vehicle, and the dynamic part based on the remaining motion of the dynamic objects. By learning separate distributions of changes in foreground and background, we can decompose the scene into static and dynamic parts and separately model the change in each. Our experiments demonstrate that disentangling structure and motion helps stochastic video prediction, leading to better future predictions in complex driving scenarios on two real-world driving datasets, KITTI and Cityscapes. ","Стохастичні моделі відеопрогнозування не підходять для моделювання тонкощів сцен реального світу, наприклад, з рухомими камерами та незалежно рухомими об’єктами переднього плану в налаштуваннях водіння. Щоб вирішити цю проблему, ми пропонуємо розділити процес генерування спостережень у відео на статичні та динамічні компоненти. Статична частина базується на структурі сцени та его-руху автомобіля, тоді як динамічна частина залежить від руху динамічних об’єктів. Наша техніка розмежування структури та руху допомагає стохастичному прогнозуванню відео, тим самим покращуючи майбутні прогнози у складних сценаріях водіння на наборах даних KITTI та Cityscapes.",1
"Хоча стохастичні моделі прогнозування відео дозволяють прогнозувати майбутнє в умовах невизначеності, вони здебільшого не в змоделі складної динаміки сцен реального світу. Наприклад, вони не можуть забезпечити надійні прогнози для сцен із рухомою камерою та незалежно рухомими об’єктами переднього плану в сценаріях водіння. Існуючі методи не можуть повністю охопити динаміку структурованого світу, зосереджуючись лише на змінах у пікселях. У цьому документі ми припускаємо, що існує базовий процес створення спостережень у відео, і пропонуємо розділити його на статичні та динамічні компоненти. Ми моделюємо статичну частину на основі структури сцени та его-руху автомобіля, а динамічну частину – на основі залишкового руху динамічних об’єктів. Вивчаючи окремі розподіли змін переднього та заднього планів, ми можемо розкласти сцену на статичні та динамічні частини та окремо моделювати зміни в кожній. Наші експерименти демонструють, що розмежування структури та руху сприяє стохастичному прогнозуванню відео, що дає змогу краще прогнозувати майбутнє у складних сценаріях водіння на двох наборах даних реального водіння, KITTI та Cityscapes.","Stochastic video prediction models are inadequate for modeling the intricacies of real-world scenes, such as those with moving cameras and independently moving foreground objects in driving settings. To address this, we propose factoring the process of generating observations in a video into static and dynamic components. The static part is based on the scene structure and the vehicle's ego-motion, while the dynamic part is dependent on the motion of the dynamic objects. Our technique of disentangling structure and motion helps stochastic video prediction, thereby enhancing future predictions in complex driving scenarios on KITTI and Cityscapes datasets.",1
"Хоча стохастичні моделі прогнозування відео дозволяють прогнозувати майбутнє в умовах невизначеності, вони здебільшого не в змоделі складної динаміки сцен реального світу. Наприклад, вони не можуть забезпечити надійні прогнози для сцен із рухомою камерою та незалежно рухомими об’єктами переднього плану в сценаріях водіння. Існуючі методи не можуть повністю охопити динаміку структурованого світу, зосереджуючись лише на змінах у пікселях. У цьому документі ми припускаємо, що існує базовий процес створення спостережень у відео, і пропонуємо розділити його на статичні та динамічні компоненти. Ми моделюємо статичну частину на основі структури сцени та его-руху автомобіля, а динамічну частину – на основі залишкового руху динамічних об’єктів. Вивчаючи окремі розподіли змін переднього та заднього планів, ми можемо розкласти сцену на статичні та динамічні частини та окремо моделювати зміни в кожній. Наші експерименти демонструють, що розмежування структури та руху сприяє стохастичному прогнозуванню відео, що дає змогу краще прогнозувати майбутнє у складних сценаріях водіння на двох наборах даних реального водіння, KITTI та Cityscapes.","Стохастичні моделі відеопрогнозування не підходять для моделювання тонкощів сцен реального світу, наприклад, з рухомими камерами та незалежно рухомими об’єктами переднього плану в налаштуваннях водіння. Щоб вирішити цю проблему, ми пропонуємо розділити процес генерування спостережень у відео на статичні та динамічні компоненти. Статична частина базується на структурі сцени та его-руху автомобіля, тоді як динамічна частина залежить від руху динамічних об’єктів. Наша техніка розмежування структури та руху допомагає стохастичному прогнозуванню відео, тим самим покращуючи майбутні прогнози у складних сценаріях водіння на наборах даних KITTI та Cityscapes.",1
"How to best integrate linguistic and perceptual processing in multi-modal tasks that involve language and vision is an important open problem. In this work, we argue that the common practice of using language in a topdown manner, to direct visual attention over high-level visual features, may not be optimal. We hypothesize that the use of language to also condition the bottom-up processing from pixels to high-level features can provide benefits to the overall performance. To support our claim, we propose a U-Net-based model and perform experiments on two language-vision dense-prediction tasks: referring expression segmentation and language-guided image colorization. We compare results where either one or both of the top-down and bottom-up visual branches are conditioned on language. Our experiments reveal that using language to control the filters for bottom-up visual processing in addition to top-down attention leads to better results on both tasks and achieves competitive performance. Our linguistic analysis suggests that bottom-up conditioning improves segmentation of objects especially when input text refers to low-level visual concepts","Integrating linguistic and perceptual processing in multi-modal tasks involving language and vision is an open problem. Commonly, language is used to direct attention to high-level visual features in a top-down manner, which may not be optimal. We propose a U-Net-based model and explore the use of language to condition bottom-up processing from pixels to high-level features in two language-vision dense-prediction tasks: referring expression segmentation and language-guided image colorization. Results show that conditioning both top-down attention and bottom-up visual processing with language leads to better results on both tasks and competitive performance. Further linguistic analysis reveals that bottom-up conditioning improves segmentation of objects when input text refers to low-level visual concepts.",1
"How to best integrate linguistic and perceptual processing in multi-modal tasks that involve language and vision is an important open problem. In this work, we argue that the common practice of using language in a topdown manner, to direct visual attention over high-level visual features, may not be optimal. We hypothesize that the use of language to also condition the bottom-up processing from pixels to high-level features can provide benefits to the overall performance. To support our claim, we propose a U-Net-based model and perform experiments on two language-vision dense-prediction tasks: referring expression segmentation and language-guided image colorization. We compare results where either one or both of the top-down and bottom-up visual branches are conditioned on language. Our experiments reveal that using language to control the filters for bottom-up visual processing in addition to top-down attention leads to better results on both tasks and achieves competitive performance. Our linguistic analysis suggests that bottom-up conditioning improves segmentation of objects especially when input text refers to low-level visual concepts","Інтеграція лінгвістичної обробки та обробки сприйняття в мультимодальних завданнях, що включають мову та зір, є відкритою проблемою. Зазвичай мова використовується для привернення уваги до візуальних особливостей високого рівня зверху вниз, що може бути неоптимальним. Ми пропонуємо модель на основі U-Net і досліджуємо використання мови для обумовлення обробки знизу вгору від пікселів до високорівневих функцій у двох завданнях щільного прогнозування мовного бачення: сегментація виразу за посиланням і кольорування зображення під керуванням мови. Результати показують, що обумовлення уваги як «зверху-вниз», так і візуальної обробки знизу вгору за допомогою мови призводить до кращих результатів як у завданнях, так і в змаганні. Подальший лінгвістичний аналіз показує, що кондиціонування знизу вгору покращує сегментацію об’єктів, коли вхідний текст відноситься до візуальних концепцій низького рівня.",1
"Як найкраще інтегрувати лінгвістичну та перцептивну обробку в мультимодальних завданнях, які включають мову та зір, є важливою відкритою проблемою. У цій роботі ми стверджуємо, що звичайна практика використання мови за принципом «зверху вниз» для спрямування візуальної уваги на візуальні функції високого рівня може бути неоптимальною. Ми припускаємо, що використання мови для обумовлення обробки знизу вгору від пікселів до функцій високого рівня може принести переваги загальній продуктивності. Щоб підтвердити наше твердження, ми пропонуємо модель на основі U-Net і проводимо експерименти з двома завданнями щільного прогнозування мовного бачення: сегментація виразу за посиланням і кольорування зображення під керуванням мови. Ми порівнюємо результати, де одна або обидві візуальні гілки зверху вниз і знизу вгору залежать від мови. Наші експерименти показують, що використання мови для керування фільтрами для візуальної обробки знизу вгору на додаток до уваги зверху вниз призводить до кращих результатів в обох завданнях і досягає конкурентоспроможності. Наш лінгвістичний аналіз показує, що кондиціонування знизу вгору покращує сегментацію об’єктів, особливо коли вхідний текст відноситься до візуальних концепцій низького рівня","Integrating linguistic and perceptual processing in multi-modal tasks involving language and vision is an open problem. Commonly, language is used to direct attention to high-level visual features in a top-down manner, which may not be optimal. We propose a U-Net-based model and explore the use of language to condition bottom-up processing from pixels to high-level features in two language-vision dense-prediction tasks: referring expression segmentation and language-guided image colorization. Results show that conditioning both top-down attention and bottom-up visual processing with language leads to better results on both tasks and competitive performance. Further linguistic analysis reveals that bottom-up conditioning improves segmentation of objects when input text refers to low-level visual concepts.",1
"Як найкраще інтегрувати лінгвістичну та перцептивну обробку в мультимодальних завданнях, які включають мову та зір, є важливою відкритою проблемою. У цій роботі ми стверджуємо, що звичайна практика використання мови за принципом «зверху вниз» для спрямування візуальної уваги на візуальні функції високого рівня може бути неоптимальною. Ми припускаємо, що використання мови для обумовлення обробки знизу вгору від пікселів до функцій високого рівня може принести переваги загальній продуктивності. Щоб підтвердити наше твердження, ми пропонуємо модель на основі U-Net і проводимо експерименти з двома завданнями щільного прогнозування мовного бачення: сегментація виразу за посиланням і кольорування зображення під керуванням мови. Ми порівнюємо результати, де одна або обидві візуальні гілки зверху вниз і знизу вгору залежать від мови. Наші експерименти показують, що використання мови для керування фільтрами для візуальної обробки знизу вгору на додаток до уваги зверху вниз призводить до кращих результатів в обох завданнях і досягає конкурентоспроможності. Наш лінгвістичний аналіз показує, що кондиціонування знизу вгору покращує сегментацію об’єктів, особливо коли вхідний текст відноситься до візуальних концепцій низького рівня","Інтеграція лінгвістичної обробки та обробки сприйняття в мультимодальних завданнях, що включають мову та зір, є відкритою проблемою. Зазвичай мова використовується для привернення уваги до візуальних особливостей високого рівня зверху вниз, що може бути неоптимальним. Ми пропонуємо модель на основі U-Net і досліджуємо використання мови для обумовлення обробки знизу вгору від пікселів до високорівневих функцій у двох завданнях щільного прогнозування мовного бачення: сегментація виразу за посиланням і кольорування зображення під керуванням мови. Результати показують, що обумовлення уваги як «зверху-вниз», так і візуальної обробки знизу вгору за допомогою мови призводить до кращих результатів як у завданнях, так і в змаганні. Подальший лінгвістичний аналіз показує, що кондиціонування знизу вгору покращує сегментацію об’єктів, коли вхідний текст відноситься до візуальних концепцій низького рівня.",1
"Capturing images under extremely low-light conditions poses significant challenges for the standard camera pipeline. Images become too dark and too noisy, which makes traditional enhancement techniques almost impossible to apply. Recently, learning-based approaches have shown very promising results for this task since they have substantially more expressive capabilities to allow for improved quality. Motivated by these studies, in this paper, we aim to leverage burst photography to boost the performance and obtain much sharper and more accurate RGB images from extremely dark raw images. The backbone of our proposed framework is a novel coarse-to-fine network architecture that generates high-quality outputs progressively. The coarse network predicts a low-resolution, denoised raw image, which is then fed to the fine network to recover fine-scale details and realistic textures. To further reduce the noise level and improve the color accuracy, we extend this network to a permutation invariant structure so that it takes a burst of low-light images as input and merges information from multiple images at the feature-level. Our experiments demonstrate that our approach leads to perceptually more pleasing results than the state-of-the-art methods by producing more detailed and considerably higher quality images.","Capturing images in dim lighting is difficult for the standard camera pipeline. Images become too dark and noisy, making traditional enhancement difficult. Recently, learning-based approaches have demonstrated potential for this task due to their ability to create improved quality. To boost performance, this paper proposes a novel coarse-to-fine network architecture that generates high-quality outputs gradually. This network predicts a low-resolution, denoised raw image, which is then fed to the fine network to recover small-scale details and textures. To reduce noise and improve color accuracy, the network is extended to a permutation invariant structure which takes a burst of low-light images as input and combines information from multiple images at the feature-level. Results show that our approach produces more detailed and higher quality images than existing methods.",1
"Capturing images under extremely low-light conditions poses significant challenges for the standard camera pipeline. Images become too dark and too noisy, which makes traditional enhancement techniques almost impossible to apply. Recently, learning-based approaches have shown very promising results for this task since they have substantially more expressive capabilities to allow for improved quality. Motivated by these studies, in this paper, we aim to leverage burst photography to boost the performance and obtain much sharper and more accurate RGB images from extremely dark raw images. The backbone of our proposed framework is a novel coarse-to-fine network architecture that generates high-quality outputs progressively. The coarse network predicts a low-resolution, denoised raw image, which is then fed to the fine network to recover fine-scale details and realistic textures. To further reduce the noise level and improve the color accuracy, we extend this network to a permutation invariant structure so that it takes a burst of low-light images as input and merges information from multiple images at the feature-level. Our experiments demonstrate that our approach leads to perceptually more pleasing results than the state-of-the-art methods by producing more detailed and considerably higher quality images.","Зйомка зображень при слабкому освітленні є складною для стандартної камери. Зображення стають надто темними та шумними, що ускладнює традиційне покращення. Нещодавно підходи, засновані на навчанні, продемонстрували потенціал для цього завдання завдяки своїй здатності створювати покращену якість. Щоб підвищити продуктивність, у цьому документі пропонується нова мережева архітектура від грубої до тонкої, яка поступово створює високоякісні результати. Ця мережа передбачає необроблене зображення з низькою роздільною здатністю, знешумлене, яке потім передається в тонку мережу для відновлення дрібних деталей і текстур. Щоб зменшити шум і підвищити точність кольору, мережа розширена до інваріантної структури, яка приймає серію зображень із слабким освітленням як вхідні дані та об’єднує інформацію з кількох зображень на рівні функцій. Результати показують, що наш підхід створює більш детальні та якісніші зображення, ніж існуючі методи.",1
"Зйомка зображень в умовах надзвичайно слабкого освітлення створює значні труднощі для стандартної камери. Зображення стають надто темними та занадто шумними, що робить практично неможливим застосування традиційних методів покращення. Нещодавно підходи, що ґрунтуються на навчанні, показали багатообіцяючі результати для цього завдання, оскільки вони мають значно більше виразних можливостей для підвищення якості. Керуючись цими дослідженнями, у цій статті ми прагнемо використовувати серійну фотографію, щоб підвищити продуктивність і отримати набагато чіткіші та точніші RGB-зображення з надзвичайно темних необроблених зображень. Основою запропонованої нами структури є нова мережева архітектура від грубої до тонкої, яка поступово генерує високоякісні результати. Груба мережа передбачає необроблене зображення з низькою роздільною здатністю, знешумлене, яке потім передається в тонку мережу для відновлення дрібних деталей і реалістичних текстур. Щоб ще більше знизити рівень шуму та підвищити точність кольору, ми розширюємо цю мережу до структури, інваріантної до перестановок, щоб вона приймала серію зображень із слабким освітленням як вхідні дані та об’єднувала інформацію з кількох зображень на рівні функцій. Наші експерименти демонструють, що наш підхід дає сприятливіші результати, ніж найсучасніші методи, створюючи детальніші та значно якісніші зображення.","Capturing images in dim lighting is difficult for the standard camera pipeline. Images become too dark and noisy, making traditional enhancement difficult. Recently, learning-based approaches have demonstrated potential for this task due to their ability to create improved quality. To boost performance, this paper proposes a novel coarse-to-fine network architecture that generates high-quality outputs gradually. This network predicts a low-resolution, denoised raw image, which is then fed to the fine network to recover small-scale details and textures. To reduce noise and improve color accuracy, the network is extended to a permutation invariant structure which takes a burst of low-light images as input and combines information from multiple images at the feature-level. Results show that our approach produces more detailed and higher quality images than existing methods.",1
"Зйомка зображень в умовах надзвичайно слабкого освітлення створює значні труднощі для стандартної камери. Зображення стають надто темними та занадто шумними, що робить практично неможливим застосування традиційних методів покращення. Нещодавно підходи, що ґрунтуються на навчанні, показали багатообіцяючі результати для цього завдання, оскільки вони мають значно більше виразних можливостей для підвищення якості. Керуючись цими дослідженнями, у цій статті ми прагнемо використовувати серійну фотографію, щоб підвищити продуктивність і отримати набагато чіткіші та точніші RGB-зображення з надзвичайно темних необроблених зображень. Основою запропонованої нами структури є нова мережева архітектура від грубої до тонкої, яка поступово генерує високоякісні результати. Груба мережа передбачає необроблене зображення з низькою роздільною здатністю, знешумлене, яке потім передається в тонку мережу для відновлення дрібних деталей і реалістичних текстур. Щоб ще більше знизити рівень шуму та підвищити точність кольору, ми розширюємо цю мережу до структури, інваріантної до перестановок, щоб вона приймала серію зображень із слабким освітленням як вхідні дані та об’єднувала інформацію з кількох зображень на рівні функцій. Наші експерименти демонструють, що наш підхід дає сприятливіші результати, ніж найсучасніші методи, створюючи детальніші та значно якісніші зображення.","Зйомка зображень при слабкому освітленні є складною для стандартної камери. Зображення стають надто темними та шумними, що ускладнює традиційне покращення. Нещодавно підходи, засновані на навчанні, продемонстрували потенціал для цього завдання завдяки своїй здатності створювати покращену якість. Щоб підвищити продуктивність, у цьому документі пропонується нова мережева архітектура від грубої до тонкої, яка поступово створює високоякісні результати. Ця мережа передбачає необроблене зображення з низькою роздільною здатністю, знешумлене, яке потім передається в тонку мережу для відновлення дрібних деталей і текстур. Щоб зменшити шум і підвищити точність кольору, мережа розширена до інваріантної структури, яка приймає серію зображень із слабким освітленням як вхідні дані та об’єднує інформацію з кількох зображень на рівні функцій. Результати показують, що наш підхід створює більш детальні та якісніші зображення, ніж існуючі методи.",1
"Image editing is a commonly studied problem in computer graphics. Despite the presence of many advanced editing tools, there is no satisfactory solution to controllably update the position of the sun using a single image. This problem is made complicated by the presence of clouds, complex landscapes, and the atmospheric effects that must be accounted for. In this paper, we tackle this problem starting with only a single photograph. With the user clicking on the initial position of the sun, our algorithm performs several estimation and segmentation processes for finding the horizon, scene depth, clouds, and the sky line. After this initial process, the user can make both fine- and large-scale changes on the position of the sun: it can be set beneath the mountains or moved behind the clouds practically turning a midday photograph into a sunset (or vice versa). We leverage a precomputed atmospheric scattering algorithm to make all of these changes not only realistic but also in real-time. We demonstrate our results using both clear and cloudy skies, showing how to add, remove, and relight clouds, all the while allowing for advanced effects such as scattering, shadows, light shafts, and lens flares","Despite advanced editing tools, there is no satisfactory way to adjust the sun's position in a single image, due to clouds, complex landscapes, and atmospheric effects. This paper proposes a solution, starting with a single photograph. With the user clicking the initial position of the sun, the algorithm performs estimation and segmentation to find the horizon, scene depth, clouds, and sky line. Subsequently, the user can make fine- and large-scale changes to the sun's position - beneath mountains or behind clouds - practically transforming a midday photo into a sunset (or vice versa). Leveraging precomputed atmospheric scattering, these changes are both realistic and real-time. Results using clear and cloudy skies demonstrate the ability to add, remove, and relight clouds, while also allowing for scattering, shadows, light shafts, and lens flares.",1
"Image editing is a commonly studied problem in computer graphics. Despite the presence of many advanced editing tools, there is no satisfactory solution to controllably update the position of the sun using a single image. This problem is made complicated by the presence of clouds, complex landscapes, and the atmospheric effects that must be accounted for. In this paper, we tackle this problem starting with only a single photograph. With the user clicking on the initial position of the sun, our algorithm performs several estimation and segmentation processes for finding the horizon, scene depth, clouds, and the sky line. After this initial process, the user can make both fine- and large-scale changes on the position of the sun: it can be set beneath the mountains or moved behind the clouds practically turning a midday photograph into a sunset (or vice versa). We leverage a precomputed atmospheric scattering algorithm to make all of these changes not only realistic but also in real-time. We demonstrate our results using both clear and cloudy skies, showing how to add, remove, and relight clouds, all the while allowing for advanced effects such as scattering, shadows, light shafts, and lens flares","Незважаючи на передові інструменти редагування, немає задовільного способу налаштувати положення сонця на одному зображенні через хмари, складні пейзажі та атмосферні ефекти. У цьому документі пропонується рішення, починаючи з однієї фотографії. Коли користувач натискає початкове положення сонця, алгоритм виконує оцінку та сегментацію, щоб знайти горизонт, глибину сцени, хмари та лінію неба. Згодом користувач може робити дрібні та масштабні зміни положення сонця – під горами чи за хмарами – практично перетворюючи полуденну фотографію на захід сонця (або навпаки). Використовуючи попередньо обчислене атмосферне розсіювання, ці зміни є реалістичними та відбуваються в реальному часі. Результати з використанням ясного та хмарного неба демонструють можливість додавати, видаляти та повторно освітлювати хмари, а також враховувати розсіювання, тіні, світлові вали та відблиски об’єктива.",1
"Редагування зображень є широко дослідженою проблемою в комп’ютерній графіці. Незважаючи на наявність багатьох розширених інструментів редагування, не існує задовільного рішення для контрольованого оновлення положення сонця за допомогою одного зображення. Ця проблема ускладнюється наявністю хмар, складними ландшафтами та атмосферними впливами, які необхідно враховувати. У цій статті ми вирішуємо цю проблему, починаючи лише з однієї фотографії. Коли користувач натискає початкове положення сонця, наш алгоритм виконує кілька процесів оцінки та сегментації для пошуку горизонту, глибини сцени, хмар і лінії неба. Після цього початкового процесу користувач може робити як дрібні, так і масштабні зміни положення сонця: його можна встановити під гори або перемістити за хмари, практично перетворивши полуденну фотографію на захід сонця (або навпаки). Ми використовуємо попередньо обчислений алгоритм атмосферного розсіювання, щоб зробити всі ці зміни не лише реалістичними, але й у реальному часі. Ми демонструємо наші результати, використовуючи як ясне, так і хмарне небо, показуючи, як додавати, видаляти та повторно освітлювати хмари, водночас допускаючи розширені ефекти, такі як розсіювання, тіні, світлові вали та відблиски від лінз.","Despite advanced editing tools, there is no satisfactory way to adjust the sun's position in a single image, due to clouds, complex landscapes, and atmospheric effects. This paper proposes a solution, starting with a single photograph. With the user clicking the initial position of the sun, the algorithm performs estimation and segmentation to find the horizon, scene depth, clouds, and sky line. Subsequently, the user can make fine- and large-scale changes to the sun's position - beneath mountains or behind clouds - practically transforming a midday photo into a sunset (or vice versa). Leveraging precomputed atmospheric scattering, these changes are both realistic and real-time. Results using clear and cloudy skies demonstrate the ability to add, remove, and relight clouds, while also allowing for scattering, shadows, light shafts, and lens flares.",1
"Редагування зображень є широко дослідженою проблемою в комп’ютерній графіці. Незважаючи на наявність багатьох розширених інструментів редагування, не існує задовільного рішення для контрольованого оновлення положення сонця за допомогою одного зображення. Ця проблема ускладнюється наявністю хмар, складними ландшафтами та атмосферними впливами, які необхідно враховувати. У цій статті ми вирішуємо цю проблему, починаючи лише з однієї фотографії. Коли користувач натискає початкове положення сонця, наш алгоритм виконує кілька процесів оцінки та сегментації для пошуку горизонту, глибини сцени, хмар і лінії неба. Після цього початкового процесу користувач може робити як дрібні, так і масштабні зміни положення сонця: його можна встановити під гори або перемістити за хмари, практично перетворивши полуденну фотографію на захід сонця (або навпаки). Ми використовуємо попередньо обчислений алгоритм атмосферного розсіювання, щоб зробити всі ці зміни не лише реалістичними, але й у реальному часі. Ми демонструємо наші результати, використовуючи як ясне, так і хмарне небо, показуючи, як додавати, видаляти та повторно освітлювати хмари, водночас допускаючи розширені ефекти, такі як розсіювання, тіні, світлові вали та відблиски від лінз.","Незважаючи на вдосконалені інструменти редагування, немає задовільного способу налаштувати положення сонця на одному зображенні через хмари, складні пейзажі та атмосферні ефекти. У цьому документі пропонується рішення, починаючи з однієї фотографії. Коли користувач натискає початкове положення сонця, алгоритм виконує оцінку та сегментацію, щоб знайти горизонт, глибину сцени, хмари та лінію неба. Згодом користувач може робити дрібні та масштабні зміни положення сонця – під горами чи за хмарами – практично перетворюючи полуденну фотографію на захід сонця (або навпаки). Використовуючи попередньо обчислене атмосферне розсіювання, ці зміни є реалістичними та реальними. Результати з використанням ясного та хмарного неба демонструють можливість додавати, видаляти та повторно освітлювати хмари, а також враховувати розсіювання, тіні, світлові вали та відблиски об’єктива.",1
"Learning robust representations is critical for the success of person re-identification and attribute recognition systems. However, to achieve this, we must use a large dataset of diverse person images as well as annotations of identity labels and/or a set of different attributes. Apart from the obvious concerns about privacy issues, the manual annotation process is both time consuming and too costly. In this paper, we instead propose to use synthetic person images for addressing these difficulties. Specifically, we first introduce Synthetic18K, a large-scale dataset of over 1 million computer generated person images of 18K unique identities with relevant attributes. Moreover, we demonstrate that pretraining of simple deep architectures on Synthetic18K for person re-identification and attribute recognition and then fine-tuning on real data leads to significant improvements in prediction performances, giving results better than or comparable to state-of-the-art models.","Obtaining strong representations is essential for the success of person re-identification and attribute recognition systems. To accomplish this, a considerable amount of diverse person images and annotations of identity labels/attributes must be employed. This manual annotation process is expensive and time consuming, as well as a potential privacy issue. We propose using synthetic person images to address these problems. To this end, we introduce Synthetic18K, a large dataset of 1 million computer generated person images of 18K distinct identities with corresponding attributes. We show that pretraining simple deep architectures on Synthetic18K for person re-identification and attribute recognition, followed by fine-tuning on real data, leads to enhanced prediction performances, surpassing or equaling state-of-the-art models.",1
"Learning robust representations is critical for the success of person re-identification and attribute recognition systems. However, to achieve this, we must use a large dataset of diverse person images as well as annotations of identity labels and/or a set of different attributes. Apart from the obvious concerns about privacy issues, the manual annotation process is both time consuming and too costly. In this paper, we instead propose to use synthetic person images for addressing these difficulties. Specifically, we first introduce Synthetic18K, a large-scale dataset of over 1 million computer generated person images of 18K unique identities with relevant attributes. Moreover, we demonstrate that pretraining of simple deep architectures on Synthetic18K for person re-identification and attribute recognition and then fine-tuning on real data leads to significant improvements in prediction performances, giving results better than or comparable to state-of-the-art models.","Отримання чітких репрезентацій є важливим для успіху систем повторної ідентифікації особи та розпізнавання атрибутів. Щоб досягти цього, необхідно використати значну кількість різноманітних зображень людей і анотацій ідентифікаційних міток/атрибутів. Цей процес анотації вручну є дорогим і трудомістким, а також потенційною проблемою конфіденційності. Ми пропонуємо використовувати зображення синтетичних людей для вирішення цих проблем. З цією метою ми представляємо Synthetic18K, великий набір даних із 1 мільйона зображень людей, згенерованих комп’ютером, із 18 тисячами окремих осіб із відповідними атрибутами. Ми показуємо, що попереднє навчання простих глибоких архітектур на Synthetic18K для повторної ідентифікації особи та розпізнавання атрибутів з подальшим тонким налаштуванням на реальних даних призводить до покращених характеристик прогнозування, перевершуючи або дорівнюючи найсучаснішим моделям.",1
"Вивчення надійних представлень має вирішальне значення для успіху систем повторної ідентифікації особи та розпізнавання атрибутів. Однак, щоб досягти цього, ми повинні використовувати великий набір даних різноманітних зображень людей, а також анотації міток ідентифікації та/або набір різних атрибутів. Крім очевидного занепокоєння щодо проблем конфіденційності, процес анотації вручну займає багато часу та надто дорого. У цій статті ми замість цього пропонуємо використовувати зображення синтетичних людей для вирішення цих труднощів. Зокрема, ми вперше представляємо Synthetic18K, великомасштабний набір даних із понад 1 мільйона зображень людей, згенерованих комп’ютером, 18K унікальних ідентифікацій із відповідними атрибутами. Крім того, ми демонструємо, що попереднє навчання простих глибоких архітектур на Synthetic18K для повторної ідентифікації особи та розпізнавання атрибутів, а потім точного налаштування на реальних даних призводить до значного покращення продуктивності прогнозування, даючи результати, кращі або порівнювані з найсучаснішими. моделі.","Obtaining strong representations is essential for the success of person re-identification and attribute recognition systems. To accomplish this, a considerable amount of diverse person images and annotations of identity labels/attributes must be employed. This manual annotation process is expensive and time consuming, as well as a potential privacy issue. We propose using synthetic person images to address these problems. To this end, we introduce Synthetic18K, a large dataset of 1 million computer generated person images of 18K distinct identities with corresponding attributes. We show that pretraining simple deep architectures on Synthetic18K for person re-identification and attribute recognition, followed by fine-tuning on real data, leads to enhanced prediction performances, surpassing or equaling state-of-the-art models.",1
"Вивчення надійних представлень має вирішальне значення для успіху систем повторної ідентифікації особи та розпізнавання атрибутів. Однак, щоб досягти цього, ми повинні використовувати великий набір даних різноманітних зображень людей, а також анотації міток ідентифікації та/або набір різних атрибутів. Крім очевидного занепокоєння щодо проблем конфіденційності, процес анотації вручну займає багато часу та надто дорого. У цій статті ми замість цього пропонуємо використовувати зображення синтетичних людей для вирішення цих труднощів. Зокрема, ми вперше представляємо Synthetic18K, великомасштабний набір даних із понад 1 мільйона зображень людей, згенерованих комп’ютером, 18K унікальних ідентифікацій із відповідними атрибутами. Крім того, ми демонструємо, що попереднє навчання простих глибоких архітектур на Synthetic18K для повторної ідентифікації особи та розпізнавання атрибутів, а потім точного налаштування на реальних даних призводить до значного покращення продуктивності прогнозування, даючи результати, кращі або порівнювані з найсучаснішими. моделі.","Отримання чітких репрезентацій є важливим для успіху систем повторної ідентифікації особи та розпізнавання атрибутів. Щоб досягти цього, необхідно використати значну кількість різноманітних зображень людей і анотацій ідентифікаційних міток/атрибутів. Цей процес анотації вручну є дорогим і трудомістким, а також потенційною проблемою конфіденційності. Ми пропонуємо використовувати зображення синтетичних людей для вирішення цих проблем. З цією метою ми представляємо Synthetic18K, великий набір даних із 1 мільйона зображень людей, згенерованих комп’ютером, із 18 тисячами окремих осіб із відповідними атрибутами. Ми показуємо, що попереднє навчання простих глибоких архітектур на Synthetic18K для повторної ідентифікації особи та розпізнавання атрибутів з подальшим тонким налаштуванням на реальних даних призводить до покращених характеристик прогнозування, перевершуючи або дорівнюючи найсучаснішим моделям.",1
"Today, the cutting edge of computer vision research greatly depends on the availability of large datasets, which are critical for effectively training and testing new methods. Manually annotating visual data, however, is not only a labor-intensive process but also prone to errors. In this study, we present NOVA, a versatile framework to create realistic-looking 3D rendered worlds containing procedurally generated humans with rich pixel-level ground truth annotations. NOVA can simulate various environmental factors such as weather conditions or different times of day, and bring an exceptionally diverse set of humans to life, each having a distinct body shape, gender and age. To demonstrate NOVA’s capabilities, we generate two synthetic datasets for person tracking. The first one includes 108 sequences, each with different levels of difficulty like tracking in crowded scenes or at nighttime and aims for testing the limits of current state-of-the-art trackers. A second dataset of 97 sequences with normal weather conditions is used to show how our synthetic sequences can be utilized to train and boost the performance of deep-learning based trackers. Our results indicate that the synthetic data generated by NOVA represents a good proxy of the real-world and can be exploited for computer vision tasks.","Research in computer vision depends on large datasets, which are essential for training and testing new methods. Creating these datasets manually, however, is time-consuming and prone to errors. This study presents NOVA, a framework for generating 3D rendered worlds with procedurally generated humans with accurate pixel-level annotations. NOVA can simulate various environmental factors, such as weather and time of day, and create distinct humans in terms of body shape, gender, and age. To showcase NOVA's potential, two synthetic datasets for person tracking were generated. The first, consisting of 108 sequences with varying levels of difficulty, was used to test the limits of current trackers. The second, with 97 sequences in normal weather conditions, was used to train and improve the performance of deep-learning based trackers. Results indicate that the synthetic data generated by NOVA is a good representation of the real world and can be used for computer vision tasks.",1
"Today, the cutting edge of computer vision research greatly depends on the availability of large datasets, which are critical for effectively training and testing new methods. Manually annotating visual data, however, is not only a labor-intensive process but also prone to errors. In this study, we present NOVA, a versatile framework to create realistic-looking 3D rendered worlds containing procedurally generated humans with rich pixel-level ground truth annotations. NOVA can simulate various environmental factors such as weather conditions or different times of day, and bring an exceptionally diverse set of humans to life, each having a distinct body shape, gender and age. To demonstrate NOVA’s capabilities, we generate two synthetic datasets for person tracking. The first one includes 108 sequences, each with different levels of difficulty like tracking in crowded scenes or at nighttime and aims for testing the limits of current state-of-the-art trackers. A second dataset of 97 sequences with normal weather conditions is used to show how our synthetic sequences can be utilized to train and boost the performance of deep-learning based trackers. Our results indicate that the synthetic data generated by NOVA represents a good proxy of the real-world and can be exploited for computer vision tasks.","Дослідження комп’ютерного зору залежать від великих наборів даних, які необхідні для навчання та тестування нових методів. Однак створення цих наборів даних вручну займає багато часу та може призвести до помилок. У цьому дослідженні представлено NOVA, структуру для генерації тривимірних візуалізованих світів із процедурно згенерованими людьми з точними анотаціями на рівні пікселів. NOVA може імітувати різні фактори навколишнього середовища, такі як погода та час доби, і створювати різних людей за формою тіла, статтю та віком. Щоб продемонструвати потенціал NOVA, було створено два синтетичних набори даних для відстеження людей. Перший, що складається зі 108 послідовностей із різними рівнями складності, використовувався для перевірки обмежень поточних трекерів. Другий, із 97 послідовностями за нормальних погодних умов, використовувався для навчання та покращення продуктивності трекерів на основі глибокого навчання. Результати показують, що синтетичні дані, згенеровані NOVA, добре відображають реальний світ і можуть використовуватися для завдань комп’ютерного зору.",1
"Сьогодні передовий край досліджень комп’ютерного зору значною мірою залежить від наявності великих наборів даних, які є критично важливими для ефективного навчання та тестування нових методів. Однак анотування візуальних даних вручну є не лише трудомістким процесом, але й схильним до помилок. У цьому дослідженні ми представляємо NOVA, універсальну структуру для створення реалістично виглядаючих тривимірних світів, що містять процедурно згенерованих людей із багатими анотаціями правдивості на рівні пікселів. NOVA може імітувати різноманітні фактори навколишнього середовища, такі як погодні умови чи різний час доби, і оживляти винятково різноманітну групу людей, кожна з яких має певну форму тіла, стать і вік. Щоб продемонструвати можливості NOVA, ми створили два синтетичних набори даних для відстеження людей. Перший включає 108 послідовностей, кожна з яких має різний рівень складності, як-от відстеження в людних сценах або вночі, і спрямована на перевірку меж поточних найсучасніших трекерів. Другий набір даних із 97 послідовностей із нормальними погодними умовами використовується, щоб показати, як наші синтетичні послідовності можна використовувати для навчання та підвищення продуктивності трекерів на основі глибокого навчання. Наші результати вказують на те, що синтетичні дані, згенеровані NOVA, є хорошим проксі реального світу та можуть використовуватися для завдань комп’ютерного зору.","Research in computer vision depends on large datasets, which are essential for training and testing new methods. Creating these datasets manually, however, is time-consuming and prone to errors. This study presents NOVA, a framework for generating 3D rendered worlds with procedurally generated humans with accurate pixel-level annotations. NOVA can simulate various environmental factors, such as weather and time of day, and create distinct humans in terms of body shape, gender, and age. To showcase NOVA's potential, two synthetic datasets for person tracking were generated. The first, consisting of 108 sequences with varying levels of difficulty, was used to test the limits of current trackers. The second, with 97 sequences in normal weather conditions, was used to train and improve the performance of deep-learning based trackers. Results indicate that the synthetic data generated by NOVA is a good representation of the real world and can be used for computer vision tasks.",1
"Сьогодні передовий край досліджень комп’ютерного зору значною мірою залежить від наявності великих наборів даних, які є критично важливими для ефективного навчання та тестування нових методів. Однак анотування візуальних даних вручну є не лише трудомістким процесом, але й схильним до помилок. У цьому дослідженні ми представляємо NOVA, універсальну структуру для створення реалістично виглядаючих тривимірних світів, що містять процедурно згенерованих людей із багатими анотаціями правдивості на рівні пікселів. NOVA може імітувати різноманітні фактори навколишнього середовища, такі як погодні умови чи різний час доби, і оживляти винятково різноманітну групу людей, кожна з яких має певну форму тіла, стать і вік. Щоб продемонструвати можливості NOVA, ми створили два синтетичних набори даних для відстеження людей. Перший включає 108 послідовностей, кожна з яких має різний рівень складності, як-от відстеження в людних сценах або вночі, і спрямована на перевірку меж поточних найсучасніших трекерів. Другий набір даних із 97 послідовностей із нормальними погодними умовами використовується, щоб показати, як наші синтетичні послідовності можна використовувати для навчання та підвищення продуктивності трекерів на основі глибокого навчання. Наші результати вказують на те, що синтетичні дані, згенеровані NOVA, є хорошим проксі реального світу та можуть використовуватися для завдань комп’ютерного зору.","Дослідження комп’ютерного зору залежать від великих наборів даних, які необхідні для навчання та тестування нових методів. Однак створення цих наборів даних вручну займає багато часу та може призвести до помилок. У цьому дослідженні представлено NOVA, структуру для генерації тривимірних візуалізованих світів із процедурно згенерованими людьми з точними анотаціями на рівні пікселів. NOVA може імітувати різні фактори навколишнього середовища, такі як погода та час доби, і створювати різних людей за формою тіла, статтю та віком. Щоб продемонструвати потенціал NOVA, було створено два синтетичних набори даних для відстеження людей. Перший, що складається з 108 послідовностей із різними рівнями складності, використовувався для перевірки обмежень поточних трекерів. Другий, із 97 послідовностями за нормальних погодних умов, використовувався для навчання та покращення продуктивності трекерів на основі глибокого навчання. Результати показують, що синтетичні дані, згенеровані NOVA, добре відображають реальний світ і можуть використовуватися для завдань комп’ютерного зору.",1
"Virtual and augmented reality (VR/AR) systems dramatically gained in popularity with various application areas such as gaming, social media, and communication. It is therefore a crucial task to have the knowhow to efficiently utilize, store or deliver 360◦ videos for end-users. Towards this aim, researchers have been developing deep neural network models for 360◦ multimedia processing and computer vision fields. In this line of work, an important research direction is to build models that can learn and predict the observers’ attention on 360◦ videos to obtain so-called saliency maps computationally. Although there are a few saliency models proposed for this purpose, these models generally consider only visual cues in video frames by neglecting audio cues from sound sources. In this study, an unsupervised frequency-based saliency model is presented for predicting the strength and location of saliency in spatial audio. The prediction of salient audio cues is then used as audio bias on the video saliency predictions of state-of-the-art models. Our experiments yield promising results and show that integrating the proposed spatial audio bias into the existing video saliency models consistently improves their performance. ","The utilization, storage and delivery of 360◦ videos for users has become increasingly important with the rise of VR/AR systems in gaming, social media and communication. To achieve this, researchers have been devising deep neural network models for 360◦ multimedia processing and computer vision. A key focus is to create models that can ascertain the attention of observers on 360◦ videos, known as saliency maps. Existing saliency models mainly take visual cues from video frames into account, ignoring audio cues from sound sources. This paper presents an unsupervised frequency-based saliency model to predict the strength and location of saliency in spatial audio. This prediction of salient audio cues is then used to augment the video saliency predictions of state-of-the-art models. Experiments show that integrating the proposed spatial audio bias into existing video saliency models improves their performance.",1
"Virtual and augmented reality (VR/AR) systems dramatically gained in popularity with various application areas such as gaming, social media, and communication. It is therefore a crucial task to have the knowhow to efficiently utilize, store or deliver 360◦ videos for end-users. Towards this aim, researchers have been developing deep neural network models for 360◦ multimedia processing and computer vision fields. In this line of work, an important research direction is to build models that can learn and predict the observers’ attention on 360◦ videos to obtain so-called saliency maps computationally. Although there are a few saliency models proposed for this purpose, these models generally consider only visual cues in video frames by neglecting audio cues from sound sources. In this study, an unsupervised frequency-based saliency model is presented for predicting the strength and location of saliency in spatial audio. The prediction of salient audio cues is then used as audio bias on the video saliency predictions of state-of-the-art models. Our experiments yield promising results and show that integrating the proposed spatial audio bias into the existing video saliency models consistently improves their performance. ","Використання, зберігання та доставка 360◦ відео для користувачів стає все більш важливим із появою систем VR/AR в іграх, соціальних мережах і комунікації. Щоб досягти цього, дослідники розробляють моделі глибокої нейронної мережі для 360◦ мультимедійної обробки та комп’ютерного зору. Основна увага полягає у створенні моделей, які можуть визначити увагу спостерігачів до 360◦ відео, відомих як карти помітності. Існуючі моделі помітності переважно враховують візуальні підказки з відеокадрів, ігноруючи звукові підказки від джерел звуку. У цьому документі представлено неконтрольовану частотну модель помітності для прогнозування сили та розташування помітності в просторовому аудіо. Це передбачення помітних аудіосигналів потім використовується для збільшення прогнозів помітності відео найсучасніших моделей. Експерименти показують, що інтеграція запропонованого просторового звукового зміщення в існуючі моделі помітності відео покращує їх продуктивність.",1
"Системи віртуальної та доповненої реальності (VR/AR) різко набули популярності в різних сферах застосування, таких як ігри, соціальні мережі та спілкування. Тому дуже важливо мати знання, як ефективно використовувати, зберігати або доставляти 360◦ відео для кінцевих користувачів. З цією метою дослідники розробляють моделі глибокої нейронної мережі для 360◦ обробки мультимедіа та полів комп’ютерного зору. У цьому напрямі роботи важливим напрямком досліджень є побудова моделей, які можуть вивчати та передбачати увагу спостерігачів на 360◦ відео, щоб отримати так звані карти помітності обчислювальним шляхом. Хоча для цієї мети запропоновано декілька моделей помітності, ці моделі зазвичай розглядають лише візуальні підказки у відеокадрах, нехтуючи звуковими підказками від джерел звуку. У цьому дослідженні представлена неконтрольована частотна модель помітності для прогнозування сили та розташування помітності в просторовому аудіо. Прогноз помітних аудіосигналів потім використовується як зміщення аудіо для прогнозів помітності відео найсучасніших моделей. Наші експерименти дають багатообіцяючі результати та показують, що інтеграція запропонованого просторового звукового зміщення в існуючі моделі помітності відео постійно покращує їх продуктивність.","The utilization, storage and delivery of 360◦ videos for users has become increasingly important with the rise of VR/AR systems in gaming, social media and communication. To achieve this, researchers have been devising deep neural network models for 360◦ multimedia processing and computer vision. A key focus is to create models that can ascertain the attention of observers on 360◦ videos, known as saliency maps. Existing saliency models mainly take visual cues from video frames into account, ignoring audio cues from sound sources. This paper presents an unsupervised frequency-based saliency model to predict the strength and location of saliency in spatial audio. This prediction of salient audio cues is then used to augment the video saliency predictions of state-of-the-art models. Experiments show that integrating the proposed spatial audio bias into existing video saliency models improves their performance.",1
"Системи віртуальної та доповненої реальності (VR/AR) різко набули популярності в різних сферах застосування, таких як ігри, соціальні мережі та спілкування. Тому дуже важливо мати знання, як ефективно використовувати, зберігати або доставляти 360◦ відео для кінцевих користувачів. З цією метою дослідники розробляють моделі глибокої нейронної мережі для 360◦ обробки мультимедіа та полів комп’ютерного зору. У цьому напрямі роботи важливим напрямком досліджень є побудова моделей, які можуть вивчати та передбачати увагу спостерігачів на 360◦ відео, щоб отримати так звані карти помітності обчислювальним шляхом. Хоча для цієї мети запропоновано декілька моделей помітності, ці моделі зазвичай розглядають лише візуальні підказки у відеокадрах, нехтуючи звуковими підказками від джерел звуку. У цьому дослідженні представлена неконтрольована частотна модель помітності для прогнозування сили та розташування помітності в просторовому аудіо. Прогноз помітних аудіосигналів потім використовується як зміщення аудіо для прогнозів помітності відео найсучасніших моделей. Наші експерименти дають багатообіцяючі результати та показують, що інтеграція запропонованого просторового звукового зміщення в існуючі моделі помітності відео постійно покращує їх продуктивність.","Використання, зберігання та доставка 360◦ відео для користувачів стає все більш важливим із появою систем VR/AR в іграх, соціальних мережах і комунікації. Щоб досягти цього, дослідники розробляють моделі глибокої нейронної мережі для 360◦ мультимедійної обробки та комп’ютерного зору. Основна увага полягає у створенні моделей, які можуть визначити увагу спостерігачів до 360◦ відео, відомих як карти помітності. Існуючі моделі помітності переважно враховують візуальні підказки з відеокадрів, ігноруючи звукові підказки від джерел звуку. У цьому документі представлено неконтрольовану частотну модель помітності для прогнозування сили та розташування помітності в просторовому аудіо. Це передбачення помітних аудіосигналів потім використовується для збільшення прогнозів помітності відео найсучасніших моделей. Експерименти показують, що інтеграція запропонованого просторового звукового зміщення в існуючі моделі помітності відео покращує їх продуктивність.",1
"Predicting saliency in videos is a challenging problem due to complex modeling of interactions between spatial and temporal information, especially when ever-changing, dynamic nature of videos is considered. Recently, researchers have proposed large-scale data sets and models that take advantage of deep learning as a way to understand what is important for video saliency. These approaches, however, learn to combine spatial and temporal features in a static manner and do not adapt themselves much to the changes in the video content. In this article, we introduce the gated fusion network for dynamic saliency (GFSalNet), the first deep saliency model capable of making predictions in a dynamic way via the gated fusion mechanism. Moreover, our model also exploits spatial and channelwise attention within a multiscale architecture that further allows for highly accurate predictions. We evaluate the proposed approach on a number of data sets, and our experimental analysis demonstrates that it outperforms or is highly competitive with the state of the art. Importantly, we show that it has a good generalization ability, and moreover, exploits temporal information more effectively via its adaptive fusion scheme.","Predicting saliency in videos is a difficult task due to complexity of spatial and temporal interactions, especially when video content is constantly changing. To understand what is important for video saliency, researchers have proposed large-scale data sets and models using deep learning. These models, however, combine spatial and temporal features in a static manner, not adapting to changes in video content. This article introduces GFSalNet, the first deep saliency model with an adaptive fusion mechanism and multiscale architecture that can make predictions dynamically. Experiments show that GFSalNet outperforms or is highly competitive with the state of the art, has good generalization ability, and exploits temporal information more effectively.",1
"Predicting saliency in videos is a challenging problem due to complex modeling of interactions between spatial and temporal information, especially when ever-changing, dynamic nature of videos is considered. Recently, researchers have proposed large-scale data sets and models that take advantage of deep learning as a way to understand what is important for video saliency. These approaches, however, learn to combine spatial and temporal features in a static manner and do not adapt themselves much to the changes in the video content. In this article, we introduce the gated fusion network for dynamic saliency (GFSalNet), the first deep saliency model capable of making predictions in a dynamic way via the gated fusion mechanism. Moreover, our model also exploits spatial and channelwise attention within a multiscale architecture that further allows for highly accurate predictions. We evaluate the proposed approach on a number of data sets, and our experimental analysis demonstrates that it outperforms or is highly competitive with the state of the art. Importantly, we show that it has a good generalization ability, and moreover, exploits temporal information more effectively via its adaptive fusion scheme.","Передбачити помітність у відео є складним завданням через складність просторових і часових взаємодій, особливо коли відеоконтент постійно змінюється. Щоб зрозуміти, що важливо для помітності відео, дослідники запропонували масштабні набори даних і моделі з використанням глибокого навчання. Ці моделі, однак, статично поєднують просторові та часові характеристики, не адаптуючись до змін відеовмісту. У цій статті представлено GFSalNet, першу модель глибокої помітності з механізмом адаптивного синтезу та багатомасштабною архітектурою, яка може динамічно робити прогнози. Експерименти показують, що GFSalNet перевершує або є висококонкурентоспроможним із сучасним обладнанням, має хорошу здатність до узагальнення та ефективніше використовує часову інформацію.",1
"Прогнозування помітності у відео є складною проблемою через складне моделювання взаємодій між просторовою та часовою інформацією, особливо коли враховується постійна мінлива динамічна природа відео. Нещодавно дослідники запропонували масштабні набори даних і моделі, які використовують переваги глибокого навчання як спосіб зрозуміти, що важливо для помітності відео. Ці підходи, однак, вчаться поєднувати просторові та часові характеристики статичним чином і не дуже адаптуються до змін у відеоконтенті. У цій статті ми представляємо мережу закритого синтезу для динамічної помітності (GFSalNet), першу модель глибокої помітності, здатну робити прогнози динамічним способом за допомогою механізму закритого синтезу. Крім того, наша модель також використовує просторову та канальну увагу в рамках багатомасштабної архітектури, що додатково дозволяє робити високоточні прогнози. Ми оцінюємо запропонований підхід на низці наборів даних, і наш експериментальний аналіз показує, що він перевершує сучасні технології або є дуже конкурентоспроможним. Важливо те, що ми показуємо, що він має хорошу здатність до узагальнення та, крім того, ефективніше використовує часову інформацію за допомогою адаптивної схеми синтезу.","Predicting saliency in videos is a difficult task due to complexity of spatial and temporal interactions, especially when video content is constantly changing. To understand what is important for video saliency, researchers have proposed large-scale data sets and models using deep learning. These models, however, combine spatial and temporal features in a static manner, not adapting to changes in video content. This article introduces GFSalNet, the first deep saliency model with an adaptive fusion mechanism and multiscale architecture that can make predictions dynamically. Experiments show that GFSalNet outperforms or is highly competitive with the state of the art, has good generalization ability, and exploits temporal information more effectively.",1
"Прогнозування помітності у відео є складною проблемою через складне моделювання взаємодій між просторовою та часовою інформацією, особливо коли враховується постійна мінлива динамічна природа відео. Нещодавно дослідники запропонували масштабні набори даних і моделі, які використовують переваги глибокого навчання як спосіб зрозуміти, що важливо для помітності відео. Ці підходи, однак, вчаться поєднувати просторові та часові характеристики статичним чином і не дуже адаптуються до змін у відеоконтенті. У цій статті ми представляємо мережу закритого синтезу для динамічної помітності (GFSalNet), першу модель глибокої помітності, здатну робити прогнози динамічним способом за допомогою механізму закритого синтезу. Крім того, наша модель також використовує просторову та канальну увагу в рамках багатомасштабної архітектури, що додатково дозволяє робити високоточні прогнози. Ми оцінюємо запропонований підхід на низці наборів даних, і наш експериментальний аналіз показує, що він перевершує сучасні технології або є дуже конкурентоспроможним. Важливо те, що ми показуємо, що він має хорошу здатність до узагальнення та, крім того, ефективніше використовує часову інформацію за допомогою адаптивної схеми синтезу.","Передбачити помітність у відео є складним завданням через складність просторових і часових взаємодій, особливо коли відеоконтент постійно змінюється. Щоб зрозуміти, що важливо для помітності відео, дослідники запропонували масштабні набори даних і моделі з використанням глибокого навчання. Ці моделі, однак, статично поєднують просторові та часові характеристики, не адаптуючись до змін відеовмісту. У цій статті представлено GFSalNet, першу модель глибокої помітності з механізмом адаптивного синтезу та багатомасштабною архітектурою, яка може динамічно робити прогнози. Експерименти показують, що GFSalNet перевершує або є висококонкурентоспроможним із сучасним обладнанням, має хорошу здатність до узагальнення та ефективніше використовує часову інформацію.",1
"Robust visual tracking plays a vital role in many areas such as autonomous cars, surveillance and robotics. Recent trackers were shown to achieve adequate results under normal tracking scenarios with clear weather conditions, standard camera setups and lighting conditions. Yet, the performance of these trackers, whether they are corre- lation filter-based or learning-based, degrade under adverse weather conditions. The lack of videos with such weather conditions, in the available visual object tracking datasets, is the prime issue behind the low perfor- mance of the learning-based tracking algorithms. In this work, we provide a new person tracking dataset of real-world sequences (PTAW172Real) captured under foggy, rainy and snowy weather conditions to assess the performance of the current trackers. We also introduce a novel person tracking dataset of synthetic sequences (PTAW217Synth) procedurally generated by our NOVA framework spanning the same weather conditions in varying severity to mitigate the problem of data scarcity. Our experimental results demonstrate that the perfor- mances of the state-of-the-art deep trackers under adverse weather conditions can be boosted when the avail- able real training sequences are complemented with our synthetically generated dataset during training.","Visual tracking is essential in autonomous cars, surveillance and robotics. Recent trackers can achieve satisfactory results in normal conditions, but their performance deteriorates in adverse weather. A major obstacle to improving deep tracker performance in such conditions is the lack of videos in available datasets. To address this issue, we present two datasets: PTAW172Real, containing real-world sequences in foggy, rainy and snowy weather, and PTAW217Synth, composed of synthetically generated sequences of varying severity. Our results demonstrate that the performance of state-of-the-art deep trackers can be improved when both real and synthetic datasets are used for training.",1
"Robust visual tracking plays a vital role in many areas such as autonomous cars, surveillance and robotics. Recent trackers were shown to achieve adequate results under normal tracking scenarios with clear weather conditions, standard camera setups and lighting conditions. Yet, the performance of these trackers, whether they are corre- lation filter-based or learning-based, degrade under adverse weather conditions. The lack of videos with such weather conditions, in the available visual object tracking datasets, is the prime issue behind the low perfor- mance of the learning-based tracking algorithms. In this work, we provide a new person tracking dataset of real-world sequences (PTAW172Real) captured under foggy, rainy and snowy weather conditions to assess the performance of the current trackers. We also introduce a novel person tracking dataset of synthetic sequences (PTAW217Synth) procedurally generated by our NOVA framework spanning the same weather conditions in varying severity to mitigate the problem of data scarcity. Our experimental results demonstrate that the perfor- mances of the state-of-the-art deep trackers under adverse weather conditions can be boosted when the avail- able real training sequences are complemented with our synthetically generated dataset during training.","Візуальне відстеження є необхідним для автономних автомобілів, відеоспостереження та робототехніки. Останні трекери можуть досягати задовільних результатів у звичайних умовах, але їх продуктивність погіршується за несприятливих погодних умов. Основною перешкодою для покращення продуктивності глибокого трекера в таких умовах є відсутність відео в доступних наборах даних. Щоб вирішити цю проблему, ми представляємо два набори даних: PTAW172Real, що містить реальні послідовності в туманну, дощову та сніжну погоду, і PTAW217Synth, що складається з синтетично згенерованих послідовностей різного ступеня тяжкості. Наші результати демонструють, що продуктивність найсучасніших глибоких трекерів можна покращити, якщо для навчання використовувати як реальні, так і синтетичні набори даних.",1
"Надійне візуальне відстеження відіграє життєво важливу роль у багатьох сферах, таких як автономні автомобілі, спостереження та робототехніка. Показано, що останні трекери досягають відповідних результатів за звичайних сценаріїв відстеження за ясних погодних умов, стандартних налаштувань камери та умов освітлення. Проте продуктивність цих трекерів, незалежно від того, чи вони засновані на кореляційному фільтрі чи на основі навчання, погіршується за несприятливих погодних умов. Відсутність відео з такими погодними умовами в доступних наборах даних візуального відстеження об’єктів є основною проблемою низької продуктивності алгоритмів відстеження на основі навчання. У цій роботі ми надаємо новий набір даних відстеження людей у реальному світі (PTAW172Real), знятих за умов туману, дощу та снігу, щоб оцінити ефективність поточних трекерів. Ми також представляємо новий набір даних відстеження людей із синтетичних послідовностей (PTAW217Synth), процедурно згенерованих нашою структурою NOVA, що охоплює однакові погодні умови різного ступеня тяжкості, щоб пом’якшити проблему дефіциту даних. Наші експериментальні результати демонструють, що продуктивність найсучасніших глибинних трекерів за несприятливих погодних умов може бути покращена, якщо доступні реальні навчальні послідовності доповнюються нашим синтетично згенерованим набором даних під час навчання.","Visual tracking is essential in autonomous cars, surveillance and robotics. Recent trackers can achieve satisfactory results in normal conditions, but their performance deteriorates in adverse weather. A major obstacle to improving deep tracker performance in such conditions is the lack of videos in available datasets. To address this issue, we present two datasets: PTAW172Real, containing real-world sequences in foggy, rainy and snowy weather, and PTAW217Synth, composed of synthetically generated sequences of varying severity. Our results demonstrate that the performance of state-of-the-art deep trackers can be improved when both real and synthetic datasets are used for training.",1
"Надійне візуальне відстеження відіграє життєво важливу роль у багатьох сферах, таких як автономні автомобілі, спостереження та робототехніка. Показано, що останні трекери досягають відповідних результатів за звичайних сценаріїв відстеження за ясних погодних умов, стандартних налаштувань камери та умов освітлення. Тим не менш, продуктивність цих трекерів, незалежно від того, засновані вони на кореляційному фільтрі чи на основі навчання, погіршується за несприятливих погодних умов. Відсутність відео з такими погодними умовами в доступних наборах даних візуального відстеження об’єктів є основною проблемою низької продуктивності алгоритмів відстеження на основі навчання. У цій роботі ми надаємо новий набір даних відстеження людей у реальному світі (PTAW172Real), знятих за умов туманної, дощової та снігової погоди, щоб оцінити ефективність поточних трекерів. Ми також представляємо новий набір даних відстеження людей із синтетичних послідовностей (PTAW217Synth), процедурно згенерованих нашою структурою NOVA, що охоплює однакові погодні умови різного ступеня тяжкості, щоб пом’якшити проблему дефіциту даних. Наші експериментальні результати демонструють, що продуктивність найсучасніших глибинних трекерів за несприятливих погодних умов може бути покращена, якщо доступні реальні навчальні послідовності доповнюються нашим синтетично згенерованим набором даних під час навчання.","Візуальне відстеження є необхідним для автономних автомобілів, відеоспостереження та робототехніки. Останні трекери можуть досягати задовільних результатів у звичайних умовах, але їх продуктивність погіршується за несприятливих погодних умов. Основною перешкодою для покращення продуктивності глибокого трекера в таких умовах є відсутність відео в доступних наборах даних. Щоб вирішити цю проблему, ми представляємо два набори даних: PTAW172Real, що містить реальні послідовності в туманну, дощову та сніжну погоду, і PTAW217Synth, що складається з синтетично згенерованих послідовностей різного ступеня тяжкості. Наші результати демонструють, що продуктивність найсучасніших глибоких трекерів можна покращити, якщо для навчання використовувати як реальні, так і синтетичні набори даних.",1
"Automatic generation of video descriptions in natural language, also called video captioning, aims to understand the visual content of the video and produce a natural language sentence depicting the objects and actions in the scene. This challenging integrated vision and language problem, however, has been predominantly addressed for English. The lack of data and the linguistic properties of other languages limit the success of existing approaches for such languages. In this paper we target Turkish, a morphologically rich and agglutinative language that has very different properties compared to English. To do so, we create the frst large-scale video captioning dataset for this language by carefully translating the English descriptions of the videos in the MSVD (Microsoft Research Video Description Corpus) dataset into Turkish. In addition to enabling research in video captioning in Turkish, the parallel English–Turkish descriptions also enable the study of the role of video context in (multimodal) machine translation. In our experiments, we build models for both video captioning and multimodal machine translation and investigate the efect of diferent word segmentation approaches and diferent neural architectures to better address the properties of Turkish. We hope that the MSVD-Turkish dataset and the results reported in this work will lead to better video captioning and multimodal machine translation models for Turkish and other morphology rich and agglutinative languages.","The goal of automatic video description generation in natural language, also known as video captioning, is to comprehend the video's visual content and form a natural language sentence illustrating the objects and actions taking place. However, this challenging combination of vision and language has mainly been studied in English. The lack of data and the linguistic characteristics of other languages impede the effectiveness of current approaches for those languages. This paper focuses on Turkish, a morphologically complex and agglutinative language that has different properties than English. To do this, we create the first extensive video captioning dataset for this language by translating the English descriptions of the MSVD (Microsoft Research Video Description Corpus) dataset into Turkish. In addition to allowing research in video captioning in Turkish, the parallel English–Turkish descriptions also enable the investigation of the role of video context in (multimodal) machine translation. Our experiments develop models for both video captioning and multimodal machine translation and examine the effect of various word segmentation approaches and various neural architectures to better address the properties of Turkish. We trust that the MSVD-Turkish dataset and the results reported in this work will result in improved video captioning and multimodal machine translation models for Turkish and other morphologically complex and agglutinative languages.",1
"Automatic generation of video descriptions in natural language, also called video captioning, aims to understand the visual content of the video and produce a natural language sentence depicting the objects and actions in the scene. This challenging integrated vision and language problem, however, has been predominantly addressed for English. The lack of data and the linguistic properties of other languages limit the success of existing approaches for such languages. In this paper we target Turkish, a morphologically rich and agglutinative language that has very different properties compared to English. To do so, we create the frst large-scale video captioning dataset for this language by carefully translating the English descriptions of the videos in the MSVD (Microsoft Research Video Description Corpus) dataset into Turkish. In addition to enabling research in video captioning in Turkish, the parallel English–Turkish descriptions also enable the study of the role of video context in (multimodal) machine translation. In our experiments, we build models for both video captioning and multimodal machine translation and investigate the efect of diferent word segmentation approaches and diferent neural architectures to better address the properties of Turkish. We hope that the MSVD-Turkish dataset and the results reported in this work will lead to better video captioning and multimodal machine translation models for Turkish and other morphology rich and agglutinative languages.","Метою автоматичного створення опису відео природною мовою, також відомого як субтитри до відео, є розуміння візуального вмісту відео та формування речення природною мовою, що ілюструє об’єкти та дії, що відбуваються. Однак це складне поєднання бачення та мови в основному вивчалося англійською мовою. Відсутність даних і лінгвістичні характеристики інших мов перешкоджають ефективності поточних підходів до цих мов. Ця стаття присвячена турецькій, морфологічно складній та аглютинативній мові, яка має інші властивості, ніж англійська. Для цього ми створюємо перший розширений набір даних субтитрів до відео для цієї мови, перекладаючи англійські описи набору даних MSVD (Microsoft Research Video Description Corpus) на турецьку мову. Окрім того, що вони дозволяють досліджувати відеосубтитри турецькою мовою, паралельні англо-турецькі описи також дозволяють досліджувати роль відеоконтексту в (мультимодальному) машинному перекладі. Наші експерименти розробляють моделі як для субтитрів до відео, так і для мультимодального машинного перекладу та вивчають вплив різних підходів до сегментації слів і різних нейронних архітектур для кращого врахування властивостей турецької мови. Ми віримо, що набір даних MSVD-Turkish і результати, наведені в цій роботі, призведуть до покращення відеосубтитрів і мультимодальних моделей машинного перекладу для турецької та інших морфологічно складних і аглютинативних мов.",1
"Автоматична генерація відеоописів природною мовою, яка також називається відеотитрами, має на меті зрозуміти візуальний вміст відео та створити речення природною мовою, що зображує об’єкти та дії в сцені. Ця складна інтегрована проблема бачення та мови, однак, була переважно розглянута для англійської мови. Брак даних і лінгвістичні властивості інших мов обмежують успіх існуючих підходів для таких мов. У цій статті ми орієнтуємося на турецьку, морфологічно багату та аглютинативну мову, яка має дуже різні властивості порівняно з англійською. Для цього ми створюємо перший великомасштабний набір даних субтитрів до відео для цієї мови, ретельно перекладаючи англійські описи відео в наборі даних MSVD (Microsoft Research Video Description Corpus) на турецьку мову. Окрім того, що вони дозволяють досліджувати субтитри до відео турецькою мовою, паралельні англо-турецькі описи також дозволяють вивчати роль відеоконтексту в (мультимодальному) машинному перекладі. У наших експериментах ми створюємо моделі як для субтитрів до відео, так і для мультимодального машинного перекладу та досліджуємо вплив різних підходів до сегментації слів і різних нейронних архітектур для кращого врахування властивостей турецької мови. Ми сподіваємося, що набір даних MSVD-Turkish і результати, наведені в цій роботі, призведуть до кращих відеосубтитрів і мультимодальних моделей машинного перекладу для турецької та інших морфологічних і аглютинативних мов.","The goal of automatic video description generation in natural language, also known as video captioning, is to comprehend the video's visual content and form a natural language sentence illustrating the objects and actions taking place. However, this challenging combination of vision and language has mainly been studied in English. The lack of data and the linguistic characteristics of other languages impede the effectiveness of current approaches for those languages. This paper focuses on Turkish, a morphologically complex and agglutinative language that has different properties than English. To do this, we create the first extensive video captioning dataset for this language by translating the English descriptions of the MSVD (Microsoft Research Video Description Corpus) dataset into Turkish. In addition to allowing research in video captioning in Turkish, the parallel English–Turkish descriptions also enable the investigation of the role of video context in (multimodal) machine translation. Our experiments develop models for both video captioning and multimodal machine translation and examine the effect of various word segmentation approaches and various neural architectures to better address the properties of Turkish. We trust that the MSVD-Turkish dataset and the results reported in this work will result in improved video captioning and multimodal machine translation models for Turkish and other morphologically complex and agglutinative languages.",1
"Автоматична генерація відеоописів природною мовою, яка також називається відеотитрами, має на меті зрозуміти візуальний вміст відео та створити речення природною мовою, що зображує об’єкти та дії в сцені. Ця складна інтегрована проблема бачення та мови, однак, була переважно розглянута для англійської мови. Брак даних і лінгвістичні властивості інших мов обмежують успіх існуючих підходів для таких мов. У цій статті ми орієнтуємося на турецьку, морфологічно багату та аглютинативну мову, яка має дуже різні властивості порівняно з англійською. Для цього ми створюємо перший великомасштабний набір даних субтитрів до відео для цієї мови, ретельно перекладаючи англійські описи відео в наборі даних MSVD (Microsoft Research Video Description Corpus) на турецьку мову. Окрім того, що вони дозволяють досліджувати субтитри до відео турецькою мовою, паралельні англо-турецькі описи також дозволяють вивчати роль відеоконтексту в (мультимодальному) машинному перекладі. У наших експериментах ми створюємо моделі як для субтитрів до відео, так і для мультимодального машинного перекладу та досліджуємо вплив різних підходів до сегментації слів і різних нейронних архітектур для кращого врахування властивостей турецької мови. Ми сподіваємося, що набір даних MSVD-Turkish і результати, наведені в цій роботі, призведуть до кращих відеосубтитрів і мультимодальних моделей машинного перекладу для турецької та інших морфологічних і аглютинативних мов.","Метою автоматичного створення опису відео природною мовою, також відомого як субтитри до відео, є розуміння візуального вмісту відео та формування речення природною мовою, що ілюструє об’єкти та дії, що відбуваються. Однак це складне поєднання бачення та мови в основному вивчалося англійською мовою. Відсутність даних і лінгвістичні характеристики інших мов перешкоджають ефективності поточних підходів для цих мов. Ця стаття присвячена турецькій, морфологічно складній та аглютинативній мові, яка має інші властивості, ніж англійська. Для цього ми створюємо перший розширений набір даних субтитрів до відео для цієї мови, перекладаючи англійські описи набору даних MSVD (Microsoft Research Video Description Corpus) на турецьку мову. Окрім того, що вони дозволяють досліджувати відеосубтитри турецькою мовою, паралельні англо-турецькі описи також дозволяють досліджувати роль відеоконтексту в (мультимодальному) машинному перекладі. Наші експерименти розробляють моделі як для субтитрів до відео, так і для мультимодального машинного перекладу та вивчають вплив різних підходів до сегментації слів і різних нейронних архітектур для кращого врахування властивостей турецької мови. Ми віримо, що набір даних MSVD-Turkish і результати, наведені в цій роботі, призведуть до покращення субтитрів до відео та мультимодальних моделей машинного перекладу для турецької та інших морфологічно складних і аглютинативних мов.",1
"Collecting textual descriptions is an especially costly task for dense video captioning, since each event in the video needs to be annotated separately and a long descriptive paragraph needs to be provided. In this paper, we investigate a way to mitigate this heavy burden and propose to leverage captions of visually similar images as auxiliary context. Our model successfully fetches visually relevant images and combines noun and verb phrases from their captions to generating coherent descriptions. To this end, we use a generator and discriminator design, together with an attention-based fusion technique, to incorporate image captions as context in the video caption generation process. The experiments on the challenging ActivityNet Captions dataset demonstrate that our proposed approach achieves more accurate and more diverse video descriptions compared to the strong baseline using METEOR, BLEU and CIDEr-D metrics and qualitative evaluations.","The cost of collecting textual descriptions for dense video captioning is high, since each event in the video must be labeled separately and a long descriptive paragraph is required. To reduce this burden, we propose using captions of visually similar images as supplementary context. Our model finds visually related images and merges noun and verb phrases from their captions to form coherent descriptions. We apply a generator and discriminator design, combined with an attention-based fusion method, to incorporate image captions into the video caption generation process. Results from experiments on the ActivityNet Captions dataset show our approach produces more accurate and varied video descriptions compared to the strong baseline using METEOR, BLEU and CIDEr-D metrics and qualitative assessments.",1
"Collecting textual descriptions is an especially costly task for dense video captioning, since each event in the video needs to be annotated separately and a long descriptive paragraph needs to be provided. In this paper, we investigate a way to mitigate this heavy burden and propose to leverage captions of visually similar images as auxiliary context. Our model successfully fetches visually relevant images and combines noun and verb phrases from their captions to generating coherent descriptions. To this end, we use a generator and discriminator design, together with an attention-based fusion technique, to incorporate image captions as context in the video caption generation process. The experiments on the challenging ActivityNet Captions dataset demonstrate that our proposed approach achieves more accurate and more diverse video descriptions compared to the strong baseline using METEOR, BLEU and CIDEr-D metrics and qualitative evaluations.","Вартість збору текстових описів для щільних субтитрів до відео є високою, оскільки кожна подія у відео має бути позначена окремо, і потрібен довгий абзац з описом. Щоб зменшити це навантаження, ми пропонуємо використовувати підписи до візуально схожих зображень як додатковий контекст. Наша модель знаходить візуально пов’язані зображення та об’єднує фрази іменників і дієслів із їхніх підписів, щоб сформувати зв’язні описи. Ми застосовуємо дизайн генератора та дискримінатора в поєднанні з методом злиття на основі уваги, щоб включити підписи до зображень у процес генерації відеозаписів. Результати експериментів із набором даних ActivityNet Captions показують, що наш підхід дає більш точні та різноманітні описи відео порівняно з сильним базовим сценарієм із використанням показників METEOR, BLEU і CIDer-D і якісних оцінок.",1
"Збір текстових описів є особливо дорогим завданням для щільних субтитрів до відео, оскільки кожна подія у відео має бути анотована окремо та має бути наданий довгий абзац з описом. У цій статті ми досліджуємо спосіб полегшити цей важкий тягар і пропонуємо використовувати підписи до візуально подібних зображень як допоміжний контекст. Наша модель успішно вибирає візуально релевантні зображення та комбінує фрази іменників і дієслів із їхніх підписів для створення зв’язних описів. З цією метою ми використовуємо дизайн генератора та дискримінатора разом із технікою злиття на основі уваги, щоб включити підписи до зображень як контекст у процес генерації субтитрів до відео. Експерименти зі складним набором даних ActivityNet Captions демонструють, що запропонований нами підхід забезпечує більш точні та різноманітні описи відео порівняно з сильним базовим сценарієм із використанням показників METEOR, BLEU і CIDer-D і якісних оцінок.","The cost of collecting textual descriptions for dense video captioning is high, since each event in the video must be labeled separately and a long descriptive paragraph is required. To reduce this burden, we propose using captions of visually similar images as supplementary context. Our model finds visually related images and merges noun and verb phrases from their captions to form coherent descriptions. We apply a generator and discriminator design, combined with an attention-based fusion method, to incorporate image captions into the video caption generation process. Results from experiments on the ActivityNet Captions dataset show our approach produces more accurate and varied video descriptions compared to the strong baseline using METEOR, BLEU and CIDEr-D metrics and qualitative assessments.",1
"Збір текстових описів є особливо дорогим завданням для щільних субтитрів до відео, оскільки кожна подія у відео має бути анотована окремо та має бути наданий довгий абзац з описом. У цій статті ми досліджуємо спосіб полегшити цей важкий тягар і пропонуємо використовувати підписи до візуально подібних зображень як допоміжний контекст. Наша модель успішно вибирає візуально релевантні зображення та комбінує фрази іменників і дієслів із їхніх підписів для створення зв’язних описів. З цією метою ми використовуємо дизайн генератора та дискримінатора разом із технікою злиття на основі уваги, щоб включити підписи до зображень як контекст у процес генерації субтитрів до відео. Експерименти зі складним набором даних ActivityNet Captions демонструють, що запропонований нами підхід забезпечує більш точні та різноманітні описи відео порівняно з сильним базовим сценарієм із використанням показників METEOR, BLEU і CIDer-D і якісних оцінок.","Вартість збору текстових описів для щільних субтитрів до відео є високою, оскільки кожна подія у відео має бути позначена окремо, і потрібен довгий абзац з описом. Щоб зменшити це навантаження, ми пропонуємо використовувати підписи до візуально схожих зображень як додатковий контекст. Наша модель знаходить візуально пов’язані зображення та об’єднує фрази іменників і дієслів із їхніх підписів, щоб сформувати зв’язні описи. Ми застосовуємо дизайн генератора та дискримінатора в поєднанні з методом злиття на основі уваги, щоб включити підписи до зображень у процес генерації відеозаписів. Результати експериментів із набором даних ActivityNet Captions показують, що наш підхід дає більш точні та різноманітні описи відео порівняно з сильним базовим сценарієм із використанням показників METEOR, BLEU і CIDer-D і якісних оцінок.",1
"Multi-contrast MRI protocols increase the level of morphological information available for diagnosis. Yet, the number and quality of contrasts are limited in practice by various factors including scan time and patient motion. Synthesis of missing or corrupted contrasts from other high-quality ones can alleviate this limitation. When a single target contrast is of interest, common approaches for multi-contrast MRI involve either one-to-one or many-to-one synthesis methods depending on their input. One-to-one meth- ods take as input a single source contrast, and they learn a latent representation sensitive to unique fea- tures of the source. Meanwhile, many-to-one methods receive multiple distinct sources, and they learn a shared latent representation more sensitive to common features across sources. For enhanced image synthesis, we propose a multi-stream approach that aggregates information across multiple source im- ages via a mixture of multiple one-to-one streams and a joint many-to-one stream. The complementary feature maps generated in the one-to-one streams and the shared feature maps generated in the many- to-one stream are combined with a fusion block. The location of the fusion block is adaptively modified to maximize task-specific performance. Quantitative and radiological assessments on T1 ,- T2-, PD-weighted, and FLAIR images clearly demonstrate the superior performance of the proposed method compared to previous state-of-the-art one-to-one and many-to-one methods.","Multi-contrast MRI protocols provide more morphological info for diagnosis, but are restricted by scan time and patient motion. Synthesizing missing or corrupted contrasts from high-quality ones can alleviate this. Common approaches for multi-contrast MRI involve either one-to-one or many-to-one synthesis methods, depending on their input. One-to-one methods take a single source contrast as input and learn a latent representation that is sensitive to the source's unique features, while many-to-one methods receive multiple distinct sources and learn a shared latent representation more sensitive to common features across sources. We propose a multi-stream approach which aggregates information from multiple source images via a mixture of one-to-one streams and a joint many-to-one stream, with a fusion block whose location is adapted to maximize task-specific performance. Quantitative and radiological assessments on T1, T2, PD-weighted, and FLAIR images show the proposed method outperforms previous one-to-one and many-to-one methods.",1
"Multi-contrast MRI protocols increase the level of morphological information available for diagnosis. Yet, the number and quality of contrasts are limited in practice by various factors including scan time and patient motion. Synthesis of missing or corrupted contrasts from other high-quality ones can alleviate this limitation. When a single target contrast is of interest, common approaches for multi-contrast MRI involve either one-to-one or many-to-one synthesis methods depending on their input. One-to-one meth- ods take as input a single source contrast, and they learn a latent representation sensitive to unique fea- tures of the source. Meanwhile, many-to-one methods receive multiple distinct sources, and they learn a shared latent representation more sensitive to common features across sources. For enhanced image synthesis, we propose a multi-stream approach that aggregates information across multiple source im- ages via a mixture of multiple one-to-one streams and a joint many-to-one stream. The complementary feature maps generated in the one-to-one streams and the shared feature maps generated in the many- to-one stream are combined with a fusion block. The location of the fusion block is adaptively modified to maximize task-specific performance. Quantitative and radiological assessments on T1 ,- T2-, PD-weighted, and FLAIR images clearly demonstrate the superior performance of the proposed method compared to previous state-of-the-art one-to-one and many-to-one methods.","Протоколи мультиконтрастної МРТ надають більше морфологічної інформації для діагностики, але обмежені часом сканування та рухами пацієнта. Синтез відсутніх або пошкоджених контрастів із високоякісних може пом’якшити це. Загальні підходи до багатоконтрастної МРТ включають методи синтезу «один до одного» або «багато до одного», залежно від їх вхідних даних. Методи «один-до-одного» приймають контраст одного джерела як вхідні дані та вивчають приховане представлення, чутливе до унікальних особливостей джерела, тоді як методи «багато-до-одного» отримують кілька різних джерел і вивчають спільне приховане представлення, більш чутливе до спільних особливостей у всьому джерела. Ми пропонуємо багатопотоковий підхід, який агрегує інформацію з кількох вихідних зображень за допомогою суміші потоків «один-до-одного» та спільного потоку «багато-до-одного» з об’єднаним блоком, розташування якого адаптовано для максимізації продуктивності для конкретного завдання. Кількісні та радіологічні оцінки зображень T1, T2, PD-зважених і FLAIR показують, що запропонований метод перевершує попередні методи один до одного та багато до одного.",1
"Мультиконтрастні протоколи МРТ підвищують рівень доступної для діагностики морфологічної інформації. Проте на практиці кількість і якість контрастів обмежена різними факторами, включаючи час сканування та рух пацієнта. Синтез відсутніх або пошкоджених контрастів з інших високоякісних може пом’якшити це обмеження. Коли цікавить одне цільове контрастування, загальні підходи до багатоконтрастної МРТ включають методи синтезу «один до одного» або «багато до одного» залежно від їх вхідних даних. Методи «один-до-одного» приймають як вхідні дані контраст одного джерела, і вони вивчають приховане представлення, чутливе до унікальних особливостей джерела. Тим часом методи «багато-до-одного» отримують кілька різних джерел, і вони вивчають спільне приховане представлення, більш чутливе до загальних функцій у джерелах. Для покращеного синтезу зображень ми пропонуємо багатопотоковий підхід, який агрегує інформацію з кількох вихідних зображень за допомогою суміші кількох потоків один до одного та спільного потоку багато до одного. Додаткові карти функцій, згенеровані в потоках «один-до-одного», і спільні карти функцій, згенеровані в потоці «багато-до-одного», поєднуються з блоком злиття. Розташування блоку злиття адаптивно змінюється, щоб максимально підвищити продуктивність для конкретного завдання. Кількісні та радіологічні оцінки T1, T2-, PD-зважених зображень і зображень FLAIR чітко демонструють чудову продуктивність запропонованого методу порівняно з попередніми сучасними методами один до одного та багато до одного.","Multi-contrast MRI protocols provide more morphological info for diagnosis, but are restricted by scan time and patient motion. Synthesizing missing or corrupted contrasts from high-quality ones can alleviate this. Common approaches for multi-contrast MRI involve either one-to-one or many-to-one synthesis methods, depending on their input. One-to-one methods take a single source contrast as input and learn a latent representation that is sensitive to the source's unique features, while many-to-one methods receive multiple distinct sources and learn a shared latent representation more sensitive to common features across sources. We propose a multi-stream approach which aggregates information from multiple source images via a mixture of one-to-one streams and a joint many-to-one stream, with a fusion block whose location is adapted to maximize task-specific performance. Quantitative and radiological assessments on T1, T2, PD-weighted, and FLAIR images show the proposed method outperforms previous one-to-one and many-to-one methods.",1
"Мультиконтрастні протоколи МРТ підвищують рівень доступної для діагностики морфологічної інформації. Проте на практиці кількість і якість контрастів обмежена різними факторами, включаючи час сканування та рух пацієнта. Синтез відсутніх або пошкоджених контрастів з інших високоякісних може пом’якшити це обмеження. Коли цікавить одне цільове контрастування, загальні підходи до багатоконтрастної МРТ включають методи синтезу «один до одного» або «багато до одного» залежно від їх вхідних даних. Методи «один-до-одного» приймають як вхідні дані контраст одного джерела, і вони вивчають приховане представлення, чутливе до унікальних особливостей джерела. Тим часом методи «багато-до-одного» отримують кілька різних джерел, і вони вивчають спільне приховане представлення, більш чутливе до загальних функцій у джерелах. Для покращеного синтезу зображень ми пропонуємо багатопотоковий підхід, який агрегує інформацію з кількох вихідних зображень за допомогою суміші кількох потоків один до одного та спільного потоку багато до одного. Додаткові карти функцій, згенеровані в потоках «один-до-одного», і спільні карти функцій, згенеровані в потоці «багато-до-одного», поєднуються з блоком злиття. Розташування блоку злиття адаптивно змінюється, щоб максимально підвищити продуктивність для конкретного завдання. Кількісні та радіологічні оцінки T1, T2-, PD-зважених зображень і зображень FLAIR чітко демонструють чудову продуктивність запропонованого методу порівняно з попередніми сучасними методами один до одного та багато до одного.","Протоколи мультиконтрастної МРТ надають більше морфологічної інформації для діагностики, але обмежені часом сканування та рухами пацієнта. Синтез відсутніх або пошкоджених контрастів із високоякісних може пом’якшити це. Загальні підходи до багатоконтрастної МРТ включають методи синтезу «один до одного» або «багато до одного», залежно від їх вхідних даних. Методи «один-до-одного» приймають контраст одного джерела як вхідні дані та вивчають приховане представлення, чутливе до унікальних особливостей джерела, тоді як методи «багато-до-одного» отримують кілька різних джерел і вивчають спільне приховане представлення, більш чутливе до спільних особливостей у всьому джерела. Ми пропонуємо багатопотоковий підхід, який агрегує інформацію з кількох вихідних зображень за допомогою суміші потоків «один-до-одного» та спільного потоку «багато-до-одного» з об’єднаним блоком, розташування якого адаптовано для максимізації продуктивності для конкретного завдання. Кількісні та радіологічні оцінки зображень T1, T2, PD-зважених і FLAIR показують, що запропонований метод перевершує попередні методи один до одного та багато до одного.",1
"Pushing is an essential non-prehensile manipulation skill used for tasks ranging from pre-grasp manipulation to scene rearrangement, reasoning about object relations in the scene, and thus pushing actions have been widely studied in robotics. The effective use of pushing actions often requires an understanding of the dynamics of the manipulated objects and adaptation to the discrepancies between prediction and reality. For this reason, effect prediction and parameter estimation with pushing actions have been heavily investigated in the literature. However, current approaches are limited because they either model systems with a fixed number of objects or use image-based representations whose outputs are not very interpretable and quickly accumulate errors. In this paper, we propose a graph neural network based framework for effect prediction and parameter estimation of pushing actions by modeling object relations based on contacts or articulations. Our framework is validated both in real and simulated environments containing different shaped multi-part objects connected via different types of joints and objects with different masses. Our approach enables the robot to predict and adapt the effect of a pushing action as it observes the scene. Further, we demonstrate 6D effect prediction in the lever-up action in the context of robot-based hard-disk disassembly","The utilization of pushing actions necessitates understanding the dynamics of manipulated objects and adapting to discrepancies between expectations and reality. Thus, effect prediction and parameter estimation with pushing actions have been researched extensively. However, existing approaches are limited due to either modeling systems with a fixed number of objects or utilizing image-based representations with non-interpretable outputs and accumulating errors quickly. This paper proposes a graph neural network based framework for effect prediction and parameter estimation of pushing actions, taking into account object relations based on contacts or articulations. The proposed framework is tested in both real and simulated environments with different shaped multi-part objects connected through various types of joints and objects with varying masses. The robot is enabled to predict and adjust the effect of a pushing action as it observes the scene, as well as 6D effect prediction in the lever-up action in the context of robot-based hard-disk disassembly.",1
"Pushing is an essential non-prehensile manipulation skill used for tasks ranging from pre-grasp manipulation to scene rearrangement, reasoning about object relations in the scene, and thus pushing actions have been widely studied in robotics. The effective use of pushing actions often requires an understanding of the dynamics of the manipulated objects and adaptation to the discrepancies between prediction and reality. For this reason, effect prediction and parameter estimation with pushing actions have been heavily investigated in the literature. However, current approaches are limited because they either model systems with a fixed number of objects or use image-based representations whose outputs are not very interpretable and quickly accumulate errors. In this paper, we propose a graph neural network based framework for effect prediction and parameter estimation of pushing actions by modeling object relations based on contacts or articulations. Our framework is validated both in real and simulated environments containing different shaped multi-part objects connected via different types of joints and objects with different masses. Our approach enables the robot to predict and adapt the effect of a pushing action as it observes the scene. Further, we demonstrate 6D effect prediction in the lever-up action in the context of robot-based hard-disk disassembly","Використання штовхаючих дій вимагає розуміння динаміки об’єктів, якими маніпулюють, і адаптації до розбіжностей між очікуваннями та реальністю. Таким чином, передбачення ефекту та оцінка параметрів за допомогою виштовхувальних дій були широко досліджені. Однак існуючі підходи обмежені через або моделювання систем із фіксованою кількістю об’єктів, або використання представлень на основі зображень із неінтерпретованими результатами та швидким накопиченням помилок. У цьому документі пропонується структура на основі графової нейронної мережі для прогнозування ефекту та оцінки параметрів штовхаючих дій, враховуючи об’єктні зв’язки на основі контактів або артикуляцій. Запропоновану структуру тестують як у реальному, так і в симульованому середовищі з багатокомпонентними об’єктами різної форми, з’єднаними різними типами з’єднань, і об’єктами різної маси. Робот може передбачати та коригувати ефект штовхання під час спостереження за сценою, а також передбачення 6D-ефекту під час дії важеля вгору в контексті розбирання жорсткого диска за допомогою робота.",1
"Поштовх — це важлива навичка маніпуляції, яка не хватає, і використовується для виконання завдань, починаючи від маніпуляцій перед захопленням до перестановки сцени, міркувань про об’єктні відносини в сцені, і, отже, дії поштовху широко вивчаються в робототехніці. Ефективне використання штовхаючих дій часто вимагає розуміння динаміки об&#39;єктів, якими маніпулюють, і адаптації до розбіжностей між прогнозом і реальністю. З цієї причини прогнозування ефекту та оцінка параметрів за допомогою виштовхувальних дій були ретельно досліджені в літературі. Однак поточні підходи обмежені, оскільки вони або моделюють системи з фіксованою кількістю об’єктів, або використовують представлення на основі зображень, результати яких не дуже добре інтерпретуються та швидко накопичують помилки. У цій статті ми пропонуємо структуру на основі графової нейронної мережі для прогнозування ефекту та оцінки параметрів штовхаючих дій шляхом моделювання об’єктних відносин на основі контактів або артикуляцій. Наша структура перевірена як у реальному, так і в симульованому середовищі, що містить багатокомпонентні об’єкти різної форми, з’єднані за допомогою різних типів з’єднань, і об’єкти з різною масою. Наш підхід дозволяє роботу передбачити та адаптувати ефект штовхання, коли він спостерігає за сценою. Крім того, ми демонструємо прогнозування 6D-ефекту під час дії важеля вгору в контексті роботизованого розбирання жорсткого диска","The utilization of pushing actions necessitates understanding the dynamics of manipulated objects and adapting to discrepancies between expectations and reality. Thus, effect prediction and parameter estimation with pushing actions have been researched extensively. However, existing approaches are limited due to either modeling systems with a fixed number of objects or utilizing image-based representations with non-interpretable outputs and accumulating errors quickly. This paper proposes a graph neural network based framework for effect prediction and parameter estimation of pushing actions, taking into account object relations based on contacts or articulations. The proposed framework is tested in both real and simulated environments with different shaped multi-part objects connected through various types of joints and objects with varying masses. The robot is enabled to predict and adjust the effect of a pushing action as it observes the scene, as well as 6D effect prediction in the lever-up action in the context of robot-based hard-disk disassembly.",1
"Поштовх — це важлива навичка маніпуляції, яка не хватає, і використовується для виконання завдань, починаючи від маніпуляцій перед захопленням до перестановки сцени, міркувань про об’єктні відносини в сцені, і, отже, дії поштовху широко вивчаються в робототехніці. Ефективне використання штовхаючих дій часто вимагає розуміння динаміки об&#39;єктів, якими маніпулюють, і адаптації до розбіжностей між прогнозом і реальністю. З цієї причини прогнозування ефекту та оцінка параметрів за допомогою виштовхувальних дій були ретельно досліджені в літературі. Однак поточні підходи обмежені, оскільки вони або моделюють системи з фіксованою кількістю об’єктів, або використовують представлення на основі зображень, результати яких не дуже добре інтерпретуються та швидко накопичують помилки. У цій статті ми пропонуємо структуру на основі графової нейронної мережі для прогнозування ефекту та оцінки параметрів штовхаючих дій шляхом моделювання об’єктних відносин на основі контактів або артикуляцій. Наша структура перевірена як у реальному, так і в симульованому середовищі, що містить багатокомпонентні об’єкти різної форми, з’єднані за допомогою різних типів з’єднань, і об’єкти з різною масою. Наш підхід дозволяє роботу передбачити та адаптувати ефект штовхання, коли він спостерігає за сценою. Крім того, ми демонструємо прогнозування 6D-ефекту під час дії важеля вгору в контексті роботизованого розбирання жорсткого диска","Використання штовхаючих дій вимагає розуміння динаміки об’єктів, якими маніпулюють, і адаптації до розбіжностей між очікуваннями та реальністю. Таким чином, передбачення ефекту та оцінка параметрів за допомогою виштовхувальних дій були широко досліджені. Однак існуючі підходи обмежені через або моделювання систем із фіксованою кількістю об’єктів, або використання представлень на основі зображень із неінтерпретованими результатами та швидким накопиченням помилок. У цьому документі пропонується структура на основі графової нейронної мережі для прогнозування ефекту та оцінки параметрів штовхаючих дій, враховуючи об’єктні зв’язки на основі контактів або артикуляцій. Запропоновану структуру тестують як у реальному, так і в симульованому середовищі з багатокомпонентними об’єктами різної форми, з’єднаними різними типами з’єднань, і об’єктами різної маси. Робот може передбачати та коригувати ефект штовхання під час спостереження за сценою, а також передбачення 6D-ефекту під час дії важеля вгору в контексті розбирання жорсткого диска за допомогою робота.",1
"Pre-trained language models have been shown to improve performance in many natural language tasks substantially. Although the early focus of such models was single language pre-training, recent advances have resulted in cross-lingual and visual pre-training methods. In this paper, we combine these two approaches to learn visually-grounded crosslingual representations. Specifically, we extend the translation language modelling (Lample and Conneau, 2019) with masked region classification and perform pre-training with three-way parallel vision & language corpora. We show that when fine-tuned for multimodal machine translation, these models obtain stateof-the-art performance. We also provide qualitative insights into the usefulness of the learned grounded representations.","Pre-trained language models have been demonstrated to drastically enhance results in many natural language tasks. Initially, focus was on single language pre-training, however recent developments have led to cross-lingual and visual pre-training techniques. This paper combines these two strategies to generate visually-grounded cross-lingual representations. We extended translation language modelling (Lample and Conneau, 2019) with masked region classification and conducted pre-training with three-way parallel vision & language corpora. When fine-tuned for multimodal machine translation, these models achieved state-of-the-art performance. Additionally, we offered qualitative evidence of the utility of the acquired grounded representations.",1
"Pre-trained language models have been shown to improve performance in many natural language tasks substantially. Although the early focus of such models was single language pre-training, recent advances have resulted in cross-lingual and visual pre-training methods. In this paper, we combine these two approaches to learn visually-grounded crosslingual representations. Specifically, we extend the translation language modelling (Lample and Conneau, 2019) with masked region classification and perform pre-training with three-way parallel vision & language corpora. We show that when fine-tuned for multimodal machine translation, these models obtain stateof-the-art performance. We also provide qualitative insights into the usefulness of the learned grounded representations.","Було продемонстровано, що попередньо підготовлені мовні моделі суттєво покращують результати багатьох завдань природної мови. Спочатку увага була зосереджена на одній мовній підготовці, однак останні розробки призвели до крослінгвальних і візуальних методів попередньої підготовки. Ця стаття поєднує ці дві стратегії для створення візуально обґрунтованих міжмовних уявлень. Ми розширили моделювання мови перекладу (Lample and Conneau, 2019) за допомогою маскової класифікації регіонів і провели попереднє навчання з тристороннім паралельним баченням і мовними корпусами. Після точного налаштування для мультимодального машинного перекладу ці моделі досягли найсучаснішої продуктивності. Крім того, ми запропонували якісні докази корисності отриманих обґрунтованих уявлень.",1
"Було показано, що попередньо підготовлені мовні моделі суттєво покращують продуктивність у багатьох завданнях природної мови. Незважаючи на те, що на ранніх етапах такі моделі зосереджувалися на одній мовній підготовці, останні досягнення призвели до міжмовних і візуальних методів попередньої підготовки. У цій статті ми поєднуємо ці два підходи, щоб вивчити візуально обґрунтовані міжмовні уявлення. Зокрема, ми розширюємо моделювання мови перекладу (Lample and Conneau, 2019) за допомогою класифікації замаскованих регіонів і виконуємо попереднє навчання за допомогою тристороннього паралельного бачення та мовних корпусів. Ми показуємо, що після точного налаштування для мультимодального машинного перекладу ці моделі отримують найсучаснішу продуктивність. Ми також надаємо якісне уявлення про корисність вивчених обґрунтованих уявлень.","Pre-trained language models have been demonstrated to drastically enhance results in many natural language tasks. Initially, focus was on single language pre-training, however recent developments have led to cross-lingual and visual pre-training techniques. This paper combines these two strategies to generate visually-grounded cross-lingual representations. We extended translation language modelling (Lample and Conneau, 2019) with masked region classification and conducted pre-training with three-way parallel vision & language corpora. When fine-tuned for multimodal machine translation, these models achieved state-of-the-art performance. Additionally, we offered qualitative evidence of the utility of the acquired grounded representations.",1
"Було показано, що попередньо підготовлені мовні моделі суттєво покращують продуктивність у багатьох завданнях природної мови. Незважаючи на те, що на ранніх етапах такі моделі зосереджувалися на одній мовній підготовці, останні досягнення призвели до міжмовних і візуальних методів попередньої підготовки. У цій статті ми поєднуємо ці два підходи, щоб вивчити візуально обґрунтовані міжмовні уявлення. Зокрема, ми розширюємо моделювання мови перекладу (Lample and Conneau, 2019) за допомогою класифікації замаскованих регіонів і виконуємо попереднє навчання за допомогою тристороннього паралельного бачення та мовних корпусів. Ми показуємо, що після точного налаштування для мультимодального машинного перекладу ці моделі отримують найсучаснішу продуктивність. Ми також надаємо якісне уявлення про корисність вивчених обґрунтованих уявлень.","Було продемонстровано, що попередньо підготовлені мовні моделі суттєво покращують результати багатьох завдань природної мови. Спочатку увага була зосереджена на одній мовній підготовці, однак останні розробки призвели до крослінгвальних і візуальних методів попередньої підготовки. Ця стаття поєднує ці дві стратегії для створення візуально обґрунтованих міжмовних уявлень. Ми розширили моделювання мови перекладу (Lample and Conneau, 2019) за допомогою маскової класифікації регіонів і провели попереднє навчання з тристороннім паралельним баченням і мовними корпусами. Після точного налаштування для мультимодального машинного перекладу ці моделі досягли найсучаснішої продуктивності. Крім того, ми запропонували якісні докази корисності отриманих обґрунтованих уявлень.",1
"Motion is an important cue for video prediction and often utilized by separating video content into static and dynamic components. Most of the previous work utilizing motion is deterministic but there are stochastic methods that can model the inherent uncertainty of the future. Existing stochastic models either do not reason about motion explicitly or make limiting assumptions about the static part. In this paper, we reason about appearance and motion in the video stochastically by predicting the future based on the motion history. Explicit reasoning about motion without history already reaches the performance of current stochastic models. The motion history further improves the results by allowing to predict consistent dynamics several frames into the future. Our model performs comparably to the stateof-the-art models on the generic video prediction datasets, however, significantly outperforms them on two challenging real-world autonomous driving datasets with complex motion and dynamic background.","Motion is an important factor for video prediction, often split into static and dynamic components. Previous work mostly used deterministic approaches, but there are stochastic methods that account for the uncertainty of the future. Existing stochastic models either don't consider motion explicitly or impose restrictions on the static part. We propose a model that takes into account both motion and appearance stochastically, predicting future frames based on motion history. Reasoning about motion without history already reaches the performance of current stochastic models, and motion history further improves results by allowing prediction of consistent dynamics multiple frames ahead. Our model performs comparably to the state-of-the-art models on generic video prediction datasets, but significantly better on two real-world autonomous driving datasets with complex motion and dynamic background.",1
"Motion is an important cue for video prediction and often utilized by separating video content into static and dynamic components. Most of the previous work utilizing motion is deterministic but there are stochastic methods that can model the inherent uncertainty of the future. Existing stochastic models either do not reason about motion explicitly or make limiting assumptions about the static part. In this paper, we reason about appearance and motion in the video stochastically by predicting the future based on the motion history. Explicit reasoning about motion without history already reaches the performance of current stochastic models. The motion history further improves the results by allowing to predict consistent dynamics several frames into the future. Our model performs comparably to the stateof-the-art models on the generic video prediction datasets, however, significantly outperforms them on two challenging real-world autonomous driving datasets with complex motion and dynamic background.","Рух є важливим фактором для передбачення відео, яке часто поділяється на статичні та динамічні компоненти. У попередній роботі здебільшого використовувалися детерміновані підходи, але існують стохастичні методи, які враховують невизначеність майбутнього. Існуючі стохастичні моделі або не розглядають рух явно, або накладають обмеження на статичну частину. Ми пропонуємо модель, яка стохастично враховує як рух, так і зовнішній вигляд, прогнозуючи майбутні кадри на основі історії руху. Роздуми про рух без історії вже досягають продуктивності поточних стохастичних моделей, а історія руху ще більше покращує результати, дозволяючи прогнозувати послідовну динаміку на кілька кадрів вперед. Наша модель працює порівнянно з найсучаснішими моделями на загальних наборах даних передбачення відео, але значно краще на двох реальних наборах даних автономного водіння зі складним рухом і динамічним фоном.",1
"Рух є важливою підказкою для передбачення відео, і часто використовується для поділу відеовмісту на статичні та динамічні компоненти. Більшість попередніх робіт із використанням руху є детермінованими, але існують стохастичні методи, які можуть моделювати притаманну невизначеність майбутнього. Існуючі стохастичні моделі або не міркують про рух явно, або роблять обмежувальні припущення щодо статичної частини. У цій статті ми розглядаємо зовнішній вигляд і рух у відео стохастично, передбачаючи майбутнє на основі історії руху. Явне міркування про рух без історії вже досягає продуктивності поточних стохастичних моделей. Історія руху ще більше покращує результати, дозволяючи передбачити узгоджену динаміку на кілька кадрів у майбутньому. Наша модель працює порівнянно з найсучаснішими моделями на загальних наборах даних прогнозування відео, однак значно перевершує їх на двох складних реальних наборах даних автономного водіння зі складним рухом і динамічним фоном.","Motion is an important factor for video prediction, often split into static and dynamic components. Previous work mostly used deterministic approaches, but there are stochastic methods that account for the uncertainty of the future. Existing stochastic models either don't consider motion explicitly or impose restrictions on the static part. We propose a model that takes into account both motion and appearance stochastically, predicting future frames based on motion history. Reasoning about motion without history already reaches the performance of current stochastic models, and motion history further improves results by allowing prediction of consistent dynamics multiple frames ahead. Our model performs comparably to the state-of-the-art models on generic video prediction datasets, but significantly better on two real-world autonomous driving datasets with complex motion and dynamic background.",1
"Рух є важливою підказкою для передбачення відео, і часто використовується для поділу відеовмісту на статичні та динамічні компоненти. Більшість попередніх робіт із використанням руху є детермінованими, але існують стохастичні методи, які можуть моделювати притаманну невизначеність майбутнього. Існуючі стохастичні моделі або не міркують про рух явно, або роблять обмежувальні припущення щодо статичної частини. У цій статті ми розглядаємо зовнішній вигляд і рух у відео стохастично, передбачаючи майбутнє на основі історії руху. Явне міркування про рух без історії вже досягає продуктивності поточних стохастичних моделей. Історія руху ще більше покращує результати, дозволяючи передбачити узгоджену динаміку на кілька кадрів у майбутньому. Наша модель працює порівнянно з найсучаснішими моделями на загальних наборах даних прогнозування відео, однак значно перевершує їх на двох складних реальних наборах даних автономного водіння зі складним рухом і динамічним фоном.","Рух є важливим фактором для передбачення відео, яке часто поділяється на статичні та динамічні компоненти. У попередній роботі здебільшого використовувалися детерміновані підходи, але існують стохастичні методи, які враховують невизначеність майбутнього. Існуючі стохастичні моделі або не розглядають рух явно, або накладають обмеження на статичну частину. Ми пропонуємо модель, яка стохастично враховує як рух, так і зовнішній вигляд, прогнозуючи майбутні кадри на основі історії руху. Роздуми про рух без історії вже досягають продуктивності поточних стохастичних моделей, а історія руху ще більше покращує результати, дозволяючи прогнозувати послідовну динаміку на кілька кадрів вперед. Наша модель працює порівнянно з найсучаснішими моделями на загальних наборах даних передбачення відео, але значно краще на двох реальних наборах даних автономного водіння зі складним рухом і динамічним фоном.",1
"Making sense of ever-growing amount of visual data available on the web is difficult, especially when considered in an unsupervised manner. As a step towards this goal, this study tackles a relatively less explored topic of generating structured summaries of large photo collections. Our framework relies on the notion of a story graph which captures the main narratives in the data and their relationships based on their visual, textual and spatio-temporal features. Its output is a directed graph with a set of possibly intersecting paths. Our proposed approach identifies coherent visual storylines and exploits sub-modularity to select a subset of these lines which covers the general narrative at most. Our experimental analysis reveals that extracted story graphs allow for obtaining better results when utilized as priors for photo album summarization. Moreover, our user studies show that our approach delivers better performance on next image prediction and coverage tasks than the state-of-the-art.","Tackling a less explored topic of generating structured summaries of large photo collections, this study presents a framework based on the concept of a story graph to make sense of the vast amount of visual data available on the web in an unsupervised manner. This graph captures main narratives and their relationships by using visual, textual and spatio-temporal features, and its output is a directed graph with intersecting paths. Our proposed approach identifies coherent visual storylines and selects a subset which covers the general narrative most, through exploiting sub-modularity. Experimental analysis reveals that the extracted story graphs lead to better results when used as priors for photo album summarization. Furthermore, user studies show that our approach outperforms the state-of-the-art in next image prediction and coverage tasks.",1
"Making sense of ever-growing amount of visual data available on the web is difficult, especially when considered in an unsupervised manner. As a step towards this goal, this study tackles a relatively less explored topic of generating structured summaries of large photo collections. Our framework relies on the notion of a story graph which captures the main narratives in the data and their relationships based on their visual, textual and spatio-temporal features. Its output is a directed graph with a set of possibly intersecting paths. Our proposed approach identifies coherent visual storylines and exploits sub-modularity to select a subset of these lines which covers the general narrative at most. Our experimental analysis reveals that extracted story graphs allow for obtaining better results when utilized as priors for photo album summarization. Moreover, our user studies show that our approach delivers better performance on next image prediction and coverage tasks than the state-of-the-art.","Розглядаючи менш вивчену тему створення структурованих підсумків великих колекцій фотографій, це дослідження представляє структуру, засновану на концепції сюжетного графіка, щоб зрозуміти величезну кількість візуальних даних, доступних в Інтернеті в неконтрольований спосіб. Цей графік фіксує основні наративи та їхні зв’язки за допомогою візуальних, текстових і просторово-часових особливостей, а його результатом є орієнтований графік із пересічними шляхами. Запропонований нами підхід визначає послідовні візуальні сюжетні лінії та вибирає підмножину, яка найбільше охоплює загальний наратив, використовуючи субмодульність. Експериментальний аналіз показує, що витягнуті графіки історій дають кращі результати, якщо їх використовувати як попередні для підсумовування фотоальбому. Крім того, дослідження користувачів показують, що наш підхід перевершує найсучасніші в наступних задачах прогнозування зображення та покриття.",1
"Важко розібратися в постійно зростаючій кількості візуальних даних, доступних в Інтернеті, особливо якщо розглядати їх без нагляду. Як крок до цієї мети це дослідження стосується відносно менш дослідженої теми створення структурованих підсумків великих колекцій фотографій. Наша структура спирається на поняття графіка історії, який фіксує основні наративи в даних та їхні зв’язки на основі їхніх візуальних, текстових і просторово-часових особливостей. Його виходом є орієнтований граф із набором можливо пересічних шляхів. Запропонований нами підхід визначає послідовні візуальні сюжетні лінії та використовує субмодульність, щоб вибрати підмножину цих ліній, яка максимально охоплює загальну розповідь. Наш експериментальний аналіз показує, що витягнуті графіки історій дозволяють отримати кращі результати, якщо їх використовувати як попередні для підсумовування фотоальбому. Крім того, наші дослідження користувачів показують, що наш підхід забезпечує кращу продуктивність у наступних задачах прогнозування зображення та покриття, ніж найсучасніший.","Tackling a less explored topic of generating structured summaries of large photo collections, this study presents a framework based on the concept of a story graph to make sense of the vast amount of visual data available on the web in an unsupervised manner. This graph captures main narratives and their relationships by using visual, textual and spatio-temporal features, and its output is a directed graph with intersecting paths. Our proposed approach identifies coherent visual storylines and selects a subset which covers the general narrative most, through exploiting sub-modularity. Experimental analysis reveals that the extracted story graphs lead to better results when used as priors for photo album summarization. Furthermore, user studies show that our approach outperforms the state-of-the-art in next image prediction and coverage tasks.",1
"Важко розібратися в постійно зростаючій кількості візуальних даних, доступних в Інтернеті, особливо якщо розглядати їх без нагляду. Як крок до цієї мети це дослідження стосується відносно менш дослідженої теми створення структурованих підсумків великих колекцій фотографій. Наша структура спирається на поняття графіка історії, який фіксує основні наративи в даних та їхні зв’язки на основі їхніх візуальних, текстових і просторово-часових особливостей. Його виходом є орієнтований граф із набором можливо пересічних шляхів. Запропонований нами підхід визначає послідовні візуальні сюжетні лінії та використовує субмодульність, щоб вибрати підмножину цих ліній, яка максимально охоплює загальну розповідь. Наш експериментальний аналіз показує, що витягнуті графіки історій дозволяють отримати кращі результати, якщо їх використовувати як попередні для підсумовування фотоальбому. Крім того, наші дослідження користувачів показують, що наш підхід забезпечує кращу продуктивність у наступних задачах прогнозування зображення та покриття, ніж найсучасніший.","Розглядаючи менш вивчену тему створення структурованих підсумків великих колекцій фотографій, це дослідження представляє структуру, засновану на концепції сюжетного графіка, щоб зрозуміти величезну кількість візуальних даних, доступних в Інтернеті в неконтрольований спосіб. Цей графік фіксує основні наративи та їхні зв’язки за допомогою візуальних, текстових і просторово-часових особливостей, а його результатом є орієнтований графік із пересічними шляхами. Запропонований нами підхід визначає послідовні візуальні сюжетні лінії та вибирає підмножину, яка найбільше охоплює загальний наратив, використовуючи субмодульність. Експериментальний аналіз показує, що витягнуті графіки історій дають кращі результати, якщо їх використовувати як попередні для підсумовування фотоальбому. Крім того, дослідження користувачів показують, що наш підхід перевершує найсучасніші в наступних задачах прогнозування зображення та покриття.",1
"Automatic generation of video descriptions in natural language, also called video captioning, aims to understand the visual content of the video and produce a natural language sentence depicting the objects and actions in the scene. This challenging integrated vision and language problem, however, has been predominantly addressed for English. The lack of data and the linguistic properties of other languages limit the success of existing approaches for such languages. In this paper we target Turkish, a morphologically rich and agglutinative language that has very different properties compared to English. To do so, we create the first large scale video captioning dataset for this language by carefully translating the English descriptions of the videos in the MSVD (Microsoft Research Video Description Corpus) dataset into Turkish. In addition to enabling research in video captioning in Turkish, the parallel English-Turkish descriptions also enables the study of the role of video context in (multimodal) machine translation. In our experiments, we build models for both video captioning and multimodal machine translation and investigate the effect of different word segmentation approaches and different neural architectures to better address the properties of Turkish. We hope that the MSVDTurkish dataset and the results reported in this work will lead to better video captioning and multimodal machine translation models for Turkish and other morphology rich and agglutinative languages.","The task of automatically generating natural language descriptions for videos, also known as video captioning, requires understanding the visual elements of the video and expressing them in a language. This integrated vision and language problem has mainly been studied in English, however, due to lack of data and the linguistic characteristics of other languages, existing approaches are not effective. In this paper, we focus on Turkish, an agglutinative language with distinct properties than English. To do this, we create the first large-scale video captioning dataset for Turkish by translating the English descriptions of the MSVD (Microsoft Research Video Description Corpus) dataset into Turkish. This dataset can be used to research video captioning in Turkish, as well as the role of video context in (multimodal) machine translation. We experiment with different word segmentation approaches and neural architectures to address the properties of Turkish and build models for both video captioning and multimodal machine translation. The MSVDTurkish dataset and results from this work can be used to improve video captioning and multimodal machine translation models for Turkish and other agglutinative languages.",1
"Automatic generation of video descriptions in natural language, also called video captioning, aims to understand the visual content of the video and produce a natural language sentence depicting the objects and actions in the scene. This challenging integrated vision and language problem, however, has been predominantly addressed for English. The lack of data and the linguistic properties of other languages limit the success of existing approaches for such languages. In this paper we target Turkish, a morphologically rich and agglutinative language that has very different properties compared to English. To do so, we create the first large scale video captioning dataset for this language by carefully translating the English descriptions of the videos in the MSVD (Microsoft Research Video Description Corpus) dataset into Turkish. In addition to enabling research in video captioning in Turkish, the parallel English-Turkish descriptions also enables the study of the role of video context in (multimodal) machine translation. In our experiments, we build models for both video captioning and multimodal machine translation and investigate the effect of different word segmentation approaches and different neural architectures to better address the properties of Turkish. We hope that the MSVDTurkish dataset and the results reported in this work will lead to better video captioning and multimodal machine translation models for Turkish and other morphology rich and agglutinative languages.","Завдання автоматичного створення описів природною мовою для відео, також відоме як субтитри до відео, вимагає розуміння візуальних елементів відео та виразу їх мовою. Ця інтегрована проблема бачення та мови в основному вивчалася англійською мовою, однак через брак даних та лінгвістичні характеристики інших мов існуючі підходи неефективні. У цій статті ми зосереджуємось на турецькій мові, аглютинативній мові з відмінними властивостями, ніж англійська. Для цього ми створюємо перший великомасштабний набір даних субтитрів до відео для турецької мови, перекладаючи англійські описи набору даних MSVD (Microsoft Research Video Description Corpus) на турецьку мову. Цей набір даних можна використовувати для дослідження субтитрів до відео турецькою мовою, а також для дослідження ролі відеоконтексту в (мультимодальному) машинному перекладі. Ми експериментуємо з різними підходами до сегментації слів і нейронними архітектурами, щоб розглянути властивості турецької мови та створити моделі для субтитрів до відео та мультимодального машинного перекладу. Набір даних MSVDTurkish і результати цієї роботи можна використовувати для покращення субтитрів до відео та мультимодальних моделей машинного перекладу для турецької та інших аглютинативних мов.",1
"Автоматична генерація відеоописів природною мовою, яка також називається відеотитрами, має на меті зрозуміти візуальний вміст відео та створити речення природною мовою, що зображує об’єкти та дії в сцені. Ця складна інтегрована проблема бачення та мови, однак, була переважно розглянута для англійської мови. Брак даних і лінгвістичні властивості інших мов обмежують успіх існуючих підходів для таких мов. У цій статті ми орієнтуємося на турецьку, морфологічно багату та аглютинативну мову, яка має дуже різні властивості порівняно з англійською. Для цього ми створюємо перший великомасштабний набір даних субтитрів до відео для цієї мови, ретельно перекладаючи англійські описи відео в наборі даних MSVD (Microsoft Research Video Description Corpus) на турецьку мову. На додаток до дослідження субтитрів до відео турецькою мовою, паралельні англо-турецькі описи також дозволяють вивчати роль відеоконтексту в (мультимодальному) машинному перекладі. У наших експериментах ми створюємо моделі як для субтитрів до відео, так і для мультимодального машинного перекладу та досліджуємо вплив різних підходів до сегментації слів і різних нейронних архітектур для кращого врахування властивостей турецької мови. Ми сподіваємося, що набір даних MSVDTurkish і результати, наведені в цій роботі, призведуть до кращих відеосубтитрів і мультимодальних моделей машинного перекладу для турецької та інших морфологічних і аглютинативних мов.","The task of automatically generating natural language descriptions for videos, also known as video captioning, requires understanding the visual elements of the video and expressing them in a language. This integrated vision and language problem has mainly been studied in English, however, due to lack of data and the linguistic characteristics of other languages, existing approaches are not effective. In this paper, we focus on Turkish, an agglutinative language with distinct properties than English. To do this, we create the first large-scale video captioning dataset for Turkish by translating the English descriptions of the MSVD (Microsoft Research Video Description Corpus) dataset into Turkish. This dataset can be used to research video captioning in Turkish, as well as the role of video context in (multimodal) machine translation. We experiment with different word segmentation approaches and neural architectures to address the properties of Turkish and build models for both video captioning and multimodal machine translation. The MSVDTurkish dataset and results from this work can be used to improve video captioning and multimodal machine translation models for Turkish and other agglutinative languages.",1
"Автоматична генерація відеоописів природною мовою, яка також називається відеотитрами, має на меті зрозуміти візуальний вміст відео та створити речення природною мовою, що зображує об’єкти та дії в сцені. Ця складна інтегрована проблема бачення та мови, однак, була переважно розглянута для англійської мови. Брак даних і лінгвістичні властивості інших мов обмежують успіх існуючих підходів для таких мов. У цій статті ми орієнтуємося на турецьку, морфологічно багату та аглютинативну мову, яка має дуже різні властивості порівняно з англійською. Для цього ми створюємо перший великомасштабний набір даних субтитрів до відео для цієї мови, ретельно перекладаючи англійські описи відео в наборі даних MSVD (Microsoft Research Video Description Corpus) на турецьку мову. На додаток до дослідження субтитрів до відео турецькою мовою, паралельні англо-турецькі описи також дозволяють вивчати роль відеоконтексту в (мультимодальному) машинному перекладі. У наших експериментах ми створюємо моделі як для субтитрів до відео, так і для мультимодального машинного перекладу та досліджуємо вплив різних підходів до сегментації слів і різних нейронних архітектур для кращого врахування властивостей турецької мови. Ми сподіваємося, що набір даних MSVDTurkish і результати, наведені в цій роботі, призведуть до кращих відеосубтитрів і мультимодальних моделей машинного перекладу для турецької та інших морфологічних і аглютинативних мов.","Завдання автоматичного створення описів природною мовою для відео, також відоме як субтитри до відео, вимагає розуміння візуальних елементів відео та виразу їх мовою. Ця інтегрована проблема бачення та мови в основному вивчалася англійською мовою, однак через брак даних та лінгвістичні характеристики інших мов існуючі підходи неефективні. У цій статті ми зосереджуємось на турецькій мові, аглютинативній мові з відмінними властивостями, ніж англійська. Для цього ми створюємо перший великомасштабний набір даних субтитрів до відео для турецької мови, перекладаючи англійські описи набору даних MSVD (Microsoft Research Video Description Corpus) на турецьку мову. Цей набір даних можна використовувати для дослідження субтитрів до відео турецькою мовою, а також для дослідження ролі контексту відео в (мультимодальному) машинному перекладі. Ми експериментуємо з різними підходами до сегментації слів і нейронними архітектурами, щоб розглянути властивості турецької мови та створити моделі для субтитрів до відео та мультимодального машинного перекладу. Набір даних MSVDTurkish і результати цієї роботи можна використовувати для покращення субтитрів до відео та мультимодальних моделей машинного перекладу для турецької та інших аглютинативних мов.",1
"Humans are able to perceive, understand and reason about causal events. Developing models with similar physical and causal understanding capabilities is a long-standing goal of artificial intelligence. As a step towards this direction, we introduce CRAFT1 , a new video question answering dataset that requires causal reasoning about physical forces and object interactions. It contains 58K video and question pairs that are generated from 10K videos from 20 different virtual environments, containing various objects in motion that interact with each other and the scene. Two question categories in CRAFT include previously studied descriptive and counterfactual questions. Additionally, inspired by the Force Dynamics Theory in cognitive linguistics, we introduce a new causal question category that involves understanding the causal interactions between objects through notions like cause, enable, and prevent. Our results show that even though the questions in CRAFT are easy for humans, the tested baseline models, including existing state-of-the-art methods, do not yet deal with the challenges posed in our benchmark.","Humans possess the capacity to identify, interpret and reason about causes and effects. Developing AI models with comparable physical and causal comprehension abilities is a long-held aspiration. To take a step towards this objective, we introduce CRAFT1 - a novel video question answering dataset that necessitates reasoning regarding physical forces and object interactions. It comprises of 58K video and question pairs generated from 10K videos from 20 virtual settings with various objects in motion that interact with each other and their environment. CRAFT comprises two question categories previously studied, as well as a novel causal question type inspired by the Force Dynamics Theory in cognitive linguistics, involving understanding the causal relationships between objects through concepts such as cause, enable and prevent. Our results demonstrate that even though humans find the questions in CRAFT straightforward, the evaluated baseline models, including existing state-of-the-art methods, are yet to address the difficulties presented in our benchmark.",1
"Humans are able to perceive, understand and reason about causal events. Developing models with similar physical and causal understanding capabilities is a long-standing goal of artificial intelligence. As a step towards this direction, we introduce CRAFT1 , a new video question answering dataset that requires causal reasoning about physical forces and object interactions. It contains 58K video and question pairs that are generated from 10K videos from 20 different virtual environments, containing various objects in motion that interact with each other and the scene. Two question categories in CRAFT include previously studied descriptive and counterfactual questions. Additionally, inspired by the Force Dynamics Theory in cognitive linguistics, we introduce a new causal question category that involves understanding the causal interactions between objects through notions like cause, enable, and prevent. Our results show that even though the questions in CRAFT are easy for humans, the tested baseline models, including existing state-of-the-art methods, do not yet deal with the challenges posed in our benchmark.","Люди мають здатність ідентифікувати, інтерпретувати та міркувати про причини та наслідки. Розробка моделей штучного інтелекту з порівнянними фізичними та причинно-наслідковими можливостями розуміння є давнім прагненням. Щоб зробити крок назустріч цій меті, ми представляємо CRAFT1 — новий набір даних із відповідями на відеозапитання, який потребує міркування щодо фізичних сил і взаємодії об’єктів. Він складається з 58K відео та пар запитань, згенерованих із 10K відео з 20 віртуальних налаштувань з різними рухомими об’єктами, які взаємодіють один з одним та своїм оточенням. CRAFT включає дві категорії запитань, які вивчалися раніше, а також новий тип причинно-наслідкових запитань, натхненний теорією динаміки сил у когнітивній лінгвістиці, що передбачає розуміння причинно-наслідкових зв’язків між об’єктами через такі поняття, як причина, можливість і запобігання. Наші результати показують, що, незважаючи на те, що люди вважають запитання в CRAFT простими, оцінені базові моделі, включаючи існуючі найсучасніші методи, ще не вирішують труднощі, представлені в нашому тесті.",1
"Люди здатні сприймати, розуміти та міркувати про причинно-наслідкові події. Розробка моделей із подібними фізичними та причинно-наслідковими можливостями розуміння є давньою метою штучного інтелекту. Як крок у цьому напрямку ми представляємо CRAFT1, новий набір даних із відповідями на відеозапитання, який вимагає причинно-наслідкових міркувань про фізичні сили та взаємодію об’єктів. Він містить 58 тис. відео та пар запитань, створених із 10 тис. відео з 20 різних віртуальних середовищ, що містять різні рухомі об’єкти, які взаємодіють один з одним і сценою. Дві категорії запитань у CRAFT включають раніше вивчені описові та контрфактичні запитання. Крім того, натхненний теорією динаміки сил у когнітивній лінгвістиці, ми вводимо нову категорію причинно-наслідкових питань, яка передбачає розуміння причинно-наслідкових взаємодій між об’єктами через такі поняття, як причина, можливість і запобігання. Наші результати показують, що, незважаючи на те, що запитання в CRAFT легкі для людей, перевірені базові моделі, включаючи існуючі найсучасніші методи, ще не справляються з проблемами, поставленими в нашому тесті.","Humans possess the capacity to identify, interpret and reason about causes and effects. Developing AI models with comparable physical and causal comprehension abilities is a long-held aspiration. To take a step towards this objective, we introduce CRAFT1 - a novel video question answering dataset that necessitates reasoning regarding physical forces and object interactions. It comprises of 58K video and question pairs generated from 10K videos from 20 virtual settings with various objects in motion that interact with each other and their environment. CRAFT comprises two question categories previously studied, as well as a novel causal question type inspired by the Force Dynamics Theory in cognitive linguistics, involving understanding the causal relationships between objects through concepts such as cause, enable and prevent. Our results demonstrate that even though humans find the questions in CRAFT straightforward, the evaluated baseline models, including existing state-of-the-art methods, are yet to address the difficulties presented in our benchmark.",1
"Люди здатні сприймати, розуміти та міркувати про причинно-наслідкові події. Розробка моделей із подібними фізичними та причинно-наслідковими можливостями розуміння є давньою метою штучного інтелекту. Як крок у цьому напрямку ми представляємо CRAFT1, новий набір даних із відповідями на відеозапитання, який вимагає причинно-наслідкових міркувань про фізичні сили та взаємодію об’єктів. Він містить 58 тис. відео та пар запитань, створених із 10 тис. відео з 20 різних віртуальних середовищ, що містять різні рухомі об’єкти, які взаємодіють один з одним і сценою. Дві категорії запитань у CRAFT включають раніше вивчені описові та контрфактичні запитання. Крім того, натхненний теорією динаміки сил у когнітивній лінгвістиці, ми вводимо нову категорію причинно-наслідкових питань, яка передбачає розуміння причинно-наслідкових взаємодій між об’єктами через такі поняття, як причина, можливість і запобігання. Наші результати показують, що, незважаючи на те, що запитання в CRAFT легкі для людей, перевірені базові моделі, включаючи існуючі найсучасніші методи, ще не справляються з проблемами, поставленими в нашому тесті.","Люди мають здатність ідентифікувати, інтерпретувати та міркувати про причини та наслідки. Розробка моделей штучного інтелекту з порівнянними фізичними та причинно-наслідковими можливостями розуміння є давнім прагненням. Щоб зробити крок назустріч цій меті, ми представляємо CRAFT1 — новий набір даних із відповідями на відеозапитання, який потребує міркування щодо фізичних сил і взаємодії об’єктів. Він складається з 58K відео та пар запитань, згенерованих із 10K відео з 20 віртуальних налаштувань з різними рухомими об’єктами, які взаємодіють один з одним та своїм оточенням. CRAFT включає дві категорії запитань, які вивчалися раніше, а також новий тип причинно-наслідкових запитань, натхненний теорією динаміки сил у когнітивній лінгвістиці, що передбачає розуміння причинно-наслідкових зв’язків між об’єктами через такі поняття, як причина, можливість і запобігання. Наші результати показують, що, незважаючи на те, що люди вважають запитання в CRAFT простими, оцінені базові моделі, включаючи існуючі найсучасніші методи, ще не вирішують труднощі, представлені в нашому тесті.",1
"In recent years, graph neural networks have been successfully applied for learning the dynamics of complex and partially observable physical systems. However, their use in the robotics domain is, to date, still limited. In this paper, we introduce Belief Regulated Dual Propagation Networks (BRDPN), a general-purpose learnable physics engine, which enables a robot to predict the effects of its actions in scenes containing groups of articulated multi-part objects. Specifically, our framework extends recently proposed propagation networks (PropNets) and consists of two complementary components, a physics predictor and a belief regulator. While the former predicts the future states of the object(s) manipulated by the robot, the latter constantly corrects the robot’s knowledge regarding the objects and their relations. Our results showed that after training in a simulator, the robot can reliably predict the consequences of its actions in object trajectory level and exploit its own interaction experience to correct its belief about the state of the environment, enabling better predictions in partially observable environments. Furthermore, the trained model was transferred to the real world and verified in predicting trajectories of pushed interacting objects whose joint relations were initially unknown. We compared BRDPN against PropNets, and showed that BRDPN performs consistently well. Moreover, BRDPN can adapt its physic predictions, since the relations can be predicted online. ","Graph neural networks have been employed to comprehend the dynamics of intricate and partially observable physical systems in recent years, yet their use in robotics is still scant. In this paper, we introduce Belief Regulated Dual Propagation Networks (BRDPN), a general-purpose learnable physics engine which allows robots to anticipate the outcomes of their actions in scenes containing multiple connected parts. Our framework develops upon recently proposed Propagation Networks (PropNets) and consists of two distinct components, a physics predictor and a belief regulator. The former forecasts the future states of the objects controlled by the robot, while the latter regularly adjusts the robot's understanding of the objects and their interactions. Our results demonstrated that after training in a simulator, the robot can dependably foresee the results of its actions in object trajectory level and take advantage of its own interaction experience to modify its belief about the state of the environment, allowing for better predictions in partially observable settings. Additionally, the trained model was transferred to the real world and confirmed in predicting trajectories of interacting objects whose joint relations were initially unknown. We compared BRDPN with PropNets, and demonstrated that BRDPN performs reliably well. Moreover, BRDPN is able to adapt its physic predictions, as the relations can be calculated online.",1
"In recent years, graph neural networks have been successfully applied for learning the dynamics of complex and partially observable physical systems. However, their use in the robotics domain is, to date, still limited. In this paper, we introduce Belief Regulated Dual Propagation Networks (BRDPN), a general-purpose learnable physics engine, which enables a robot to predict the effects of its actions in scenes containing groups of articulated multi-part objects. Specifically, our framework extends recently proposed propagation networks (PropNets) and consists of two complementary components, a physics predictor and a belief regulator. While the former predicts the future states of the object(s) manipulated by the robot, the latter constantly corrects the robot’s knowledge regarding the objects and their relations. Our results showed that after training in a simulator, the robot can reliably predict the consequences of its actions in object trajectory level and exploit its own interaction experience to correct its belief about the state of the environment, enabling better predictions in partially observable environments. Furthermore, the trained model was transferred to the real world and verified in predicting trajectories of pushed interacting objects whose joint relations were initially unknown. We compared BRDPN against PropNets, and showed that BRDPN performs consistently well. Moreover, BRDPN can adapt its physic predictions, since the relations can be predicted online. ","Графові нейронні мережі в останні роки використовувалися для розуміння динаміки складних і частково спостережуваних фізичних систем, але їх використання в робототехніці все ще мало. У цьому документі ми представляємо регульовані переконаннями подвійні мережі розповсюдження (BRDPN), універсальний навчальний фізичний механізм, який дозволяє роботам передбачати результати своїх дій у сценах, що містять кілька з’єднаних частин. Наша структура розроблена на основі нещодавно запропонованих мереж розповсюдження (PropNets) і складається з двох окремих компонентів: фізичного предиктора та регулятора переконань. Перший прогнозує майбутні стани об’єктів, якими керує робот, а другий регулярно коригує розуміння роботом об’єктів та їх взаємодії. Наші результати продемонстрували, що після навчання на симуляторі робот може надійно передбачати результати своїх дій на рівні траєкторії об’єкта та використовувати переваги свого власного досвіду взаємодії, щоб змінити свої уявлення про стан навколишнього середовища, дозволяючи краще прогнозувати в частково спостережуваних налаштування. Крім того, навчена модель була перенесена в реальний світ і підтверджена в прогнозуванні траєкторій взаємодіючих об’єктів, спільні відносини яких спочатку були невідомі. Ми порівняли BRDPN із PropNets і продемонстрували, що BRDPN працює надійно добре. Крім того, BRDPN може адаптувати свої фізичні прогнози, оскільки співвідношення можна розрахувати онлайн.",1
"В останні роки графові нейронні мережі успішно застосовуються для вивчення динаміки складних і частково спостережуваних фізичних систем. Однак їх використання в області робототехніки на сьогоднішній день все ще обмежене. У цій статті ми представляємо мережу подвійного розповсюдження, регульовану переконаннями (BRDPN), універсальний навчальний фізичний механізм, який дозволяє роботу передбачати наслідки його дій у сценах, що містять групи шарнірних багатокомпонентних об’єктів. Зокрема, наша структура розширює нещодавно запропоновані мережі розповсюдження (PropNets) і складається з двох взаємодоповнюючих компонентів, фізичного предиктора та регулятора переконань. У той час як перший передбачає майбутні стани об’єктів, якими маніпулює робот, другий постійно коригує знання робота щодо об’єктів та їхніх зв’язків. Наші результати показали, що після навчання на симуляторі робот може надійно передбачати наслідки своїх дій на рівні траєкторії об’єкта та використовувати власний досвід взаємодії, щоб виправити свої переконання щодо стану навколишнього середовища, забезпечуючи кращі прогнози в частково доступних для спостереження середовищах. Крім того, навчена модель була перенесена в реальний світ і перевірена у передбаченні траєкторій штовхаючих взаємодіючих об’єктів, спільні зв’язки яких спочатку були невідомі. Ми порівняли BRDPN із PropNets і показали, що BRDPN стабільно добре працює. Крім того, BRDPN може адаптувати свої фізичні прогнози, оскільки співвідношення можна передбачити онлайн.","Graph neural networks have been employed to comprehend the dynamics of intricate and partially observable physical systems in recent years, yet their use in robotics is still scant. In this paper, we introduce Belief Regulated Dual Propagation Networks (BRDPN), a general-purpose learnable physics engine which allows robots to anticipate the outcomes of their actions in scenes containing multiple connected parts. Our framework develops upon recently proposed Propagation Networks (PropNets) and consists of two distinct components, a physics predictor and a belief regulator. The former forecasts the future states of the objects controlled by the robot, while the latter regularly adjusts the robot's understanding of the objects and their interactions. Our results demonstrated that after training in a simulator, the robot can dependably foresee the results of its actions in object trajectory level and take advantage of its own interaction experience to modify its belief about the state of the environment, allowing for better predictions in partially observable settings. Additionally, the trained model was transferred to the real world and confirmed in predicting trajectories of interacting objects whose joint relations were initially unknown. We compared BRDPN with PropNets, and demonstrated that BRDPN performs reliably well. Moreover, BRDPN is able to adapt its physic predictions, as the relations can be calculated online.",1
"В останні роки графові нейронні мережі успішно застосовуються для вивчення динаміки складних і частково спостережуваних фізичних систем. Однак їх використання в області робототехніки на сьогоднішній день все ще обмежене. У цьому документі ми представляємо мережі подвійного розповсюдження, регульовані переконаннями (BRDPN), універсальний навчальний фізичний механізм, який дозволяє роботу передбачати наслідки його дій у сценах, що містять групи шарнірних багатокомпонентних об’єктів. Зокрема, наша структура розширює нещодавно запропоновані мережі розповсюдження (PropNets) і складається з двох додаткових компонентів, фізичного предиктора та регулятора віри. Тоді як перший передбачає майбутні стани об’єктів, якими маніпулює робот, другий постійно коригує знання робота щодо об’єктів та їхніх зв’язків. Наші результати показали, що після навчання на симуляторі робот може надійно передбачати наслідки своїх дій на рівні траєкторії об’єкта та використовувати власний досвід взаємодії, щоб виправити свої переконання щодо стану навколишнього середовища, забезпечуючи кращі прогнози в частково доступних для спостереження середовищах. Крім того, навчена модель була перенесена в реальний світ і перевірена у прогнозуванні траєкторій штовхаючих взаємодіючих об’єктів, спільні зв’язки яких спочатку були невідомі. Ми порівняли BRDPN із PropNets і показали, що BRDPN стабільно добре працює. Крім того, BRDPN може адаптувати свої фізичні прогнози, оскільки співвідношення можна передбачити онлайн.","Графові нейронні мережі в останні роки використовувалися для розуміння динаміки складних і частково спостережуваних фізичних систем, але їх використання в робототехніці все ще мало. У цьому документі ми представляємо регульовані переконаннями подвійні мережі розповсюдження (BRDPN), універсальний навчальний фізичний механізм, який дозволяє роботам передбачати результати своїх дій у сценах, що містять кілька з’єднаних частин. Наша структура розроблена на основі нещодавно запропонованих мереж розповсюдження (PropNets) і складається з двох окремих компонентів: фізичного предиктора та регулятора переконань. Перший прогнозує майбутні стани об’єктів, якими керує робот, тоді як другий регулярно коригує розуміння роботом об’єктів та їх взаємодії. Наші результати продемонстрували, що після навчання на симуляторі робот може надійно передбачати результати своїх дій на рівні траєкторії об’єкта та використовувати переваги свого власного досвіду взаємодії, щоб змінити свої переконання щодо стану навколишнього середовища, дозволяючи краще прогнозувати в частково спостережуваних налаштування. Крім того, навчена модель була перенесена в реальний світ і підтверджена в прогнозуванні траєкторій взаємодіючих об’єктів, спільні відносини яких спочатку були невідомі. Ми порівняли BRDPN із PropNets і продемонстрували, що BRDPN працює надійно добре. Крім того, BRDPN може адаптувати свої фізичні прогнози, оскільки співвідношення можна розрахувати онлайн.",1
"In recent years, many computational models for saliency prediction have been introduced. For dynamic scenes, the existing models typically combine different feature maps extracted from spatial and temporal domains either by following generic integration strategies such as averaging or winners take all or using machine learning techniques to set each feature’s importance. Rather than resorting to these fixed feature integration schemes, in this paper, we propose a novel weakly supervised dynamic saliency model called HedgeSal, which is based on a decision-theoretic online learning scheme. Our framework uses two pretrained deep static saliency models as experts to extract individual saliency maps from appearance and motion streams, and then generates the final saliency map by weighted decisions of all these models. As visual characteristics of dynamic scenes constantly vary, the models providing consistently good predictions in the past are automatically assigned higher weights, allowing each expert to adjust itself to the current conditions. We demonstrate the effectiveness of our model on the CRCNS, UCFSports and CITIUS datasets.","In recent years, a novel weakly supervised dynamic saliency model, called HedgeSal, has been proposed. It is based on a decision-theoretic online learning scheme, which uses two pretrained deep static saliency models as experts to extract individual saliency maps from appearance and motion streams, and then generates the final saliency map by weighted decisions of all these models. Rather than employing generic integration strategies such as averaging or winners take all or using machine learning techniques to set each feature’s importance, this framework allows each expert to adjust itself to the current conditions by assigning higher weights to the models providing consistently good predictions in the past. The effectiveness of the model has been demonstrated on the CRCNS, UCFSports and CITIUS datasets.",1
"In recent years, many computational models for saliency prediction have been introduced. For dynamic scenes, the existing models typically combine different feature maps extracted from spatial and temporal domains either by following generic integration strategies such as averaging or winners take all or using machine learning techniques to set each feature’s importance. Rather than resorting to these fixed feature integration schemes, in this paper, we propose a novel weakly supervised dynamic saliency model called HedgeSal, which is based on a decision-theoretic online learning scheme. Our framework uses two pretrained deep static saliency models as experts to extract individual saliency maps from appearance and motion streams, and then generates the final saliency map by weighted decisions of all these models. As visual characteristics of dynamic scenes constantly vary, the models providing consistently good predictions in the past are automatically assigned higher weights, allowing each expert to adjust itself to the current conditions. We demonstrate the effectiveness of our model on the CRCNS, UCFSports and CITIUS datasets.","В останні роки була запропонована нова модель динамічної помітності зі слабким наглядом, яка називається HedgeSal. Він заснований на схемі онлайн-навчання на основі прийняття рішень, яка використовує дві попередньо підготовлені глибокі статичні моделі помітності як експертів для отримання індивідуальних карт помітності з потоків зовнішнього вигляду та руху, а потім генерує остаточну карту помітності шляхом зважених рішень усіх цих моделей. Замість використання загальних стратегій інтеграції, таких як усереднення або переможці отримують все, або використання методів машинного навчання для встановлення важливості кожної функції, ця структура дозволяє кожному експерту адаптуватися до поточних умов, призначаючи вищі ваги моделям, що забезпечують незмінно хороші прогнози в минулому. . Ефективність моделі була продемонстрована на наборах даних CRCNS, UCFSports і CITIUS.",1
"В останні роки було введено багато обчислювальних моделей для прогнозування помітності. Для динамічних сцен існуючі моделі зазвичай поєднують різні карти функцій, отримані з просторових і часових доменів, дотримуючись загальних стратегій інтеграції, таких як усереднення або переможці отримують усе, або використовуючи методи машинного навчання для встановлення важливості кожної функції. Замість того, щоб вдаватися до цих схем інтеграції фіксованих функцій, у цій статті ми пропонуємо нову слабко контрольовану динамічну модель помітності під назвою HedgeSal, яка базується на схемі онлайн-навчання, що базується на теоретичному прийнятті рішень. Наш фреймворк використовує дві попередньо підготовлені глибокі статичні моделі помітності як експертів для отримання індивідуальних карт помітності з потоків зовнішнього вигляду та руху, а потім генерує остаточну карту помітності шляхом зважених рішень усіх цих моделей. Оскільки візуальні характеристики динамічних сцен постійно змінюються, моделям, що забезпечують незмінно хороші прогнози в минулому, автоматично призначаються вищі ваги, що дозволяє кожному експерту пристосуватися до поточних умов. Ми демонструємо ефективність нашої моделі на наборах даних CRCNS, UCFSports і CITIUS.","In recent years, a novel weakly supervised dynamic saliency model, called HedgeSal, has been proposed. It is based on a decision-theoretic online learning scheme, which uses two pretrained deep static saliency models as experts to extract individual saliency maps from appearance and motion streams, and then generates the final saliency map by weighted decisions of all these models. Rather than employing generic integration strategies such as averaging or winners take all or using machine learning techniques to set each feature’s importance, this framework allows each expert to adjust itself to the current conditions by assigning higher weights to the models providing consistently good predictions in the past. The effectiveness of the model has been demonstrated on the CRCNS, UCFSports and CITIUS datasets.",1
"В останні роки було введено багато обчислювальних моделей для прогнозування помітності. Для динамічних сцен існуючі моделі зазвичай поєднують різні карти функцій, отримані з просторових і часових доменів, дотримуючись загальних стратегій інтеграції, таких як усереднення або переможці отримують усе, або використовуючи методи машинного навчання для встановлення важливості кожної функції. Замість того, щоб вдаватися до цих схем інтеграції фіксованих функцій, у цій статті ми пропонуємо нову слабко контрольовану динамічну модель помітності під назвою HedgeSal, яка базується на схемі онлайн-навчання, що базується на теоретичному прийнятті рішень. Наш фреймворк використовує дві попередньо підготовлені глибокі статичні моделі помітності як експертів для отримання індивідуальних карт помітності з потоків зовнішнього вигляду та руху, а потім генерує остаточну карту помітності шляхом зважених рішень усіх цих моделей. Оскільки візуальні характеристики динамічних сцен постійно змінюються, моделям, що забезпечують незмінно хороші прогнози в минулому, автоматично призначаються вищі ваги, що дозволяє кожному експерту пристосуватися до поточних умов. Ми демонструємо ефективність нашої моделі на наборах даних CRCNS, UCFSports і CITIUS.","В останні роки була запропонована нова модель динамічної помітності зі слабким наглядом, яка називається HedgeSal. Він заснований на схемі онлайн-навчання на основі прийняття рішень, яка використовує дві попередньо підготовлені глибокі статичні моделі помітності як експертів для отримання індивідуальних карт помітності з потоків зовнішнього вигляду та руху, а потім генерує остаточну карту помітності шляхом зважених рішень усіх цих моделей. Замість використання загальних стратегій інтеграції, таких як усереднення або переможці отримують все, або використання методів машинного навчання для встановлення важливості кожної функції, ця структура дозволяє кожному експерту адаптуватися до поточних умов, призначаючи вищі ваги моделям, що забезпечують незмінно хороші прогнози в минулому. . Ефективність моделі була продемонстрована на наборах даних CRCNS, UCFSports і CITIUS.",1
"In recent years, deep learning methods have come to the forefront in many areas that require remote sensing, from medicine to agriculture, from defense industry to space research; and these methods have achieved tremendous success as compared to traditional methods. Together with substantial growth in available data with high-quality labels and computational resources, these deep neural network architectures and techniques have seen remarkable developments. The major difference between deep learning and classical recognition methods is that deep learning methods consider an end-to-end learning scheme which gives rise to learning features from raw data. Better regularization techniques and robust optimization algorithms introduced with state-of-the-art deep learning models are other factors leading this difference. In this paper, we discuss the remote sensing problems and how deep learning can be used to solve these problems with a special focus on medical and remote sensing applications. In particular, we briefly review outperforming architectures within the deep learning literature and their use cases. ","In recent years, deep learning methods have been very successful in remote sensing applications such as medicine, agriculture, defense industry and space research, compared to traditional methods. The cause for this is due to the growth in available data with labels, along with computational resources. These deep neural network architectures and techniques have seen remarkable developments, due to the end-to-end learning scheme which allows for learning from raw data. Furthermore, improved regularization techniques and optimization algorithms in the state-of-the-art deep learning models are another contributing factor. In this paper, we discuss how deep learning can be used to solve remote sensing problems, with a focus on medical and remote sensing applications. We also review architectures within the deep learning literature and their respective use cases.",1
"In recent years, deep learning methods have come to the forefront in many areas that require remote sensing, from medicine to agriculture, from defense industry to space research; and these methods have achieved tremendous success as compared to traditional methods. Together with substantial growth in available data with high-quality labels and computational resources, these deep neural network architectures and techniques have seen remarkable developments. The major difference between deep learning and classical recognition methods is that deep learning methods consider an end-to-end learning scheme which gives rise to learning features from raw data. Better regularization techniques and robust optimization algorithms introduced with state-of-the-art deep learning models are other factors leading this difference. In this paper, we discuss the remote sensing problems and how deep learning can be used to solve these problems with a special focus on medical and remote sensing applications. In particular, we briefly review outperforming architectures within the deep learning literature and their use cases. ","В останні роки методи глибокого навчання були дуже успішними в додатках дистанційного зондування, таких як медицина, сільське господарство, оборонна промисловість і космічні дослідження, порівняно з традиційними методами. Причиною цього є зростання доступних даних із мітками разом із обчислювальними ресурсами. Ці глибокі архітектури та методи нейронних мереж зазнали чудових змін завдяки наскрізній схемі навчання, яка дозволяє навчатися на необроблених даних. Крім того, вдосконалені методи регулярізації та алгоритми оптимізації в найсучасніших моделях глибокого навчання є ще одним сприяючим фактором. У цій статті ми обговорюємо, як глибоке навчання можна використовувати для вирішення проблем дистанційного зондування, зосереджуючись на медичних і дистанційних застосуваннях. Ми також розглядаємо архітектури в літературі про глибоке навчання та їх відповідні випадки використання.",1
"В останні роки методи глибокого навчання вийшли на передній план у багатьох сферах, де потрібне дистанційне зондування, від медицини до сільського господарства, від оборонної промисловості до космічних досліджень; і ці методи досягли величезного успіху в порівнянні з традиційними методами. Разом зі значним зростанням доступних даних із високоякісними мітками та обчислювальними ресурсами ці глибокі архітектури й методи нейронних мереж зазнали значних змін. Основна відмінність між методами глибокого навчання та класичними методами розпізнавання полягає в тому, що методи глибокого навчання розглядають наскрізну схему навчання, яка дає початок функціям навчання з необроблених даних. Кращі методи регулярізації та надійні алгоритми оптимізації, запроваджені з найсучаснішими моделями глибокого навчання, є іншими факторами, що призводять до цієї різниці. У цій статті ми обговорюємо проблеми дистанційного зондування та те, як глибоке навчання можна використовувати для вирішення цих проблем, з особливим акцентом на медичні та дистанційні програми. Зокрема, ми коротко оглядаємо найкращі архітектури в літературі про глибоке навчання та приклади їх використання.","In recent years, deep learning methods have been very successful in remote sensing applications such as medicine, agriculture, defense industry and space research, compared to traditional methods. The cause for this is due to the growth in available data with labels, along with computational resources. These deep neural network architectures and techniques have seen remarkable developments, due to the end-to-end learning scheme which allows for learning from raw data. Furthermore, improved regularization techniques and optimization algorithms in the state-of-the-art deep learning models are another contributing factor. In this paper, we discuss how deep learning can be used to solve remote sensing problems, with a focus on medical and remote sensing applications. We also review architectures within the deep learning literature and their respective use cases.",1
"В останні роки методи глибокого навчання вийшли на передній план у багатьох сферах, де потрібне дистанційне зондування, від медицини до сільського господарства, від оборонної промисловості до космічних досліджень; і ці методи досягли величезного успіху в порівнянні з традиційними методами. Разом зі значним зростанням доступних даних із високоякісними мітками та обчислювальними ресурсами ці глибокі архітектури й методи нейронних мереж зазнали значних змін. Основна відмінність між методами глибокого навчання та класичними методами розпізнавання полягає в тому, що методи глибокого навчання розглядають наскрізну схему навчання, яка дає початок функціям навчання з необроблених даних. Кращі методи регулярізації та надійні алгоритми оптимізації, запроваджені з найсучаснішими моделями глибокого навчання, є іншими факторами, що призводять до цієї різниці. У цій статті ми обговорюємо проблеми дистанційного зондування та те, як глибоке навчання можна використовувати для вирішення цих проблем, з особливим акцентом на медичні та дистанційні програми. Зокрема, ми коротко оглядаємо найкращі архітектури в літературі про глибоке навчання та приклади їх використання.","В останні роки методи глибокого навчання були дуже успішними в додатках дистанційного зондування, таких як медицина, сільське господарство, оборонна промисловість і космічні дослідження, порівняно з традиційними методами. Причиною цього є зростання доступних даних із мітками разом із обчислювальними ресурсами. Ці глибокі архітектури та методи нейронних мереж зазнали чудових змін завдяки наскрізній схемі навчання, яка дозволяє навчатися на необроблених даних. Крім того, вдосконалені методи регулярізації та алгоритми оптимізації в найсучасніших моделях глибокого навчання є ще одним сприяючим фактором. У цій статті ми обговорюємо, як глибоке навчання можна використовувати для вирішення проблем дистанційного зондування, зосереджуючись на медичних і дистанційних застосуваннях. Ми також розглядаємо архітектури в літературі про глибоке навчання та їх відповідні випадки використання.",1
"In this study, we explore building a two-stage framework for enabling users to directly manipulate high-level attributes of a natural scene. The key to our approach is a deep generative network that can hallucinate images of a scene as if they were taken in a different season (e.g., during winter), weather condition (e.g., on a cloudy day), or at a different time of the day (e.g., at sunset). Once the scene is hallucinated with the given attributes, the corresponding look is then transferred to the input image while preserving the semantic details intact, giving a photo-realistic manipulation result. As the proposed framework hallucinates what the scene will look like, it does not require any reference style image as commonly utilized in most of the appearance or style transfer approaches. Moreover, it allows to simultaneously manipulate a given scene according to a diverse set of transient attributes within a single model, eliminating the need of training multiple networks per each translation task. Our comprehensive set of qualitative and quantitative results demonstrates the effectiveness of our approach against the competing methods.","This study proposes a two-stage framework to allow users to manipulate high-level attributes of a natural scene. A deep generative network is used to hallucinate images of the scene in different seasons, weather conditions, and times of day. The corresponding look is then transferred to the input image while preserving semantic details, creating a photo-realistic manipulation result. The framework does not require any reference style image, and enables simultaneous manipulation of a scene according to a diverse set of transient attributes within a single model, eliminating the need for multiple networks per task. Comprehensive qualitative and quantitative results demonstrate the effectiveness of the approach.",1
"In this study, we explore building a two-stage framework for enabling users to directly manipulate high-level attributes of a natural scene. The key to our approach is a deep generative network that can hallucinate images of a scene as if they were taken in a different season (e.g., during winter), weather condition (e.g., on a cloudy day), or at a different time of the day (e.g., at sunset). Once the scene is hallucinated with the given attributes, the corresponding look is then transferred to the input image while preserving the semantic details intact, giving a photo-realistic manipulation result. As the proposed framework hallucinates what the scene will look like, it does not require any reference style image as commonly utilized in most of the appearance or style transfer approaches. Moreover, it allows to simultaneously manipulate a given scene according to a diverse set of transient attributes within a single model, eliminating the need of training multiple networks per each translation task. Our comprehensive set of qualitative and quantitative results demonstrates the effectiveness of our approach against the competing methods.","Це дослідження пропонує двоетапну структуру, яка дозволяє користувачам маніпулювати високорівневими атрибутами природної сцени. Глибока генеративна мережа використовується для галюцинації зображень сцени в різні пори року, погодні умови та час доби. Відповідний вигляд потім переноситься на вхідне зображення зі збереженням семантичних деталей, створюючи фотореалістичний результат маніпуляції. Фреймворк не потребує зображення еталонного стилю та дозволяє одночасно маніпулювати сценою відповідно до різноманітного набору тимчасових атрибутів у межах однієї моделі, усуваючи потребу в кількох мережах для виконання завдання. Комплексні якісні та кількісні результати демонструють ефективність підходу.",1
"У цьому дослідженні ми досліджуємо побудову двоетапної структури, яка дозволяє користувачам безпосередньо маніпулювати високорівневими атрибутами природної сцени. Ключем до нашого підходу є глибока генеративна мережа, яка може галюцинувати зображення сцени, ніби вони були зроблені в інший сезон (наприклад, взимку), погодні умови (наприклад, у похмурий день) або в інший час дня (наприклад, на заході сонця). Після галюцинації сцени з заданими атрибутами відповідний вигляд переноситься на вхідне зображення, зберігаючи семантичні деталі недоторканими, що дає фотореалістичний результат маніпуляції. Оскільки запропонований фреймворк показує, як буде виглядати сцена, він не потребує жодного еталонного стилю зображення, яке зазвичай використовується в більшості підходів до зовнішнього вигляду чи передачі стилю. Крім того, це дозволяє одночасно маніпулювати певною сценою відповідно до різноманітного набору тимчасових атрибутів у межах однієї моделі, усуваючи потребу в навчанні кількох мереж для кожного завдання перекладу. Наш комплексний набір якісних і кількісних результатів демонструє ефективність нашого підходу проти конкурентних методів.","This study proposes a two-stage framework to allow users to manipulate high-level attributes of a natural scene. A deep generative network is used to hallucinate images of the scene in different seasons, weather conditions, and times of day. The corresponding look is then transferred to the input image while preserving semantic details, creating a photo-realistic manipulation result. The framework does not require any reference style image, and enables simultaneous manipulation of a scene according to a diverse set of transient attributes within a single model, eliminating the need for multiple networks per task. Comprehensive qualitative and quantitative results demonstrate the effectiveness of the approach.",1
"У цьому дослідженні ми досліджуємо побудову двоетапної структури, яка дозволяє користувачам безпосередньо маніпулювати високорівневими атрибутами природної сцени. Ключем до нашого підходу є глибока генеративна мережа, яка може галюцинувати зображення сцени, ніби вони були зроблені в інший сезон (наприклад, взимку), погодні умови (наприклад, у похмурий день) або в інший час дня (наприклад, на заході сонця). Після галюцинації сцени з заданими атрибутами відповідний вигляд переноситься на вхідне зображення, зберігаючи семантичні деталі недоторканими, що дає фотореалістичний результат маніпуляції. Оскільки запропонований фреймворк показує, як буде виглядати сцена, він не потребує жодного еталонного стилю зображення, яке зазвичай використовується в більшості підходів до зовнішнього вигляду чи передачі стилю. Крім того, це дозволяє одночасно маніпулювати певною сценою відповідно до різноманітного набору тимчасових атрибутів у межах однієї моделі, усуваючи потребу в навчанні кількох мереж для кожного завдання перекладу. Наш комплексний набір якісних і кількісних результатів демонструє ефективність нашого підходу проти конкурентних методів.","Це дослідження пропонує двоетапну структуру, яка дозволяє користувачам маніпулювати високорівневими атрибутами природної сцени. Глибока генеративна мережа використовується для галюцинації зображень сцени в різні пори року, погодні умови та час доби. Відповідний вигляд потім переноситься на вхідне зображення зі збереженням семантичних деталей, створюючи фотореалістичний результат маніпуляції. Фреймворк не потребує зображення еталонного стилю та дозволяє одночасно маніпулювати сценою відповідно до різноманітного набору тимчасових атрибутів у межах однієї моделі, усуваючи потребу в кількох мережах для виконання завдання. Комплексні якісні та кількісні результати демонструють ефективність підходу.",1
"In this paper, we address the problem of learning to summarize personal photo albums. That is, given a photo album, we aim to select a small set of representative images from the album so that the extracted summary captures most of the story that is being told through the images. More specifically, we extend a recently proposed recurrent neural network based framework by employing a more effective way to represent images and, more importantly, adding a diversity term to the main objective. Our diversity term is based on the idea of jointly training a discriminator network to evaluate the diversity of the selected images. This alleviates the issue of selecting near-duplicate or semantically similar images, which is the primary shortcoming of the base approach. The experimental results show that our improved model produces better or comparable summaries, providing a good balance between quality and diversity. ","Addressing the problem of summarizing personal photo albums, this paper proposes an extension of a recurrent neural network-based framework. A more effective way of representing images is employed, and a diversity term is added to the main objective. This term is based on a discriminator network evaluating the diversity of the chosen images. The improved model produces better or equal summaries, offering a good balance between quality and diversity, while avoiding the issue of selecting near-duplicate or semantically similar images.",1
"In this paper, we address the problem of learning to summarize personal photo albums. That is, given a photo album, we aim to select a small set of representative images from the album so that the extracted summary captures most of the story that is being told through the images. More specifically, we extend a recently proposed recurrent neural network based framework by employing a more effective way to represent images and, more importantly, adding a diversity term to the main objective. Our diversity term is based on the idea of jointly training a discriminator network to evaluate the diversity of the selected images. This alleviates the issue of selecting near-duplicate or semantically similar images, which is the primary shortcoming of the base approach. The experimental results show that our improved model produces better or comparable summaries, providing a good balance between quality and diversity. ","Звертаючись до проблеми узагальнення особистих фотоальбомів, ця стаття пропонує розширення рекурентної нейронної мережі. Використовується більш ефективний спосіб представлення зображень, а до основної мети додається термін різноманітності. Цей термін базується на мережі дискримінаторів, що оцінює різноманітність вибраних зображень. Удосконалена модель створює кращі або однакові підсумки, пропонуючи гарний баланс між якістю та різноманітністю, уникаючи проблеми вибору майже дубльованих або семантично схожих зображень.",1
"У цій роботі ми торкнемося проблеми навчання узагальненню особистих фотоальбомів. Тобто, враховуючи фотоальбом, ми прагнемо вибрати невеликий набір репрезентативних зображень з альбому, щоб витягнутий підсумок охоплював більшу частину історії, яка розповідається через зображення. Зокрема, ми розширюємо нещодавно запропоновану структуру на основі рекурентної нейронної мережі, використовуючи більш ефективний спосіб представлення зображень і, що більш важливо, додаючи термін різноманітності до основної мети. Наш термін різноманітності базується на ідеї спільного навчання мережі дискримінаторів для оцінки різноманітності вибраних зображень. Це полегшує проблему вибору майже повторюваних або семантично подібних зображень, що є основним недоліком базового підходу. Експериментальні результати показують, що наша вдосконалена модель створює кращі або порівнювані підсумки, забезпечуючи хороший баланс між якістю та різноманітністю.","Addressing the problem of summarizing personal photo albums, this paper proposes an extension of a recurrent neural network-based framework. A more effective way of representing images is employed, and a diversity term is added to the main objective. This term is based on a discriminator network evaluating the diversity of the chosen images. The improved model produces better or equal summaries, offering a good balance between quality and diversity, while avoiding the issue of selecting near-duplicate or semantically similar images.",1
"У цій роботі ми торкнемося проблеми навчання узагальненню особистих фотоальбомів. Тобто, враховуючи фотоальбом, ми прагнемо вибрати невеликий набір репрезентативних зображень з альбому, щоб витягнутий підсумок охоплював більшу частину історії, яка розповідається через зображення. Зокрема, ми розширюємо нещодавно запропоновану структуру на основі рекурентної нейронної мережі, використовуючи більш ефективний спосіб представлення зображень і, що більш важливо, додаючи термін різноманітності до основної мети. Наш термін різноманітності базується на ідеї спільного навчання мережі дискримінаторів для оцінки різноманітності вибраних зображень. Це полегшує проблему вибору майже повторюваних або семантично схожих зображень, що є основним недоліком базового підходу. Експериментальні результати показують, що наша вдосконалена модель створює кращі або порівнювані підсумки, забезпечуючи хороший баланс між якістю та різноманітністю.","Звертаючись до проблеми узагальнення особистих фотоальбомів, ця стаття пропонує розширення рекурентної нейронної мережі. Використовується більш ефективний спосіб представлення зображень, а до основної мети додається термін різноманітності. Цей термін базується на мережі дискримінаторів, що оцінює різноманітність вибраних зображень. Удосконалена модель створює кращі або однакові підсумки, пропонуючи гарний баланс між якістю та різноманітністю, уникаючи проблеми вибору майже дубльованих або семантично схожих зображень.",1
"This paper addresses the problem of comprehending procedural commonsense knowledge. This is a challenging task as it requires identifying key entities, keeping track of their state changes, and understanding temporal and causal relations. Contrary to most of the previous work, in this study, we do not rely on strong inductive bias and explore the question of how multimodality can be exploited to provide a complementary semantic signal. Towards this end, we introduce a new entity-aware neural comprehension model augmented with external relational memory units. Our model learns to dynamically update entity states in relation to each other while reading the text instructions. Our experimental analysis on the visual reasoning tasks in the recently proposed RecipeQA dataset reveals that our approach improves the accuracy of the previously reported models by a large margin. Moreover, we find that our model learns effective dynamic representations of entities even though we do not use any supervision at the level of entity states.1","Addressing the difficult task of understanding procedural commonsense knowledge, this paper investigates how multimodality can be utilized to supply a semantic signal. We introduce an entity-aware neural comprehension model with external relational memory units to dynamically update entity states as the text instructions are read. Results from the RecipeQA dataset indicate that our model significantly surpasses the accuracy of prior models. Furthermore, our model learns dynamic entity representations without any state supervision.",1
"This paper addresses the problem of comprehending procedural commonsense knowledge. This is a challenging task as it requires identifying key entities, keeping track of their state changes, and understanding temporal and causal relations. Contrary to most of the previous work, in this study, we do not rely on strong inductive bias and explore the question of how multimodality can be exploited to provide a complementary semantic signal. Towards this end, we introduce a new entity-aware neural comprehension model augmented with external relational memory units. Our model learns to dynamically update entity states in relation to each other while reading the text instructions. Our experimental analysis on the visual reasoning tasks in the recently proposed RecipeQA dataset reveals that our approach improves the accuracy of the previously reported models by a large margin. Moreover, we find that our model learns effective dynamic representations of entities even though we do not use any supervision at the level of entity states.1","Звертаючись до складного завдання розуміння процедурних знань здорового глузду, ця стаття досліджує, як мультимодальність може бути використана для надання семантичного сигналу. Ми представляємо модель нейронного розуміння з урахуванням сутностей із зовнішніми реляційними блоками пам’яті для динамічного оновлення станів сутностей під час читання текстових інструкцій. Результати набору даних RecipeQA показують, що наша модель значно перевищує точність попередніх моделей. Крім того, наша модель вивчає динамічні представлення сутностей без будь-якого контролю з боку держави.",1
"У цій статті розглядається проблема розуміння процедурних знань здорового глузду. Це складне завдання, оскільки вимагає визначення ключових об’єктів, відстеження змін їхнього стану та розуміння часових і причинно-наслідкових зв’язків. На відміну від більшості попередніх робіт, у цьому дослідженні ми не покладаємося на сильне індуктивне упередження та досліджуємо питання про те, як мультимодальність можна використовувати для надання додаткового семантичного сигналу. З цією метою ми представляємо нову модель нейронного розуміння з урахуванням сутностей, доповнену блоками зовнішньої реляційної пам’яті. Наша модель вчиться динамічно оновлювати стани сутностей по відношенню один до одного під час читання текстових інструкцій. Наш експериментальний аналіз завдань візуального мислення в нещодавно запропонованому наборі даних RecipeQA показує, що наш підхід значно покращує точність раніше повідомлених моделей. Крім того, ми виявили, що наша модель вивчає ефективні динамічні представлення сутностей, навіть якщо ми не використовуємо жодного контролю на рівні станів сутності.1","Addressing the difficult task of understanding procedural commonsense knowledge, this paper investigates how multimodality can be utilized to supply a semantic signal. We introduce an entity-aware neural comprehension model with external relational memory units to dynamically update entity states as the text instructions are read. Results from the RecipeQA dataset indicate that our model significantly surpasses the accuracy of prior models. Furthermore, our model learns dynamic entity representations without any state supervision.",1
"У цій статті розглядається проблема розуміння процедурних знань здорового глузду. Це складне завдання, оскільки вимагає визначення ключових об’єктів, відстеження змін їхнього стану та розуміння часових і причинно-наслідкових зв’язків. На відміну від більшості попередніх робіт, у цьому дослідженні ми не покладаємося на сильне індуктивне упередження та досліджуємо питання про те, як мультимодальність можна використовувати для надання додаткового семантичного сигналу. З цією метою ми представляємо нову модель нейронного розуміння з урахуванням сутностей, доповнену блоками зовнішньої реляційної пам’яті. Наша модель вчиться динамічно оновлювати стани сутностей по відношенню один до одного під час читання текстових інструкцій. Наш експериментальний аналіз завдань візуального мислення в нещодавно запропонованому наборі даних RecipeQA показує, що наш підхід значно покращує точність раніше повідомлених моделей. Крім того, ми виявили, що наша модель вивчає ефективні динамічні представлення сутностей, навіть якщо ми не використовуємо жодного контролю на рівні станів сутності.1","Звертаючись до складного завдання розуміння процедурних знань здорового глузду, ця стаття досліджує, як мультимодальність може бути використана для надання семантичного сигналу. Ми представляємо модель нейронного розуміння з урахуванням сутностей із зовнішніми реляційними блоками пам’яті для динамічного оновлення станів сутностей під час читання текстових інструкцій. Результати набору даних RecipeQA показують, що наша модель значно перевищує точність попередніх моделей. Крім того, наша модель вивчає динамічні представлення сутностей без будь-якого контролю з боку держави.",1
"Acquiring images of the same anatomy with multiple different contrasts increases the diversity of diagnostic information available in an MR exam. Yet, the scan time limitations may prohibit the acquisition of certain contrasts, and some contrasts may be corrupted by noise and artifacts. In such cases, the ability to synthesize unacquired or corrupted contrasts can improve diagnostic utility. For multi-contrast synthesis, the current methods learn a nonlinear intensity transformation between the source and target images, either via nonlinear regression or deterministic neural networks. These methods can, in turn, suffer from the loss of structural details in synthesized images. Here, in this paper, we propose a new approach for multi-contrast MRI synthesis based on conditional generative adversarial networks. The proposed approach preserves intermediate-to-high frequency details via an adversarial loss, and it offers enhanced synthesis performance via pixel-wise and perceptual losses for registered multi-contrast images and a cycle-consistency loss for unregistered images. Information from neighboring cross-sections are utilized to further improve synthesis quality. Demonstrations on T1 - and T2- weighted images from healthy subjects and patients clearly indicate the superior performance of the proposed approach compared to the previous state-of-the-art methods. Our synthesis approach can help improve the quality and versatility of the multi-contrast MRI exams without the need for prolonged or repeated examinations.","Obtaining images of the same anatomy with numerous different contrasts increases the variety of diagnostic data available in an MR exam. However, scan time restrictions may hinder the acquisition of certain contrasts, and some contrasts may be impaired by noise and artifacts. In such scenarios, the capacity to synthesize unacquired or corrupted contrasts can enhance diagnostic utility. For multi-contrast synthesis, current methods learn a nonlinear intensity transformation between source and target images, either via nonlinear regression or deterministic neural networks. These methods may, however, lead to the loss of structural details in synthesized images. This paper presents a new approach for multi-contrast MRI synthesis based on conditional generative adversarial networks. The proposed approach preserves intermediate-to-high frequency details via an adversarial loss, and it provides improved synthesis performance via pixel-wise and perceptual losses for registered multi-contrast images and a cycle-consistency loss for unregistered images. Utilizing information from neighboring cross-sections can further enhance synthesis quality. Demonstrations on T1 - and T2- weighted images from healthy subjects and patients demonstrate the superior performance of the proposed approach compared to the previous state-of-the-art methods. Our synthesis approach can help improve the quality and versatility of the multi-contrast MRI exams without the need for prolonged or repeated examinations.",1
"Acquiring images of the same anatomy with multiple different contrasts increases the diversity of diagnostic information available in an MR exam. Yet, the scan time limitations may prohibit the acquisition of certain contrasts, and some contrasts may be corrupted by noise and artifacts. In such cases, the ability to synthesize unacquired or corrupted contrasts can improve diagnostic utility. For multi-contrast synthesis, the current methods learn a nonlinear intensity transformation between the source and target images, either via nonlinear regression or deterministic neural networks. These methods can, in turn, suffer from the loss of structural details in synthesized images. Here, in this paper, we propose a new approach for multi-contrast MRI synthesis based on conditional generative adversarial networks. The proposed approach preserves intermediate-to-high frequency details via an adversarial loss, and it offers enhanced synthesis performance via pixel-wise and perceptual losses for registered multi-contrast images and a cycle-consistency loss for unregistered images. Information from neighboring cross-sections are utilized to further improve synthesis quality. Demonstrations on T1 - and T2- weighted images from healthy subjects and patients clearly indicate the superior performance of the proposed approach compared to the previous state-of-the-art methods. Our synthesis approach can help improve the quality and versatility of the multi-contrast MRI exams without the need for prolonged or repeated examinations.","Отримання зображень однієї анатомії з численними різними контрастами збільшує різноманітність діагностичних даних, доступних під час МРТ-дослідження. Однак обмеження часу сканування можуть перешкоджати отриманню певних контрастів, а деякі контрасти можуть погіршуватися через шум і артефакти. У таких сценаріях здатність синтезувати неотримані або пошкоджені контрасти може підвищити діагностичну корисність. Для багатоконтрастного синтезу сучасні методи вивчають нелінійне перетворення інтенсивності між вихідним і цільовим зображеннями за допомогою нелінійної регресії або детермінованих нейронних мереж. Однак ці методи можуть призвести до втрати структурних деталей у синтезованих зображеннях. У цій статті представлено новий підхід для синтезу мультиконтрастної МРТ на основі умовних генеративних змагальних мереж. Запропонований підхід зберігає проміжні та високі частотні деталі через суперечливі втрати, і він забезпечує покращену продуктивність синтезу через піксельні та перцептивні втрати для зареєстрованих багатоконтрастних зображень і втрату узгодженості циклу для незареєстрованих зображень. Використання інформації з сусідніх перерізів може додатково підвищити якість синтезу. Демонстрації на T1- і T2-зважених зображеннях здорових суб&#39;єктів і пацієнтів демонструють чудову продуктивність запропонованого підходу порівняно з попередніми сучасними методами. Наш підхід до синтезу може допомогти покращити якість і універсальність багатоконтрастних МРТ-досліджень без необхідності тривалих або повторних обстежень.",1
"Отримання зображень однієї анатомії з кількома різними контрастами збільшує різноманітність діагностичної інформації, доступної під час МРТ-дослідження. Проте обмеження часу сканування можуть забороняти отримання певних контрастів, а деякі контрасти можуть бути зіпсовані шумом і артефактами. У таких випадках здатність синтезувати неотримані або пошкоджені контрасти може покращити діагностичну корисність. Для багатоконтрастного синтезу поточні методи вивчають нелінійне перетворення інтенсивності між вихідним і цільовим зображеннями за допомогою нелінійної регресії або детермінованих нейронних мереж. Ці методи, у свою чергу, можуть страждати від втрати структурних деталей у синтезованих зображеннях. Тут, у цій статті, ми пропонуємо новий підхід для синтезу мультиконтрастної МРТ на основі умовних генеративних змагальних мереж. Запропонований підхід зберігає проміжні та високі частотні деталі через суперечливі втрати та пропонує покращену продуктивність синтезу через піксельні та перцептивні втрати для зареєстрованих багатоконтрастних зображень і втрату узгодженості циклу для незареєстрованих зображень. Інформація з сусідніх перерізів використовується для подальшого покращення якості синтезу. Демонстрації на T1- і T2-зважених зображеннях здорових суб&#39;єктів і пацієнтів чітко вказують на кращу продуктивність запропонованого підходу порівняно з попередніми сучасними методами. Наш підхід до синтезу може допомогти покращити якість і універсальність багатоконтрастних МРТ-досліджень без необхідності тривалих або повторних обстежень.","Obtaining images of the same anatomy with numerous different contrasts increases the variety of diagnostic data available in an MR exam. However, scan time restrictions may hinder the acquisition of certain contrasts, and some contrasts may be impaired by noise and artifacts. In such scenarios, the capacity to synthesize unacquired or corrupted contrasts can enhance diagnostic utility. For multi-contrast synthesis, current methods learn a nonlinear intensity transformation between source and target images, either via nonlinear regression or deterministic neural networks. These methods may, however, lead to the loss of structural details in synthesized images. This paper presents a new approach for multi-contrast MRI synthesis based on conditional generative adversarial networks. The proposed approach preserves intermediate-to-high frequency details via an adversarial loss, and it provides improved synthesis performance via pixel-wise and perceptual losses for registered multi-contrast images and a cycle-consistency loss for unregistered images. Utilizing information from neighboring cross-sections can further enhance synthesis quality. Demonstrations on T1 - and T2- weighted images from healthy subjects and patients demonstrate the superior performance of the proposed approach compared to the previous state-of-the-art methods. Our synthesis approach can help improve the quality and versatility of the multi-contrast MRI exams without the need for prolonged or repeated examinations.",1
"Отримання зображень однієї анатомії з кількома різними контрастами збільшує різноманітність діагностичної інформації, доступної під час МРТ-дослідження. Проте обмеження часу сканування можуть забороняти отримання певних контрастів, а деякі контрасти можуть бути зіпсовані шумом і артефактами. У таких випадках здатність синтезувати неотримані або пошкоджені контрасти може покращити діагностичну корисність. Для багатоконтрастного синтезу поточні методи вивчають нелінійне перетворення інтенсивності між вихідним і цільовим зображеннями за допомогою нелінійної регресії або детермінованих нейронних мереж. Ці методи, у свою чергу, можуть страждати від втрати структурних деталей у синтезованих зображеннях. Тут, у цій статті, ми пропонуємо новий підхід для синтезу мультиконтрастної МРТ на основі умовних генеративних змагальних мереж. Запропонований підхід зберігає проміжні та високі частотні деталі через суперечливі втрати та пропонує покращену продуктивність синтезу через піксельні та перцептивні втрати для зареєстрованих багатоконтрастних зображень і втрату узгодженості циклу для незареєстрованих зображень. Інформація з сусідніх перерізів використовується для подальшого покращення якості синтезу. Демонстрації на T1- і T2-зважених зображеннях здорових суб&#39;єктів і пацієнтів чітко вказують на кращу продуктивність запропонованого підходу порівняно з попередніми сучасними методами. Наш підхід до синтезу може допомогти покращити якість і універсальність багатоконтрастних МРТ-досліджень без необхідності тривалих або повторних обстежень.","Отримання зображень однієї анатомії з численними різними контрастами збільшує різноманітність діагностичних даних, доступних під час МРТ-дослідження. Однак обмеження часу сканування можуть перешкоджати отриманню певних контрастів, а деякі контрасти можуть погіршуватися через шум і артефакти. У таких сценаріях здатність синтезувати неотримані або пошкоджені контрасти може підвищити діагностичну корисність. Для багатоконтрастного синтезу сучасні методи вивчають нелінійне перетворення інтенсивності між вихідним і цільовим зображеннями за допомогою нелінійної регресії або детермінованих нейронних мереж. Однак ці методи можуть призвести до втрати структурних деталей у синтезованих зображеннях. У цій статті представлено новий підхід для синтезу мультиконтрастної МРТ на основі умовних генеративних змагальних мереж. Запропонований підхід зберігає проміжні та високі частотні деталі через суперечливі втрати, і він забезпечує покращену продуктивність синтезу через піксельні та перцептивні втрати для зареєстрованих багатоконтрастних зображень і втрату узгодженості циклу для незареєстрованих зображень. Використання інформації з сусідніх перерізів може додатково підвищити якість синтезу. Демонстрації на T1- і T2-зважених зображеннях здорових суб&#39;єктів і пацієнтів демонструють чудову продуктивність запропонованого підходу порівняно з попередніми сучасними методами. Наш підхід до синтезу може допомогти покращити якість і універсальність багатоконтрастних МРТ-досліджень без необхідності тривалих або повторних обстежень.",1
"Understanding and reasoning about cooking recipes is a fruitful research direction towards enabling machines to interpret procedural text. In this work, we introduce RecipeQA, a dataset for multimodal comprehension of cooking recipes. It comprises of approximately 20K instructional recipes with multiple modalities such as titles, descriptions and aligned set of images. With over 36K automatically generated question-answer pairs, we design a set of comprehension and reasoning tasks that require joint understanding of images and text, capturing the temporal flow of events and making sense of procedural knowledge. Our preliminary results indicate that RecipeQA will serve as a challenging test bed and an ideal benchmark for evaluating machine comprehension systems.","We present RecipeQA, a dataset for machines to comprehend cooking recipes, comprising of around 20K instructional recipes with titles, descriptions, images and 36K generated Q&A pairs. Tasks set up require joint comprehension of images and text, accounting for temporal flow and knowledge of procedures. Early results suggest RecipeQA will be a demanding test bed and benchmark to assess machine comprehension systems.",1
"Understanding and reasoning about cooking recipes is a fruitful research direction towards enabling machines to interpret procedural text. In this work, we introduce RecipeQA, a dataset for multimodal comprehension of cooking recipes. It comprises of approximately 20K instructional recipes with multiple modalities such as titles, descriptions and aligned set of images. With over 36K automatically generated question-answer pairs, we design a set of comprehension and reasoning tasks that require joint understanding of images and text, capturing the temporal flow of events and making sense of procedural knowledge. Our preliminary results indicate that RecipeQA will serve as a challenging test bed and an ideal benchmark for evaluating machine comprehension systems.","Ми представляємо RecipeQA, набір даних для машин для розуміння кулінарних рецептів, що складається з близько 20 тисяч навчальних рецептів із назвами, описами, зображеннями та 36 тисяч згенерованих пар запитань і відповідей. Поставлені завдання вимагають спільного розуміння зображень і тексту, обліку тимчасового потоку та знання процедур. Перші результати свідчать про те, що RecipeQA буде вимогливим випробувальним майданчиком і еталоном для оцінки систем машинного розуміння.",1
"Розуміння та обґрунтування кулінарних рецептів є плідним напрямком дослідження, спрямованим на те, щоб дозволити машинам інтерпретувати процедурний текст. У цій роботі ми представляємо RecipeQA, набір даних для мультимодального розуміння кулінарних рецептів. Він складається з приблизно 20 тисяч навчальних рецептів із різними модальностями, такими як заголовки, описи та вирівняний набір зображень. Маючи понад 36 тисяч автоматично згенерованих пар запитань-відповідей, ми розробляємо набір завдань на розуміння й міркування, які вимагають спільного розуміння зображень і тексту, фіксації тимчасового потоку подій і осмислення процедурних знань. Наші попередні результати вказують на те, що RecipeQA слугуватиме складним випробувальним майданчиком і ідеальним еталоном для оцінки систем машинного розуміння.","We present RecipeQA, a dataset for machines to comprehend cooking recipes, comprising of around 20K instructional recipes with titles, descriptions, images and 36K generated Q&A pairs. Tasks set up require joint comprehension of images and text, accounting for temporal flow and knowledge of procedures. Early results suggest RecipeQA will be a demanding test bed and benchmark to assess machine comprehension systems.",1
"Розуміння та обґрунтування кулінарних рецептів є плідним напрямком дослідження, спрямованим на те, щоб дозволити машинам інтерпретувати процедурний текст. У цій роботі ми представляємо RecipeQA, набір даних для мультимодального розуміння кулінарних рецептів. Він складається з приблизно 20 тисяч навчальних рецептів із різними модальностями, такими як заголовки, описи та вирівняний набір зображень. Маючи понад 36 тисяч автоматично згенерованих пар запитань-відповідей, ми розробляємо набір завдань на розуміння й міркування, які вимагають спільного розуміння зображень і тексту, фіксації тимчасового потоку подій і осмислення процедурних знань. Наші попередні результати вказують на те, що RecipeQA слугуватиме складним випробувальним майданчиком і ідеальним еталоном для оцінки систем машинного розуміння.","Ми представляємо RecipeQA, набір даних для машин для розуміння кулінарних рецептів, що складається з близько 20 тисяч навчальних рецептів із назвами, описами, зображеннями та 36 тисяч згенерованих пар запитань і відповідей. Поставлені завдання вимагають спільного розуміння зображень і тексту, обліку тимчасового потоку та знання процедур. Перші результати свідчать про те, що RecipeQA буде вимогливим випробувальним майданчиком і еталоном для оцінки систем машинного розуміння.",1
"Developing techniques for editing an outfit image through natural sentences and accordingly generating new outfits has promising applications for art, fashion and design. However, it is considered as a certainly challenging task since image manipulation should be carried out only on the relevant parts of the image while keeping the remaining sections untouched. Moreover, this manipulation process should generate an image that is as realistic as possible. In this work, we propose FiLMedGAN, which leverages feature-wise linear modulation (FiLM) to relate and transform visual features with natural language representations without using extra spatial information. Our experiments demonstrate that this approach, when combined with skip connections and total variation regularization, produces more plausible results than the baseline work, and has a better localization capability when generating new outfits consistent with the target description.","Generating new outfits from an image via natural language commands is a difficult task, as it necessitates modification of only certain parts of the image while leaving the rest intact, and creating an image that looks realistic. We introduce FiLMedGAN, which couples feature-wise linear modulation (FiLM) to link visual features with language descriptions, and does not require additional spatial information. Our experiments prove that, when combined with skip connections and total variation regularization, FiLMedGAN produces more convincing results than the baseline work, and has a superior ability to localize new outfits in accordance with the specified description.",1
"Developing techniques for editing an outfit image through natural sentences and accordingly generating new outfits has promising applications for art, fashion and design. However, it is considered as a certainly challenging task since image manipulation should be carried out only on the relevant parts of the image while keeping the remaining sections untouched. Moreover, this manipulation process should generate an image that is as realistic as possible. In this work, we propose FiLMedGAN, which leverages feature-wise linear modulation (FiLM) to relate and transform visual features with natural language representations without using extra spatial information. Our experiments demonstrate that this approach, when combined with skip connections and total variation regularization, produces more plausible results than the baseline work, and has a better localization capability when generating new outfits consistent with the target description.","Створення нових нарядів із зображення за допомогою команд природної мови є складним завданням, оскільки вимагає модифікації лише певних частин зображення, залишаючи решту недоторканою та створюючи зображення, яке виглядає реалістичним. Ми представляємо FiLMedGAN, який поєднує функціональну лінійну модуляцію (FiLM), щоб зв’язати візуальні функції з мовними описами, і не потребує додаткової просторової інформації. Наші експерименти доводять, що в поєднанні зі з’єднаннями пропуску та повною регуляризацією варіацій FiLMedGAN дає більш переконливі результати, ніж базова робота, і має чудову здатність локалізувати нові комплекти відповідно до зазначеного опису.",1
"Розробка методів редагування зображення вбрання за допомогою природних речень і відповідно генерування нових вбрань має перспективні застосування для мистецтва, моди та дизайну. Однак це вважається, безумовно, складним завданням, оскільки маніпуляції із зображенням слід виконувати лише на відповідних частинах зображення, залишаючи решту ділянок недоторканими. Більше того, цей процес маніпуляції має створити зображення, яке є максимально реалістичним. У цій роботі ми пропонуємо FiLMedGAN, який використовує функціональну лінійну модуляцію (FiLM), щоб зв’язати та трансформувати візуальні функції з представленнями природної мови без використання додаткової просторової інформації. Наші експерименти демонструють, що цей підхід у поєднанні з пропускаючими з’єднаннями та регуляризацією повної варіації дає більш вірогідні результати, ніж базова робота, і має кращу здатність до локалізації під час створення нових комплектів, що відповідають цільовому опису.","Generating new outfits from an image via natural language commands is a difficult task, as it necessitates modification of only certain parts of the image while leaving the rest intact, and creating an image that looks realistic. We introduce FiLMedGAN, which couples feature-wise linear modulation (FiLM) to link visual features with language descriptions, and does not require additional spatial information. Our experiments prove that, when combined with skip connections and total variation regularization, FiLMedGAN produces more convincing results than the baseline work, and has a superior ability to localize new outfits in accordance with the specified description.",1
"Розробка методів редагування зображення вбрання за допомогою природних речень і відповідно генерування нових вбрань має перспективні застосування для мистецтва, моди та дизайну. Однак це вважається, безумовно, складним завданням, оскільки маніпуляції із зображенням слід виконувати лише на відповідних частинах зображення, залишаючи решту ділянок недоторканими. Більше того, цей процес маніпуляції має створити зображення, яке є максимально реалістичним. У цій роботі ми пропонуємо FiLMedGAN, який використовує функціональну лінійну модуляцію (FiLM), щоб зв’язати та трансформувати візуальні функції з представленнями природної мови без використання додаткової просторової інформації. Наші експерименти демонструють, що цей підхід у поєднанні з пропускаючими з’єднаннями та регуляризацією повної варіації дає більш вірогідні результати, ніж базова робота, і має кращу здатність до локалізації під час створення нових комплектів, що відповідають цільовому опису.","Створення нових нарядів із зображення за допомогою команд природної мови є складним завданням, оскільки вимагає модифікації лише певних частин зображення, залишаючи решту недоторканою та створюючи зображення, яке виглядає реалістичним. Ми представляємо FiLMedGAN, який поєднує функціональну лінійну модуляцію (FiLM), щоб зв’язати візуальні функції з мовними описами, і не потребує додаткової просторової інформації. Наші експерименти доводять, що в поєднанні зі з’єднаннями пропуску та повною регуляризацією варіацій FiLMedGAN дає більш переконливі результати, ніж базова робота, і має чудову здатність локалізувати нові комплекти відповідно до зазначеного опису.",1
"Moving object detection is an imperative task in computer vision, where it is primarily used for surveillance applications. With the increasing availability of low-altitude aerial vehicles, new challenges for moving object detection have surfaced, both for academia and industry. In this paper, we propose a new approach that can detect moving objects efficiently and handle parallax cases. By introducing sparse flow based parallax handling and downscale processing, we push the boundaries of real-time performance with 16 FPS on limited embedded resources (a five-fold improvement over existing baselines), while managing to perform comparably or even improve the state-of-the-art in two different datasets. We also present a roadmap for extending our approach to exploit multi-modal data in order to mitigate the need for parameter tuning.","Moving object detection is a significant task in computer vision, particularly for surveillance purposes. The rise of low-altitude aerial vehicles has presented new difficulties for moving object detection for both industry and academia. This paper suggests a novel technique to detect moving objects competently and handle parallax cases. Sparse flow based parallax handling and downscale processing are used to reach real-time performance at 16 FPS on limited embedded resources (five times better than existing baselines), while still performing similarly or even better than the current state-of-the-art on two different datasets. Additionally, a plan is proposed to expand the approach to make use of multi-modal data in order to reduce the requirement for parameter tuning.",1
"Moving object detection is an imperative task in computer vision, where it is primarily used for surveillance applications. With the increasing availability of low-altitude aerial vehicles, new challenges for moving object detection have surfaced, both for academia and industry. In this paper, we propose a new approach that can detect moving objects efficiently and handle parallax cases. By introducing sparse flow based parallax handling and downscale processing, we push the boundaries of real-time performance with 16 FPS on limited embedded resources (a five-fold improvement over existing baselines), while managing to perform comparably or even improve the state-of-the-art in two different datasets. We also present a roadmap for extending our approach to exploit multi-modal data in order to mitigate the need for parameter tuning.","Виявлення рухомих об&#39;єктів є важливим завданням комп&#39;ютерного зору, особливо для цілей спостереження. Розвиток низьковисотних літальних апаратів створив нові труднощі для виявлення рухомих об’єктів як для промисловості, так і для наукових кіл. У цій статті запропоновано нову техніку для грамотного виявлення рухомих об’єктів і обробки випадків паралакса. Обробка паралакса на основі розрідженого потоку та обробка зменшення масштабу використовуються для досягнення продуктивності в реальному часі зі швидкістю 16 кадрів/с на обмежених вбудованих ресурсах (у п’ять разів краще, ніж існуючі базові показники), але при цьому працюють так само або навіть краще, ніж поточний стан техніки на два різні набори даних. Крім того, пропонується план розширення підходу для використання мультимодальних даних, щоб зменшити вимоги до налаштування параметрів.",1
"Виявлення рухомих об&#39;єктів є обов&#39;язковим завданням комп&#39;ютерного зору, де воно в основному використовується для програм спостереження. Зі збільшенням доступності літальних апаратів на низькій висоті виникли нові виклики для виявлення рухомих об’єктів як для наукових кіл, так і для промисловості. У цій статті ми пропонуємо новий підхід, який може ефективно виявляти рухомі об’єкти та обробляти випадки паралакса. Запроваджуючи обробку паралакса на основі розрідженого потоку та обробку з низьким масштабом, ми розширюємо межі продуктивності в реальному часі з 16 кадрами в секунду на обмежених вбудованих ресурсах (п’ятикратне покращення порівняно з існуючими базовими показниками), водночас зуміючи досягти порівнянної продуктивності або навіть покращити стан -the-art у двох різних наборах даних. Ми також представляємо дорожню карту для розширення нашого підходу до використання мультимодальних даних, щоб зменшити потребу в налаштуванні параметрів.","Moving object detection is a significant task in computer vision, particularly for surveillance purposes. The rise of low-altitude aerial vehicles has presented new difficulties for moving object detection for both industry and academia. This paper suggests a novel technique to detect moving objects competently and handle parallax cases. Sparse flow based parallax handling and downscale processing are used to reach real-time performance at 16 FPS on limited embedded resources (five times better than existing baselines), while still performing similarly or even better than the current state-of-the-art on two different datasets. Additionally, a plan is proposed to expand the approach to make use of multi-modal data in order to reduce the requirement for parameter tuning.",1
"Виявлення рухомих об’єктів є обов’язковим завданням комп’ютерного зору, де воно в основному використовується для програм спостереження. Зі збільшенням доступності літальних апаратів на низькій висоті виникли нові виклики для виявлення рухомих об’єктів як для наукових кіл, так і для промисловості. У цій статті ми пропонуємо новий підхід, який може ефективно виявляти рухомі об’єкти та обробляти випадки паралакса. Запроваджуючи обробку паралакса на основі розрідженого потоку та обробку з низьким масштабом, ми розширюємо межі продуктивності в реальному часі з 16 кадрами в секунду на обмежених вбудованих ресурсах (п’ятикратне покращення порівняно з існуючими базовими показниками), водночас зуміючи досягти порівнянної продуктивності або навіть покращити стан -the-art у двох різних наборах даних. Ми також представляємо дорожню карту для розширення нашого підходу до використання мультимодальних даних, щоб зменшити потребу в налаштуванні параметрів.","Виявлення рухомих об&#39;єктів є важливим завданням комп&#39;ютерного зору, особливо для цілей спостереження. Розвиток низьковисотних літальних апаратів створив нові труднощі для виявлення рухомих об’єктів як для промисловості, так і для наукових кіл. У цій статті запропоновано нову техніку для грамотного виявлення рухомих об’єктів і обробки випадків паралакса. Обробка паралакса на основі розрідженого потоку та обробка зменшення масштабу використовуються для досягнення продуктивності в реальному часі зі швидкістю 16 кадрів на секунду на обмежених вбудованих ресурсах (у п’ять разів краще, ніж існуючі базові лінії), при цьому продуктивність така ж або навіть краща, ніж поточний стан техніки на два різні набори даних. Крім того, пропонується план розширення підходу для використання мультимодальних даних, щоб зменшити вимоги до налаштування параметрів.",1
"Computational saliency models for still images have gained significant popularity in recent years. Saliency prediction from videos, on the other hand, has received relatively little interest from the community. Motivated by this, in this paper, we study the use of deep learning for dynamic saliency prediction and propose the so-called spatio-temporal saliency networks. The key to our models is the architecture of two-stream networks where we investigate different fusion mechanisms to integrate spatial and temporal information. We evaluate our models on the dynamic images and eye movements and University of Central Florida-Sports datasets and present highly competitive results against the existing state-of-the-art models. We also carry out some experiments on a number of still images from the MIT300 dataset by exploiting the optical flow maps predicted from these images. Our results show that considering inherent motion information in this way can be helpful for static saliency estimation.","The utilization of deep learning for dynamic saliency prediction has recently been neglected by the community, despite computational saliency models for still images being increasingly popular. We therefore introduce the spatio-temporal saliency networks, which utilize a two-stream network architecture and investigate different fusion mechanisms to combine spatial and temporal information. These models are evaluated on dynamic images and eye movements, as well as the University of Central Florida-Sports dataset, yielding highly competitive results compared to the current state-of-the-art models. Experiments on the MIT300 dataset with optical flow maps predicted from these images demonstrate that considering motion information can improve static saliency estimation.",1
"Computational saliency models for still images have gained significant popularity in recent years. Saliency prediction from videos, on the other hand, has received relatively little interest from the community. Motivated by this, in this paper, we study the use of deep learning for dynamic saliency prediction and propose the so-called spatio-temporal saliency networks. The key to our models is the architecture of two-stream networks where we investigate different fusion mechanisms to integrate spatial and temporal information. We evaluate our models on the dynamic images and eye movements and University of Central Florida-Sports datasets and present highly competitive results against the existing state-of-the-art models. We also carry out some experiments on a number of still images from the MIT300 dataset by exploiting the optical flow maps predicted from these images. Our results show that considering inherent motion information in this way can be helpful for static saliency estimation.","Спільнота нещодавно нехтувала використанням глибокого навчання для динамічного прогнозування помітності, незважаючи на те, що обчислювальні моделі помітності для нерухомих зображень стають все більш популярними. Тому ми представляємо мережі просторово-часової помітності, які використовують двопотокову мережеву архітектуру та досліджуємо різні механізми злиття для поєднання просторової та часової інформації. Ці моделі оцінюються на основі динамічних зображень і рухів очей, а також на основі спортивних даних Університету Центральної Флориди, що дає дуже конкурентоспроможні результати порівняно з поточними найсучаснішими моделями. Експерименти на наборі даних MIT300 із оптичними картами потоку, передбаченими на цих зображеннях, демонструють, що врахування інформації про рух може покращити оцінку статичної помітності.",1
"Обчислювальні моделі помітності для нерухомих зображень набули значної популярності в останні роки. З іншого боку, передбачення помітності відеозаписів викликало відносно невеликий інтерес у спільноти. Мотивуючись цим, у цій статті ми вивчаємо використання глибокого навчання для динамічного прогнозування помітності та пропонуємо так звані просторово-часові мережі помітності. Ключем до наших моделей є архітектура двопотокових мереж, де ми досліджуємо різні механізми злиття для інтеграції просторової та часової інформації. Ми оцінюємо наші моделі на основі динамічних зображень і рухів очей, а також наборів даних Університету Центральної Флориди-Спорт і представляємо конкурентоспроможні результати порівняно з існуючими найсучаснішими моделями. Ми також проводимо деякі експерименти на кількох нерухомих зображеннях із набору даних MIT300, використовуючи карти оптичного потоку, передбачені на цих зображеннях. Наші результати показують, що врахування інформації про внутрішній рух таким чином може бути корисним для оцінки статичної помітності.","The utilization of deep learning for dynamic saliency prediction has recently been neglected by the community, despite computational saliency models for still images being increasingly popular. We therefore introduce the spatio-temporal saliency networks, which utilize a two-stream network architecture and investigate different fusion mechanisms to combine spatial and temporal information. These models are evaluated on dynamic images and eye movements, as well as the University of Central Florida-Sports dataset, yielding highly competitive results compared to the current state-of-the-art models. Experiments on the MIT300 dataset with optical flow maps predicted from these images demonstrate that considering motion information can improve static saliency estimation.",1
"Обчислювальні моделі помітності для нерухомих зображень набули значної популярності в останні роки. З іншого боку, передбачення помітності відеозаписів викликало відносно невеликий інтерес у спільноти. Мотивуючись цим, у цій статті ми вивчаємо використання глибокого навчання для динамічного прогнозування помітності та пропонуємо так звані просторово-часові мережі помітності. Ключем до наших моделей є архітектура двопотокових мереж, де ми досліджуємо різні механізми злиття для інтеграції просторової та часової інформації. Ми оцінюємо наші моделі на основі динамічних зображень і рухів очей, а також наборів даних Університету Центральної Флориди-Спорт і представляємо конкурентоспроможні результати порівняно з існуючими найсучаснішими моделями. Ми також проводимо деякі експерименти на кількох нерухомих зображеннях із набору даних MIT300, використовуючи карти оптичного потоку, передбачені на цих зображеннях. Наші результати показують, що врахування інформації про внутрішній рух таким чином може бути корисним для оцінки статичної помітності.","Спільнота нещодавно нехтувала використанням глибокого навчання для динамічного прогнозування помітності, незважаючи на те, що обчислювальні моделі помітності для нерухомих зображень стають все більш популярними. Тому ми представляємо мережі просторово-часової помітності, які використовують двопотокову мережеву архітектуру та досліджуємо різні механізми злиття для поєднання просторової та часової інформації. Ці моделі оцінюються на основі динамічних зображень і рухів очей, а також на основі спортивних даних Університету Центральної Флориди, що дає дуже конкурентоспроможні результати порівняно з поточними найсучаснішими моделями. Експерименти на наборі даних MIT300 із оптичними картами потоку, передбаченими на цих зображеннях, демонструють, що врахування інформації про рух може покращити оцінку статичної помітності.",1
" In the past few years, automatically generating descriptions for images has attracted a lot of attention in computer vision and natural language processing research. Among the existing approaches, data-driven methods have been proven to be highly effective. These methods compare the given image against a large set of training images to determine a set of relevant images, then generate a description using the associated captions. In this study, the authors propose to integrate an objectbased semantic image representation into a deep features-based retrieval framework to select the relevant images. Moreover, they present a novel phrase selection paradigm and a sentence generation model which depends on a joint analysis of salient regions in the input and retrieved images within a clustering framework. The authors demonstrate the effectiveness of their proposed approach on Flickr8K and Flickr30K benchmark datasets and show that their model gives highly competitive results compared with the state-of-the-art models","Recently, the task of automatically generating descriptions for images has gained much interest in computer vision and natural language processing. Data-driven methods, which compare the given image to a set of training images to identify relevant images and generate descriptions based on the associated captions, have been shown to be successful. This study introduces an object-based semantic image representation into a deep features-based retrieval framework to select the relevant images. Additionally, a novel phrase selection paradigm and sentence generation model is proposed which performs a joint analysis of salient regions in the input and retrieved images within a clustering framework. The effectiveness of this approach was tested on the Flickr8K and Flickr30K benchmark datasets, and was found to give highly competitive results compared to the state-of-the-art models.",1
" In the past few years, automatically generating descriptions for images has attracted a lot of attention in computer vision and natural language processing research. Among the existing approaches, data-driven methods have been proven to be highly effective. These methods compare the given image against a large set of training images to determine a set of relevant images, then generate a description using the associated captions. In this study, the authors propose to integrate an objectbased semantic image representation into a deep features-based retrieval framework to select the relevant images. Moreover, they present a novel phrase selection paradigm and a sentence generation model which depends on a joint analysis of salient regions in the input and retrieved images within a clustering framework. The authors demonstrate the effectiveness of their proposed approach on Flickr8K and Flickr30K benchmark datasets and show that their model gives highly competitive results compared with the state-of-the-art models","Останнім часом завдання автоматичного створення описів для зображень привернуло великий інтерес до комп’ютерного зору та обробки природної мови. Методи, керовані даними, які порівнюють дане зображення з набором навчальних зображень, щоб ідентифікувати релевантні зображення та генерувати описи на основі пов’язаних підписів, виявилися успішними. Це дослідження вводить об’єктно-орієнтоване семантичне представлення зображень у глибоку структуру пошуку на основі функцій для вибору відповідних зображень. Крім того, запропоновано нову парадигму вибору фраз і модель генерації речень, яка виконує спільний аналіз помітних областей у вхідних і отриманих зображеннях у рамках кластеризації. Ефективність цього підходу було перевірено на контрольних наборах даних Flickr8K і Flickr30K, і було встановлено, що він дає висококонкурентні результати порівняно з найсучаснішими моделями.",1
"За останні кілька років автоматичне генерування описів для зображень привернуло багато уваги в дослідженнях комп’ютерного зору та обробки природної мови. Серед існуючих підходів методи, керовані даними, виявилися високоефективними. Ці методи порівнюють задане зображення з великим набором навчальних зображень, щоб визначити набір релевантних зображень, а потім генерують опис, використовуючи відповідні підписи. У цьому дослідженні автори пропонують інтегрувати об’єктно-орієнтоване представлення семантичного зображення в глибоку структуру пошуку на основі ознак, щоб вибрати відповідні зображення. Крім того, вони представляють нову парадигму вибору фраз і модель генерації речень, яка залежить від спільного аналізу помітних областей у вхідних і отриманих зображеннях у рамках кластеризації. Автори демонструють ефективність запропонованого ними підходу на базах даних Flickr8K і Flickr30K і показують, що їх модель дає висококонкурентні результати порівняно з найсучаснішими моделями","Recently, the task of automatically generating descriptions for images has gained much interest in computer vision and natural language processing. Data-driven methods, which compare the given image to a set of training images to identify relevant images and generate descriptions based on the associated captions, have been shown to be successful. This study introduces an object-based semantic image representation into a deep features-based retrieval framework to select the relevant images. Additionally, a novel phrase selection paradigm and sentence generation model is proposed which performs a joint analysis of salient regions in the input and retrieved images within a clustering framework. The effectiveness of this approach was tested on the Flickr8K and Flickr30K benchmark datasets, and was found to give highly competitive results compared to the state-of-the-art models.",1
"За останні кілька років автоматичне генерування описів для зображень привернуло багато уваги в дослідженнях комп’ютерного зору та обробки природної мови. Серед існуючих підходів методи, керовані даними, виявилися високоефективними. Ці методи порівнюють задане зображення з великим набором навчальних зображень, щоб визначити набір релевантних зображень, а потім генерують опис, використовуючи відповідні підписи. У цьому дослідженні автори пропонують інтегрувати об’єктно-орієнтоване представлення семантичного зображення в глибоку структуру пошуку на основі ознак, щоб вибрати відповідні зображення. Крім того, вони представляють нову парадигму вибору фраз і модель генерації речень, яка залежить від спільного аналізу помітних областей у вхідних і отриманих зображеннях у рамках кластеризації. Автори демонструють ефективність запропонованого ними підходу на базах даних Flickr8K і Flickr30K і показують, що їх модель дає висококонкурентні результати порівняно з найсучаснішими моделями","Останнім часом завдання автоматичного створення описів для зображень привернуло великий інтерес до комп’ютерного зору та обробки природної мови. Методи, керовані даними, які порівнюють дане зображення з набором навчальних зображень, щоб ідентифікувати релевантні зображення та генерувати описи на основі пов’язаних підписів, виявилися успішними. Це дослідження вводить об’єктно-орієнтоване семантичне представлення зображень у глибоку структуру пошуку на основі функцій для вибору відповідних зображень. Крім того, запропоновано нову парадигму вибору фраз і модель генерації речень, яка виконує спільний аналіз помітних областей у вхідних і отриманих зображеннях у рамках кластеризації. Ефективність цього підходу було перевірено на контрольних наборах даних Flickr8K і Flickr30K, і було встановлено, що він дає висококонкурентні результати порівняно з найсучаснішими моделями.",1
"In this paper, we present a new sampling-based alpha matting approach for the accurate estimation of foreground and background layers of an image. Previous samplingbased methods typically rely on certain heuristics in collecting representative samples from known regions, and thus their performance deteriorates if the underlying assumptions are not satisfied. To alleviate this, we take an entirely new approach and formulate sampling as a sparse subset selection problem where we propose to pick a small set of candidate samples that best explains the unknown pixels. Moreover, we describe a new dissimilarity measure for comparing two samples which is based on KL-divergence between the distributions of features extracted in the vicinity of the samples. The proposed framework is general and could be easily extended to video matting by additionally taking temporal information into account in the sampling process. Evaluation on standard benchmark data sets for image and video matting demonstrates that our approach provides more accurate results compared with the state-of-the-art methods.","We propose a sampling-based alpha matting approach to accurately estimate foreground and background layers of an image. Existing sampling-based methods depend on heuristics to gather samples from known regions, and thus, are not reliable if the assumptions are not met. To resolve this, we address sampling as a sparse subset selection issue and suggest a small set of candidate samples to explain unknown pixels. Furthermore, we introduce a dissimilarity measure based on KL-divergence between distributions of features around the samples. Our framework is versatile and can be extended to video matting by taking temporal information into account in the sampling process. Evaluation on standard benchmarks for image and video matting shows that our approach provides more precise results compared to existing methods.",1
"In this paper, we present a new sampling-based alpha matting approach for the accurate estimation of foreground and background layers of an image. Previous samplingbased methods typically rely on certain heuristics in collecting representative samples from known regions, and thus their performance deteriorates if the underlying assumptions are not satisfied. To alleviate this, we take an entirely new approach and formulate sampling as a sparse subset selection problem where we propose to pick a small set of candidate samples that best explains the unknown pixels. Moreover, we describe a new dissimilarity measure for comparing two samples which is based on KL-divergence between the distributions of features extracted in the vicinity of the samples. The proposed framework is general and could be easily extended to video matting by additionally taking temporal information into account in the sampling process. Evaluation on standard benchmark data sets for image and video matting demonstrates that our approach provides more accurate results compared with the state-of-the-art methods.","Ми пропонуємо підхід альфа-матування на основі вибірки для точної оцінки переднього та фонового шарів зображення. Існуючі методи на основі вибірки залежать від евристичних методів збору зразків із відомих регіонів і, таким чином, не є надійними, якщо припущення не виконуються. Щоб вирішити цю проблему, ми розглядаємо вибірку як проблему вибору розрідженої підмножини та пропонуємо невеликий набір зразків-кандидатів для пояснення невідомих пікселів. Крім того, ми вводимо міру відмінності, засновану на KL-розбіжності між розподілами ознак навколо зразків. Наша структура є універсальною і може бути розширена до відеоматінгу, враховуючи часову інформацію в процесі вибірки. Оцінка стандартних тестів для матування зображень і відео показує, що наш підхід забезпечує більш точні результати порівняно з існуючими методами.",1
"У цій статті ми представляємо новий підхід альфа-матування на основі вибірки для точної оцінки переднього та фонового шарів зображення. Попередні методи, засновані на вибірці, зазвичай покладаються на певні евристики при зборі репрезентативних вибірок із відомих регіонів, і тому їх продуктивність погіршується, якщо базові припущення не задовольняються. Щоб полегшити це, ми застосували абсолютно новий підхід і сформулювали вибірку як проблему відбору розрідженої підмножини, де ми пропонуємо вибрати невеликий набір вибірок-кандидатів, які найкраще пояснюють невідомі пікселі. Крім того, ми описуємо нову міру відмінності для порівняння двох зразків, яка базується на KL-розбіжності між розподілами ознак, виділених поблизу зразків. Запропонована структура є загальною і може бути легко розширена до відеоматінгу шляхом додаткового врахування тимчасової інформації в процесі вибірки. Оцінка стандартних контрольних наборів даних для матування зображень і відео демонструє, що наш підхід забезпечує більш точні результати порівняно з найсучаснішими методами.","We propose a sampling-based alpha matting approach to accurately estimate foreground and background layers of an image. Existing sampling-based methods depend on heuristics to gather samples from known regions, and thus, are not reliable if the assumptions are not met. To resolve this, we address sampling as a sparse subset selection issue and suggest a small set of candidate samples to explain unknown pixels. Furthermore, we introduce a dissimilarity measure based on KL-divergence between distributions of features around the samples. Our framework is versatile and can be extended to video matting by taking temporal information into account in the sampling process. Evaluation on standard benchmarks for image and video matting shows that our approach provides more precise results compared to existing methods.",1
"У цій статті ми представляємо новий підхід альфа-матування на основі вибірки для точної оцінки переднього та фонового шарів зображення. Попередні методи, засновані на вибірці, зазвичай покладаються на певні евристики при зборі репрезентативних вибірок із відомих регіонів, і тому їх продуктивність погіршується, якщо базові припущення не задовольняються. Щоб полегшити це, ми застосували абсолютно новий підхід і сформулювали вибірку як проблему відбору розрідженої підмножини, де ми пропонуємо вибрати невеликий набір вибірок-кандидатів, які найкраще пояснюють невідомі пікселі. Крім того, ми описуємо нову міру відмінності для порівняння двох зразків, яка базується на KL-розбіжності між розподілами ознак, виділених поблизу зразків. Запропонована структура є загальною і може бути легко розширена до відеоматінгу шляхом додаткового врахування тимчасової інформації в процесі вибірки. Оцінка стандартних контрольних наборів даних для матування зображень і відео демонструє, що наш підхід забезпечує більш точні результати порівняно з найсучаснішими методами.","Ми пропонуємо підхід альфа-матування на основі вибірки для точної оцінки переднього та фонового шарів зображення. Існуючі методи на основі вибірки залежать від евристичних методів збору зразків із відомих регіонів і, таким чином, не є надійними, якщо припущення не виконуються. Щоб вирішити цю проблему, ми розглядаємо вибірку як проблему вибору розрідженої підмножини та пропонуємо невеликий набір зразків-кандидатів для пояснення невідомих пікселів. Крім того, ми вводимо міру відмінності, засновану на KL-розбіжності між розподілами ознак навколо зразків. Наша структура є універсальною і може бути розширена до відеоматінгу, враховуючи часову інформацію в процесі вибірки. Оцінка стандартних тестів для матування зображень і відео показує, що наш підхід забезпечує більш точні результати порівняно з існуючими методами.",1
"With the growing interest in computational models of visual attention, saliency prediction has become an important research topic in computer vision. Over the past years, many different successful saliency models have been proposed especially for image saliency prediction. However, these models generally do not consider the dynamic nature of the scenes, and hence, they work better on static images. To date, there has been relatively little work on dynamic saliency that deals with predicting where humans look at videos. In addition, previous studies showed that how the feature integration is carried out is very crucial for more accurate results. Yet, many dynamic saliency models follow a similar simple design and extract separate spatial and temporal saliency maps which are then integrated together to obtain the final saliency map. In this paper, we present a comparative study for different feature integration strategies in dynamic saliency estimation. We employ a number of low and high-level visual features such as static saliency, motion, faces, humans and text, some of which have not been previously used in dynamic saliency estimation. In order to explore the strength of feature integration strategies, we investigate four learning-based (SVM, Gradient Boosting, NNLS, Random Forest) and two transformation- based (Mean, Max) fusion methods, resulting in six new dynamic saliency models. Our experimental analysis on two different dynamic saliency benchmark datasets reveal that our models achieve better performance than the individual features. In addition, our learning-based models outperform the state-of-the-art dynamic saliency models.","Interest in computational models of visual attention has increased, leading to the importance of saliency prediction in computer vision. Over recent years, many successful saliency models for image saliency prediction have been proposed, yet they do not take into account dynamic scenes. Little research has been done on dynamic saliency with regards to predicting where humans look in videos. It has been shown that how feature integration is carried out is critical for more accurate results, yet many dynamic saliency models employ a simple design which extracts separate spatial and temporal saliency maps which are integrated together to form the final saliency map. This paper presents a comparative study of different feature integration strategies in dynamic saliency estimation. Low and high-level visual features such as static saliency, motion, faces, humans and text (some of which have not been used in dynamic saliency estimation before) are employed. To explore the strength of feature integration strategies, six new dynamic saliency models are created, based on four learning-based (SVM, Gradient Boosting, NNLS, Random Forest) and two transformation-based (Mean, Max) fusion methods. Experimental analysis on two different dynamic saliency benchmark datasets shows that our models outperform individual features, and that our learning-based models are better than the state-of-the-art dynamic saliency models.",1
"With the growing interest in computational models of visual attention, saliency prediction has become an important research topic in computer vision. Over the past years, many different successful saliency models have been proposed especially for image saliency prediction. However, these models generally do not consider the dynamic nature of the scenes, and hence, they work better on static images. To date, there has been relatively little work on dynamic saliency that deals with predicting where humans look at videos. In addition, previous studies showed that how the feature integration is carried out is very crucial for more accurate results. Yet, many dynamic saliency models follow a similar simple design and extract separate spatial and temporal saliency maps which are then integrated together to obtain the final saliency map. In this paper, we present a comparative study for different feature integration strategies in dynamic saliency estimation. We employ a number of low and high-level visual features such as static saliency, motion, faces, humans and text, some of which have not been previously used in dynamic saliency estimation. In order to explore the strength of feature integration strategies, we investigate four learning-based (SVM, Gradient Boosting, NNLS, Random Forest) and two transformation- based (Mean, Max) fusion methods, resulting in six new dynamic saliency models. Our experimental analysis on two different dynamic saliency benchmark datasets reveal that our models achieve better performance than the individual features. In addition, our learning-based models outperform the state-of-the-art dynamic saliency models.","Інтерес до обчислювальних моделей зорової уваги зріс, що призвело до важливості прогнозування помітності в комп’ютерному зорі. За останні роки було запропоновано багато успішних моделей помітності для прогнозування помітності зображення, але вони не враховують динамічні сцени. Було проведено мало досліджень щодо динамічної помітності щодо передбачення того, куди дивляться люди у відео. Було показано, що те, як виконується інтеграція функцій, має вирішальне значення для отримання більш точних результатів, але багато динамічних моделей помітності використовують просту структуру, яка виділяє окремі просторові та часові карти помітності, які інтегруються разом, щоб сформувати остаточну карту помітності. У цій статті представлено порівняльне дослідження різних стратегій інтеграції ознак у динамічній оцінці помітності. Використовуються візуальні характеристики низького та високого рівня, такі як статична помітність, рух, обличчя, люди та текст (деякі з яких раніше не використовувалися в динамічній оцінці помітності). Щоб дослідити силу стратегій інтеграції функцій, створено шість нових динамічних моделей помітності на основі чотирьох методів злиття на основі навчання (SVM, Gradient Boosting, NNLS, Random Forest) і двох методів злиття на основі трансформації (Mean, Max). Експериментальний аналіз двох різних наборів даних динамічної помітності показує, що наші моделі перевершують окремі функції, а наші моделі на основі навчання є кращими за найсучасніші моделі динамічної помітності.",1
"Зі зростанням інтересу до обчислювальних моделей зорової уваги прогнозування помітності стало важливою темою дослідження комп’ютерного зору. За останні роки було запропоновано багато різних успішних моделей помітності, особливо для прогнозування помітності зображення. Однак ці моделі зазвичай не враховують динамічну природу сцен, а отже, вони краще працюють на статичних зображеннях. На сьогоднішній день було проведено відносно мало робіт щодо динамічної помітності, яка стосується передбачення того, де люди дивляться відео. Крім того, попередні дослідження показали, що для більш точних результатів дуже важливо, як здійснюється інтеграція функцій. Тим не менш, багато динамічних моделей помітності дотримуються схожої простої конструкції та виділяють окремі просторові та часові карти помітності, які потім інтегруються разом, щоб отримати остаточну карту помітності. У цій статті ми представляємо порівняльне дослідження для різних стратегій інтеграції функцій у динамічній оцінці помітності. Ми використовуємо ряд візуальних функцій низького та високого рівня, таких як статична помітність, рух, обличчя, люди та текст, деякі з яких раніше не використовувалися в динамічній оцінці помітності. Щоб дослідити силу стратегій інтеграції функцій, ми досліджуємо чотири методи злиття на основі навчання (SVM, Gradient Boosting, NNLS, Random Forest) і два на основі трансформації (Mean, Max), у результаті чого створено шість нових динамічних моделей помітності. Наш експериментальний аналіз двох різних наборів даних динамічного порівняння показує, що наші моделі досягають кращої продуктивності, ніж окремі функції. Крім того, наші моделі, засновані на навчанні, перевершують найсучасніші моделі динамічної помітності.","Interest in computational models of visual attention has increased, leading to the importance of saliency prediction in computer vision. Over recent years, many successful saliency models for image saliency prediction have been proposed, yet they do not take into account dynamic scenes. Little research has been done on dynamic saliency with regards to predicting where humans look in videos. It has been shown that how feature integration is carried out is critical for more accurate results, yet many dynamic saliency models employ a simple design which extracts separate spatial and temporal saliency maps which are integrated together to form the final saliency map. This paper presents a comparative study of different feature integration strategies in dynamic saliency estimation. Low and high-level visual features such as static saliency, motion, faces, humans and text (some of which have not been used in dynamic saliency estimation before) are employed. To explore the strength of feature integration strategies, six new dynamic saliency models are created, based on four learning-based (SVM, Gradient Boosting, NNLS, Random Forest) and two transformation-based (Mean, Max) fusion methods. Experimental analysis on two different dynamic saliency benchmark datasets shows that our models outperform individual features, and that our learning-based models are better than the state-of-the-art dynamic saliency models.",1
"Зі зростанням інтересу до обчислювальних моделей зорової уваги прогнозування помітності стало важливою темою дослідження комп’ютерного зору. За останні роки було запропоновано багато різних успішних моделей помітності, особливо для прогнозування помітності зображення. Однак ці моделі зазвичай не враховують динамічну природу сцен, а отже, вони краще працюють на статичних зображеннях. На сьогоднішній день було проведено відносно мало робіт щодо динамічної помітності, яка стосується передбачення того, де люди дивляться відео. Крім того, попередні дослідження показали, що для більш точних результатів дуже важливо, як здійснюється інтеграція функцій. Тим не менш, багато динамічних моделей помітності дотримуються схожої простої конструкції та виділяють окремі просторові та часові карти помітності, які потім інтегруються разом, щоб отримати остаточну карту помітності. У цій статті ми представляємо порівняльне дослідження для різних стратегій інтеграції функцій у динамічній оцінці помітності. Ми використовуємо ряд візуальних функцій низького та високого рівня, таких як статична помітність, рух, обличчя, люди та текст, деякі з яких раніше не використовувалися в динамічній оцінці помітності. Щоб дослідити силу стратегій інтеграції функцій, ми досліджуємо чотири методи злиття на основі навчання (SVM, Gradient Boosting, NNLS, Random Forest) і два на основі трансформації (Mean, Max), у результаті чого створено шість нових динамічних моделей помітності. Наш експериментальний аналіз двох різних наборів даних динамічного порівняння показує, що наші моделі досягають кращої продуктивності, ніж окремі функції. Крім того, наші моделі, засновані на навчанні, перевершують найсучасніші моделі динамічної помітності.","Інтерес до обчислювальних моделей зорової уваги зріс, що призвело до важливості прогнозування помітності в комп’ютерному зорі. За останні роки було запропоновано багато успішних моделей помітності для прогнозування помітності зображення, але вони не враховують динамічні сцени. Було проведено мало досліджень щодо динамічної помітності щодо передбачення того, куди дивляться люди у відео. Було показано, що те, як виконується інтеграція функцій, має вирішальне значення для отримання більш точних результатів, але багато динамічних моделей помітності використовують просту структуру, яка виділяє окремі просторові та часові карти помітності, які інтегруються разом, щоб сформувати остаточну карту помітності. У цій статті представлено порівняльне дослідження різних стратегій інтеграції ознак у динамічній оцінці помітності. Використовуються візуальні характеристики низького та високого рівня, такі як статична помітність, рух, обличчя, люди та текст (деякі з яких раніше не використовувалися в динамічній оцінці помітності). Щоб дослідити силу стратегій інтеграції функцій, створено шість нових динамічних моделей помітності на основі чотирьох методів злиття на основі навчання (SVM, Gradient Boosting, NNLS, Random Forest) і двох методів злиття на основі трансформації (Mean, Max). Експериментальний аналіз двох різних наборів даних динамічної помітності показує, що наші моделі перевершують окремі функції, а наші моделі на основі навчання є кращими за найсучасніші моделі динамічної помітності.",1
"Moving Object Detection is one of the integral tasks for aerial reconnaissance and surveillance applications. Despite the problem’s rising potential due to increasing availability of unmanned aerial vehicles, moving object detection suffers from a lack of widely-accepted, correctly labelled dataset that would facilitate a robust evaluation of the techniques published by the community. Towards this end, we compile a new dataset by manually annotating several sequences from VIVID and UAV123 datasets for moving object detection. We also propose a feature-based, efficient pipeline that is optimized for near real-time performance on GPU-based embedded SoMs (system on module). We evaluate our pipeline on this extended dataset for low altitude moving object detection. Ground-truth annotations are made publicly available to the community to foster further research in moving object detection field. ","Moving Object Detection is essential for aerial reconnaissance and surveillance. However, a lack of labelled datasets hinders the evaluation of techniques proposed by the community. We compile a new dataset, manually annotating sequences from VIVID and UAV123 datasets. We also present a feature-based, efficient pipeline optimized for near real-time performance on GPU-based embedded SoMs. Our pipeline is evaluated on the extended dataset for low altitude moving object detection. We make the ground-truth annotations publicly available to further research in the field.",1
"Moving Object Detection is one of the integral tasks for aerial reconnaissance and surveillance applications. Despite the problem’s rising potential due to increasing availability of unmanned aerial vehicles, moving object detection suffers from a lack of widely-accepted, correctly labelled dataset that would facilitate a robust evaluation of the techniques published by the community. Towards this end, we compile a new dataset by manually annotating several sequences from VIVID and UAV123 datasets for moving object detection. We also propose a feature-based, efficient pipeline that is optimized for near real-time performance on GPU-based embedded SoMs (system on module). We evaluate our pipeline on this extended dataset for low altitude moving object detection. Ground-truth annotations are made publicly available to the community to foster further research in moving object detection field. ","Виявлення рухомих об&#39;єктів має важливе значення для повітряної розвідки та спостереження. Однак відсутність маркованих наборів даних перешкоджає оцінці методів, запропонованих спільнотою. Ми збираємо новий набір даних, вручну анотуючи послідовності з наборів даних VIVID і UAV123. Ми також представляємо ефективний конвеєр на основі функцій, оптимізований для продуктивності майже в реальному часі на вбудованих SoM на основі GPU. Наш конвеєр оцінюється на основі розширеного набору даних для виявлення рухомих об’єктів на малій висоті. Ми робимо анотації загальної правди загальнодоступними для подальших досліджень у цій галузі.",1
"Виявлення рухомих об&#39;єктів є одним із невід&#39;ємних завдань для програм повітряної розвідки та спостереження. Незважаючи на зростаючий потенціал проблеми через збільшення доступності безпілотних літальних апаратів, виявлення рухомих об’єктів страждає від відсутності загальноприйнятого, правильно позначеного набору даних, який би полегшив надійну оцінку методів, опублікованих спільнотою. З цією метою ми збираємо новий набір даних, вручну анотуючи кілька послідовностей із наборів даних VIVID і UAV123 для виявлення рухомих об’єктів. Ми також пропонуємо ефективний конвеєр на основі функцій, який оптимізований для роботи майже в реальному часі на вбудованих SoM на основі GPU (система на модулі). Ми оцінюємо наш конвеєр на цьому розширеному наборі даних для виявлення рухомих об’єктів на малій висоті. Наземні правдиві анотації стають загальнодоступними для спільноти, щоб сприяти подальшим дослідженням у сфері виявлення рухомих об’єктів.","Moving Object Detection is essential for aerial reconnaissance and surveillance. However, a lack of labelled datasets hinders the evaluation of techniques proposed by the community. We compile a new dataset, manually annotating sequences from VIVID and UAV123 datasets. We also present a feature-based, efficient pipeline optimized for near real-time performance on GPU-based embedded SoMs. Our pipeline is evaluated on the extended dataset for low altitude moving object detection. We make the ground-truth annotations publicly available to further research in the field.",1
"Виявлення рухомих об&#39;єктів є одним із невід&#39;ємних завдань для програм повітряної розвідки та спостереження. Незважаючи на зростаючий потенціал проблеми через збільшення доступності безпілотних літальних апаратів, виявлення рухомих об’єктів страждає від відсутності загальноприйнятого, правильно позначеного набору даних, який би полегшив надійну оцінку методів, опублікованих спільнотою. З цією метою ми збираємо новий набір даних, вручну анотуючи кілька послідовностей із наборів даних VIVID і UAV123 для виявлення рухомих об’єктів. Ми також пропонуємо ефективний конвеєр на основі функцій, який оптимізований для роботи майже в реальному часі на вбудованих SoM на основі GPU (система на модулі). Ми оцінюємо наш конвеєр на цьому розширеному наборі даних для виявлення рухомих об’єктів на малій висоті. Наземні правдиві анотації стають загальнодоступними для спільноти, щоб сприяти подальшим дослідженням у сфері виявлення рухомих об’єктів.","Виявлення рухомих об&#39;єктів має важливе значення для повітряної розвідки та спостереження. Однак відсутність маркованих наборів даних перешкоджає оцінці методів, запропонованих спільнотою. Ми збираємо новий набір даних, вручну анотуючи послідовності з наборів даних VIVID і UAV123. Ми також представляємо ефективний конвеєр на основі функцій, оптимізований для продуктивності майже в реальному часі на вбудованих SoM на основі GPU. Наш конвеєр оцінюється на основі розширеного набору даних для виявлення рухомих об’єктів на малій висоті. Ми робимо анотації загальної правди загальнодоступними для подальших досліджень у цій галузі.",1
"The task of generating natural language descriptions from images has received a lot of attention in recent years. Consequently, it is becoming increasingly important to evaluate such image captioning approaches in an automatic manner. In this paper, we provide an in-depth evaluation of the existing image captioning metrics through a series of carefully designed experiments. Moreover, we explore the utilization of the recently proposed Word Mover’s Distance (WMD) document metric for the purpose of image captioning. Our findings outline the differences and/or similarities between metrics and their relative robustness by means of extensive correlation, accuracy and distraction based evaluations. Our results also demonstrate that WMD provides strong advantages over other metrics.","Recently, there has been a great deal of interest in the task of generating natural language descriptions from images. Therefore, it is becoming essential to automatically assess such image captioning approaches. This paper provides a thorough assessment of existing image captioning metrics through experiments designed carefully. Additionally, we investigate the utilization of the recently developed Word Mover's Distance (WMD) document metric for image captioning. Our findings demonstrate the differences and/or similarities between metrics, their relative robustness via extensive correlation, accuracy and distraction-based evaluations. Furthermore, our results demonstrate that WMD offers considerable benefits compared to other metrics.",1
"The task of generating natural language descriptions from images has received a lot of attention in recent years. Consequently, it is becoming increasingly important to evaluate such image captioning approaches in an automatic manner. In this paper, we provide an in-depth evaluation of the existing image captioning metrics through a series of carefully designed experiments. Moreover, we explore the utilization of the recently proposed Word Mover’s Distance (WMD) document metric for the purpose of image captioning. Our findings outline the differences and/or similarities between metrics and their relative robustness by means of extensive correlation, accuracy and distraction based evaluations. Our results also demonstrate that WMD provides strong advantages over other metrics.","Останнім часом виник великий інтерес до завдання створення описів природною мовою з зображень. Тому важливо автоматично оцінювати такі підходи до підписів до зображень. Ця стаття містить ретельну оцінку існуючих метрик субтитрів до зображень за допомогою ретельно розроблених експериментів. Крім того, ми досліджуємо використання нещодавно розробленої метрики документа Word Mover&#39;s Distance (WMD) для підписів до зображень. Наші висновки демонструють відмінності та/або схожість між показниками, їхню відносну стійкість через широку кореляцію, точність і оцінювання на основі відволікання. Крім того, наші результати демонструють, що ЗМЗ пропонує значні переваги порівняно з іншими показниками.",1
"Завдання генерації описів природною мовою з зображень привернуло багато уваги в останні роки. Отже, стає все більш важливим автоматично оцінювати такі підходи до підписів до зображень. У цій статті ми надаємо поглиблену оцінку існуючих показників субтитрів до зображень за допомогою серії ретельно розроблених експериментів. Крім того, ми досліджуємо використання нещодавно запропонованої метрики документа Word Mover Distance (WMD) для підписів до зображень. Наші висновки окреслюють відмінності та/або подібності між показниками та їх відносною стійкістю за допомогою обширних оцінок на основі кореляції, точності та відволікання. Наші результати також демонструють, що ЗМЗ має значні переваги перед іншими показниками.","Recently, there has been a great deal of interest in the task of generating natural language descriptions from images. Therefore, it is becoming essential to automatically assess such image captioning approaches. This paper provides a thorough assessment of existing image captioning metrics through experiments designed carefully. Additionally, we investigate the utilization of the recently developed Word Mover's Distance (WMD) document metric for image captioning. Our findings demonstrate the differences and/or similarities between metrics, their relative robustness via extensive correlation, accuracy and distraction-based evaluations. Furthermore, our results demonstrate that WMD offers considerable benefits compared to other metrics.",1
"Завдання генерації описів природною мовою з зображень привернуло багато уваги в останні роки. Отже, стає все більш важливим автоматично оцінювати такі підходи до підписів до зображень. У цій статті ми надаємо поглиблену оцінку існуючих показників субтитрів до зображень за допомогою серії ретельно розроблених експериментів. Крім того, ми досліджуємо використання нещодавно запропонованої метрики документа Word Mover Distance (WMD) для підписів до зображень. Наші висновки окреслюють відмінності та/або схожість між показниками та їх відносною надійністю за допомогою обширних оцінок на основі кореляції, точності та відволікання. Наші результати також демонструють, що ЗМЗ має значні переваги перед іншими показниками.","Останнім часом виник великий інтерес до завдання створення описів природною мовою з зображень. Тому важливо автоматично оцінювати такі підходи до підписів до зображень. Ця стаття містить ретельну оцінку існуючих метрик субтитрів до зображень за допомогою ретельно розроблених експериментів. Крім того, ми досліджуємо використання нещодавно розробленої метрики документа Word Mover Distance (WMD) для підписів до зображень. Наші висновки демонструють відмінності та/або схожість між показниками, їхню відносну стійкість через широку кореляцію, точність і оцінювання на основі відволікання. Крім того, наші результати демонструють, що ЗМЗ пропонує значні переваги порівняно з іншими показниками.",1
"Existing computational models of visual attention generally employ simple image features such as color, intensity or orientation to generate a saliency map which highlights the image parts that attract human attention. Interestingly, most of these models do not process any depth information and operate only on standard two-dimensional RGB images. On the other hand, depth processing through stereo vision is a key characteristics of the human visual system. In line with this observation, in this study, we propose to extend two state-of-the-art static saliency models that depend on region covariances to process additional depth information available in RGB-D images. We evaluate our proposed models on NUS-3D benchmark dataset by taking into account different evaluation metrics. Our results reveal that using the additional depth information improves the saliency prediction in a statistically significant manner, giving more accurate saliency maps.  ","Computational models of visual attention generally utilize basic image components like color, intensity, and orientation to create a saliency map which shows the image parts that attract human attention. Most of these models don't take into account depth info, instead just analyzing standard 2D RGB images. This study proposes to modify two state-of-the-art static saliency models that depend on region covariances to include depth info from RGB-D images. Evaluation on NUS-3D benchmark dataset via different metrics demonstrates that using the additional depth info enhances saliency prediction in a statistically significant way, producing more precise saliency maps.",1
"Existing computational models of visual attention generally employ simple image features such as color, intensity or orientation to generate a saliency map which highlights the image parts that attract human attention. Interestingly, most of these models do not process any depth information and operate only on standard two-dimensional RGB images. On the other hand, depth processing through stereo vision is a key characteristics of the human visual system. In line with this observation, in this study, we propose to extend two state-of-the-art static saliency models that depend on region covariances to process additional depth information available in RGB-D images. We evaluate our proposed models on NUS-3D benchmark dataset by taking into account different evaluation metrics. Our results reveal that using the additional depth information improves the saliency prediction in a statistically significant manner, giving more accurate saliency maps.  ","Обчислювальні моделі зорової уваги зазвичай використовують основні компоненти зображення, такі як колір, інтенсивність і орієнтація, щоб створити карту помітності, яка показує частини зображення, які привертають увагу людини. Більшість із цих моделей не враховують інформацію про глибину, натомість лише аналізують стандартні 2D-зображення RGB. У цьому дослідженні пропонується модифікувати дві найсучасніші статичні моделі помітності, які залежать від коваріацій регіону, щоб включити інформацію про глибину із зображень RGB-D. Оцінка порівняльного набору даних NUS-3D за допомогою різних показників демонструє, що використання додаткової інформації про глибину покращує прогнозування помітності статистично значущим чином, створюючи більш точні карти помітності.",1
"Існуючі обчислювальні моделі зорової уваги зазвичай використовують прості характеристики зображення, такі як колір, інтенсивність або орієнтація, щоб створити карту помітності, яка виділяє частини зображення, які привертають увагу людини. Цікаво, що більшість із цих моделей не обробляють інформацію про глибину й працюють лише зі стандартними двовимірними зображеннями RGB. З іншого боку, обробка глибини за допомогою стереозору є ключовою характеристикою зорової системи людини. Згідно з цим спостереженням, у цьому дослідженні ми пропонуємо розширити дві найсучасніші статичні моделі помітності, які залежать від коваріацій регіону, для обробки додаткової інформації про глибину, доступної в зображеннях RGB-D. Ми оцінюємо запропоновані нами моделі на основі порівняльного набору даних NUS-3D, беручи до уваги різні метрики оцінювання. Наші результати показують, що використання додаткової інформації про глибину покращує прогнозування помітності статистично значущим чином, надаючи точніші карти помітності.","Computational models of visual attention generally utilize basic image components like color, intensity, and orientation to create a saliency map which shows the image parts that attract human attention. Most of these models don't take into account depth info, instead just analyzing standard 2D RGB images. This study proposes to modify two state-of-the-art static saliency models that depend on region covariances to include depth info from RGB-D images. Evaluation on NUS-3D benchmark dataset via different metrics demonstrates that using the additional depth info enhances saliency prediction in a statistically significant way, producing more precise saliency maps.",1
"Існуючі обчислювальні моделі зорової уваги зазвичай використовують прості характеристики зображення, такі як колір, інтенсивність або орієнтація, щоб створити карту помітності, яка виділяє частини зображення, які привертають увагу людини. Цікаво, що більшість із цих моделей не обробляють інформацію про глибину й працюють лише зі стандартними двовимірними зображеннями RGB. З іншого боку, обробка глибини за допомогою стереозору є ключовою характеристикою зорової системи людини. Згідно з цим спостереженням, у цьому дослідженні ми пропонуємо розширити дві найсучасніші статичні моделі помітності, які залежать від коваріацій регіону, для обробки додаткової інформації про глибину, доступної в зображеннях RGB-D. Ми оцінюємо запропоновані нами моделі на основі порівняльного набору даних NUS-3D, беручи до уваги різні метрики оцінювання. Наші результати показують, що використання додаткової інформації про глибину покращує прогнозування помітності статистично значущим чином, надаючи точніші карти помітності.","Обчислювальні моделі зорової уваги зазвичай використовують основні компоненти зображення, такі як колір, інтенсивність і орієнтація, щоб створити карту помітності, яка показує частини зображення, які привертають увагу людини. Більшість із цих моделей не враховують інформацію про глибину, натомість лише аналізують стандартні 2D-зображення RGB. У цьому дослідженні пропонується модифікувати дві найсучасніші статичні моделі помітності, які залежать від коваріацій регіону, щоб включити інформацію про глибину із зображень RGB-D. Оцінка порівняльного набору даних NUS-3D за допомогою різних показників демонструє, що використання додаткової інформації про глибину покращує прогнозування помітності статистично значущим чином, створюючи більш точні карти помітності.",1
"Automatic image synthesis research has been rapidly growing with deep networks getting more and more expressive. In the last couple of years, we have observed images of digits, indoor scenes, birds, chairs, etc. being automatically generated. The expressive power of image generators have also been enhanced by introducing several forms of conditioning variables such as object names, sentences, bounding box and key-point locations. In this work, we propose a novel deep conditional generative adversarial network architecture that takes its strength from the semantic layout and scene attributes integrated as conditioning variables. We show that our architecture is able to generate realistic outdoor scene images under different conditions, e.g. daynight, sunny-foggy, with clear object boundaries. ","Research into automatic image synthesis has been advancing rapidly, with deep networks becoming increasingly expressive. In the past few years, we have seen images of digits, indoor scenes, birds, chairs etc. automatically generated, and their expressive power boosted by incorporating various conditioning variables, e.g. object names, sentences, bounding boxes and key-point locations. Here, we introduce a novel deep conditional generative adversarial network architecture that harnesses semantic layout and scene attributes as conditioning variables. We demonstrate that this architecture is able to produce realistic outdoor scene images in different conditions, such as day/night, sunny/foggy, with distinct object outlines.",1
"Automatic image synthesis research has been rapidly growing with deep networks getting more and more expressive. In the last couple of years, we have observed images of digits, indoor scenes, birds, chairs, etc. being automatically generated. The expressive power of image generators have also been enhanced by introducing several forms of conditioning variables such as object names, sentences, bounding box and key-point locations. In this work, we propose a novel deep conditional generative adversarial network architecture that takes its strength from the semantic layout and scene attributes integrated as conditioning variables. We show that our architecture is able to generate realistic outdoor scene images under different conditions, e.g. daynight, sunny-foggy, with clear object boundaries. ","Дослідження автоматичного синтезу зображень швидко просуваються, а глибокі мережі стають все більш виразними. Протягом останніх кількох років ми спостерігали автоматичне генерування зображень цифр, сцен у приміщенні, птахів, стільців тощо, а їх виразність посилювалася шляхом включення різноманітних умовних змінних, наприклад назв об’єктів, речень, обмежувальних рамок і розташування ключових точок. Тут ми представляємо нову глибоку умовну генеруючу змагальну мережеву архітектуру, яка використовує семантичний макет і атрибути сцени як змінні умови. Ми демонструємо, що ця архітектура здатна створювати реалістичні зображення зовнішньої сцени в різних умовах, наприклад день/ніч, сонячно/туман, із чіткими контурами об’єктів.",1
"Дослідження автоматичного синтезу зображень стрімко розвиваються, а глибокі мережі стають дедалі виразнішими. За останні пару років ми спостерігали автоматичне генерування зображень цифр, сцен у приміщенні, птахів, стільців тощо. Виражальну силу генераторів зображень також було покращено завдяки введенню кількох форм обумовлюючих змінних, таких як назви об’єктів, речення, обмежувальна рамка та розташування ключових точок. У цій роботі ми пропонуємо нову глибоку умовну генеруючу змагальну архітектуру мережі, яка бере свою силу від семантичного макету та атрибутів сцени, інтегрованих як змінні умови. Ми показуємо, що наша архітектура здатна генерувати реалістичні зображення зовнішньої сцени за різних умов, наприклад, вдень, у сонячну погоду з туманом, із чіткими межами об’єктів.","Research into automatic image synthesis has been advancing rapidly, with deep networks becoming increasingly expressive. In the past few years, we have seen images of digits, indoor scenes, birds, chairs etc. automatically generated, and their expressive power boosted by incorporating various conditioning variables, e.g. object names, sentences, bounding boxes and key-point locations. Here, we introduce a novel deep conditional generative adversarial network architecture that harnesses semantic layout and scene attributes as conditioning variables. We demonstrate that this architecture is able to produce realistic outdoor scene images in different conditions, such as day/night, sunny/foggy, with distinct object outlines.",1
"Дослідження автоматичного синтезу зображень стрімко розвиваються, а глибокі мережі стають дедалі виразнішими. За останні пару років ми спостерігали автоматичне генерування зображень цифр, сцен у приміщенні, птахів, стільців тощо. Виражальну силу генераторів зображень також було покращено завдяки введенню кількох форм обумовлюючих змінних, таких як назви об’єктів, речення, обмежувальна рамка та розташування ключових точок. У цій роботі ми пропонуємо нову глибоку умовну генеруючу змагальну архітектуру мережі, яка бере свою силу від семантичного макету та атрибутів сцени, інтегрованих як змінні умови. Ми показуємо, що наша архітектура здатна генерувати реалістичні зображення зовнішньої сцени за різних умов, наприклад, вдень, у сонячну погоду з туманом, із чіткими межами об’єктів.","Дослідження автоматичного синтезу зображень швидко просуваються, а глибокі мережі стають все більш виразними. Протягом останніх кількох років ми спостерігали автоматичне генерування зображень цифр, сцен у приміщенні, птахів, стільців тощо, а їх виразність посилювалася шляхом включення різноманітних умовних змінних, наприклад назв об’єктів, речень, обмежувальних рамок і розташування ключових точок. Тут ми представляємо нову глибоку умовну генеруючу змагальну мережеву архітектуру, яка використовує семантичний макет і атрибути сцени як змінні умови. Ми демонструємо, що ця архітектура здатна створювати реалістичні зображення зовнішньої сцени в різних умовах, наприклад день/ніч, сонячно/туман, із чіткими контурами об’єктів.",1
"In this study, we explore whether the captions in the wild can boost the performance of object detection in images. Captions that accompany images usually provide significant information about the visual content of the image, making them an important resource for image understanding. However, captions in the wild are likely to include numerous types of noises which can hurt visual estimation. In this paper, we propose data-driven methods to deal with the noisy captions and utilize them to improve object detection. We show how a pre-trained state-of-theart object detector can take advantage of noisy captions. Our experiments demonstrate that captions provide promising cues about the visual content of the images and can aid in improving object detection.","This study examines if captions in the wild can enhance object detection in images. Such captions usually give important details about the image's visual content, thus being a vital resource for image comprehension. But, captions in the wild could include various types of noise which can affect visual estimation. To counter this, data-driven methods were proposed to manage noisy captions and make use of them to improve object detection. The paper shows how a pre-trained, state-of-the-art object detector can benefit from noisy captions. Experiments illustrate that captions offer promising clues about the visual content of the images and can help in boosting object detection.",1
"In this study, we explore whether the captions in the wild can boost the performance of object detection in images. Captions that accompany images usually provide significant information about the visual content of the image, making them an important resource for image understanding. However, captions in the wild are likely to include numerous types of noises which can hurt visual estimation. In this paper, we propose data-driven methods to deal with the noisy captions and utilize them to improve object detection. We show how a pre-trained state-of-theart object detector can take advantage of noisy captions. Our experiments demonstrate that captions provide promising cues about the visual content of the images and can aid in improving object detection.","Це дослідження перевіряє, чи можуть підписи в дикій природі покращити виявлення об’єктів на зображеннях. Такі підписи зазвичай надають важливі деталі про візуальний вміст зображення, тому є життєво важливим ресурсом для розуміння зображення. Але титри в дикій природі можуть містити різні типи шуму, які можуть впливати на візуальну оцінку. Щоб протистояти цьому, були запропоновані методи, керовані даними, для керування шумними титрами та їх використання для покращення виявлення об’єктів. У статті показано, як попередньо навчений сучасний детектор об’єктів може отримати вигоду від зашумлених субтитрів. Експерименти показують, що підписи пропонують багатообіцяючі підказки щодо візуального вмісту зображень і можуть допомогти у покращенні виявлення об’єктів.",1
"У цьому дослідженні ми досліджуємо, чи можуть підписи в дикій природі підвищити ефективність виявлення об’єктів на зображеннях. Підписи, які супроводжують зображення, зазвичай надають значну інформацію про візуальний вміст зображення, що робить їх важливим ресурсом для розуміння зображення. Проте підписи в дикій природі, ймовірно, містять численні типи шумів, які можуть зашкодити візуальній оцінці. У цій статті ми пропонуємо керовані даними методи боротьби з зашумленими титрами та використовуємо їх для покращення виявлення об’єктів. Ми показуємо, як попередньо навчений сучасний детектор об’єктів може використовувати шумні титри. Наші експерименти демонструють, що підписи дають багатообіцяючі підказки щодо візуального вмісту зображень і можуть допомогти покращити виявлення об’єктів.","This study examines if captions in the wild can enhance object detection in images. Such captions usually give important details about the image's visual content, thus being a vital resource for image comprehension. But, captions in the wild could include various types of noise which can affect visual estimation. To counter this, data-driven methods were proposed to manage noisy captions and make use of them to improve object detection. The paper shows how a pre-trained, state-of-the-art object detector can benefit from noisy captions. Experiments illustrate that captions offer promising clues about the visual content of the images and can help in boosting object detection.",1
"У цьому дослідженні ми досліджуємо, чи можуть підписи в дикій природі підвищити ефективність виявлення об’єктів на зображеннях. Підписи, які супроводжують зображення, зазвичай надають значну інформацію про візуальний вміст зображення, що робить їх важливим ресурсом для розуміння зображення. Проте підписи в дикій природі, ймовірно, містять численні типи шумів, які можуть зашкодити візуальній оцінці. У цій статті ми пропонуємо керовані даними методи боротьби з зашумленими титрами та використовуємо їх для покращення виявлення об’єктів. Ми показуємо, як попередньо навчений сучасний детектор об’єктів може використовувати шумні титри. Наші експерименти демонструють, що підписи дають багатообіцяючі підказки щодо візуального вмісту зображень і можуть допомогти покращити виявлення об’єктів.","Це дослідження перевіряє, чи можуть підписи в дикій природі покращити виявлення об’єктів на зображеннях. Такі підписи зазвичай надають важливі деталі про візуальний вміст зображення, тому є життєво важливим ресурсом для розуміння зображення. Але титри в дикій природі можуть містити різні типи шуму, які можуть впливати на візуальну оцінку. Щоб протистояти цьому, були запропоновані методи, керовані даними, для керування шумними титрами та їх використання для покращення виявлення об’єктів. У статті показано, як попередньо навчений сучасний детектор об’єктів може отримати вигоду від зашумлених субтитрів. Експерименти показують, що підписи пропонують багатообіцяючі підказки щодо візуального вмісту зображень і можуть допомогти у покращенні виявлення об’єктів.",1
"In recent years, visual saliency estimation in images has attracted much attention in the computer vision community. However, predicting saliency in videos has received relatively little attention. Inspired by the recent success of deep convolutional neural networks based static saliency models, in this work, we study two different two-stream convolutional networks for dynamic saliency prediction. To improve the generalization capability of our models, we also introduce a novel, empirically grounded data augmentation technique for this task. We test our models on DIEM dataset and report superior results against the existing models. Moreover, we perform transfer learning experiments on SALICON, a recently proposed static saliency dataset, by finetuning our models on the optical flows estimated from static images. Our experiments show that taking motion into account in this way can be helpful for static saliency estimation. ","Recent research in the computer vision community has focused on estimating visual saliency in images. However, predicting saliency in videos has not been widely explored. This work examines two two-stream convolutional networks for dynamic saliency prediction, utilizing a novel data augmentation technique to enhance the models' generalization capability. Tests on the DIEM dataset reveal the models' superiority over existing models. Additionally, transfer learning experiments on the static saliency dataset SALICON, with the models fine-tuned on optical flows estimated from static images, demonstrate that motion consideration is advantageous for static saliency estimation.",1
"In recent years, visual saliency estimation in images has attracted much attention in the computer vision community. However, predicting saliency in videos has received relatively little attention. Inspired by the recent success of deep convolutional neural networks based static saliency models, in this work, we study two different two-stream convolutional networks for dynamic saliency prediction. To improve the generalization capability of our models, we also introduce a novel, empirically grounded data augmentation technique for this task. We test our models on DIEM dataset and report superior results against the existing models. Moreover, we perform transfer learning experiments on SALICON, a recently proposed static saliency dataset, by finetuning our models on the optical flows estimated from static images. Our experiments show that taking motion into account in this way can be helpful for static saliency estimation. ","Останні дослідження в спільноті комп’ютерного бачення зосереджені на оцінці візуальної помітності зображень. Однак прогнозування помітності у відео не було широко досліджено. У цій роботі розглядаються дві двопотокові згорткові мережі для динамічного прогнозування помітності, використовуючи нову техніку доповнення даних для покращення можливостей узагальнення моделей. Тести на наборі даних DIEM показують перевагу моделей над існуючими. Крім того, експерименти з перенесенням навчання на наборі даних статичної помітності SALICON з моделями, точно налаштованими на оптичних потоках, оцінених зі статичних зображень, демонструють, що врахування руху є перевагою для оцінки статичної помітності.",1
"В останні роки оцінка візуальної помітності зображень привернула велику увагу в спільноті комп’ютерного зору. Однак прогнозуванню помітності у відео приділено відносно мало уваги. Натхненні нещодавнім успіхом глибоких згорткових нейронних мереж на основі статичних моделей помітності, у цій роботі ми вивчаємо дві різні двопотокові згорткові мережі для динамічного прогнозування помітності. Щоб покращити можливості узагальнення наших моделей, ми також запровадили нову, емпірично обґрунтовану техніку збільшення даних для цього завдання. Ми перевіряємо наші моделі на наборі даних DIEM і повідомляємо про кращі результати порівняно з існуючими моделями. Крім того, ми проводимо експерименти з навчанням переносу на SALICON, нещодавно запропонованому наборі даних статичної помітності, шляхом точного налаштування наших моделей на оптичних потоках, оцінених за статичними зображеннями. Наші експерименти показують, що врахування руху таким чином може бути корисним для оцінки статичної помітності.","Recent research in the computer vision community has focused on estimating visual saliency in images. However, predicting saliency in videos has not been widely explored. This work examines two two-stream convolutional networks for dynamic saliency prediction, utilizing a novel data augmentation technique to enhance the models' generalization capability. Tests on the DIEM dataset reveal the models' superiority over existing models. Additionally, transfer learning experiments on the static saliency dataset SALICON, with the models fine-tuned on optical flows estimated from static images, demonstrate that motion consideration is advantageous for static saliency estimation.",1
"В останні роки оцінка візуальної помітності зображень привернула велику увагу в спільноті комп’ютерного зору. Однак прогнозуванню помітності у відео приділено відносно мало уваги. Натхненні нещодавнім успіхом глибоких згорткових нейронних мереж на основі статичних моделей помітності, у цій роботі ми вивчаємо дві різні двопотокові згорткові мережі для динамічного прогнозування помітності. Щоб покращити можливості узагальнення наших моделей, ми також запровадили нову, емпірично обґрунтовану техніку збільшення даних для цього завдання. Ми перевіряємо наші моделі на наборі даних DIEM і повідомляємо про кращі результати порівняно з існуючими моделями. Крім того, ми проводимо експерименти з навчанням переносу на SALICON, нещодавно запропонованому наборі даних статичної помітності, шляхом точного налаштування наших моделей на оптичних потоках, оцінених за статичними зображеннями. Наші експерименти показують, що врахування руху таким чином може бути корисним для оцінки статичної помітності.","Останні дослідження в спільноті комп’ютерного бачення зосереджені на оцінці візуальної помітності зображень. Однак прогнозування помітності у відео не було широко досліджено. У цій роботі розглядаються дві двопотокові згорткові мережі для динамічного прогнозування помітності, використовуючи нову техніку доповнення даних для покращення можливостей узагальнення моделей. Тести на наборі даних DIEM показують перевагу моделей над існуючими. Крім того, експерименти з перенесенням навчання на наборі даних статичної помітності SALICON з моделями, точно налаштованими на оптичних потоках, оцінених зі статичних зображень, демонструють, що врахування руху є перевагою для оцінки статичної помітності.",1
"Correlation filters have recently attracted attention in visual tracking due to their efficiency and high per- formance. However, their application to long-term tracking is somewhat limited since these trackers are not equipped with mechanisms to cope with challenging cases like partial occlusion, deformation or scale changes. In this paper, we propose a deformable part-based correlation filter tracking approach which depends on coupled interactions between a global filter and several part filters. Specifically, local filters provide an initial estimate, which is then used by the global filter as a reference to determine the final result. Then, the global filter provides a feedback to the part filters regarding their updates and the related deformation parameters. In this way, our proposed collaborative model handles not only partial occlusion but also scale changes. Experiments on two large public benchmark datasets demonstrate that our approach gives significantly better results compared with the state-of-the-art trackers.","Correlation filters, due to their efficiency and high performance, have recently gained attention in visual tracking. However, they are not suitable for long-term tracking, as they lack mechanisms to handle challenging cases such as partial occlusion, deformation or scale changes. To address this, we present a deformable part-based correlation filter tracking approach which relies on interactions between a global filter and several part filters. Local filters first provide an initial estimate which is used by the global filter to compute the final result. The global filter then provides feedback to the part filters on their updates and deformation parameters. This collaborative model is able to cope with both partial occlusion and scale changes. Experiments on two large public benchmark datasets show that our approach achieves significantly better results compared to the state-of-the-art trackers.",1
"Correlation filters have recently attracted attention in visual tracking due to their efficiency and high per- formance. However, their application to long-term tracking is somewhat limited since these trackers are not equipped with mechanisms to cope with challenging cases like partial occlusion, deformation or scale changes. In this paper, we propose a deformable part-based correlation filter tracking approach which depends on coupled interactions between a global filter and several part filters. Specifically, local filters provide an initial estimate, which is then used by the global filter as a reference to determine the final result. Then, the global filter provides a feedback to the part filters regarding their updates and the related deformation parameters. In this way, our proposed collaborative model handles not only partial occlusion but also scale changes. Experiments on two large public benchmark datasets demonstrate that our approach gives significantly better results compared with the state-of-the-art trackers.","Кореляційні фільтри завдяки своїй ефективності та високій продуктивності нещодавно привернули увагу у візуальному відстеженні. Однак вони не підходять для тривалого відстеження, оскільки їм бракує механізмів для обробки складних випадків, таких як часткова оклюзія, деформація або зміни масштабу. Щоб вирішити цю проблему, ми представляємо підхід відстеження кореляційного фільтра на основі деформованих частин, який ґрунтується на взаємодії між глобальним фільтром і декількома фільтрами частин. Локальні фільтри спочатку надають початкову оцінку, яка використовується глобальним фільтром для обчислення кінцевого результату. Глобальний фільтр потім надає зворотний зв’язок фільтрам частин щодо їхніх оновлень і параметрів деформації. Ця спільна модель здатна впоратися як з частковою оклюзією, так і зі зміною масштабу. Експерименти з двома великими публічними контрольними наборами даних показують, що наш підхід дає значно кращі результати порівняно з найсучаснішими трекерами.",1
"Кореляційні фільтри нещодавно привернули увагу у візуальному відстеженні завдяки своїй ефективності та високій продуктивності. Однак їхнє застосування для тривалого відстеження дещо обмежене, оскільки ці трекери не оснащені механізмами, щоб впоратися зі складними випадками, такими як часткова оклюзія, деформація або зміни масштабу. У цій статті ми пропонуємо підхід відстеження кореляційного фільтра на основі деформованих частин, який залежить від зв’язаних взаємодій між глобальним фільтром і декількома фільтрами частин. Зокрема, локальні фільтри забезпечують початкову оцінку, яка потім використовується глобальним фільтром як еталон для визначення кінцевого результату. Потім глобальний фільтр надає зворотний зв’язок фільтрам частин щодо їх оновлень і відповідних параметрів деформації. Таким чином, наша запропонована спільна модель обробляє не лише часткову оклюзію, але й зміни масштабу. Експерименти на двох великих загальнодоступних наборах даних показують, що наш підхід дає значно кращі результати порівняно з найсучаснішими трекерами.","Correlation filters, due to their efficiency and high performance, have recently gained attention in visual tracking. However, they are not suitable for long-term tracking, as they lack mechanisms to handle challenging cases such as partial occlusion, deformation or scale changes. To address this, we present a deformable part-based correlation filter tracking approach which relies on interactions between a global filter and several part filters. Local filters first provide an initial estimate which is used by the global filter to compute the final result. The global filter then provides feedback to the part filters on their updates and deformation parameters. This collaborative model is able to cope with both partial occlusion and scale changes. Experiments on two large public benchmark datasets show that our approach achieves significantly better results compared to the state-of-the-art trackers.",1
"Кореляційні фільтри нещодавно привернули увагу у візуальному відстеженні завдяки своїй ефективності та високій продуктивності. Однак їхнє застосування для тривалого відстеження дещо обмежене, оскільки ці трекери не оснащені механізмами, щоб впоратися зі складними випадками, такими як часткова оклюзія, деформація або зміни масштабу. У цій статті ми пропонуємо підхід відстеження кореляційного фільтра на основі деформованих частин, який залежить від зв’язаних взаємодій між глобальним фільтром і декількома фільтрами частин. Зокрема, локальні фільтри забезпечують початкову оцінку, яка потім використовується глобальним фільтром як еталон для визначення кінцевого результату. Потім глобальний фільтр надає зворотний зв’язок фільтрам частин щодо їх оновлень і відповідних параметрів деформації. Таким чином, наша запропонована спільна модель обробляє не лише часткову оклюзію, але й зміни масштабу. Експерименти на двох великих загальнодоступних наборах даних показують, що наш підхід дає значно кращі результати порівняно з найсучаснішими трекерами.","Кореляційні фільтри завдяки своїй ефективності та високій продуктивності нещодавно привернули увагу у візуальному відстеженні. Однак вони не підходять для тривалого відстеження, оскільки їм бракує механізмів для обробки складних випадків, таких як часткова оклюзія, деформація або зміни масштабу. Щоб вирішити цю проблему, ми представляємо підхід відстеження кореляційного фільтра на основі деформованих частин, який ґрунтується на взаємодії між глобальним фільтром і декількома фільтрами частин. Локальні фільтри спочатку надають початкову оцінку, яка використовується глобальним фільтром для обчислення кінцевого результату. Глобальний фільтр потім надає зворотний зв’язок фільтрам частин щодо їхніх оновлень і параметрів деформації. Ця спільна модель здатна впоратися як з частковою оклюзією, так і зі зміною масштабу. Експерименти з двома великими публічними контрольними наборами даних показують, що наш підхід дає значно кращі результати порівняно з найсучаснішими трекерами.",1
"Reconstructing high dynamic range (HDR) images of a complex scene involving moving objects and dynamic backgrounds is prone to artifacts. A large number of methods have been proposed that attempt to alleviate these artifacts, known as HDR deghosting algorithms. Currently, the quality of these algorithms are judged by subjective evaluations, which are tedious to conduct and get quickly outdated as new algorithms are proposed on a rapid basis. In this paper, we propose an objective metric which aims to simplify this process. Our metric takes a stack of input exposures and the deghosting result and produces a set of artifact maps for different types of artifacts. These artifact maps can be combined to yield a single quality score. We performed a subjective experiment involving 52 subjects and 16 different scenes to validate the agreement of our quality scores with subjective judgements and observed a concordance of almost 80%. Our metric also enables a novel application that we call as hybrid deghosting, in which the output of different deghosting algorithms are combined to obtain a superior deghosting result.","The reconstruction of HDR images of a complex scene with moving objects and dynamic backgrounds can cause artifacts. Numerous methods, known as HDR deghosting algorithms, have been developed to reduce these artifacts. Subjective evaluations are used to assess the quality of these algorithms, but this process is time-consuming and not up-to-date with newer algorithms. We present an objective metric that simplifies this process by creating artifact maps from input exposures and the deghosting result, which can be combined to form a single quality score. To validate our score's agreement with subjective judgements, we conducted a subjective experiment with 52 subjects and 16 scenes and obtained a concordance of nearly 80%. Our metric also enables a novel application, hybrid deghosting, which combines the output of multiple deghosting algorithms to create a better deghosting result.",1
"Reconstructing high dynamic range (HDR) images of a complex scene involving moving objects and dynamic backgrounds is prone to artifacts. A large number of methods have been proposed that attempt to alleviate these artifacts, known as HDR deghosting algorithms. Currently, the quality of these algorithms are judged by subjective evaluations, which are tedious to conduct and get quickly outdated as new algorithms are proposed on a rapid basis. In this paper, we propose an objective metric which aims to simplify this process. Our metric takes a stack of input exposures and the deghosting result and produces a set of artifact maps for different types of artifacts. These artifact maps can be combined to yield a single quality score. We performed a subjective experiment involving 52 subjects and 16 different scenes to validate the agreement of our quality scores with subjective judgements and observed a concordance of almost 80%. Our metric also enables a novel application that we call as hybrid deghosting, in which the output of different deghosting algorithms are combined to obtain a superior deghosting result.","Реконструкція HDR-зображень складної сцени з рухомими об’єктами та динамічним фоном може спричинити артефакти. Щоб зменшити ці артефакти, було розроблено численні методи, відомі як алгоритми видалення фантом HDR. Для оцінки якості цих алгоритмів використовуються суб’єктивні оцінки, але цей процес займає багато часу та не відповідає новим алгоритмам. Ми представляємо об’єктивну метрику, яка спрощує цей процес, створюючи карти артефактів із вхідних експозицій і результатів видалення фантомних зображень, які можна об’єднати, щоб сформувати єдиний показник якості. Щоб підтвердити узгодженість нашої оцінки з суб’єктивними судженнями, ми провели суб’єктивний експеримент із 52 суб’єктами та 16 сценами та отримали відповідність майже на 80%. Наша метрика також дає змогу використовувати нову програму, гібридне видалення фантомних зображень, яке поєднує результати кількох алгоритмів видалення фантомних зображень для створення кращого результату видалення фантомних зображень.",1
"Реконструкція зображень із широким динамічним діапазоном (HDR) складної сцени з рухомими об’єктами та динамічним фоном схильна до артефактів. Було запропоновано велику кількість методів, які намагаються усунути ці артефакти, відомих як алгоритми видалення фантом HDR. Наразі якість цих алгоритмів оцінюється за суб’єктивними оцінками, які нудно проводити та швидко застарівають, оскільки нові алгоритми пропонуються швидко. У цій статті ми пропонуємо об’єктивну метрику, яка має на меті спростити цей процес. Наша метрика бере набір вхідних експозицій і результат видалення фантомних зображень і створює набір карт артефактів для різних типів артефактів. Ці карти артефактів можна поєднувати, щоб отримати єдиний показник якості. Ми провели суб’єктивний експеримент із залученням 52 суб’єктів і 16 різних сцен, щоб перевірити узгодженість наших показників якості з суб’єктивними судженнями та спостерігали узгодженість майже на 80%. Наша метрика також дозволяє нову програму, яку ми називаємо гібридним видаленням фантомних зображень, у якому вихідні дані різних алгоритмів видалення фантомних зображень поєднуються для отримання чудового результату видалення фантомних зображень.","The reconstruction of HDR images of a complex scene with moving objects and dynamic backgrounds can cause artifacts. Numerous methods, known as HDR deghosting algorithms, have been developed to reduce these artifacts. Subjective evaluations are used to assess the quality of these algorithms, but this process is time-consuming and not up-to-date with newer algorithms. We present an objective metric that simplifies this process by creating artifact maps from input exposures and the deghosting result, which can be combined to form a single quality score. To validate our score's agreement with subjective judgements, we conducted a subjective experiment with 52 subjects and 16 scenes and obtained a concordance of nearly 80%. Our metric also enables a novel application, hybrid deghosting, which combines the output of multiple deghosting algorithms to create a better deghosting result.",1
"Реконструкція зображень із широким динамічним діапазоном (HDR) складної сцени з рухомими об’єктами та динамічним фоном схильна до артефактів. Було запропоновано велику кількість методів, які намагаються усунути ці артефакти, відомих як алгоритми видалення фантом HDR. Наразі якість цих алгоритмів оцінюється за суб’єктивними оцінками, які нудно проводити та швидко застарівають, оскільки нові алгоритми пропонуються швидко. У цій статті ми пропонуємо об’єктивну метрику, яка має на меті спростити цей процес. Наша метрика бере набір вхідних експозицій і результат видалення фантомних зображень і створює набір карт артефактів для різних типів артефактів. Ці карти артефактів можна поєднувати, щоб отримати єдиний показник якості. Ми провели суб’єктивний експеримент із залученням 52 суб’єктів і 16 різних сцен, щоб перевірити узгодженість наших показників якості з суб’єктивними судженнями та спостерігали узгодженість майже на 80%. Наша метрика також дозволяє нову програму, яку ми називаємо гібридним видаленням фантомних зображень, у якому вихідні дані різних алгоритмів видалення фантомних зображень поєднуються для отримання чудового результату видалення фантомних зображень.","Реконструкція HDR-зображень складної сцени з рухомими об’єктами та динамічним фоном може спричинити артефакти. Щоб зменшити ці артефакти, було розроблено численні методи, відомі як алгоритми видалення фантом HDR. Для оцінки якості цих алгоритмів використовуються суб’єктивні оцінки, але цей процес займає багато часу та не відповідає новим алгоритмам. Ми представляємо об’єктивну метрику, яка спрощує цей процес, створюючи карти артефактів із вхідних експозицій і результатів видалення фантомних зображень, які можна об’єднати, щоб сформувати єдиний показник якості. Щоб підтвердити узгодженість нашої оцінки з суб’єктивними судженнями, ми провели суб’єктивний експеримент із 52 суб’єктами та 16 сценами та отримали відповідність майже на 80%. Наша метрика також дає змогу використовувати нову програму, гібридне видалення фантомних зображень, яке поєднує результати кількох алгоритмів видалення фантомних зображень для створення кращого результату видалення фантомних зображень.",1
"Automatic description generation from natural images is a challenging problem thathas recently received a large amount of interest from the computer vision and natural lan-guage processing communities.  In this survey,  we classify the existing approaches basedon how they conceptualize this problem, viz., models that cast description as either gen-eration  problem  or  as  a  retrieval  problem  over  a  visual  or  multimodal  representationalspace.  We provide a detailed review of existing models, highlighting their advantages anddisadvantages.  Moreover, we give an overview of the benchmark image datasets and theevaluation measures that have been developed to assess the quality of machine-generatedimage descriptions.  Finally we extrapolate future directions in the area of automatic imagedescription generation.","Generating descriptions of natural images is a difficult task that has recently sparked a lot of interest among computer vision and natural language processing researchers. In this review, we categorize existing approaches based on how they view the problem, i.e. models that see description generation as a problem of either generation or retrieval in a visual or multimodal representation space. We analyze the benefits and drawbacks of current models, provide an overview of benchmark image datasets and evaluation measures for assessing the quality of machine-generated image descriptions, and discuss potential future directions for automatic image description generation.",1
"Automatic description generation from natural images is a challenging problem thathas recently received a large amount of interest from the computer vision and natural lan-guage processing communities.  In this survey,  we classify the existing approaches basedon how they conceptualize this problem, viz., models that cast description as either gen-eration  problem  or  as  a  retrieval  problem  over  a  visual  or  multimodal  representationalspace.  We provide a detailed review of existing models, highlighting their advantages anddisadvantages.  Moreover, we give an overview of the benchmark image datasets and theevaluation measures that have been developed to assess the quality of machine-generatedimage descriptions.  Finally we extrapolate future directions in the area of automatic imagedescription generation.","Створення описів природних зображень є складним завданням, яке нещодавно викликало великий інтерес серед дослідників комп’ютерного зору та обробки природної мови. У цьому огляді ми класифікуємо існуючі підходи на основі того, як вони бачать проблему, тобто моделі, які розглядають генерацію опису як проблему генерації або пошуку у візуальному чи мультимодальному просторі представлення. Ми аналізуємо переваги та недоліки поточних моделей, надаємо огляд еталонних наборів даних зображень і заходи оцінки для оцінки якості машинно створених описів зображень, а також обговорюємо потенційні майбутні напрямки для автоматичного створення описів зображень.",1
"Автоматичне створення опису з природних зображень є складною проблемою, яка нещодавно привернула великий інтерес у спільнот комп’ютерного зору та обробки природної мови. У цьому дослідженні ми класифікуємо існуючі підходи на основі того, як вони концептуалізують цю проблему, а саме моделі, які перетворюють опис як проблему генерації або як проблему пошуку у візуальному чи мультимодальному репрезентативному просторі. Ми надаємо детальний огляд існуючих моделей, висвітлюючи їх переваги та недоліки. Крім того, ми надаємо огляд еталонних наборів даних зображень і заходів оцінки, які були розроблені для оцінки якості описів зображень, створених машиною. Нарешті ми екстраполюємо майбутні напрямки в області автоматичного створення опису зображення.","Generating descriptions of natural images is a difficult task that has recently sparked a lot of interest among computer vision and natural language processing researchers. In this review, we categorize existing approaches based on how they view the problem, i.e. models that see description generation as a problem of either generation or retrieval in a visual or multimodal representation space. We analyze the benefits and drawbacks of current models, provide an overview of benchmark image datasets and evaluation measures for assessing the quality of machine-generated image descriptions, and discuss potential future directions for automatic image description generation.",1
"Автоматичне створення опису з природних зображень є складною проблемою, яка нещодавно привернула великий інтерес у спільнот комп’ютерного зору та обробки природної мови. У цьому дослідженні ми класифікуємо існуючі підходи на основі того, як вони концептуалізують цю проблему, а саме моделі, які перетворюють опис як проблему генерації або як проблему пошуку у візуальному чи мультимодальному репрезентативному просторі. Ми надаємо детальний огляд існуючих моделей, висвітлюючи їх переваги та недоліки. Крім того, ми надаємо огляд еталонних наборів даних зображень і заходів оцінки, які були розроблені для оцінки якості описів зображень, створених машиною. Нарешті ми екстраполюємо майбутні напрямки в області автоматичного створення опису зображення.","Створення описів природних зображень є складним завданням, яке нещодавно викликало великий інтерес серед дослідників комп’ютерного зору та обробки природної мови. У цьому огляді ми класифікуємо існуючі підходи на основі того, як вони бачать проблему, тобто моделі, які розглядають генерацію опису як проблему генерації або пошуку у візуальному чи мультимодальному просторі представлення. Ми аналізуємо переваги та недоліки поточних моделей, надаємо огляд еталонних наборів даних зображень і заходи оцінки для оцінки якості машинно створених описів зображень, а також обговорюємо потенційні майбутні напрямки для автоматичного створення описів зображень.",1
"In daily life, humans demonstrate an amazing ability to remember images they see on magazines, commercials, TV, web pages, etc. but automatic prediction of intrinsic memorability of images using computer vision and machine learning techniques has only been investigated very recently. Our goal in this article is to explore the role of visual attention and image semantics in understanding image memorability. In particular, we present an attention-driven spatial pooling strategy and show that considering image features from the salient parts of images improves the results of the previous models. We also investigate different semantic properties of images by carrying out an analysis of a diverse set of recently proposed semantic features which encode meta-level object categories, scene attributes, and invoked feelings. We show that these features which are automatically extracted from images provide memorability predictions as nearly accurate as those derived from human anno- tations. Moreover, our combined model yields results superior to those of state-of-the art fully automatic models.","Humans display remarkable capability to recall images they observe in magazines, commercials, TV, web pages, etc. However, utilization of computer vision and machine learning to automatically forecast intrinsic memorability of images has only recently been studied. Here, we analyze the role of visual attention and image semantics in comprehending image memorability. We propose an attention-driven spatial pooling approach and find that incorporating image features from salient parts of images enhances the results of earlier models. We also examine diverse semantic properties of images through an analysis of semantic features which encode meta-level object categories, scene attributes, and invoked feelings. Results show that these features, extracted automatically from images, provide memorability predictions as accurate as those derived from human annotations. Furthermore, our combined model produces results that surpass state-of-the art fully automatic models.",1
"In daily life, humans demonstrate an amazing ability to remember images they see on magazines, commercials, TV, web pages, etc. but automatic prediction of intrinsic memorability of images using computer vision and machine learning techniques has only been investigated very recently. Our goal in this article is to explore the role of visual attention and image semantics in understanding image memorability. In particular, we present an attention-driven spatial pooling strategy and show that considering image features from the salient parts of images improves the results of the previous models. We also investigate different semantic properties of images by carrying out an analysis of a diverse set of recently proposed semantic features which encode meta-level object categories, scene attributes, and invoked feelings. We show that these features which are automatically extracted from images provide memorability predictions as nearly accurate as those derived from human anno- tations. Moreover, our combined model yields results superior to those of state-of-the art fully automatic models.","Люди виявляють надзвичайну здатність запам’ятовувати зображення, які вони спостерігали в журналах, рекламі, на телебаченні, на веб-сторінках тощо. Однак використання комп’ютерного зору та машинного навчання для автоматичного прогнозування внутрішньої запам’ятовуваності зображень було вивчено лише нещодавно. Тут ми аналізуємо роль візуальної уваги та семантики зображення в розумінні запам’ятовуваності зображення. Ми пропонуємо орієнтований на увагу підхід до просторового об’єднання та виявили, що включення характеристик зображення з помітних частин зображення покращує результати попередніх моделей. Ми також вивчаємо різноманітні семантичні властивості зображень за допомогою аналізу семантичних особливостей, які кодують категорії об’єктів метарівня, атрибути сцени та викликані почуття. Результати показують, що ці функції, автоматично витягнуті із зображень, забезпечують прогнози щодо запам’ятовування так само точні, як і ті, що отримані з анотацій людини. Крім того, наша комбінована модель дає результати, які перевершують сучасні повністю автоматичні моделі.",1
"У повсякденному житті люди демонструють дивовижну здатність запам’ятовувати зображення, які вони бачать у журналах, рекламі, на телебаченні, на веб-сторінках тощо, але автоматичне передбачення внутрішньої запам’ятовуваності зображень за допомогою комп’ютерного зору та методів машинного навчання було досліджено зовсім недавно. Наша мета в цій статті — дослідити роль візуальної уваги та семантики зображення в розумінні запам’ятовуваності зображення. Зокрема, ми представляємо орієнтовану на увагу стратегію просторового об’єднання та показуємо, що врахування особливостей зображень із помітних частин зображень покращує результати попередніх моделей. Ми також досліджуємо різні семантичні властивості зображень, проводячи аналіз різноманітного набору нещодавно запропонованих семантичних особливостей, які кодують категорії об’єктів метарівня, атрибути сцени та викликані почуття. Ми показуємо, що ці функції, які автоматично витягуються із зображень, забезпечують прогнози запам’ятовуваності настільки ж точні, як і ті, що отримані з анотацій людини. Крім того, наша комбінована модель дає результати, які перевершують результати сучасних повністю автоматичних моделей.","Humans display remarkable capability to recall images they observe in magazines, commercials, TV, web pages, etc. However, utilization of computer vision and machine learning to automatically forecast intrinsic memorability of images has only recently been studied. Here, we analyze the role of visual attention and image semantics in comprehending image memorability. We propose an attention-driven spatial pooling approach and find that incorporating image features from salient parts of images enhances the results of earlier models. We also examine diverse semantic properties of images through an analysis of semantic features which encode meta-level object categories, scene attributes, and invoked feelings. Results show that these features, extracted automatically from images, provide memorability predictions as accurate as those derived from human annotations. Furthermore, our combined model produces results that surpass state-of-the art fully automatic models.",1
"У повсякденному житті люди демонструють дивовижну здатність запам’ятовувати зображення, які вони бачать у журналах, рекламі, на телебаченні, на веб-сторінках тощо, але автоматичне передбачення внутрішньої запам’ятовуваності зображень за допомогою комп’ютерного зору та методів машинного навчання було досліджено зовсім недавно. Наша мета в цій статті — дослідити роль візуальної уваги та семантики зображення в розумінні запам’ятовуваності зображення. Зокрема, ми представляємо орієнтовану на увагу стратегію просторового об’єднання та показуємо, що врахування особливостей зображень із помітних частин зображень покращує результати попередніх моделей. Ми також досліджуємо різні семантичні властивості зображень, проводячи аналіз різноманітного набору нещодавно запропонованих семантичних особливостей, які кодують категорії об’єктів метарівня, атрибути сцени та викликані почуття. Ми показуємо, що ці функції, які автоматично витягуються із зображень, забезпечують прогнози запам’ятовуваності настільки ж точні, як і ті, що отримані з анотацій людини. Крім того, наша комбінована модель дає результати, які перевершують результати сучасних повністю автоматичних моделей.","Люди виявляють надзвичайну здатність запам’ятовувати зображення, які вони спостерігали в журналах, рекламі, на телебаченні, на веб-сторінках тощо. Однак використання комп’ютерного зору та машинного навчання для автоматичного прогнозування внутрішньої запам’ятовуваності зображень було вивчено лише нещодавно. Тут ми аналізуємо роль візуальної уваги та семантики зображення в розумінні запам’ятовуваності зображення. Ми пропонуємо орієнтований на увагу підхід до просторового об’єднання та виявили, що включення характеристик зображення з помітних частин зображення покращує результати попередніх моделей. Ми також вивчаємо різноманітні семантичні властивості зображень за допомогою аналізу семантичних особливостей, які кодують категорії об’єктів метарівня, атрибути сцени та викликані почуття. Результати показують, що ці функції, автоматично витягнуті із зображень, забезпечують прогнози щодо запам’ятовування так само точні, як і ті, що отримані з анотацій людини. Крім того, наша комбінована модель дає результати, які перевершують сучасні повністю автоматичні моделі.",1
"In this paper, we propose a novel query expansion approach for improving transferbased automatic image captioning. The core idea of our method is to translate the given visual query into a distributional semantics based form, which is generated by the average of the sentence vectors extracted from the captions of images visually similar to the input image. Using three image captioning benchmark datasets, we show that our approach provides more accurate results compared to the state-of-theart data-driven methods in terms of both automatic metrics and subjective evaluation.","A new query expansion strategy is proposed to enhance transfer-based image captioning. This method translates the visual query into a distributional semantics form, created by the mean of sentence vectors obtained from captions of images similar to the input image. Through 3 benchmark datasets, it is demonstrated that this technique surpasses current data-driven methods in terms of both automated metrics and subjective evaluation.",1
"In this paper, we propose a novel query expansion approach for improving transferbased automatic image captioning. The core idea of our method is to translate the given visual query into a distributional semantics based form, which is generated by the average of the sentence vectors extracted from the captions of images visually similar to the input image. Using three image captioning benchmark datasets, we show that our approach provides more accurate results compared to the state-of-theart data-driven methods in terms of both automatic metrics and subjective evaluation.","Пропонується нова стратегія розширення запитів для покращення субтитрів до зображень на основі передачі. Цей метод переводить візуальний запит у форму розподільної семантики, створену за допомогою векторів речень, отриманих із підписів зображень, подібних до вхідного зображення. За допомогою 3 контрольних наборів даних було продемонстровано, що ця техніка перевершує поточні методи, керовані даними, з точки зору як автоматизованих показників, так і суб’єктивної оцінки.",1
"У цьому документі ми пропонуємо новий підхід до розширення запиту для покращення автоматичного створення підписів до зображень на основі передачі. Основна ідея нашого методу полягає в тому, щоб перевести заданий візуальний запит у форму, засновану на семантиці розподілу, яка генерується середнім значенням векторів речень, витягнутих із підписів зображень, візуально схожих на вхідне зображення. Використовуючи три контрольних набори даних субтитрів до зображень, ми показуємо, що наш підхід дає точніші результати порівняно з сучасними методами, що керуються даними, з точки зору як автоматичних показників, так і суб’єктивної оцінки.","A new query expansion strategy is proposed to enhance transfer-based image captioning. This method translates the visual query into a distributional semantics form, created by the mean of sentence vectors obtained from captions of images similar to the input image. Through 3 benchmark datasets, it is demonstrated that this technique surpasses current data-driven methods in terms of both automated metrics and subjective evaluation.",1
"У цьому документі ми пропонуємо новий підхід розширення запиту для покращення автоматичного створення підписів до зображень на основі передачі. Основна ідея нашого методу полягає в тому, щоб перевести заданий візуальний запит у форму, засновану на семантиці розподілу, яка генерується середнім значенням векторів речень, витягнутих із підписів зображень, візуально схожих на вхідне зображення. Використовуючи три контрольних набори даних субтитрів до зображень, ми показуємо, що наш підхід забезпечує більш точні результати порівняно з сучасними методами, що керуються даними, з точки зору як автоматичних показників, так і суб’єктивної оцінки.","Пропонується нова стратегія розширення запитів для покращення субтитрів зображень на основі передачі. Цей метод переводить візуальний запит у форму розподільної семантики, створену за допомогою векторів речень, отриманих із підписів зображень, подібних до вхідного зображення. За допомогою 3 контрольних наборів даних було продемонстровано, що ця техніка перевершує поточні методи, керовані даними, з точки зору як автоматизованих показників, так і суб’єктивної оцінки.",1
"Obtaining a high quality high dynamic range (HDR) image in the presence of camera and object movement has been a long-standing challenge. Many methods, known as HDR deghosting algorithms, have been developed over the past ten years to undertake this challenge. Each of these algorithms approaches the deghosting problem from a different perspective, providing solutions with different degrees of complexity, solutions that range from rudimentary heuristics to advanced computer vision techniques. The proposed solutions generally differ in two ways: (1) how to detect ghost regions and (2) what to do to eliminate ghosts. Some algorithms choose to completely discard moving objects giving rise to HDR images which only contain the static regions. Some other algorithms try to find the best image to use for each dynamic region. Yet others try to register moving objects from different images in the spirit of maximizing dynamic range in dynamic regions. Furthermore, each algorithm may introduce different types of artifacts as they aim to eliminate ghosts. These artifacts may come in the form of noise, broken objects, under- and over-exposed regions, and residual ghosting. Given the high volume of studies conducted in this field over the recent years, a comprehensive survey of the state of the art is required. Thus, the first goal of this paper is to provide this survey. Secondly, the large number of algorithms brings about the need to classify them. Thus the second goal of this paper is to propose a taxonomy of deghosting algorithms which can be used to group existing and future algorithms into meaningful classes. Thirdly, the existence of a large number of algorithms brings about the need to evaluate their effectiveness, as each new algorithm claims to outperform its precedents. Therefore, the last goal of this paper is to share the results of a subjective experiment which aims to evaluate various state-of-the-art deghosting algorithms.","The difficulty of obtaining a high quality, high dynamic range (HDR) image when camera and object movement is present has been a long-standing issue. To tackle this challenge, numerous HDR deghosting algorithms have been created over the past decade. Each of these algorithms looks at the deghosting problem from a different point of view, providing solutions with various levels of complexity, from simple heuristics to advanced computer vision techniques. The solutions vary in two ways: (1) how to detect ghost regions and (2) what to do to get rid of ghosts. Some algorithms opt to remove moving objects, producing HDR images that only feature static regions. Others try to identify the best image to use for each dynamic region, while others try to register moving objects from various images in order to maximize dynamic range in dynamic regions. Additionally, each algorithm may create various types of artifacts as they work to eliminate ghosts, such as noise, broken objects, under- and over-exposed regions, and residual ghosting. Due to the high number of studies conducted in this field in recent years, an extensive survey of the state of the art is necessary. Therefore, the first goal of this paper is to provide such a survey. The second goal is to propose a taxonomy of deghosting algorithms which can be used to organize existing and future algorithms into meaningful classes. Lastly, the existence of a large number of algorithms leads to the need to assess their effectiveness, as each new algorithm claims to be better than its predecessors. Therefore, the last goal of this paper is to present the results of a subjective experiment that aims to evaluate different state-of-the-art deghosting algorithms.",1
"Obtaining a high quality high dynamic range (HDR) image in the presence of camera and object movement has been a long-standing challenge. Many methods, known as HDR deghosting algorithms, have been developed over the past ten years to undertake this challenge. Each of these algorithms approaches the deghosting problem from a different perspective, providing solutions with different degrees of complexity, solutions that range from rudimentary heuristics to advanced computer vision techniques. The proposed solutions generally differ in two ways: (1) how to detect ghost regions and (2) what to do to eliminate ghosts. Some algorithms choose to completely discard moving objects giving rise to HDR images which only contain the static regions. Some other algorithms try to find the best image to use for each dynamic region. Yet others try to register moving objects from different images in the spirit of maximizing dynamic range in dynamic regions. Furthermore, each algorithm may introduce different types of artifacts as they aim to eliminate ghosts. These artifacts may come in the form of noise, broken objects, under- and over-exposed regions, and residual ghosting. Given the high volume of studies conducted in this field over the recent years, a comprehensive survey of the state of the art is required. Thus, the first goal of this paper is to provide this survey. Secondly, the large number of algorithms brings about the need to classify them. Thus the second goal of this paper is to propose a taxonomy of deghosting algorithms which can be used to group existing and future algorithms into meaningful classes. Thirdly, the existence of a large number of algorithms brings about the need to evaluate their effectiveness, as each new algorithm claims to outperform its precedents. Therefore, the last goal of this paper is to share the results of a subjective experiment which aims to evaluate various state-of-the-art deghosting algorithms.","Труднощі отримання високоякісного зображення з широким динамічним діапазоном (HDR) за наявності руху камери та об’єкта були проблемою, яка актуальна давно. Щоб впоратися з цією проблемою, за останнє десятиліття було створено численні алгоритми видалення фантом HDR. Кожен із цих алгоритмів розглядає проблему видалення фантом з іншої точки зору, забезпечуючи рішення з різними рівнями складності, від простих евристичних до розширених методів комп’ютерного зору. Рішення відрізняються двома способами: (1) як виявити області-привиди та (2) що робити, щоб позбутися привидів. Деякі алгоритми вирішують видалити рухомі об’єкти, створюючи HDR-зображення, які містять лише статичні області. Інші намагаються визначити найкраще зображення для кожної динамічної області, а інші намагаються зареєструвати рухомі об’єкти з різних зображень, щоб максимізувати динамічний діапазон у динамічних областях. Крім того, кожен алгоритм може створювати різні типи артефактів, коли вони працюють над усуненням привидів, як-от шум, зламані об’єкти, недо- та переекспоновані області та залишкові ореоли. У зв’язку з великою кількістю досліджень, проведених у цій галузі за останні роки, необхідний широкий огляд сучасного стану. Тому першою метою цієї роботи є проведення такого опитування. Друга мета полягає в тому, щоб запропонувати таксономію алгоритмів видалення фантомних зображень, які можна використовувати для організації існуючих і майбутніх алгоритмів у значущі класи. Нарешті, існування великої кількості алгоритмів призводить до необхідності оцінки їх ефективності, оскільки кожен новий алгоритм претендує на те, щоб бути кращим за своїх попередників. Тому останньою метою цієї статті є представлення результатів суб’єктивного експерименту, метою якого є оцінка різних найсучасніших алгоритмів видалення фантомних зображень.",1
"Отримання високоякісного зображення з широким динамічним діапазоном (HDR) за наявності камери та руху об’єктів було давньою проблемою. За останні десять років для вирішення цього завдання було розроблено багато методів, відомих як алгоритми видалення фантом HDR. Кожен із цих алгоритмів підходить до проблеми видалення фантом з іншої точки зору, надаючи рішення з різним ступенем складності, рішення, що варіюються від рудиментарної евристики до вдосконалених методів комп’ютерного зору. Пропоновані рішення зазвичай відрізняються двома способами: (1) як виявити привиди та (2) що робити, щоб усунути привиди. Деякі алгоритми вирішують повністю відкидати рухомі об’єкти, створюючи зображення HDR, які містять лише статичні області. Деякі інші алгоритми намагаються знайти найкраще зображення для використання для кожної динамічної області. Треті намагаються зареєструвати рухомі об’єкти з різних зображень у дусі максимізації динамічного діапазону в динамічних областях. Крім того, кожен алгоритм може вводити різні типи артефактів, оскільки вони спрямовані на усунення привидів. Ці артефакти можуть проявлятися у вигляді шуму, зламаних об’єктів, недо- та переекспонованих областей, а також залишкових ореолів. Враховуючи великий обсяг досліджень, проведених у цій галузі за останні роки, необхідний комплексний огляд сучасного стану. Таким чином, першою метою цієї статті є проведення цього опитування. По-друге, велика кількість алгоритмів викликає необхідність їх класифікації. Таким чином, друга мета цієї статті полягає в тому, щоб запропонувати таксономію алгоритмів видалення фантомних зображень, яка може бути використана для групування існуючих і майбутніх алгоритмів у значущі класи. По-третє, існування великої кількості алгоритмів викликає необхідність оцінки їх ефективності, оскільки кожен новий алгоритм претендує на перевершення своїх прецедентів. Тому останньою метою цієї статті є поділитися результатами суб’єктивного експерименту, який має на меті оцінити різні найсучасніші алгоритми видалення фантомних зображень.","The difficulty of obtaining a high quality, high dynamic range (HDR) image when camera and object movement is present has been a long-standing issue. To tackle this challenge, numerous HDR deghosting algorithms have been created over the past decade. Each of these algorithms looks at the deghosting problem from a different point of view, providing solutions with various levels of complexity, from simple heuristics to advanced computer vision techniques. The solutions vary in two ways: (1) how to detect ghost regions and (2) what to do to get rid of ghosts. Some algorithms opt to remove moving objects, producing HDR images that only feature static regions. Others try to identify the best image to use for each dynamic region, while others try to register moving objects from various images in order to maximize dynamic range in dynamic regions. Additionally, each algorithm may create various types of artifacts as they work to eliminate ghosts, such as noise, broken objects, under- and over-exposed regions, and residual ghosting. Due to the high number of studies conducted in this field in recent years, an extensive survey of the state of the art is necessary. Therefore, the first goal of this paper is to provide such a survey. The second goal is to propose a taxonomy of deghosting algorithms which can be used to organize existing and future algorithms into meaningful classes. Lastly, the existence of a large number of algorithms leads to the need to assess their effectiveness, as each new algorithm claims to be better than its predecessors. Therefore, the last goal of this paper is to present the results of a subjective experiment that aims to evaluate different state-of-the-art deghosting algorithms.",1
"Отримання високоякісного зображення з широким динамічним діапазоном (HDR) за наявності камери та руху об’єктів було давньою проблемою. За останні десять років було розроблено багато методів, відомих як алгоритми видалення фантом HDR, щоб вирішити цю проблему. Кожен із цих алгоритмів підходить до проблеми видалення фантом з іншої точки зору, надаючи рішення з різним ступенем складності, рішення, що варіюються від рудиментарної евристики до вдосконалених методів комп’ютерного зору. Пропоновані рішення зазвичай відрізняються двома способами: (1) як виявити привиди та (2) що робити, щоб усунути привиди. Деякі алгоритми вирішують повністю відкидати рухомі об’єкти, створюючи зображення HDR, які містять лише статичні області. Деякі інші алгоритми намагаються знайти найкраще зображення для використання для кожної динамічної області. Треті намагаються зареєструвати рухомі об’єкти з різних зображень у дусі максимізації динамічного діапазону в динамічних областях. Крім того, кожен алгоритм може вводити різні типи артефактів, оскільки вони спрямовані на усунення привидів. Ці артефакти можуть проявлятися у вигляді шуму, зламаних об’єктів, недо- та переекспонованих областей, а також залишкових ореолів. Враховуючи великий обсяг досліджень, проведених у цій галузі за останні роки, необхідний комплексний огляд сучасного стану. Таким чином, першою метою цієї статті є проведення цього опитування. По-друге, велика кількість алгоритмів викликає необхідність їх класифікації. Таким чином, друга мета цієї статті полягає в тому, щоб запропонувати таксономію алгоритмів видалення фантомних зображень, яка може бути використана для групування існуючих і майбутніх алгоритмів у значущі класи. По-третє, існування великої кількості алгоритмів викликає необхідність оцінки їх ефективності, оскільки кожен новий алгоритм претендує на перевершення своїх прецедентів. Тому останньою метою цієї статті є поділитися результатами суб’єктивного експерименту, який має на меті оцінити різні найсучасніші алгоритми видалення фантомних зображень.","Труднощі отримання високоякісного зображення з широким динамічним діапазоном (HDR) за наявності руху камери та об’єкта були проблемою, яка актуальна давно. Щоб впоратися з цією проблемою, протягом останнього десятиліття було створено численні алгоритми видалення фантом HDR. Кожен із цих алгоритмів розглядає проблему видалення фантом з іншої точки зору, забезпечуючи рішення з різними рівнями складності, від простих евристичних до розширених методів комп’ютерного зору. Рішення відрізняються двома способами: (1) як виявити області-привиди та (2) що робити, щоб позбутися привидів. Деякі алгоритми вирішують видалити рухомі об’єкти, створюючи HDR-зображення, які містять лише статичні області. Інші намагаються визначити найкраще зображення для кожної динамічної області, а інші намагаються зареєструвати рухомі об’єкти з різних зображень, щоб максимізувати динамічний діапазон у динамічних областях. Крім того, кожен алгоритм може створювати різні типи артефактів, коли вони працюють над усуненням привидів, як-от шум, зламані об’єкти, недо- та переекспоновані області та залишкові ореоли. У зв’язку з великою кількістю досліджень, проведених у цій галузі за останні роки, необхідний широкий огляд сучасного стану. Тому першою метою цієї роботи є проведення такого опитування. Друга мета полягає в тому, щоб запропонувати таксономію алгоритмів видалення фантомних зображень, які можна використовувати для організації існуючих і майбутніх алгоритмів у значущі класи. Нарешті, існування великої кількості алгоритмів призводить до необхідності оцінки їх ефективності, оскільки кожен новий алгоритм претендує на те, щоб бути кращим за своїх попередників. Тому останньою метою цієї статті є представлення результатів суб’єктивного експерименту, метою якого є оцінка різних найсучасніших алгоритмів видалення фантомних зображень.",1
"Predicting where a photo was taken is quite important and yet a challenging task for computer vision algorithms. Our motivation is to solve this difficult problem in a cityscale setting by employing a data-driven approach. In order to pursue this goal, we developed a fast and robust scene matching method that follows a coarse-to-fine strategy. In particular, we combine scene retrieval via global features and dense scene alignment and use a large set of geo-tagged images of downtown San Francisco in our evaluation. The experimental results show that the proposed approach, despite its simplicity, is surprisingly effective and achieves comparable results with the state-of-the-art.","Solving the difficult task of predicting a photo's location is a challenge for computer vision algorithms. To tackle this problem on a city-scale, we employed a data-driven approach and developed a fast and robust scene matching method with a coarse-to-fine strategy. We tested it on a large set of geo-tagged images from downtown San Francisco, and the results demonstrate that our approach is surprisingly effective, even with its simplicity, and yields comparable performance to the current state-of-the-art.",1
"Predicting where a photo was taken is quite important and yet a challenging task for computer vision algorithms. Our motivation is to solve this difficult problem in a cityscale setting by employing a data-driven approach. In order to pursue this goal, we developed a fast and robust scene matching method that follows a coarse-to-fine strategy. In particular, we combine scene retrieval via global features and dense scene alignment and use a large set of geo-tagged images of downtown San Francisco in our evaluation. The experimental results show that the proposed approach, despite its simplicity, is surprisingly effective and achieves comparable results with the state-of-the-art.","Вирішення складного завдання прогнозування розташування фотографії є проблемою для алгоритмів комп’ютерного зору. Щоб вирішити цю проблему в масштабі міста, ми застосували підхід, керований даними, і розробили швидкий і надійний метод зіставлення сцен із стратегією від грубого до точного. Ми протестували його на великому наборі зображень із геотегами в центрі Сан-Франциско, і результати показали, що наш підхід є напрочуд ефективним, навіть незважаючи на його простоту, і забезпечує порівняну продуктивність із поточним сучасним рівнем техніки.",1
"Передбачити, де було зроблено фотографію, є досить важливим і водночас складним завданням для алгоритмів комп’ютерного зору. Наша мотивація полягає в тому, щоб вирішити цю складну проблему в умовах міста за допомогою підходу, керованого даними. Щоб досягти цієї мети, ми розробили швидкий і надійний метод зіставлення сцен, який дотримується стратегії від грубого до точного. Зокрема, ми поєднуємо пошук сцени за допомогою глобальних функцій і щільного вирівнювання сцени та використовуємо великий набір зображень центру Сан-Франциско з геотегами в нашій оцінці. Експериментальні результати показують, що запропонований підхід, незважаючи на його простоту, є напрочуд ефективним і досягає результатів, порівнянних із сучасними.","Solving the difficult task of predicting a photo's location is a challenge for computer vision algorithms. To tackle this problem on a city-scale, we employed a data-driven approach and developed a fast and robust scene matching method with a coarse-to-fine strategy. We tested it on a large set of geo-tagged images from downtown San Francisco, and the results demonstrate that our approach is surprisingly effective, even with its simplicity, and yields comparable performance to the current state-of-the-art.",1
"Передбачити, де було зроблено фотографію, є досить важливим і водночас складним завданням для алгоритмів комп’ютерного зору. Наша мотивація полягає в тому, щоб вирішити цю складну проблему в умовах міста за допомогою підходу, керованого даними. Щоб досягти цієї мети, ми розробили швидкий і надійний метод зіставлення сцен, який дотримується стратегії від грубого до точного. Зокрема, ми поєднуємо пошук сцени за допомогою глобальних функцій і щільного вирівнювання сцени та використовуємо великий набір зображень центру Сан-Франциско з геотегами в нашій оцінці. Експериментальні результати показують, що запропонований підхід, незважаючи на його простоту, є напрочуд ефективним і досягає результатів, порівнянних із сучасними.","Вирішення складного завдання прогнозування розташування фотографії є проблемою для алгоритмів комп’ютерного зору. Щоб вирішити цю проблему в масштабі міста, ми застосували підхід, керований даними, і розробили швидкий і надійний метод зіставлення сцен із стратегією від грубого до точного. Ми протестували його на великому наборі зображень із геотегами в центрі Сан-Франциско, і результати показали, що наш підхід є напрочуд ефективним, навіть незважаючи на його простоту, і забезпечує порівняну продуктивність із поточним сучасним рівнем техніки.",1
"Previous sampling-based image matting methods typically rely on certain heuristics in collecting representative samples from known regions, and thus their performance deteriorates if the underlying assumptions are not satisfied. To alleviate this, in this paper we take an entirely new approach and formulate sampling as a sparse subset selection problem where we propose to pick a small set of candidate samples that best explains the unknown pixels. Moreover, we describe a new distance measure for comparing two samples which is based on KL-divergence between the distributions of features extracted in the vicinity of the samples. Using a standard benchmark dataset for image matting, we demonstrate that our approach provides more accurate results compared with the state-of-the-art methods. ","Sampling-based image matting techniques usually depend on particular heuristics to obtain samples from known areas, leading to poor results when underlying assumptions are not met. To address this, we introduce a new method where sample selection is viewed as a sparse subset selection problem, selecting a small set of samples that best explain unknown pixels. Additionally, we introduce a new distance measure to compare two samples, based on KL-divergence between the distributions of features from the vicinity of the samples. Results from a standard benchmark dataset for image matting show our approach gives more accurate outcomes compared to current state-of-the-art methods.",1
"Previous sampling-based image matting methods typically rely on certain heuristics in collecting representative samples from known regions, and thus their performance deteriorates if the underlying assumptions are not satisfied. To alleviate this, in this paper we take an entirely new approach and formulate sampling as a sparse subset selection problem where we propose to pick a small set of candidate samples that best explains the unknown pixels. Moreover, we describe a new distance measure for comparing two samples which is based on KL-divergence between the distributions of features extracted in the vicinity of the samples. Using a standard benchmark dataset for image matting, we demonstrate that our approach provides more accurate results compared with the state-of-the-art methods. ","Методи матування зображень на основі вибірки зазвичай залежать від певної евристики для отримання зразків із відомих областей, що призводить до поганих результатів, якщо основні припущення не виконуються. Щоб вирішити цю проблему, ми представляємо новий метод, у якому відбір вибірки розглядається як проблема відбору розрідженої підмножини, вибираючи невеликий набір вибірок, які найкраще пояснюють невідомі пікселі. Крім того, ми вводимо нову міру відстані для порівняння двох зразків на основі KL-розбіжності між розподілами ознак з околиць зразків. Результати стандартного порівняльного набору даних для матування зображення показують, що наш підхід дає точніші результати порівняно з поточними найсучаснішими методами.",1
"Попередні методи матування зображень на основі вибірки зазвичай покладаються на певні евристики при зборі репрезентативних зразків із відомих регіонів, і, отже, їх продуктивність погіршується, якщо базові припущення не задовольняються. Щоб полегшити це, у цій статті ми застосували абсолютно новий підхід і сформулювали вибірку як проблему вибору розрідженої підмножини, де ми пропонуємо вибрати невеликий набір вибірок-кандидатів, які найкраще пояснюють невідомі пікселі. Крім того, ми описуємо нову міру відстані для порівняння двох зразків, яка базується на KL-розбіжності між розподілами ознак, виділених поблизу зразків. Використовуючи стандартний контрольний набір даних для матування зображення, ми демонструємо, що наш підхід забезпечує більш точні результати порівняно з найсучаснішими методами.","Sampling-based image matting techniques usually depend on particular heuristics to obtain samples from known areas, leading to poor results when underlying assumptions are not met. To address this, we introduce a new method where sample selection is viewed as a sparse subset selection problem, selecting a small set of samples that best explain unknown pixels. Additionally, we introduce a new distance measure to compare two samples, based on KL-divergence between the distributions of features from the vicinity of the samples. Results from a standard benchmark dataset for image matting show our approach gives more accurate outcomes compared to current state-of-the-art methods.",1
"Попередні методи матування зображень на основі вибірки зазвичай покладаються на певні евристики при зборі репрезентативних зразків із відомих регіонів, і, отже, їх продуктивність погіршується, якщо базові припущення не задовольняються. Щоб полегшити це, у цій статті ми застосували абсолютно новий підхід і сформулювали вибірку як проблему вибору розрідженої підмножини, де ми пропонуємо вибрати невеликий набір вибірок-кандидатів, які найкраще пояснюють невідомі пікселі. Крім того, ми описуємо нову міру відстані для порівняння двох зразків, яка базується на KL-розбіжності між розподілами ознак, виділених поблизу зразків. Використовуючи стандартний контрольний набір даних для матування зображення, ми демонструємо, що наш підхід забезпечує більш точні результати порівняно з найсучаснішими методами.","Методи матування зображень на основі вибірки зазвичай залежать від певної евристики для отримання зразків із відомих областей, що призводить до поганих результатів, якщо основні припущення не виконуються. Щоб вирішити цю проблему, ми представляємо новий метод, у якому відбір вибірки розглядається як проблема відбору розрідженої підмножини, вибираючи невеликий набір вибірок, які найкраще пояснюють невідомі пікселі. Крім того, ми вводимо нову міру відстані для порівняння двох зразків на основі KL-розбіжності між розподілами ознак з околиць зразків. Результати стандартного порівняльного набору даних для матування зображення показують, що наш підхід дає точніші результати порівняно з поточними найсучаснішими методами.",1
"Predicting where humans look in images has gained significant popularity in recent years. In this work, we present a novel method for learning top-down visual saliency, which is well-suited to locate objects of interest in complex scenes. During training, we jointly learn a superpixel based class-specific dictionary and a Conditional Random Field (CRF). While using such a discriminative dictionary helps to distinguish target objects from the background, performing the computations at the superpixel level allows us to improve accuracy of object localizations. Experimental results on the Graz-02 and PASCAL VOC 2007 datasets show that the proposed approach is able to achieve stateof-the-art results and provides much better saliency maps.","In recent years, predicting where humans look in images has become increasingly popular. We present a new approach to top-down visual saliency that is effective for locating objects of interest in complex scenes. Training entails the joint learning of a class-specific superpixel dictionary and a Conditional Random Field (CRF). The discriminative dictionary enables distinguishing between target objects and background, while the superpixel level calculations enhance accuracy of object localizations. Experiments on Graz-02 and PASCAL VOC 2007 datasets demonstrate that the proposed method outperforms existing approaches and produces superior saliency maps.",1
"Predicting where humans look in images has gained significant popularity in recent years. In this work, we present a novel method for learning top-down visual saliency, which is well-suited to locate objects of interest in complex scenes. During training, we jointly learn a superpixel based class-specific dictionary and a Conditional Random Field (CRF). While using such a discriminative dictionary helps to distinguish target objects from the background, performing the computations at the superpixel level allows us to improve accuracy of object localizations. Experimental results on the Graz-02 and PASCAL VOC 2007 datasets show that the proposed approach is able to achieve stateof-the-art results and provides much better saliency maps.","Останніми роками прогнозування того, куди дивляться люди на зображеннях, стає все більш популярним. Ми представляємо новий підхід до візуальної помітності зверху вниз, який ефективний для визначення місцезнаходження об’єктів, що цікавлять, у складних сценах. Навчання передбачає спільне вивчення словника суперпікселів класу та умовного випадкового поля (CRF). Дискримінаційний словник дозволяє розрізняти цільові об’єкти та фон, а обчислення рівня суперпікселя підвищує точність локалізації об’єктів. Експерименти на наборах даних Graz-02 і PASCAL VOC 2007 демонструють, що запропонований метод перевершує існуючі підходи та створює чудові карти помітності.",1
"Передбачення, куди дивляться люди на зображеннях, набуло значної популярності в останні роки. У цій роботі ми представляємо новий метод вивчення візуальної помітності зверху вниз, який добре підходить для визначення місцезнаходження цікавих об’єктів у складних сценах. Під час навчання ми спільно вивчаємо словник класу на основі суперпікселів і умовне випадкове поле (CRF). Хоча використання такого розрізнювального словника допомагає відрізнити цільові об’єкти від фону, виконання обчислень на рівні суперпікселя дозволяє підвищити точність локалізації об’єктів. Експериментальні результати на наборах даних Graz-02 і PASCAL VOC 2007 показують, що запропонований підхід здатний досягти найсучасніших результатів і забезпечує набагато кращі карти помітності.","In recent years, predicting where humans look in images has become increasingly popular. We present a new approach to top-down visual saliency that is effective for locating objects of interest in complex scenes. Training entails the joint learning of a class-specific superpixel dictionary and a Conditional Random Field (CRF). The discriminative dictionary enables distinguishing between target objects and background, while the superpixel level calculations enhance accuracy of object localizations. Experiments on Graz-02 and PASCAL VOC 2007 datasets demonstrate that the proposed method outperforms existing approaches and produces superior saliency maps.",1
"Передбачення, куди дивляться люди на зображеннях, набуло значної популярності в останні роки. У цій роботі ми представляємо новий метод вивчення візуальної помітності зверху вниз, який добре підходить для визначення місцезнаходження цікавих об’єктів у складних сценах. Під час навчання ми спільно вивчаємо словник класу на основі суперпікселів і умовне випадкове поле (CRF). Хоча використання такого розрізнювального словника допомагає відрізнити цільові об’єкти від фону, виконання обчислень на рівні суперпікселя дозволяє підвищити точність локалізації об’єктів. Експериментальні результати на наборах даних Graz-02 і PASCAL VOC 2007 показують, що запропонований підхід здатний досягти найсучасніших результатів і забезпечує набагато кращі карти помітності.","Останніми роками прогнозування того, куди дивляться люди на зображеннях, стає все більш популярним. Ми представляємо новий підхід до візуальної помітності зверху вниз, який ефективний для визначення місцезнаходження об’єктів, що цікавлять, у складних сценах. Навчання передбачає спільне вивчення словника суперпікселів класу та умовного випадкового поля (CRF). Дискримінаційний словник дозволяє розрізняти цільові об’єкти та фон, а обчислення рівня суперпікселя підвищує точність локалізації об’єктів. Експерименти на наборах даних Graz-02 і PASCAL VOC 2007 демонструють, що запропонований метод перевершує існуючі підходи та створює чудові карти помітності.",1
"Recent years have witnessed the emergence of new image smoothing techniques which have provided new insights and raised new questions about the nature of this well-studied problem. Specifically, these models separate a given image into its structure and texture layers by utilizing non-gradient based definitions for edges or special measures that distinguish edges from oscillations. In this study, we propose an alternative yet simple image smoothing approach which depends on covariance matrices of simple image features, aka the region covariances. The use of second order statistics as a patch descriptor allows us to implicitly capture local structure and texture information and makes our approach particularly effective for structure extraction from texture. Our experimental results have shown that the proposed approach leads to better image decompositions as compared to the state-of-the-art methods and preserves prominent edges and shading well. Moreover, we also demonstrate the applicability of our approach on some image editing and manipulation tasks such as image abstraction, texture and detail enhancement, image composition, inverse halftoning and seam carving.","Recent years have seen the introduction of new image smoothing techniques which have deepened understanding and raised questions about this long-studied problem. These models separate a given image into its structure and texture layers using non-gradient based definitions for edges and distinguishing features for oscillations. In this study, we propose a straightforward image smoothing approach based on covariance matrices of simple image features, the region covariances. Utilizing second order statistics as a patch descriptor allows us to capture local structure and texture information, making it especially effective for structure extraction from texture. Results demonstrate that our approach surpasses state-of-the-art methods in terms of image decomposition, preserving prominent edges and shading. Additionally, it is applicable to various image editing and manipulation tasks such as image abstraction, texture and detail enhancement, image composition, inverse halftoning and seam carving.",1
"Recent years have witnessed the emergence of new image smoothing techniques which have provided new insights and raised new questions about the nature of this well-studied problem. Specifically, these models separate a given image into its structure and texture layers by utilizing non-gradient based definitions for edges or special measures that distinguish edges from oscillations. In this study, we propose an alternative yet simple image smoothing approach which depends on covariance matrices of simple image features, aka the region covariances. The use of second order statistics as a patch descriptor allows us to implicitly capture local structure and texture information and makes our approach particularly effective for structure extraction from texture. Our experimental results have shown that the proposed approach leads to better image decompositions as compared to the state-of-the-art methods and preserves prominent edges and shading well. Moreover, we also demonstrate the applicability of our approach on some image editing and manipulation tasks such as image abstraction, texture and detail enhancement, image composition, inverse halftoning and seam carving.","Останніми роками з’явилися нові методи згладжування зображень, які поглибили розуміння та поставили питання про цю давно досліджену проблему. Ці моделі поділяють дане зображення на шари структури та текстури, використовуючи неградієнтні визначення для країв і відмінних ознак для коливань. У цьому дослідженні ми пропонуємо простий підхід до згладжування зображення на основі коваріаційних матриць простих характеристик зображення, коваріацій регіону. Використання статистичних даних другого порядку як дескриптора патча дозволяє нам отримувати інформацію про локальну структуру та текстуру, що робить його особливо ефективним для вилучення структури з текстури. Результати демонструють, що наш підхід перевершує найсучасніші методи з точки зору декомпозиції зображення, збереження помітних країв і затінення. Крім того, він застосовний для різноманітних завдань редагування та обробки зображень, таких як абстракція зображення, покращення текстури та деталей, композиція зображення, інверсне напівтонування та вирізання швів.",1
"Останні роки стали свідками появи нових методів згладжування зображень, які дали нові знання та поставили нові питання щодо природи цієї добре вивченої проблеми. Зокрема, ці моделі поділяють дане зображення на шари структури та текстури, використовуючи неградієнтні визначення для країв або спеціальні заходи, які відрізняють краї від коливань. У цьому дослідженні ми пропонуємо альтернативний, але простий підхід до згладжування зображення, який залежить від коваріаційних матриць простих характеристик зображення, також відомих як коваріації регіонів. Використання статистики другого порядку як дескриптора патча дозволяє нам неявно отримувати інформацію про локальну структуру та текстуру та робить наш підхід особливо ефективним для вилучення структури з текстури. Результати наших експериментів показали, що запропонований підхід забезпечує кращу декомпозицію зображення порівняно з сучасними методами та добре зберігає помітні краї та затінення. Крім того, ми також демонструємо застосовність нашого підходу до деяких завдань редагування та маніпулювання зображеннями, таких як абстракція зображення, покращення текстури та деталей, композиція зображення, інверсне півтонування та різьблення швів.","Recent years have seen the introduction of new image smoothing techniques which have deepened understanding and raised questions about this long-studied problem. These models separate a given image into its structure and texture layers using non-gradient based definitions for edges and distinguishing features for oscillations. In this study, we propose a straightforward image smoothing approach based on covariance matrices of simple image features, the region covariances. Utilizing second order statistics as a patch descriptor allows us to capture local structure and texture information, making it especially effective for structure extraction from texture. Results demonstrate that our approach surpasses state-of-the-art methods in terms of image decomposition, preserving prominent edges and shading. Additionally, it is applicable to various image editing and manipulation tasks such as image abstraction, texture and detail enhancement, image composition, inverse halftoning and seam carving.",1
"Останні роки стали свідками появи нових методів згладжування зображень, які дали нові знання та поставили нові питання щодо природи цієї добре вивченої проблеми. Зокрема, ці моделі поділяють дане зображення на шари структури та текстури, використовуючи неградієнтні визначення для країв або спеціальні заходи, які відрізняють краї від коливань. У цьому дослідженні ми пропонуємо альтернативний, але простий підхід до згладжування зображення, який залежить від коваріаційних матриць простих характеристик зображення, також відомих як коваріації регіонів. Використання статистики другого порядку як дескриптора патча дозволяє нам неявно отримувати інформацію про локальну структуру та текстуру та робить наш підхід особливо ефективним для вилучення структури з текстури. Результати наших експериментів показали, що запропонований підхід забезпечує кращу декомпозицію зображення порівняно з сучасними методами та добре зберігає помітні краї та затінення. Крім того, ми також демонструємо застосовність нашого підходу до деяких завдань редагування та маніпулювання зображеннями, таких як абстракція зображення, покращення текстури та деталей, композиція зображення, інверсне півтонування та різьблення швів.","Останніми роками з’явилися нові методи згладжування зображень, які поглибили розуміння та поставили питання про цю давно досліджену проблему. Ці моделі поділяють дане зображення на шари структури та текстури, використовуючи неградієнтні визначення для країв і відмінних ознак для коливань. У цьому дослідженні ми пропонуємо простий підхід до згладжування зображення на основі коваріаційних матриць простих характеристик зображення, коваріацій регіону. Використання статистичних даних другого порядку як дескриптора патча дозволяє нам отримувати інформацію про локальну структуру та текстуру, що робить його особливо ефективним для вилучення структури з текстури. Результати демонструють, що наш підхід перевершує найсучасніші методи з точки зору декомпозиції зображення, збереження помітних країв і затінення. Крім того, він застосовний для різноманітних завдань редагування та обробки зображень, таких як абстракція зображення, покращення текстури та деталей, композиція зображення, інверсне напівтонування та вирізання швів.",1
"In the last few decades, significant achievements have been attained in predicting where humans look at images through different computational models. However, how to determine contributions of different visual features to overall saliency still remains an open problem. To overcome this issue, a recent class of models formulates saliency estimation as a supervised learning problem and accordingly apply machine learning techniques. In this paper, we also address this challenging problem and propose to use multiple kernel learning (MKL) to combine information coming from different feature dimensions and to perform integration at an intermediate level. Besides, we suggest to use responses of a recently proposed filterbank of object detectors, known as ObjectBank, as additional semantic high-level features. Here we show that our MKL-based framework together with the proposed object-specific features provide state-of-the-art performance as compared to SVM or AdaBoost-based saliency models. ","In recent years, predicting human visual attention in images has been achieved through computational models. However, how to measure the influence of various visual features on overall saliency is still unsolved. To tackle this, a recent type of models treats saliency estimation as a supervised learning task and applies machine learning methods. In this article, we use multiple kernel learning (MKL) to integrate data from multiple feature dimensions and conduct integration at an intermediate level. Moreover, we use responses of a recently developed ObjectBank filterbank of object detectors as extra semantic high-level features. Our MKL-based framework combined with the proposed object-specific features show superior performance compared to SVM or AdaBoost-based saliency models.",1
"In the last few decades, significant achievements have been attained in predicting where humans look at images through different computational models. However, how to determine contributions of different visual features to overall saliency still remains an open problem. To overcome this issue, a recent class of models formulates saliency estimation as a supervised learning problem and accordingly apply machine learning techniques. In this paper, we also address this challenging problem and propose to use multiple kernel learning (MKL) to combine information coming from different feature dimensions and to perform integration at an intermediate level. Besides, we suggest to use responses of a recently proposed filterbank of object detectors, known as ObjectBank, as additional semantic high-level features. Here we show that our MKL-based framework together with the proposed object-specific features provide state-of-the-art performance as compared to SVM or AdaBoost-based saliency models. ","Останніми роками прогнозування зорової уваги людини на зображеннях було досягнуто за допомогою обчислювальних моделей. Однак, як виміряти вплив різних візуальних особливостей на загальну помітність, досі не з’ясовано. Щоб вирішити цю проблему, останній тип моделей розглядає оцінку помітності як контрольоване навчальне завдання та застосовує методи машинного навчання. У цій статті ми використовуємо багатоядерне навчання (MKL) для інтеграції даних із кількох вимірів функцій та проведення інтеграції на середньому рівні. Крім того, ми використовуємо відповіді нещодавно розробленого банку фільтрів ObjectBank детекторів об’єктів як додаткові семантичні функції високого рівня. Наш фреймворк на основі MKL у поєднанні з пропонованими об’єктно-специфічними функціями показує кращу продуктивність порівняно з моделями помітності на основі SVM або AdaBoost.",1
"За останні кілька десятиліть було досягнуто значних досягнень у передбаченні того, де люди дивляться на зображення, за допомогою різних обчислювальних моделей. Проте, як визначити внесок різних візуальних особливостей у загальну помітність, все ще залишається відкритою проблемою. Щоб подолати цю проблему, останній клас моделей формулює оцінку помітності як проблему контрольованого навчання та відповідно застосовує методи машинного навчання. У цьому документі ми також розглядаємо цю складну проблему та пропонуємо використовувати багатоядерне навчання (MKL) для об’єднання інформації, що надходить з різних вимірів функцій, і для виконання інтеграції на середньому рівні. Крім того, ми пропонуємо використовувати відгуки нещодавно запропонованого банку фільтрів детекторів об’єктів, відомого як ObjectBank, як додаткові семантичні функції високого рівня. Тут ми показуємо, що наш фреймворк на основі MKL разом із пропонованими об’єктно-специфічними функціями забезпечує найсучаснішу продуктивність порівняно з моделями помітності на основі SVM або AdaBoost.","In recent years, predicting human visual attention in images has been achieved through computational models. However, how to measure the influence of various visual features on overall saliency is still unsolved. To tackle this, a recent type of models treats saliency estimation as a supervised learning task and applies machine learning methods. In this article, we use multiple kernel learning (MKL) to integrate data from multiple feature dimensions and conduct integration at an intermediate level. Moreover, we use responses of a recently developed ObjectBank filterbank of object detectors as extra semantic high-level features. Our MKL-based framework combined with the proposed object-specific features show superior performance compared to SVM or AdaBoost-based saliency models.",1
"За останні кілька десятиліть було досягнуто значних досягнень у передбаченні того, де люди дивляться на зображення, за допомогою різних обчислювальних моделей. Проте, як визначити внесок різних візуальних особливостей у загальну помітність, все ще залишається відкритою проблемою. Щоб подолати цю проблему, останній клас моделей формулює оцінку помітності як проблему контрольованого навчання та відповідно застосовує методи машинного навчання. У цьому документі ми також розглядаємо цю складну проблему та пропонуємо використовувати багатоядерне навчання (MKL) для об’єднання інформації, що надходить з різних вимірів функцій, і для виконання інтеграції на середньому рівні. Крім того, ми пропонуємо використовувати відгуки нещодавно запропонованого банку фільтрів детекторів об’єктів, відомого як ObjectBank, як додаткові семантичні функції високого рівня. Тут ми показуємо, що наш фреймворк на основі MKL разом із пропонованими об’єктно-специфічними функціями забезпечує найсучаснішу продуктивність порівняно з моделями помітності на основі SVM або AdaBoost.","Останніми роками прогнозування зорової уваги людини на зображеннях було досягнуто за допомогою обчислювальних моделей. Проте, як виміряти вплив різних візуальних особливостей на загальну помітність, досі не з’ясовано. Щоб вирішити цю проблему, останній тип моделей розглядає оцінку помітності як контрольоване навчальне завдання та застосовує методи машинного навчання. У цій статті ми використовуємо багатоядерне навчання (MKL) для інтеграції даних із кількох вимірів функцій та проведення інтеграції на середньому рівні. Крім того, ми використовуємо відповіді нещодавно розробленого банку фільтрів ObjectBank детекторів об’єктів як додаткові семантичні функції високого рівня. Наш фреймворк на основі MKL у поєднанні з пропонованими об’єктно-специфічними функціями показує кращу продуктивність порівняно з моделями помітності на основі SVM або AdaBoost.",1
"Abstract  To detect visually salient elements of complex natural scenes, computational bottom-up saliency models commonly examine several feature channels such as color and orientation in parallel. They compute a separate feature map for each channel and then linearly combine these maps to produce a master saliency map. However, only a few studies have investigated how different feature dimensions contribute to the overall visual saliency. We address this integration issue and propose to use covariance matrices of simple image features (known as region covariance descriptors in the computer vision community; Tuzel, Porikli, & Meer, 2006) as meta-features for saliency estimation. As low-dimensional representations of image patches, region covariances capture local image structures better than standard linear filters, but more importantly, they naturally provide nonlinear integration of different features by modeling their correlations. We also show that first-order statistics of features could be easily incorporated to the proposed approach to improve the performance. Our experimental evaluation on several benchmark data sets demonstrate that the proposed approach outperforms the state-of-art models on various tasks including prediction of human eye fixations, salient object detection, and image-retargeting.","Computational bottom-up saliency models usually process several feature channels, e.g. color and orientation, in parallel to detect salient elements of complex natural scenes. Maps for each feature are combined linearly to create a master saliency map; however, few studies have investigated the contribution of different feature dimensions to overall visual saliency. To address this integration issue, we propose using region covariance descriptors (Tuzel, Porikli, & Meer, 2006) as meta-features for saliency estimation. These low-dimensional representations capture local image structures better than linear filters and nonlinearly integrate different features by modeling their correlations. We also show that incorporating first-order statistics of features can further improve performance. Our experiments on several benchmark data sets show our approach outperforms state-of-the-art models for predicting human eye fixations, salient object detection, and image-retargeting.",1
"Abstract  To detect visually salient elements of complex natural scenes, computational bottom-up saliency models commonly examine several feature channels such as color and orientation in parallel. They compute a separate feature map for each channel and then linearly combine these maps to produce a master saliency map. However, only a few studies have investigated how different feature dimensions contribute to the overall visual saliency. We address this integration issue and propose to use covariance matrices of simple image features (known as region covariance descriptors in the computer vision community; Tuzel, Porikli, & Meer, 2006) as meta-features for saliency estimation. As low-dimensional representations of image patches, region covariances capture local image structures better than standard linear filters, but more importantly, they naturally provide nonlinear integration of different features by modeling their correlations. We also show that first-order statistics of features could be easily incorporated to the proposed approach to improve the performance. Our experimental evaluation on several benchmark data sets demonstrate that the proposed approach outperforms the state-of-art models on various tasks including prediction of human eye fixations, salient object detection, and image-retargeting.","Обчислювальні моделі помітності «знизу вгору» зазвичай обробляють кілька каналів ознак, наприклад колір і орієнтацію, паралельно для виявлення помітних елементів складних природних сцен. Карти для кожного об’єкта об’єднуються лінійно, щоб створити головну карту помітності; однак мало досліджень досліджували внесок різних вимірів ознак у загальну візуальну помітність. Щоб вирішити цю проблему інтеграції, ми пропонуємо використовувати дескриптори коваріації регіону (Tuzel, Porikli, &amp; Meer, 2006) як метафункції для оцінки помітності. Ці низьковимірні представлення фіксують локальні структури зображення краще, ніж лінійні фільтри, і нелінійно інтегрують різні функції, моделюючи їх кореляції. Ми також показуємо, що включення статистики функцій першого порядку може ще більше підвищити продуктивність. Наші експерименти на кількох контрольних наборах даних показують, що наш підхід перевершує найсучасніші моделі для прогнозування фіксації людського ока, виявлення помітних об’єктів і перенацілювання зображення.",1
"Анотація Щоб виявити візуально помітні елементи складних природних сцен, обчислювальні моделі помітності знизу вгору зазвичай досліджують кілька каналів ознак, таких як колір і орієнтація паралельно. Вони обчислюють окрему карту функцій для кожного каналу, а потім лінійно об’єднують ці карти, щоб отримати головну карту помітності. Однак лише кілька досліджень досліджували, як різні розміри функцій впливають на загальну візуальну помітність. Ми розглядаємо цю проблему інтеграції та пропонуємо використовувати коваріаційні матриці простих характеристик зображення (відомі як дескриптори коваріації регіонів у спільноті комп’ютерного зору; Tuzel, Porikli та Meer, 2006) як мета-ознаки для оцінки помітності. Будучи низьковимірними представленнями фрагментів зображення, коваріації регіонів фіксують локальні структури зображення краще, ніж стандартні лінійні фільтри, але, що більш важливо, вони природним чином забезпечують нелінійну інтеграцію різних функцій шляхом моделювання їхніх кореляцій. Ми також показуємо, що статистика функцій першого порядку може бути легко включена в запропонований підхід для покращення продуктивності. Наша експериментальна оцінка кількох еталонних наборів даних демонструє, що запропонований підхід перевершує сучасні моделі для різних завдань, включаючи прогнозування фіксації людського ока, виявлення помітних об’єктів і перенацілювання зображень.","Computational bottom-up saliency models usually process several feature channels, e.g. color and orientation, in parallel to detect salient elements of complex natural scenes. Maps for each feature are combined linearly to create a master saliency map; however, few studies have investigated the contribution of different feature dimensions to overall visual saliency. To address this integration issue, we propose using region covariance descriptors (Tuzel, Porikli, & Meer, 2006) as meta-features for saliency estimation. These low-dimensional representations capture local image structures better than linear filters and nonlinearly integrate different features by modeling their correlations. We also show that incorporating first-order statistics of features can further improve performance. Our experiments on several benchmark data sets show our approach outperforms state-of-the-art models for predicting human eye fixations, salient object detection, and image-retargeting.",1
"Анотація Щоб виявити візуально помітні елементи складних природних сцен, обчислювальні моделі помітності знизу вгору зазвичай досліджують кілька каналів ознак, таких як колір і орієнтація паралельно. Вони обчислюють окрему карту функцій для кожного каналу, а потім лінійно об’єднують ці карти, щоб отримати головну карту помітності. Однак лише кілька досліджень досліджували, як різні розміри функцій впливають на загальну візуальну помітність. Ми розглядаємо цю проблему інтеграції та пропонуємо використовувати коваріаційні матриці простих характеристик зображення (відомі як дескриптори коваріації регіонів у спільноті комп’ютерного зору; Tuzel, Porikli та Meer, 2006) як мета-ознаки для оцінки помітності. Будучи низьковимірними представленнями фрагментів зображення, коваріації регіонів фіксують локальні структури зображення краще, ніж стандартні лінійні фільтри, але, що більш важливо, вони природним чином забезпечують нелінійну інтеграцію різних функцій шляхом моделювання їхніх кореляцій. Ми також показуємо, що статистика функцій першого порядку може бути легко включена в запропонований підхід для покращення продуктивності. Наша експериментальна оцінка кількох еталонних наборів даних демонструє, що запропонований підхід перевершує сучасні моделі для різних завдань, включаючи прогнозування фіксації людського ока, виявлення помітних об’єктів і перенацілювання зображень.","Обчислювальні моделі помітності «знизу вгору» зазвичай обробляють кілька каналів ознак, наприклад колір і орієнтацію, паралельно для виявлення помітних елементів складних природних сцен. Карти для кожного об’єкта об’єднуються лінійно, щоб створити головну карту помітності; однак мало досліджень досліджували внесок різних вимірів ознак у загальну візуальну помітність. Щоб вирішити цю проблему інтеграції, ми пропонуємо використовувати дескриптори коваріації регіону (Tuzel, Porikli, &amp; Meer, 2006) як метафункції для оцінки помітності. Ці низьковимірні представлення фіксують локальні структури зображення краще, ніж лінійні фільтри, і нелінійно інтегрують різні функції, моделюючи їх кореляції. Ми також показуємо, що включення статистики функцій першого порядку може ще більше підвищити продуктивність. Наші експерименти на кількох контрольних наборах даних показують, що наш підхід перевершує найсучасніші моделі для прогнозування фіксації людського ока, виявлення помітних об’єктів і перенацілювання зображення.",1
"In daily life, humans demonstrate astounding ability to remember images they see on magazines, commercials, TV, the web and so on, but automatic prediction of intrinsic memorability of images using computer vision and machine learning techniques was not investigated until a few years ago. However, despite these recent advances, none of the available approaches makes use of any attentional mechanism, a fundamental aspect of human vision, which selects relevant image regions for higher-level processing. Our goal in this paper is to explore the role of visual attention in understanding memorability of images. In particular, we present an attention-driven spatial pooling strategy for image memorability and show that the regions estimated by bottom-up and object-level saliency maps are more effective in predicting memorability than considering a fixed spatial pyramid structure as in the previous studies.","Humans possess an impressive ability to recall images they observe in everyday life, yet the potential of automatic memorability prediction of images through computer vision and machine learning had not been explored until a few years ago. Despite the advancements, none of the existing methods use an attentional mechanism, a significant part of human vision, to select important image areas for more advanced processing. In this paper, we research the role of visual attention in understanding image memorability. Specifically, we present an attention-driven spatial pooling approach for image memorability and prove that the regions identified by bottom-up and object-level saliency maps are more efficient in predicting memorability than utilizing a fixed spatial pyramid structure, as in the previous studies.",1
"In daily life, humans demonstrate astounding ability to remember images they see on magazines, commercials, TV, the web and so on, but automatic prediction of intrinsic memorability of images using computer vision and machine learning techniques was not investigated until a few years ago. However, despite these recent advances, none of the available approaches makes use of any attentional mechanism, a fundamental aspect of human vision, which selects relevant image regions for higher-level processing. Our goal in this paper is to explore the role of visual attention in understanding memorability of images. In particular, we present an attention-driven spatial pooling strategy for image memorability and show that the regions estimated by bottom-up and object-level saliency maps are more effective in predicting memorability than considering a fixed spatial pyramid structure as in the previous studies.","Люди володіють вражаючою здатністю запам’ятовувати зображення, які вони спостерігають у повсякденному житті, але потенціал автоматичного прогнозування запам’ятовування зображень за допомогою комп’ютерного зору та машинного навчання не досліджувався кілька років тому. Незважаючи на досягнення, жоден із існуючих методів не використовує механізм уваги, значну частину людського зору, для вибору важливих областей зображення для більш розширеної обробки. У цій статті ми досліджуємо роль зорової уваги в розумінні запам’ятовуваності зображення. Зокрема, ми представляємо орієнтований на увагу підхід просторового об’єднання для запам’ятовування зображення та доводимо, що регіони, ідентифіковані за допомогою карт помітності знизу вгору та на рівні об’єктів, ефективніші для прогнозування запам’ятовування, ніж використання фіксованої просторової пірамідальної структури, як у попередніх дослідженнях.",1
"У повсякденному житті люди демонструють приголомшливу здатність запам’ятовувати зображення, які вони бачать у журналах, рекламі, на телебаченні, в Інтернеті тощо, але автоматичне передбачення внутрішньої запам’ятовуваності зображень за допомогою комп’ютерного зору та методів машинного навчання досліджувалося лише кілька років тому. Однак, незважаючи на ці нещодавні досягнення, жоден із доступних підходів не використовує жодного механізму уваги, фундаментального аспекту людського зору, який вибирає відповідні області зображення для обробки на вищому рівні. Наша мета в цій статті — дослідити роль візуальної уваги в розумінні запам’ятовуваності зображень. Зокрема, ми представляємо орієнтовану на увагу стратегію просторового об’єднання для запам’ятовуваності зображення та показуємо, що регіони, оцінені за допомогою карт помітності знизу вгору та на рівні об’єктів, є більш ефективними для прогнозування запам’ятовуваності, ніж розгляд фіксованої просторової пірамідальної структури, як у попередніх дослідженнях.","Humans possess an impressive ability to recall images they observe in everyday life, yet the potential of automatic memorability prediction of images through computer vision and machine learning had not been explored until a few years ago. Despite the advancements, none of the existing methods use an attentional mechanism, a significant part of human vision, to select important image areas for more advanced processing. In this paper, we research the role of visual attention in understanding image memorability. Specifically, we present an attention-driven spatial pooling approach for image memorability and prove that the regions identified by bottom-up and object-level saliency maps are more efficient in predicting memorability than utilizing a fixed spatial pyramid structure, as in the previous studies.",1
"У повсякденному житті люди демонструють приголомшливу здатність запам’ятовувати зображення, які вони бачать у журналах, рекламі, на телебаченні, в Інтернеті тощо, але автоматичне передбачення внутрішньої запам’ятовуваності зображень за допомогою комп’ютерного зору та методів машинного навчання досліджувалося лише кілька років тому. Однак, незважаючи на ці нещодавні досягнення, жоден із доступних підходів не використовує жодного механізму уваги, фундаментального аспекту людського зору, який вибирає відповідні області зображення для обробки на вищому рівні. Наша мета в цій статті — дослідити роль візуальної уваги в розумінні запам’ятовуваності зображень. Зокрема, ми представляємо орієнтовану на увагу стратегію просторового об’єднання для запам’ятовуваності зображення та показуємо, що регіони, оцінені за допомогою карт помітності знизу вгору та на рівні об’єктів, є більш ефективними для прогнозування запам’ятовуваності, ніж розгляд фіксованої просторової пірамідальної структури, як у попередніх дослідженнях.","Люди володіють вражаючою здатністю запам’ятовувати зображення, які вони спостерігають у повсякденному житті, але потенціал автоматичного прогнозування запам’ятовування зображень за допомогою комп’ютерного зору та машинного навчання не досліджувався кілька років тому. Незважаючи на досягнення, жоден із існуючих методів не використовує механізм уваги, значну частину людського зору, для вибору важливих областей зображення для більш розширеної обробки. У цій статті ми досліджуємо роль візуальної уваги в розумінні запам’ятовуваності зображення. Зокрема, ми представляємо орієнтований на увагу підхід просторового об’єднання для запам’ятовування зображення та доводимо, що регіони, ідентифіковані за допомогою карт помітності знизу вгору та на рівні об’єктів, ефективніші для прогнозування запам’ятовування, ніж використання фіксованої просторової пірамідальної структури, як у попередніх дослідженнях.",1
"In this paper, we address the issue of part-based tracking by proposing a new fragments-based tracker. The proposed tracker enhances the recently suggested FragTrack algorithm to employ an adaptive cue integration scheme. This is done by embedding the original tracker into a particle filter framework, asso- ciating a reliability value to each fragment that describes a different part of the target object and dynam- ically adjusting these reliabilities at each frame with respect to the current context. Particularly, the vote of each fragment contributes to the joint tracking result according to its reliability, and this allows us to achieve a better accuracy in handling partial occlusions and pose changes while preserving and even improving the efficiency of the original tracker. In order to demonstrate the performance and the effec- tiveness of the proposed algorithm we present qualitative and quantitative results on a number of chal- lenging video sequences","In this paper, we propose a new fragments-based tracker that addresses the issue of part-based tracking. This tracker, called FragTrack, is enhanced with an adaptive cue integration scheme embedded into a particle filter framework. Each fragment, which describes a different part of the target object, is associated with a reliability value that is dynamically adjusted in each frame with respect to the current context. Consequently, each fragment's vote contributes to the joint tracking result based on its reliability, thus improving accuracy in handling partial occlusions and pose changes while maintaining the efficiency of the original tracker. To illustrate the performance and effectiveness of the proposed algorithm, qualitative and quantitative results are presented on various challenging video sequences.",1
"In this paper, we address the issue of part-based tracking by proposing a new fragments-based tracker. The proposed tracker enhances the recently suggested FragTrack algorithm to employ an adaptive cue integration scheme. This is done by embedding the original tracker into a particle filter framework, asso- ciating a reliability value to each fragment that describes a different part of the target object and dynam- ically adjusting these reliabilities at each frame with respect to the current context. Particularly, the vote of each fragment contributes to the joint tracking result according to its reliability, and this allows us to achieve a better accuracy in handling partial occlusions and pose changes while preserving and even improving the efficiency of the original tracker. In order to demonstrate the performance and the effec- tiveness of the proposed algorithm we present qualitative and quantitative results on a number of chal- lenging video sequences","У цьому документі ми пропонуємо новий трекер на основі фрагментів, який вирішує проблему відстеження на основі частин. Цей трекер, який називається FragTrack, удосконалено схемою адаптивної інтеграції підказок, вбудованою в структуру фільтра частинок. Кожен фрагмент, який описує іншу частину цільового об’єкта, пов’язаний зі значенням надійності, яке динамічно коригується в кожному кадрі щодо поточного контексту. Отже, голосування кожного фрагмента сприяє спільному результату відстеження на основі його надійності, таким чином покращуючи точність у обробці часткових оклюзій і змін пози, зберігаючи при цьому ефективність оригінального трекера. Щоб проілюструвати продуктивність і ефективність запропонованого алгоритму, якісні та кількісні результати представлені на різних складних відеопослідовностях.",1
"У цій статті ми розглядаємо питання відстеження на основі частин, пропонуючи новий трекер на основі фрагментів. Запропонований трекер покращує нещодавно запропонований алгоритм FragTrack для використання адаптивної схеми інтеграції сигналів. Це робиться шляхом вбудовування оригінального трекера в структуру фільтра частинок, пов’язування значення надійності з кожним фрагментом, який описує іншу частину цільового об’єкта, і динамічного коригування цих надійностей у кожному кадрі відповідно до поточного контексту. Зокрема, голосування кожного фрагмента вносить свій внесок у результат спільного відстеження відповідно до його надійності, і це дозволяє нам досягти кращої точності в обробці часткових оклюзій і змін пози, зберігаючи та навіть покращуючи ефективність оригінального трекера. Щоб продемонструвати продуктивність та ефективність запропонованого алгоритму, ми представляємо якісні та кількісні результати на низці складних відеопослідовностей","In this paper, we propose a new fragments-based tracker that addresses the issue of part-based tracking. This tracker, called FragTrack, is enhanced with an adaptive cue integration scheme embedded into a particle filter framework. Each fragment, which describes a different part of the target object, is associated with a reliability value that is dynamically adjusted in each frame with respect to the current context. Consequently, each fragment's vote contributes to the joint tracking result based on its reliability, thus improving accuracy in handling partial occlusions and pose changes while maintaining the efficiency of the original tracker. To illustrate the performance and effectiveness of the proposed algorithm, qualitative and quantitative results are presented on various challenging video sequences.",1
"У цій статті ми розглядаємо питання відстеження на основі частин, пропонуючи новий трекер на основі фрагментів. Запропонований трекер покращує нещодавно запропонований алгоритм FragTrack для використання адаптивної схеми інтеграції сигналів. Це робиться шляхом вбудовування оригінального трекера в структуру фільтра частинок, пов’язування значення надійності з кожним фрагментом, який описує іншу частину цільового об’єкта, і динамічного коригування цих надійностей у кожному кадрі відповідно до поточного контексту. Зокрема, голосування кожного фрагмента вносить свій внесок у результат спільного відстеження відповідно до його надійності, і це дозволяє нам досягти кращої точності в обробці часткових оклюзій і змін пози, зберігаючи та навіть покращуючи ефективність оригінального трекера. Щоб продемонструвати продуктивність та ефективність запропонованого алгоритму, ми представляємо якісні та кількісні результати на низці складних відеопослідовностей","У цьому документі ми пропонуємо новий трекер на основі фрагментів, який вирішує проблему відстеження на основі частин. Цей трекер, який називається FragTrack, удосконалено схемою адаптивної інтеграції підказок, вбудованою в структуру фільтра частинок. Кожен фрагмент, який описує іншу частину цільового об’єкта, пов’язаний зі значенням надійності, яке динамічно коригується в кожному кадрі щодо поточного контексту. Отже, голосування кожного фрагмента сприяє спільному результату відстеження на основі його надійності, таким чином покращуючи точність у обробці часткових оклюзій і змін пози, зберігаючи при цьому ефективність оригінального трекера. Щоб проілюструвати продуктивність і ефективність запропонованого алгоритму, якісні та кількісні результати представлені на різних складних відеопослідовностях.",1
"Many researchers argue that fusing multiple cues increases the reliability and robustness of visual tracking. However, how the multi-cue integration is realized during tracking is still an open issue. In this work, we present a novel data fusion approach for multi-cue tracking using particle filter. Our method differs from previous approaches in a number of ways. First, we carry out the integration of cues both in making predictions about the target object and in verifying them through observations. Our second and more significant contribution is that both stages of integration directly depend on the dynamically changing reliabilities of visual cues. These two aspects of our method allow the tracker to easily adapt itself to the changes in the context, and accordingly improve the tracking accuracy by resolving the ambiguities","Many researchers believe combining multiple cues boosts reliability and robustness of visual tracking; yet, how this integration is realized during tracking is still unknown. In this work, we propose a new data fusion approach for multi-cue tracking using particle filter. It differs from previous approaches in two ways. First, integration of cues is used to make predictions and verify observations. Second, and more importantly, both integration stages depend on dynamically changing reliabilities of visual cues. These two aspects of our method allow the tracker to adjust to context changes, thus improving tracking accuracy by resolving ambiguities.",1
"Many researchers argue that fusing multiple cues increases the reliability and robustness of visual tracking. However, how the multi-cue integration is realized during tracking is still an open issue. In this work, we present a novel data fusion approach for multi-cue tracking using particle filter. Our method differs from previous approaches in a number of ways. First, we carry out the integration of cues both in making predictions about the target object and in verifying them through observations. Our second and more significant contribution is that both stages of integration directly depend on the dynamically changing reliabilities of visual cues. These two aspects of our method allow the tracker to easily adapt itself to the changes in the context, and accordingly improve the tracking accuracy by resolving the ambiguities","Багато дослідників вважають, що поєднання кількох підказок підвищує надійність і стійкість візуального відстеження; однак, як ця інтеграція реалізована під час відстеження, досі невідомо. У цій роботі ми пропонуємо новий підхід об’єднання даних для відстеження кількох сигналів за допомогою фільтра частинок. Він відрізняється від попередніх підходів у двох аспектах. По-перше, інтеграція підказок використовується для прогнозування та перевірки спостережень. По-друге, і що більш важливо, обидва етапи інтеграції залежать від динамічної зміни надійності візуальних сигналів. Ці два аспекти нашого методу дозволяють трекеру підлаштовуватися під зміни контексту, таким чином покращуючи точність відстеження шляхом вирішення неоднозначностей.",1
"Багато дослідників стверджують, що об’єднання кількох підказок підвищує надійність і стійкість візуального відстеження. Проте те, як реалізується інтеграція кількох підказок під час відстеження, залишається відкритим питанням. У цій роботі ми представляємо новий підхід об’єднання даних для відстеження кількох сигналів за допомогою фільтра частинок. Наш метод відрізняється від попередніх підходів кількома параметрами. По-перше, ми здійснюємо інтеграцію підказок як для створення прогнозів щодо цільового об’єкта, так і для їх перевірки за допомогою спостережень. Наш другий і більш значний внесок полягає в тому, що обидва етапи інтеграції безпосередньо залежать від надійності візуальних підказок, що динамічно змінюється. Ці два аспекти нашого методу дозволяють трекеру легко адаптуватися до змін у контексті та, відповідно, підвищити точність відстеження шляхом вирішення неоднозначностей.","Many researchers believe combining multiple cues boosts reliability and robustness of visual tracking; yet, how this integration is realized during tracking is still unknown. In this work, we propose a new data fusion approach for multi-cue tracking using particle filter. It differs from previous approaches in two ways. First, integration of cues is used to make predictions and verify observations. Second, and more importantly, both integration stages depend on dynamically changing reliabilities of visual cues. These two aspects of our method allow the tracker to adjust to context changes, thus improving tracking accuracy by resolving ambiguities.",1
"Багато дослідників стверджують, що об’єднання кількох підказок підвищує надійність і стійкість візуального відстеження. Проте те, як реалізується інтеграція кількох підказок під час відстеження, залишається відкритим питанням. У цій роботі ми представляємо новий підхід об’єднання даних для відстеження кількох сигналів за допомогою фільтра частинок. Наш метод відрізняється від попередніх підходів кількома параметрами. По-перше, ми здійснюємо інтеграцію підказок як для створення прогнозів щодо цільового об’єкта, так і для їх перевірки за допомогою спостережень. Наш другий і більш значний внесок полягає в тому, що обидва етапи інтеграції безпосередньо залежать від надійності візуальних підказок, що динамічно змінюється. Ці два аспекти нашого методу дозволяють трекеру легко адаптуватися до змін у контексті та, відповідно, підвищити точність відстеження шляхом вирішення неоднозначностей.","Багато дослідників вважають, що поєднання кількох підказок підвищує надійність і стійкість візуального відстеження; однак, як ця інтеграція реалізована під час відстеження, досі невідомо. У цій роботі ми пропонуємо новий підхід об’єднання даних для відстеження кількох сигналів за допомогою фільтра частинок. Він відрізняється від попередніх підходів у двох аспектах. По-перше, інтеграція сигналів використовується для прогнозування та перевірки спостережень. По-друге, і що більш важливо, обидва етапи інтеграції залежать від динамічної зміни надійності візуальних сигналів. Ці два аспекти нашого методу дозволяють трекеру підлаштовуватися під зміни контексту, таким чином покращуючи точність відстеження шляхом вирішення неоднозначностей.",1
"This paper presents a new image segmentation framework which employs a shape prior in the form of an edge strength function to introduce a higher-level influence on the segmentation process. We formulate segmentation as the minimization of three coupled functionals, respectively, defining three processes: prior-guided segmentation, shape feature extraction and local deformation estimation. Particularly, the shape feature extraction process is in charge of estimating an edge strength function from the evolving object region. The local deformation estimation process uses this function to determine a meaningful correspondence between a given prior and the evolving object region, and the deformation map estimated in return supervises the segmentation by enforcing the evolving object boundary towards the prior shape.","A new image segmentation framework is presented which incorporates an edge strength function as a shape prior to introduce higher-level control over the segmentation process. The segmentation is formulated as the minimization of three functionals, each representing a distinct process: prior-guided segmentation, shape feature extraction, and local deformation estimation. The shape feature extraction process estimates an edge strength function from the object region while the local deformation estimation process utilizes the function to find a correspondence between the prior and the object region, the deformation map then guiding the segmentation by guiding the object boundary to the prior shape.",1
"This paper presents a new image segmentation framework which employs a shape prior in the form of an edge strength function to introduce a higher-level influence on the segmentation process. We formulate segmentation as the minimization of three coupled functionals, respectively, defining three processes: prior-guided segmentation, shape feature extraction and local deformation estimation. Particularly, the shape feature extraction process is in charge of estimating an edge strength function from the evolving object region. The local deformation estimation process uses this function to determine a meaningful correspondence between a given prior and the evolving object region, and the deformation map estimated in return supervises the segmentation by enforcing the evolving object boundary towards the prior shape.","Представлено нову структуру сегментації зображення, яка включає функцію міцності країв як форми перед впровадженням контролю вищого рівня над процесом сегментації. Сегментація сформульована як мінімізація трьох функціоналів, кожен з яких представляє окремий процес: попередньо керована сегментація, виділення ознак форми та локальна оцінка деформації. Процес виділення ознак форми оцінює функцію міцності краю з області об’єкта, тоді як процес оцінки локальної деформації використовує функцію для пошуку відповідності між попередньою та областю об’єкта, а потім карта деформації спрямовує сегментацію, спрямовуючи межу об’єкта до попередньої форму.",1
"У цьому документі представлено нову структуру сегментації зображення, яка використовує попередні форми у формі функції міцності країв, щоб запровадити вплив вищого рівня на процес сегментації. Ми формулюємо сегментацію як мінімізацію трьох пов’язаних функціоналів відповідно, визначаючи три процеси: попередньо керовану сегментацію, виділення ознак форми та локальну оцінку деформації. Зокрема, процес виділення ознак форми відповідає за оцінку функції міцності країв з області об’єкта, що розвивається. Процес локальної оцінки деформації використовує цю функцію для визначення значущої відповідності між даним попереднім і еволюційною областю об’єкта, а карта деформації, оцінена у свою чергу, контролює сегментацію, примушуючи еволюцію межі об’єкта до попередньої форми.","A new image segmentation framework is presented which incorporates an edge strength function as a shape prior to introduce higher-level control over the segmentation process. The segmentation is formulated as the minimization of three functionals, each representing a distinct process: prior-guided segmentation, shape feature extraction, and local deformation estimation. The shape feature extraction process estimates an edge strength function from the object region while the local deformation estimation process utilizes the function to find a correspondence between the prior and the object region, the deformation map then guiding the segmentation by guiding the object boundary to the prior shape.",1
"У цьому документі представлено нову структуру сегментації зображення, яка використовує попередні форми у формі функції міцності країв, щоб запровадити вплив вищого рівня на процес сегментації. Ми формулюємо сегментацію як мінімізацію трьох пов’язаних функціоналів, відповідно, визначаючи три процеси: попередньо керовану сегментацію, виділення ознак форми та локальну оцінку деформації. Зокрема, процес виділення ознак форми відповідає за оцінку функції міцності країв з області об’єкта, що розвивається. Процес локальної оцінки деформації використовує цю функцію для визначення значущої відповідності між даним попереднім і еволюційною областю об’єкта, а карта деформації, оцінена у свою чергу, контролює сегментацію, примушуючи еволюцію межі об’єкта до попередньої форми.","Представлено нову структуру сегментації зображення, яка включає функцію міцності країв як форми перед впровадженням контролю вищого рівня над процесом сегментації. Сегментація сформульована як мінімізація трьох функціоналів, кожен з яких представляє окремий процес: попередньо керована сегментація, виділення ознак форми та локальна оцінка деформації. Процес виділення ознак форми оцінює функцію міцності краю з області об’єкта, тоді як процес оцінки локальної деформації використовує функцію для пошуку відповідності між попередньою та областю об’єкта, а потім карта деформації спрямовує сегментацію, спрямовуючи межу об’єкта до попередньої форму.",1
"We present a simple and robust feature preserving image regularization by letting local region measures modulate the diffusivity. The purpose of this modulation is to disambiguate low level cues in early vision. We interpret the Ambrosio-Tortorelli approximation of the Mumford-Shah model as a system with modulatory feedback and utilize this interpretation to integrate high level information into the regularization process. The method does not require any prior model or learning; the high level information is extracted from local regions and fed back to the regularization step. An important characteristic of the method is that both negative and positive feedback can be simultaneously used without creating oscillations. Experiments performed with both gray and color natural images demonstrate the potential of the method under difficult noise types, non-uniform contrast, existence of multi-scale patterns and textures.","We propose a robust, feature-preserving image regularization that modulates diffusivity using local region measures. This disambiguates low-level cues in early vision. The Ambrosio-Tortorelli approximation of the Mumford-Shah model is interpreted as a system with modulatory feedback, which allows us to integrate high-level information into the regularization process. The method does not need prior models or learning; local regions provide the high-level information which is fed back to the regularization. Our method has the advantage of using both negative and positive feedback simultaneously, without oscillations. Experiments on gray and color natural images demonstrate the potential of our method for dealing with difficult noise types, non-uniform contrast, multi-scale patterns, and textures.",1
"We present a simple and robust feature preserving image regularization by letting local region measures modulate the diffusivity. The purpose of this modulation is to disambiguate low level cues in early vision. We interpret the Ambrosio-Tortorelli approximation of the Mumford-Shah model as a system with modulatory feedback and utilize this interpretation to integrate high level information into the regularization process. The method does not require any prior model or learning; the high level information is extracted from local regions and fed back to the regularization step. An important characteristic of the method is that both negative and positive feedback can be simultaneously used without creating oscillations. Experiments performed with both gray and color natural images demonstrate the potential of the method under difficult noise types, non-uniform contrast, existence of multi-scale patterns and textures.","Ми пропонуємо надійну регулярізацію зображення зі збереженням функцій, яка модулює дифузію за допомогою локальних регіональних заходів. Це усуває неоднозначність сигналів низького рівня в ранньому баченні. Апроксимація Амбросіо-Тортореллі моделі Мамфорда-Шаха інтерпретується як система з модулюючим зворотним зв&#39;язком, що дозволяє нам інтегрувати інформацію високого рівня в процес регуляризації. Метод не потребує попередніх моделей або навчання; локальні регіони надають інформацію високого рівня, яка повертається до регулярізації. Перевага нашого методу полягає в тому, що він використовує як негативний, так і позитивний зворотний зв’язок одночасно, без коливань. Експерименти на сірих і кольорових природних зображеннях демонструють потенціал нашого методу для роботи зі складними типами шуму, нерівномірним контрастом, багатомасштабними візерунками та текстурами.",1
"Ми представляємо просту та надійну функцію, яка зберігає регулярізацію зображення, дозволяючи вимірюванням локальних регіонів модулювати дифузію. Метою цієї модуляції є усунення неоднозначності сигналів низького рівня в ранньому баченні. Ми інтерпретуємо наближення Амбросіо-Тортореллі моделі Мамфорда-Шаха як систему з модулюючим зворотним зв&#39;язком і використовуємо цю інтерпретацію для інтеграції інформації високого рівня в процес регулярізації. Метод не потребує попередньої моделі чи навчання; інформація високого рівня витягується з локальних регіонів і повертається на етап регулярізації. Важливою характеристикою методу є те, що як негативний, так і позитивний зворотний зв&#39;язок можуть використовуватися одночасно без створення коливань. Експерименти, проведені як з сірими, так і з кольоровими природними зображеннями, демонструють потенціал методу в умовах складних типів шуму, нерівномірного контрасту, наявності багатомасштабних візерунків і текстур.","We propose a robust, feature-preserving image regularization that modulates diffusivity using local region measures. This disambiguates low-level cues in early vision. The Ambrosio-Tortorelli approximation of the Mumford-Shah model is interpreted as a system with modulatory feedback, which allows us to integrate high-level information into the regularization process. The method does not need prior models or learning; local regions provide the high-level information which is fed back to the regularization. Our method has the advantage of using both negative and positive feedback simultaneously, without oscillations. Experiments on gray and color natural images demonstrate the potential of our method for dealing with difficult noise types, non-uniform contrast, multi-scale patterns, and textures.",1
"Ми представляємо просту та надійну функцію, яка зберігає регулярізацію зображення, дозволяючи вимірюванням локальних регіонів модулювати дифузію. Метою цієї модуляції є усунення неоднозначності сигналів низького рівня в ранньому баченні. Ми інтерпретуємо наближення Амбросіо-Тортореллі моделі Мамфорда-Шаха як систему з модулюючим зворотним зв&#39;язком і використовуємо цю інтерпретацію для інтеграції інформації високого рівня в процес регулярізації. Метод не потребує попередньої моделі чи навчання; інформація високого рівня витягується з локальних регіонів і повертається на етап регулярізації. Важливою характеристикою методу є те, що як негативний, так і позитивний зворотний зв&#39;язок можуть використовуватися одночасно без створення коливань. Експерименти, проведені як з сірими, так і з кольоровими природними зображеннями, демонструють потенціал методу в умовах складних типів шуму, нерівномірного контрасту, наявності багатомасштабних візерунків і текстур.","Ми пропонуємо надійну регулярізацію зображення зі збереженням функцій, яка модулює дифузію за допомогою локальних регіональних заходів. Це усуває неоднозначність сигналів низького рівня в ранньому баченні. Апроксимація Амбросіо-Тортореллі моделі Мамфорда-Шаха інтерпретується як система з модулюючим зворотним зв&#39;язком, що дозволяє нам інтегрувати інформацію високого рівня в процес регуляризації. Метод не потребує попередніх моделей або навчання; локальні регіони надають інформацію високого рівня, яка повертається до регулярізації. Перевага нашого методу полягає в тому, що він використовує як негативний, так і позитивний зворотний зв’язок одночасно, без коливань. Експерименти на сірих і кольорових природних зображеннях демонструють потенціал нашого методу для роботи зі складними типами шуму, нерівномірним контрастом, багатомасштабними візерунками та текстурами.",1
"We present a new skeletal representation along with a matching framework to address the deformable shape recognition problem. The disconnectedness arises as a result of excessive regularization that we use to describe a shape at an attainably coarse scale. Our motivation is to rely on the stable properties of the shape instead of inaccurately measured secondary details. The new representation does not suffer from the common instability problems of traditional connected skeletons and the matching process gives quite successful results on a diverse database of 2D shapes. An important difference of our approach from the conventional use of the skeleton is that we replace the local coordinate frame with a global euclidean frame supported by additional mechanisms to handle articulations and local boundary deformations. As a result, we can produce descriptions that are sensitive to any combination of changes in scale, position, orientation, and articulation, as well as invariant ones.","We propose a new skeletal representation and a matching framework to address deformable shape recognition. We regularize the shape at a coarse scale to focus on its stable properties instead of inaccurately measured secondary details. Our representation is immune to instability problems of connected skeletons, and the matching process yields successful results on a diverse 2D shape database. Instead of local coordinates, our approach utilizes a global Euclidean frame with additional mechanisms for articulations and boundary deformations. Consequently, we can generate descriptions that are sensitive to any combination of changes in scale, position, orientation, and articulation, as well as invariant ones.",1
"We present a new skeletal representation along with a matching framework to address the deformable shape recognition problem. The disconnectedness arises as a result of excessive regularization that we use to describe a shape at an attainably coarse scale. Our motivation is to rely on the stable properties of the shape instead of inaccurately measured secondary details. The new representation does not suffer from the common instability problems of traditional connected skeletons and the matching process gives quite successful results on a diverse database of 2D shapes. An important difference of our approach from the conventional use of the skeleton is that we replace the local coordinate frame with a global euclidean frame supported by additional mechanisms to handle articulations and local boundary deformations. As a result, we can produce descriptions that are sensitive to any combination of changes in scale, position, orientation, and articulation, as well as invariant ones.","Ми пропонуємо нове скелетне представлення та відповідну структуру для розпізнавання форми, що деформується. Ми регулюємо форму в грубому масштабі, щоб зосередитися на її стабільних властивостях замість неточно виміряних вторинних деталей. Наше представлення захищено від проблем нестабільності з’єднаних скелетів, і процес зіставлення дає успішні результати в різноманітній базі даних двовимірних форм. Замість локальних координат наш підхід використовує глобальну евклідову систему з додатковими механізмами для артикуляції та граничних деформацій. Отже, ми можемо генерувати описи, чутливі до будь-якої комбінації змін у масштабі, положенні, орієнтації та артикуляції, а також інваріантні.",1
"Ми представляємо нове скелетне представлення разом із відповідним каркасом для вирішення проблеми розпізнавання форми, що деформується. Роз’єднаність виникає в результаті надмірної регуляризації, яку ми використовуємо для опису форми в досяжно грубому масштабі. Наша мотивація полягає в тому, щоб покладатися на стабільні властивості форми замість неточно виміряних вторинних деталей. Нове представлення не страждає від поширених проблем нестабільності традиційних з’єднаних скелетів, і процес зіставлення дає досить успішні результати на різноманітній базі даних двовимірних форм. Важлива відмінність нашого підходу від звичайного використання скелета полягає в тому, що ми замінюємо локальну систему координат на глобальну евклідову систему, яка підтримується додатковими механізмами для обробки артикуляцій і локальних граничних деформацій. У результаті ми можемо створювати описи, чутливі до будь-якої комбінації змін у масштабі, положенні, орієнтації та артикуляції, а також інваріантні.","We propose a new skeletal representation and a matching framework to address deformable shape recognition. We regularize the shape at a coarse scale to focus on its stable properties instead of inaccurately measured secondary details. Our representation is immune to instability problems of connected skeletons, and the matching process yields successful results on a diverse 2D shape database. Instead of local coordinates, our approach utilizes a global Euclidean frame with additional mechanisms for articulations and boundary deformations. Consequently, we can generate descriptions that are sensitive to any combination of changes in scale, position, orientation, and articulation, as well as invariant ones.",1
"Ми представляємо нове скелетне представлення разом із відповідним каркасом для вирішення проблеми розпізнавання форми, що деформується. Роз’єднаність виникає в результаті надмірної регуляризації, яку ми використовуємо для опису форми в досяжно грубому масштабі. Наша мотивація полягає в тому, щоб покладатися на стабільні властивості форми замість неточно виміряних вторинних деталей. Нове представлення не страждає від поширених проблем нестабільності традиційних з’єднаних скелетів, і процес зіставлення дає досить успішні результати на різноманітній базі даних двовимірних форм. Важлива відмінність нашого підходу від звичайного використання скелета полягає в тому, що ми замінюємо локальну систему координат на глобальну евклідову систему, яка підтримується додатковими механізмами для обробки артикуляцій і локальних граничних деформацій. У результаті ми можемо створювати описи, чутливі до будь-якої комбінації змін у масштабі, положенні, орієнтації та артикуляції, а також інваріантні.","Ми пропонуємо нове скелетне представлення та відповідну структуру для розпізнавання форми, що деформується. Ми регулюємо форму в грубому масштабі, щоб зосередитися на її стабільних властивостях замість неточно виміряних вторинних деталей. Наше представлення захищено від проблем нестабільності з’єднаних скелетів, і процес зіставлення дає успішні результати в різноманітній базі даних двовимірних форм. Замість локальних координат наш підхід використовує глобальну евклідову систему з додатковими механізмами для артикуляції та граничних деформацій. Отже, ми можемо генерувати описи, чутливі до будь-якої комбінації змін у масштабі, положенні, орієнтації та артикуляції, а також інваріантні.",1
"As recently discussed by Bar, Kiryati, and Sochen in [3], the Ambrosio-Tortorelli approximation of the Mumford-Shah functional defines an extended line process regularization where the regularizer has an additional constraint introduced by the term ρ|∇v| . This term mildly forces some spatial organization by demanding that the edges are smooth. However, it does not force spatial coherence such as edge direction compatibility or edge connectivity, as in the traditional edge detectors such as Canny. Using the connection between regularization and diffusion filters, we incorporate further spatial structure into the regularization process of the Mumford-Shah model. The new model combines smoothing, edge detection and edge linking steps of the traditional approach to boundary detection. Importance of spatial coherence is best observed if the image noise is salt and pepper like. Proposed approach is able to deal with difficult noise cases without using non-smooth cost functions such as  in the data fidelity or regularizer. ","Bar, Kiryati, and Sochen [3] discussed the Ambrosio-Tortorelli approximation of the Mumford-Shah functional, which introduces an extra constraint to the regularizer in the form of ρ|∇v|, thus mildly encouraging spatial organization. However, it does not enforce coherence such as edge direction compatibility or edge connectivity as seen in traditional edge detectors such as Canny. To incorporate further spatial structure into the regularization process of the Mumford-Shah model, we established a connection between regularization and diffusion filters. The new model combines smoothing, edge detection and edge linking steps of the traditional approach for boundary detection, particularly when the image noise is salt and pepper like. This approach is able to handle difficult noise cases without using non-smooth cost functions in the data fidelity or regularizer.",1
"As recently discussed by Bar, Kiryati, and Sochen in [3], the Ambrosio-Tortorelli approximation of the Mumford-Shah functional defines an extended line process regularization where the regularizer has an additional constraint introduced by the term ρ|∇v| . This term mildly forces some spatial organization by demanding that the edges are smooth. However, it does not force spatial coherence such as edge direction compatibility or edge connectivity, as in the traditional edge detectors such as Canny. Using the connection between regularization and diffusion filters, we incorporate further spatial structure into the regularization process of the Mumford-Shah model. The new model combines smoothing, edge detection and edge linking steps of the traditional approach to boundary detection. Importance of spatial coherence is best observed if the image noise is salt and pepper like. Proposed approach is able to deal with difficult noise cases without using non-smooth cost functions such as  in the data fidelity or regularizer. ","Бар, Кіріаті та Сочен [3] обговорювали наближення Амбросіо-Тортореллі функціоналу Мамфорда-Шаха, який вводить додаткове обмеження на регуляризатор у формі ρ|∇v|, таким чином помірно заохочуючи просторову організацію. Однак він не забезпечує когерентності, такої як сумісність напрямків країв або з’єднання країв, як це видно в традиційних детекторах країв, таких як Canny. Щоб включити подальшу просторову структуру в процес регуляризації моделі Мамфорда-Шаха, ми встановили зв’язок між регуляризацією та дифузійними фільтрами. Нова модель поєднує згладжування, виявлення країв і з’єднання країв традиційного підходу до виявлення меж, особливо коли шум зображення схожий на сіль і перець. Цей підхід здатний обробляти складні випадки шуму без використання негладких функцій вартості в точності даних або регуляризаторі.",1
"Як нещодавно обговорювали Бар, Кіріаті та Сочен у [3], апроксимація Амбросіо-Тортореллі функціоналу Мамфорда-Шаха визначає розширену регулярізацію лінійного процесу, де регулязатор має додаткове обмеження, введене членом ρ|∇v| . Цей термін м’яко нав’язує певну просторову організацію, вимагаючи гладкості країв. Однак він не забезпечує просторову когерентність, таку як сумісність напрямків країв або з’єднання країв, як у традиційних детекторах країв, таких як Canny. Використовуючи зв’язок між регуляризацією та дифузійними фільтрами, ми включаємо подальшу просторову структуру в процес регуляризації моделі Мамфорда-Шаха. Нова модель поєднує згладжування, визначення країв і з’єднання країв традиційного підходу до визначення меж. Важливість просторової когерентності найкраще спостерігається, якщо шум зображення схожий на сіль і перець. Запропонований підхід здатний працювати зі складними випадками шуму без використання негладких функцій вартості, таких як у точності даних або регуляризаторі.","Bar, Kiryati, and Sochen [3] discussed the Ambrosio-Tortorelli approximation of the Mumford-Shah functional, which introduces an extra constraint to the regularizer in the form of ρ|∇v|, thus mildly encouraging spatial organization. However, it does not enforce coherence such as edge direction compatibility or edge connectivity as seen in traditional edge detectors such as Canny. To incorporate further spatial structure into the regularization process of the Mumford-Shah model, we established a connection between regularization and diffusion filters. The new model combines smoothing, edge detection and edge linking steps of the traditional approach for boundary detection, particularly when the image noise is salt and pepper like. This approach is able to handle difficult noise cases without using non-smooth cost functions in the data fidelity or regularizer.",1
"Як нещодавно обговорювали Бар, Кіріаті та Сочен у [3], апроксимація Амбросіо-Тортореллі функціоналу Мамфорда-Шаха визначає розширену регулярізацію лінійного процесу, де регулязатор має додаткове обмеження, введене членом ρ|∇v| . Цей термін м’яко вимагає певної просторової організації, вимагаючи гладкості країв. Однак він не забезпечує просторову когерентність, таку як сумісність напрямків країв або з’єднання країв, як у традиційних детекторах країв, таких як Canny. Використовуючи зв’язок між регуляризацією та дифузійними фільтрами, ми включаємо подальшу просторову структуру в процес регуляризації моделі Мамфорда-Шаха. Нова модель поєднує згладжування, визначення країв і з’єднання країв традиційного підходу до виявлення меж. Важливість просторової когерентності найкраще спостерігається, якщо шум зображення схожий на сіль і перець. Запропонований підхід здатний працювати зі складними випадками шуму без використання негладких функцій вартості, таких як у точності даних або регуляризаторі.","Бар, Кіріаті та Сочен [3] обговорювали наближення Амбросіо-Тортореллі функціоналу Мамфорда-Шаха, який вводить додаткове обмеження на регуляризатор у формі ρ|∇v|, таким чином помірно заохочуючи просторову організацію. Однак він не забезпечує когерентності, такої як сумісність напрямків країв або з’єднання країв, як це видно в традиційних детекторах країв, таких як Canny. Щоб включити подальшу просторову структуру в процес регуляризації моделі Мамфорда-Шаха, ми встановили зв’язок між регуляризацією та дифузійними фільтрами. Нова модель поєднує згладжування, виявлення країв і з’єднання країв традиційного підходу до виявлення меж, особливо коли шум зображення схожий на сіль і перець. Цей підхід здатний обробляти складні випадки шуму без використання негладких функцій вартості в точності даних або регуляризаторі.",1
"Local symmetry axis based schemes have been used for generic shape recognition as they lead to articulation insensitive representations. Despite their strengths, purely syntactic level of axial representations precludes the possibility of distinguishing a likely articulation from an unlikely one. In order to overcome this weakness, syntax should be combined with pragmatics and/or semantics. As a solution we propose a novel articulation space which enables inferences on the likelihood of possible articulations. Articulation priors can be constructed directly from examples (pragmatics) or set externally (semantics). We incorporate articulation priors to a skeletal matching scheme to arrive at an enriched axial representation which is sensitive to unlikely articulations but insensitive to likely ones","Axial representations, while providing articulation-insensitive representations for generic shape recognition, are unable to distinguish between likely and unlikely articulations. To address this issue, we propose a novel articulation space which allows inferences on the probability of possible articulations. Articulation priors can be either derived from examples or set externally, and are incorporated into a skeletal matching scheme to create an enriched axial representation that is sensitive to unlikely articulations yet insensitive to likely ones.",1
"Local symmetry axis based schemes have been used for generic shape recognition as they lead to articulation insensitive representations. Despite their strengths, purely syntactic level of axial representations precludes the possibility of distinguishing a likely articulation from an unlikely one. In order to overcome this weakness, syntax should be combined with pragmatics and/or semantics. As a solution we propose a novel articulation space which enables inferences on the likelihood of possible articulations. Articulation priors can be constructed directly from examples (pragmatics) or set externally (semantics). We incorporate articulation priors to a skeletal matching scheme to arrive at an enriched axial representation which is sensitive to unlikely articulations but insensitive to likely ones","Осьові представлення, хоча й забезпечують нечутливі до артикуляції представлення для загального розпізнавання форми, не можуть розрізнити вірогідні та малоймовірні артикуляції. Щоб вирішити цю проблему, ми пропонуємо новий артикуляційний простір, який дозволяє робити висновки про ймовірність можливих артикуляцій. Артикуляційні пріоритети можуть бути отримані з прикладів або встановлені ззовні та включені в скелетну схему відповідності для створення збагаченого осьового представлення, чутливого до малоймовірних артикуляцій, але нечутливого до ймовірних.",1
"Схеми, засновані на локальній осі симетрії, використовувалися для загального розпізнавання форми, оскільки вони призводили до представлень, нечутливих до артикуляції. Незважаючи на їхні сильні сторони, суто синтаксичний рівень осьових репрезентацій виключає можливість відрізнити ймовірну артикуляцію від малоймовірної. Щоб подолати цю слабкість, синтаксис слід поєднувати з прагматикою та/або семантикою. Як рішення ми пропонуємо новий артикуляційний простір, який дозволяє робити висновки про ймовірність можливих артикуляцій. Пріори артикуляції можуть бути побудовані безпосередньо з прикладів (прагматика) або задані зовні (семантика). Ми включаємо артикуляцію перед скелетною схемою відповідності, щоб досягти збагаченого осьового представлення, чутливого до малоймовірних артикуляцій, але нечутливого до ймовірних","Axial representations, while providing articulation-insensitive representations for generic shape recognition, are unable to distinguish between likely and unlikely articulations. To address this issue, we propose a novel articulation space which allows inferences on the probability of possible articulations. Articulation priors can be either derived from examples or set externally, and are incorporated into a skeletal matching scheme to create an enriched axial representation that is sensitive to unlikely articulations yet insensitive to likely ones.",1
"Схеми, засновані на локальній осі симетрії, використовувалися для загального розпізнавання форми, оскільки вони призводили до представлень, нечутливих до артикуляції. Незважаючи на їхні сильні сторони, суто синтаксичний рівень осьових репрезентацій виключає можливість відрізнити ймовірну артикуляцію від малоймовірної. Щоб подолати цю слабкість, синтаксис слід поєднувати з прагматикою та/або семантикою. Як рішення ми пропонуємо новий артикуляційний простір, який дозволяє робити висновки про ймовірність можливих артикуляцій. Пріори артикуляції можуть бути побудовані безпосередньо з прикладів (прагматика) або задані зовні (семантика). Ми включаємо артикуляцію перед скелетною схемою відповідності, щоб досягти збагаченого осьового представлення, чутливого до малоймовірних артикуляцій, але нечутливого до ймовірних","Осьові представлення, хоча й забезпечують нечутливі до артикуляції представлення для загального розпізнавання форми, не можуть розрізнити вірогідні та малоймовірні артикуляції. Щоб вирішити цю проблему, ми пропонуємо новий артикуляційний простір, який дозволяє робити висновки про ймовірність можливих артикуляцій. Артикуляційні пріоритети можуть бути отримані з прикладів або встановлені ззовні та включені в скелетну схему відповідності для створення збагаченого осьового представлення, чутливого до малоймовірних артикуляцій, але нечутливого до ймовірних.",1
"Many applications of computer vision requires segmenting out of an object of interest from a given image. Motivated by unlevel-sets formulation of Raviv, Kiryati and Sochen [8] and statistical formulation of Leventon, Grimson and Faugeras [6], we present a new image segmentation method which accounts for prior shape information. Our method depends on Ambrosio-Tortorelli approximation of Mumford-Shah functional. The prior shape is represented by a by-product of this functional, a smooth edge indicator function, known as the “edge strength function”, which provides a distance-like surface for the shape boundary. Our method can handle arbitrary deformations due to shape variability as well as plane Euclidean transformations. The method is also robust with respect to noise and missing parts. Furthermore, this formulation does not require simple closed curves as in a typical level set formulation.","Motivated by the unlevel-sets formulation of Raviv, Kiryati and Sochen [8] and the statistical formulation of Leventon, Grimson and Faugeras [6], a new image segmentation method has been proposed which accounts for prior shape information. This method relies on the Ambrosio-Tortorelli approximation of the Mumford-Shah functional and uses the “edge strength function”, a smooth edge indicator function, as a distance-like surface for the shape boundary. This method is capable of handling arbitrary deformations due to shape variability as well as plane Euclidean transformations, while being robust with respect to noise and missing parts. Additionally, this formulation does not require simple closed curves as in a typical level set formulation.",1
"Many applications of computer vision requires segmenting out of an object of interest from a given image. Motivated by unlevel-sets formulation of Raviv, Kiryati and Sochen [8] and statistical formulation of Leventon, Grimson and Faugeras [6], we present a new image segmentation method which accounts for prior shape information. Our method depends on Ambrosio-Tortorelli approximation of Mumford-Shah functional. The prior shape is represented by a by-product of this functional, a smooth edge indicator function, known as the “edge strength function”, which provides a distance-like surface for the shape boundary. Our method can handle arbitrary deformations due to shape variability as well as plane Euclidean transformations. The method is also robust with respect to noise and missing parts. Furthermore, this formulation does not require simple closed curves as in a typical level set formulation.","На підставі формулювання безрівневих наборів Равіва, Кір’яті та Сочена [8] та статистичного формулювання Левентона, Грімсона та Фогераса [6] було запропоновано новий метод сегментації зображення, який враховує попередню інформацію про форму. Цей метод ґрунтується на апроксимації Амбросіо-Тортореллі функціоналу Мамфорда-Ша та використовує «функцію міцності краю», функцію індикатора гладкого краю, як поверхню, подібну до відстані, для межі форми. Цей метод здатний обробляти довільні деформації через мінливість форми, а також плоскі евклідові перетворення, водночас він надійний щодо шуму та відсутніх частин. Крім того, це формулювання не потребує простих замкнутих кривих, як у типовому формулюванні набору рівнів.",1
"Багато застосувань комп’ютерного зору вимагають сегментування об’єкта інтересу з даного зображення. Спираючись на формулювання нерівневих наборів Равіва, Кір’яті та Сочена [8] та статистичне формулювання Левентона, Грімсона та Фогера [6], ми представляємо новий метод сегментації зображення, який враховує попередню інформацію про форму. Наш метод залежить від апроксимації Амбросіо-Тортореллі функціоналу Мамфорда-Шаха. Попередня форма представлена побічним продуктом цього функціоналу, функцією індикатора гладких країв, відомою як «функція сили краю», яка забезпечує поверхню, подібну до відстані, для межі форми. Наш метод може обробляти довільні деформації через мінливість форми, а також плоскі евклідові перетворення. Метод також надійний щодо шуму та відсутніх частин. Крім того, це формулювання не вимагає простих замкнутих кривих, як у типовому формулюванні набору рівнів.","Motivated by the unlevel-sets formulation of Raviv, Kiryati and Sochen [8] and the statistical formulation of Leventon, Grimson and Faugeras [6], a new image segmentation method has been proposed which accounts for prior shape information. This method relies on the Ambrosio-Tortorelli approximation of the Mumford-Shah functional and uses the “edge strength function”, a smooth edge indicator function, as a distance-like surface for the shape boundary. This method is capable of handling arbitrary deformations due to shape variability as well as plane Euclidean transformations, while being robust with respect to noise and missing parts. Additionally, this formulation does not require simple closed curves as in a typical level set formulation.",1
"Багато застосувань комп’ютерного зору вимагають сегментування об’єкта інтересу з даного зображення. Спираючись на формулювання нерівневих наборів Равіва, Кір’яті та Сочена [8] та статистичне формулювання Левентона, Грімсона та Фогера [6], ми представляємо новий метод сегментації зображення, який враховує попередню інформацію про форму. Наш метод залежить від апроксимації Амбросіо-Тортореллі функціонала Мамфорда-Шаха. Попередня форма представлена побічним продуктом цього функціоналу, функцією індикатора гладких країв, відомою як «функція сили краю», яка забезпечує поверхню, подібну до відстані, для межі форми. Наш метод може обробляти довільні деформації через мінливість форми, а також плоскі евклідові перетворення. Метод також надійний щодо шуму та відсутніх частин. Крім того, це формулювання не вимагає простих замкнутих кривих, як у типовому формулюванні набору рівнів.","На підставі формулювання безрівневих наборів Равіва, Кір’яті та Сочена [8] та статистичного формулювання Левентона, Грімсона та Фогераса [6] було запропоновано новий метод сегментації зображення, який враховує попередню інформацію про форму. Цей метод ґрунтується на апроксимації Амбросіо-Тортореллі функціоналу Мамфорда-Ша та використовує «функцію міцності краю», функцію індикатора гладкого краю, як поверхню, подібну до відстані, для межі форми. Цей метод здатний обробляти довільні деформації через мінливість форми, а також плоскі евклідові перетворення, водночас він надійний щодо шуму та відсутніх частин. Крім того, це формулювання не потребує простих замкнутих кривих, як у типовому формулюванні набору рівнів.",1
"It is now possible to design real-time, low-cost computer vision systems even in personal computers due to the recent advances in electronics and the computer industry. Due to this reason, it is feasible to develop computer-vision-based human-computer interaction systems. A vision-based continuous Graffiti™-like text entry system is presented. The user sketches characters in a Graffiti™-like alphabet in a continuous manner on a flat surface using a laser pointer. The beam of the laser pointer is tracked on the image sequences captured by a camera, and the corresponding written word is recognized from the extracted trace of the laser beam.","Recent advancements in electronics and the computer industry have enabled the creation of low-cost, real-time computer vision systems on personal computers. This has made it possible to build human-computer interaction systems based on computer vision. An example of this is a vision-based continuous Graffiti™-like text entry system. This system allows the user to sketch characters in a Graffiti™-like alphabet using a laser pointer on a flat surface. The camera captures image sequences of the laser beam and the corresponding written word is then recognized from the trace of the laser beam.",1
"It is now possible to design real-time, low-cost computer vision systems even in personal computers due to the recent advances in electronics and the computer industry. Due to this reason, it is feasible to develop computer-vision-based human-computer interaction systems. A vision-based continuous Graffiti™-like text entry system is presented. The user sketches characters in a Graffiti™-like alphabet in a continuous manner on a flat surface using a laser pointer. The beam of the laser pointer is tracked on the image sequences captured by a camera, and the corresponding written word is recognized from the extracted trace of the laser beam.","Останні досягнення в електроніці та комп’ютерній індустрії дозволили створити недорогі системи комп’ютерного зору в реальному часі на персональних комп’ютерах. Це дозволило створювати системи взаємодії людини з комп’ютером на основі комп’ютерного зору. Прикладом цього є безперервна система введення тексту, подібна до Graffiti™ на основі візуалізації. Ця система дозволяє користувачеві малювати символи алфавітом, схожим на Graffiti™, за допомогою лазерної указки на плоскій поверхні. Камера фіксує послідовності зображень лазерного променя, а потім розпізнає відповідне написане слово за слідом лазерного променя.",1
"Завдяки останнім досягненням електроніки та комп’ютерної індустрії тепер можливо розробляти недорогі системи комп’ютерного зору в режимі реального часу навіть у персональних комп’ютерах. З цієї причини можливо розробити системи взаємодії людини з комп’ютером на основі комп’ютерного зору. Представлена безперервна система введення тексту, подібна до Graffiti™ на основі vision. Користувач безперервно малює символи алфавіту, схожого на Graffiti™, на плоскій поверхні за допомогою лазерної указки. Промінь лазерної вказівки відстежується на послідовності зображень, знятих камерою, і відповідне написане слово розпізнається за вилученим слідом лазерного променя.","Recent advancements in electronics and the computer industry have enabled the creation of low-cost, real-time computer vision systems on personal computers. This has made it possible to build human-computer interaction systems based on computer vision. An example of this is a vision-based continuous Graffiti™-like text entry system. This system allows the user to sketch characters in a Graffiti™-like alphabet using a laser pointer on a flat surface. The camera captures image sequences of the laser beam and the corresponding written word is then recognized from the trace of the laser beam.",1
"Завдяки останнім досягненням електроніки та комп’ютерної індустрії тепер можливо розробляти недорогі системи комп’ютерного зору в режимі реального часу навіть у персональних комп’ютерах. З цієї причини можливо розробити системи взаємодії людини з комп’ютером на основі комп’ютерного зору. Представлена безперервна система введення тексту, подібна до Graffiti™ на основі vision. Користувач безперервно малює символи алфавіту, схожого на Graffiti™, на плоскій поверхні за допомогою лазерної указки. Промінь лазерної вказівки відстежується на послідовності зображень, знятих камерою, і відповідне написане слово розпізнається за вилученим слідом лазерного променя.","Останні досягнення в електроніці та комп’ютерній індустрії дозволили створити недорогі системи комп’ютерного зору в реальному часі на персональних комп’ютерах. Це дозволило створювати системи взаємодії людини з комп’ютером на основі комп’ютерного зору. Прикладом цього є безперервна система введення тексту, подібна до Graffiti™ на основі візуалізації. Ця система дозволяє користувачеві малювати символи алфавітом, схожим на Graffiti™, за допомогою лазерної указки на плоскій поверхні. Камера фіксує послідовності зображень лазерного променя, а потім розпізнає відповідне написане слово за слідом лазерного променя.",1
"In this paper, a unistroke keyboard based on computer vision is described for the handicapped. The keyboard can be made of paper or fabric containing an image of a keyboard, which has an upside down U-shape. It can even be displayed on a computer screen. Each character is represented by a non-overlapping rectangular region on the keyboard image and the user enters a character by illuminating a character region with a laser pointer. The keyboard image is monitored by a camera and illuminated key locations are recognized. During the text entry process the user neither have to tum the laser light off nor raise the laser light from the keyboard. A disabled person who bas difficulty using hisiher hands may attach the laser pointer to an eyeglass and easily enter text by moving hisher head to point the laser beam on a character location. In addition, a mouse-like device can be developed based on the same principle. The user can move the cursor by moving the laser light on the computer screen which is monitored by a camera. ","A unistroke keyboard is proposed for the handicapped which can be made of paper or fabric in an upside down U-shape. Each character is represented by a non-overlapping rectangular region and is entered by illuminating the region with a laser pointer. The user doesn't have to turn off or raise the laser light while entering text. A disabled person may attach the laser pointer to an eyeglass to enter text by moving their head. Furthermore, a mouse-like device based on the same principle can be developed where the user can move the cursor by moving the laser light on the computer screen which is monitored by a camera.",1
"In this paper, a unistroke keyboard based on computer vision is described for the handicapped. The keyboard can be made of paper or fabric containing an image of a keyboard, which has an upside down U-shape. It can even be displayed on a computer screen. Each character is represented by a non-overlapping rectangular region on the keyboard image and the user enters a character by illuminating a character region with a laser pointer. The keyboard image is monitored by a camera and illuminated key locations are recognized. During the text entry process the user neither have to tum the laser light off nor raise the laser light from the keyboard. A disabled person who bas difficulty using hisiher hands may attach the laser pointer to an eyeglass and easily enter text by moving hisher head to point the laser beam on a character location. In addition, a mouse-like device can be developed based on the same principle. The user can move the cursor by moving the laser light on the computer screen which is monitored by a camera. ","Для людей з обмеженими можливостями пропонується одноштрихова клавіатура, яку можна зробити з паперу або тканини у перевернутій U-подібній формі. Кожен символ представлено прямокутною областю, яка не перекривається, і вводиться шляхом освітлення області лазерним покажчиком. Користувачеві не потрібно вимикати або піднімати лазерне світло під час введення тексту. Людина з обмеженими можливостями може прикріпити лазерну указку до окуляра, щоб вводити текст, рухаючи головою. Крім того, можна розробити пристрій, схожий на мишу, заснований на тому ж принципі, де користувач може рухати курсор, переміщаючи лазерне світло на екрані комп’ютера, який контролюється камерою.",1
"У цій статті описано одноштрихову клавіатуру на основі комп’ютерного зору для людей з обмеженими можливостями. Клавіатура може бути зроблена з паперу або тканини, на якій зображено клавіатуру, яка має перевернуту U-подібну форму. Його можна навіть відобразити на екрані комп’ютера. Кожен символ представлений прямокутною областю, яка не перекривається, на зображенні клавіатури, і користувач вводить символ, підсвічуючи область символу лазерною вказівкою. Зображення клавіатури контролюється камерою, і розпізнаються підсвічені клавіші. Під час процесу введення тексту користувачеві не потрібно ні вимикати лазерне світло, ні піднімати його з клавіатури. Людина з обмеженими можливостями, якій важко користуватися своїми руками, може прикріпити лазерну указку до окулярів і легко вводити текст, рухаючи головою, щоб навести лазерний промінь на місце символу. Крім того, за таким же принципом можна розробити пристрій, схожий на мишу. Користувач може переміщати курсор, пересуваючи лазерне світло на екрані комп’ютера, який контролюється камерою.","A unistroke keyboard is proposed for the handicapped which can be made of paper or fabric in an upside down U-shape. Each character is represented by a non-overlapping rectangular region and is entered by illuminating the region with a laser pointer. The user doesn't have to turn off or raise the laser light while entering text. A disabled person may attach the laser pointer to an eyeglass to enter text by moving their head. Furthermore, a mouse-like device based on the same principle can be developed where the user can move the cursor by moving the laser light on the computer screen which is monitored by a camera.",1
"У цій статті описано одноштрихову клавіатуру на основі комп’ютерного зору для людей з обмеженими можливостями. Клавіатура може бути зроблена з паперу або тканини, на якій зображено клавіатуру, яка має перевернуту U-подібну форму. Його можна навіть відобразити на екрані комп’ютера. Кожен символ представлений прямокутною областю, яка не перекривається, на зображенні клавіатури, і користувач вводить символ, підсвічуючи область символу лазерною вказівкою. Зображення клавіатури контролюється камерою, і розпізнаються підсвічені клавіші. Під час процесу введення тексту користувачеві не потрібно ні вимикати лазерне світло, ні піднімати його з клавіатури. Людина з обмеженими можливостями, якій важко користуватися своїми руками, може прикріпити лазерну указку до окулярів і легко вводити текст, рухаючи головою, щоб навести лазерний промінь на місце символу. Крім того, за таким же принципом можна розробити пристрій, схожий на мишу. Користувач може переміщати курсор, пересуваючи лазерне світло на екрані комп’ютера, який контролюється камерою.","Для людей з обмеженими можливостями пропонується одноштрихова клавіатура, яку можна зробити з паперу або тканини у перевернутій U-подібній формі. Кожен символ представлено прямокутною областю, яка не перекривається, і вводиться шляхом освітлення області лазерним покажчиком. Користувачеві не потрібно вимикати або піднімати лазерне світло під час введення тексту. Людина з обмеженими можливостями може прикріпити лазерну указку до окуляра, щоб вводити текст, рухаючи головою. Крім того, можна розробити пристрій, схожий на мишу, заснований на тому ж принципі, де користувач може рухати курсор, переміщаючи лазерне світло на екрані комп’ютера, який контролюється камерою.",1
"In MediaEval 2016, we focus on the image interestingness subtask which involves predicting interesting key frames of a video in the form of a movie trailer. We specifically propose three different deep models for this subtask. The first two models are based on fine-tuning two pretrained models, namely AlexNet and MemNet, where we cast the interestingness prediction as a regression problem. Our third deep model, on the other hand, depends on a triplet network which is comprised of three instances of the same feedforward network with shared weights, and trained according to a triplet ranking loss. Our experiments demonstrate that all these models provide relatively similar and promising results on the image interestingness subtask.","For MediaEval 2016, we investigate the image interestingness subtask, which requires predicting interesting key frames of a video in the form of a movie trailer. We present three deep models for this task. The first two are based on fine-tuning AlexNet and MemNet, with the prediction being a regression problem. The third model is a triplet network, consisting of three feedforward networks with shared weights, and trained with a triplet ranking loss. Results from our experiments show that all these models provide comparable and satisfactory performance on the image interestingness subtask.",1
"In MediaEval 2016, we focus on the image interestingness subtask which involves predicting interesting key frames of a video in the form of a movie trailer. We specifically propose three different deep models for this subtask. The first two models are based on fine-tuning two pretrained models, namely AlexNet and MemNet, where we cast the interestingness prediction as a regression problem. Our third deep model, on the other hand, depends on a triplet network which is comprised of three instances of the same feedforward network with shared weights, and trained according to a triplet ranking loss. Our experiments demonstrate that all these models provide relatively similar and promising results on the image interestingness subtask.","Для MediaEval 2016 ми досліджуємо підзавдання щодо цікавості зображення, яке вимагає прогнозування цікавих ключових кадрів відео у формі трейлера фільму. Ми представляємо три глибокі моделі для цього завдання. Перші два засновані на точному налаштуванні AlexNet і MemNet, причому передбачення є проблемою регресії. Третя модель — це триплетна мережа, що складається з трьох мереж прямого зв’язку зі спільними вагами та навчена з втратою рейтингу триплетів. Результати наших експериментів показують, що всі ці моделі забезпечують порівнянну та задовільну продуктивність підзавдання щодо цікавості зображення.",1
"У MediaEval 2016 ми зосереджуємося на підзавданні щодо цікавості зображення, яке передбачає прогнозування цікавих ключових кадрів відео у формі трейлера фільму. Ми спеціально пропонуємо три різні глибокі моделі для цього підзавдання. Перші дві моделі базуються на тонкому налаштуванні двох попередньо навчених моделей, а саме AlexNet і MemNet, де ми використовуємо прогноз цікавості як проблему регресії. Наша третя глибока модель, з іншого боку, залежить від триплетної мережі, яка складається з трьох екземплярів тієї самої прямої мережі зі спільними вагами та навчена відповідно до втрати рейтингу триплетів. Наші експерименти демонструють, що всі ці моделі дають відносно схожі та багатообіцяючі результати щодо підзавдання щодо цікавості зображення.","For MediaEval 2016, we investigate the image interestingness subtask, which requires predicting interesting key frames of a video in the form of a movie trailer. We present three deep models for this task. The first two are based on fine-tuning AlexNet and MemNet, with the prediction being a regression problem. The third model is a triplet network, consisting of three feedforward networks with shared weights, and trained with a triplet ranking loss. Results from our experiments show that all these models provide comparable and satisfactory performance on the image interestingness subtask.",1
"У MediaEval 2016 ми зосереджуємося на підзавданні щодо цікавості зображення, яке передбачає прогнозування цікавих ключових кадрів відео у формі трейлера фільму. Ми спеціально пропонуємо три різні глибокі моделі для цього підзавдання. Перші дві моделі базуються на тонкому налаштуванні двох попередньо навчених моделей, а саме AlexNet і MemNet, де ми використовуємо прогноз цікавості як проблему регресії. Наша третя глибока модель, з іншого боку, залежить від триплетної мережі, яка складається з трьох екземплярів тієї самої прямої мережі зі спільними вагами та навчена відповідно до втрати рейтингу триплетів. Наші експерименти демонструють, що всі ці моделі дають відносно схожі та багатообіцяючі результати щодо підзавдання щодо цікавості зображення.","Для MediaEval 2016 ми досліджуємо підзавдання щодо цікавості зображення, яке вимагає прогнозування цікавих ключових кадрів відео у формі трейлера фільму. Ми представляємо три глибокі моделі для цього завдання. Перші два засновані на точному налаштуванні AlexNet і MemNet, причому передбачення є проблемою регресії. Третя модель — це триплетна мережа, що складається з трьох мереж прямого зв’язку зі спільними вагами та навчена з втратою рейтингу триплетів. Результати наших експериментів показують, що всі ці моделі забезпечують порівнянну та задовільну продуктивність підзавдання щодо цікавості зображення.",1
"This paper describes our two-stage system1 for the Euphemism Detection shared task hosted by the 3rd Workshop on Figurative Language Processing in conjunction with EMNLP 2022. Euphemisms tone down expressions about sensitive or unpleasant issues like addiction and death. The ambiguous nature of euphemistic words or expressions makes it challenging to detect their actual meaning within a context. In the first stage, we seek to mitigate this ambiguity by incorporating literal descriptions into input text prompts to our baseline model. It turns out that this kind of direct supervision yields remarkable performance improvement. In the second stage, we integrate visual supervision into our system using visual imageries, two sets of images generated by a text-to-image model by taking terms and descriptions as input. Our experiments demonstrate that visual supervision also gives a statistically significant performance boost. Our system achieved the second place with an F1 score of 87.2%, only about 0.9% worse than the best submission.","In this paper, we address the issue of part-based tracking by proposing a new fragments-based tracker. The proposed tracker enhances the recently suggested FragTrack algorithm to employ an adaptive cue integration scheme. This is done by embedding the original tracker into a particle filter framework, asso- ciating a reliability value to each fragment that describes a different part of the target object and dynam- ically adjusting these reliabilities at each frame with respect to the current context. Particularly, the vote of each fragment contributes to the joint tracking result according to its reliability, and this allows us to achieve a better accuracy in handling partial occlusions and pose changes while preserving and even improving the efficiency of the original tracker. In order to demonstrate the performance and the effec- tiveness of the proposed algorithm we present qualitative and quantitative results on a number of chal- lenging video sequences",0
"This paper describes our two-stage system1 for the Euphemism Detection shared task hosted by the 3rd Workshop on Figurative Language Processing in conjunction with EMNLP 2022. Euphemisms tone down expressions about sensitive or unpleasant issues like addiction and death. The ambiguous nature of euphemistic words or expressions makes it challenging to detect their actual meaning within a context. In the first stage, we seek to mitigate this ambiguity by incorporating literal descriptions into input text prompts to our baseline model. It turns out that this kind of direct supervision yields remarkable performance improvement. In the second stage, we integrate visual supervision into our system using visual imageries, two sets of images generated by a text-to-image model by taking terms and descriptions as input. Our experiments demonstrate that visual supervision also gives a statistically significant performance boost. Our system achieved the second place with an F1 score of 87.2%, only about 0.9% worse than the best submission.","У цій статті ми розглядаємо питання відстеження на основі частин, пропонуючи новий трекер на основі фрагментів. Запропонований трекер покращує нещодавно запропонований алгоритм FragTrack для використання адаптивної схеми інтеграції сигналів. Це робиться шляхом вбудовування оригінального трекера в структуру фільтра частинок, пов’язування значення надійності з кожним фрагментом, який описує іншу частину цільового об’єкта, і динамічного коригування цих надійностей у кожному кадрі відповідно до поточного контексту. Зокрема, голосування кожного фрагмента вносить свій внесок у результат спільного відстеження відповідно до його надійності, і це дозволяє нам досягти кращої точності в обробці часткових оклюзій і змін пози, зберігаючи та навіть покращуючи ефективність оригінального трекера. Щоб продемонструвати продуктивність та ефективність запропонованого алгоритму, ми представляємо якісні та кількісні результати на низці складних відеопослідовностей",0
"У цьому документі описано нашу двоетапну систему1 для спільного завдання «Виявлення евфемізмів», організованого 3-м семінаром з обробки образної мови спільно з EMNLP 2022. Евфемізми пом’якшують вирази про чутливі або неприємні питання, такі як залежність і смерть. Неоднозначний характер евфемістичних слів або виразів ускладнює виявлення їх справжнього значення в контексті. На першому етапі ми намагаємося пом’якшити цю неоднозначність, включивши літеральні описи до вхідних текстових підказок нашої базової моделі. Виявляється, такий вид прямого контролю дає значне покращення продуктивності. На другому етапі ми інтегруємо візуальний нагляд у нашу систему за допомогою візуальних зображень, двох наборів зображень, створених за допомогою моделі тексту в зображення, використовуючи терміни та описи як вхідні дані. Наші експерименти демонструють, що візуальний контроль також дає статистично значуще підвищення продуктивності. Наша система посіла друге місце з результатом F1 87,2%, що лише на 0,9% гірше, ніж найкраще подання.","In this paper, we address the issue of part-based tracking by proposing a new fragments-based tracker. The proposed tracker enhances the recently suggested FragTrack algorithm to employ an adaptive cue integration scheme. This is done by embedding the original tracker into a particle filter framework, asso- ciating a reliability value to each fragment that describes a different part of the target object and dynam- ically adjusting these reliabilities at each frame with respect to the current context. Particularly, the vote of each fragment contributes to the joint tracking result according to its reliability, and this allows us to achieve a better accuracy in handling partial occlusions and pose changes while preserving and even improving the efficiency of the original tracker. In order to demonstrate the performance and the effec- tiveness of the proposed algorithm we present qualitative and quantitative results on a number of chal- lenging video sequences",0
"У цьому документі описано нашу двоетапну систему1 для спільного завдання «Виявлення евфемізмів», організованого 3-м семінаром з обробки образної мови спільно з EMNLP 2022. Евфемізми пом’якшують вирази про чутливі або неприємні питання, такі як залежність і смерть. Неоднозначний характер евфемістичних слів або виразів ускладнює виявлення їх справжнього значення в контексті. На першому етапі ми намагаємося пом’якшити цю неоднозначність, включивши літеральні описи до вхідних текстових підказок нашої базової моделі. Виявляється, такий вид прямого контролю дає значне покращення продуктивності. На другому етапі ми інтегруємо візуальний нагляд у нашу систему за допомогою візуальних зображень, двох наборів зображень, створених за допомогою моделі тексту в зображення, використовуючи терміни та описи як вхідні дані. Наші експерименти демонструють, що візуальний контроль також дає статистично значуще підвищення продуктивності. Наша система посіла друге місце з результатом F1 87,2%, що лише на 0,9% гірше, ніж найкраще подання.","У цій статті ми розглядаємо питання відстеження на основі частин, пропонуючи новий трекер на основі фрагментів. Запропонований трекер покращує нещодавно запропонований алгоритм FragTrack для використання адаптивної схеми інтеграції сигналів. Це робиться шляхом вбудовування оригінального трекера в структуру фільтра частинок, пов’язування значення надійності з кожним фрагментом, який описує іншу частину цільового об’єкта, і динамічного коригування цих надійностей у кожному кадрі відповідно до поточного контексту. Зокрема, голосування кожного фрагмента вносить свій внесок у результат спільного відстеження відповідно до його надійності, і це дозволяє нам досягти кращої точності в обробці часткових оклюзій і змін пози, зберігаючи та навіть покращуючи ефективність оригінального трекера. Щоб продемонструвати продуктивність та ефективність запропонованого алгоритму, ми представляємо якісні та кількісні результати на низці складних відеопослідовностей",0
"Giving machines the ability to imagine possible new objects or scenes from linguistic descriptions and produce their realistic renderings is arguably one of the most challenging problems in computer vision. Recent advances in deep generative models have led to new approaches that give promising results towards this goal. In this paper, we introduce a new method called DiCoMoGAN for manipulating videos with natural language, aiming to perform local and semantic edits on a video clip to alter the appearances of an object of interest. Our GAN architecture allows for better utilization of multiple observations by disentangling content and motion to enable controllable semantic edits. To this end, we introduce two tightly coupled networks: (i) a representation network for constructing a concise understanding of motion dynamics and temporally invariant content, and (ii) a translation network that exploits the extracted latent content representation to actuate the manipulation according to the target description. Our qualitative and quantitative evaluations demonstrate that DiCoMoGAN significantly outperforms existing frame-based methods, producing temporally coherent and semantically more meaningful results.","In this paper, a unistroke keyboard based on computer vision is described for the handicapped. The keyboard can be made of paper or fabric containing an image of a keyboard, which has an upside down U-shape. It can even be displayed on a computer screen. Each character is represented by a non-overlapping rectangular region on the keyboard image and the user enters a character by illuminating a character region with a laser pointer. The keyboard image is monitored by a camera and illuminated key locations are recognized. During the text entry process the user neither have to tum the laser light off nor raise the laser light from the keyboard. A disabled person who bas difficulty using hisiher hands may attach the laser pointer to an eyeglass and easily enter text by moving hisher head to point the laser beam on a character location. In addition, a mouse-like device can be developed based on the same principle. The user can move the cursor by moving the laser light on the computer screen which is monitored by a camera. ",0
"Giving machines the ability to imagine possible new objects or scenes from linguistic descriptions and produce their realistic renderings is arguably one of the most challenging problems in computer vision. Recent advances in deep generative models have led to new approaches that give promising results towards this goal. In this paper, we introduce a new method called DiCoMoGAN for manipulating videos with natural language, aiming to perform local and semantic edits on a video clip to alter the appearances of an object of interest. Our GAN architecture allows for better utilization of multiple observations by disentangling content and motion to enable controllable semantic edits. To this end, we introduce two tightly coupled networks: (i) a representation network for constructing a concise understanding of motion dynamics and temporally invariant content, and (ii) a translation network that exploits the extracted latent content representation to actuate the manipulation according to the target description. Our qualitative and quantitative evaluations demonstrate that DiCoMoGAN significantly outperforms existing frame-based methods, producing temporally coherent and semantically more meaningful results.","У цій статті описано одноштрихову клавіатуру на основі комп’ютерного зору для людей з обмеженими можливостями. Клавіатура може бути зроблена з паперу або тканини із зображенням клавіатури, яка має перевернуту U-подібну форму. Його можна навіть відобразити на екрані комп’ютера. Кожен символ представлений прямокутною областю, яка не перекривається, на зображенні клавіатури, і користувач вводить символ, підсвічуючи область символу лазерною вказівкою. Зображення клавіатури контролюється камерою, а підсвічені клавіші розпізнаються. Під час процесу введення тексту користувачеві не потрібно ні вимикати лазерне світло, ні піднімати його з клавіатури. Людина з обмеженими можливостями, якій важко користуватися своїми руками, може прикріпити лазерну указку до окулярів і легко вводити текст, рухаючи головою, щоб навести лазерний промінь на місце символу. Крім того, за таким же принципом можна розробити пристрій, схожий на мишу. Користувач може переміщати курсор, пересуваючи лазерне світло на екрані комп’ютера, який контролюється камерою.",0
"Надання машинам здатності уявляти можливі нові об’єкти чи сцени на основі лінгвістичних описів і виробляти їх реалістичне відтворення є, мабуть, однією з найскладніших проблем комп’ютерного зору. Останні досягнення в глибоких генеративних моделях привели до нових підходів, які дають багатообіцяючі результати для досягнення цієї мети. У цій статті ми представляємо новий метод під назвою DiCoMoGAN для обробки відео за допомогою природної мови, спрямований на виконання локальних і семантичних редагувань відеокліпу, щоб змінити зовнішній вигляд цікавого об’єкта. Наша архітектура GAN дозволяє краще використовувати кілька спостережень, розмежовуючи вміст і рух, щоб уможливити кероване семантичне редагування. З цією метою ми представляємо дві тісно пов’язані мережі: (i) репрезентативну мережу для побудови стислого розуміння динаміки руху та тимчасово незмінного вмісту, і (ii) мережу перекладу, яка використовує виділене латентне представлення вмісту для активації маніпуляції відповідно до цільовий опис. Наші якісні та кількісні оцінки демонструють, що DiCoMoGAN значно перевершує існуючі методи на основі фреймів, створюючи узгоджені в часі та семантично більш значущі результати.","In this paper, a unistroke keyboard based on computer vision is described for the handicapped. The keyboard can be made of paper or fabric containing an image of a keyboard, which has an upside down U-shape. It can even be displayed on a computer screen. Each character is represented by a non-overlapping rectangular region on the keyboard image and the user enters a character by illuminating a character region with a laser pointer. The keyboard image is monitored by a camera and illuminated key locations are recognized. During the text entry process the user neither have to tum the laser light off nor raise the laser light from the keyboard. A disabled person who bas difficulty using hisiher hands may attach the laser pointer to an eyeglass and easily enter text by moving hisher head to point the laser beam on a character location. In addition, a mouse-like device can be developed based on the same principle. The user can move the cursor by moving the laser light on the computer screen which is monitored by a camera. ",0
"Надання машинам здатності уявляти можливі нові об’єкти чи сцени на основі лінгвістичних описів і виробляти їх реалістичне відтворення є, мабуть, однією з найскладніших проблем комп’ютерного зору. Останні досягнення в глибоких генеративних моделях привели до нових підходів, які дають багатообіцяючі результати для досягнення цієї мети. У цій статті ми представляємо новий метод під назвою DiCoMoGAN для обробки відео за допомогою природної мови, спрямований на виконання локальних і семантичних редагувань відеокліпу, щоб змінити зовнішній вигляд цікавого об’єкта. Наша архітектура GAN дозволяє краще використовувати кілька спостережень, розмежовуючи вміст і рух, щоб уможливити кероване семантичне редагування. З цією метою ми представляємо дві тісно пов’язані мережі: (i) репрезентативну мережу для побудови стислого розуміння динаміки руху та тимчасово незмінного вмісту, і (ii) мережу перекладу, яка використовує виділене латентне представлення вмісту для активації маніпуляції відповідно до цільовий опис. Наші якісні та кількісні оцінки демонструють, що DiCoMoGAN значно перевершує існуючі методи на основі фреймів, створюючи узгоджені в часі та семантично більш значущі результати.","У цій статті описано одноштрихову клавіатуру на основі комп’ютерного зору для людей з обмеженими можливостями. Клавіатура може бути зроблена з паперу або тканини, на якій зображено клавіатуру, яка має перевернуту U-подібну форму. Його можна навіть відобразити на екрані комп’ютера. Кожен символ представлений прямокутною областю, яка не перекривається, на зображенні клавіатури, і користувач вводить символ, підсвічуючи область символу лазерною вказівкою. Зображення клавіатури контролюється камерою, і розпізнаються підсвічені клавіші. Під час процесу введення тексту користувачеві не потрібно ні вимикати лазерне світло, ні піднімати його з клавіатури. Людина з обмеженими можливостями, якій важко користуватися своїми руками, може прикріпити лазерну указку до окулярів і легко вводити текст, рухаючи головою, щоб навести лазерний промінь на місце символу. Крім того, за таким же принципом можна розробити пристрій, схожий на мишу. Користувач може переміщати курсор, пересуваючи лазерне світло на екрані комп’ютера, який контролюється камерою.",0
"Flow-based generative super-resolution (SR) models learn to produce a diverse set of feasible SR solutions, called the SR space. Diversity of SR solutions increases with the temperature (τ ) of latent variables, which introduces random variations of texture among sample solutions, resulting in visual artifacts and low fidelity. In this paper, we present a simple but effective image ensembling/fusion approach to obtain a single SR image eliminating random artifacts and improving fidelity without significantly compromising perceptual quality. We achieve this by benefiting from a diverse set of feasible photorealistic solutions in the SR space spanned by flow models. We propose different image ensembling and fusion strategies which offer multiple paths to move sample solutions in the SR space to more desired destinations in the perception-distortion plane in a controllable manner depending on the fidelity vs. perceptual quality requirements of the task at hand. Experimental results demonstrate that our image ensembling/fusion strategy achieves more promising perception-distortion tradeoff compared to sample SR images produced by flow models and adversarially trained models in terms of both quantitative metrics and visual quality.","Image editing is a commonly studied problem in computer graphics. Despite the presence of many advanced editing tools, there is no satisfactory solution to controllably update the position of the sun using a single image. This problem is made complicated by the presence of clouds, complex landscapes, and the atmospheric effects that must be accounted for. In this paper, we tackle this problem starting with only a single photograph. With the user clicking on the initial position of the sun, our algorithm performs several estimation and segmentation processes for finding the horizon, scene depth, clouds, and the sky line. After this initial process, the user can make both fine- and large-scale changes on the position of the sun: it can be set beneath the mountains or moved behind the clouds practically turning a midday photograph into a sunset (or vice versa). We leverage a precomputed atmospheric scattering algorithm to make all of these changes not only realistic but also in real-time. We demonstrate our results using both clear and cloudy skies, showing how to add, remove, and relight clouds, all the while allowing for advanced effects such as scattering, shadows, light shafts, and lens flares",0
"Flow-based generative super-resolution (SR) models learn to produce a diverse set of feasible SR solutions, called the SR space. Diversity of SR solutions increases with the temperature (τ ) of latent variables, which introduces random variations of texture among sample solutions, resulting in visual artifacts and low fidelity. In this paper, we present a simple but effective image ensembling/fusion approach to obtain a single SR image eliminating random artifacts and improving fidelity without significantly compromising perceptual quality. We achieve this by benefiting from a diverse set of feasible photorealistic solutions in the SR space spanned by flow models. We propose different image ensembling and fusion strategies which offer multiple paths to move sample solutions in the SR space to more desired destinations in the perception-distortion plane in a controllable manner depending on the fidelity vs. perceptual quality requirements of the task at hand. Experimental results demonstrate that our image ensembling/fusion strategy achieves more promising perception-distortion tradeoff compared to sample SR images produced by flow models and adversarially trained models in terms of both quantitative metrics and visual quality.","Редагування зображень є широко дослідженою проблемою в комп’ютерній графіці. Незважаючи на наявність багатьох розширених інструментів редагування, не існує задовільного рішення для контрольованого оновлення положення сонця за допомогою одного зображення. Ця проблема ускладнюється наявністю хмар, складними ландшафтами та атмосферними впливами, які необхідно враховувати. У цій статті ми вирішуємо цю проблему, починаючи лише з однієї фотографії. Коли користувач натискає початкове положення сонця, наш алгоритм виконує кілька процесів оцінки та сегментації для пошуку горизонту, глибини сцени, хмар і лінії неба. Після цього початкового процесу користувач може робити як дрібні, так і масштабні зміни положення сонця: його можна встановити під гори або перемістити за хмари, практично перетворивши полуденну фотографію на захід сонця (або навпаки). Ми використовуємо попередньо обчислений алгоритм атмосферного розсіювання, щоб зробити всі ці зміни не лише реалістичними, але й у реальному часі. Ми демонструємо наші результати, використовуючи як ясне, так і хмарне небо, показуючи, як додавати, видаляти та повторно освітлювати хмари, водночас допускаючи розширені ефекти, такі як розсіювання, тіні, світлові вали та відблиски від лінз.",0
"Моделі generative super-resolution (SR) на основі потоку вчаться створювати різноманітний набір можливих рішень SR, які називають простором SR. Різноманітність розчинів SR збільшується з температурою (τ) прихованих змінних, що вводить випадкові варіації текстури серед розчинів зразків, що призводить до візуальних артефактів і низької точності. У цій статті ми представляємо простий, але ефективний підхід до ансамблювання/злиття зображень для отримання єдиного зображення SR, усуваючи випадкові артефакти та покращуючи точність без значного погіршення якості сприйняття. Ми досягаємо цього, використовуючи переваги різноманітного набору можливих фотореалістичних рішень у просторі SR, охопленому моделями потоку. Ми пропонуємо різні стратегії поєднання зображень і злиття, які пропонують кілька шляхів для переміщення зразків рішень у просторі SR до більш бажаних пунктів призначення в площині сприйняття-спотворення контрольованим способом залежно від вимог щодо точності та якості сприйняття для поточного завдання. Експериментальні результати демонструють, що наша стратегія ансамблювання/злиття зображень досягає більш перспективного компромісу між сприйняттям і спотворенням порівняно з зразками SR-зображень, створених потоковими моделями та конкурентно навченими моделями як з точки зору кількісних показників, так і якості зображення.","Image editing is a commonly studied problem in computer graphics. Despite the presence of many advanced editing tools, there is no satisfactory solution to controllably update the position of the sun using a single image. This problem is made complicated by the presence of clouds, complex landscapes, and the atmospheric effects that must be accounted for. In this paper, we tackle this problem starting with only a single photograph. With the user clicking on the initial position of the sun, our algorithm performs several estimation and segmentation processes for finding the horizon, scene depth, clouds, and the sky line. After this initial process, the user can make both fine- and large-scale changes on the position of the sun: it can be set beneath the mountains or moved behind the clouds practically turning a midday photograph into a sunset (or vice versa). We leverage a precomputed atmospheric scattering algorithm to make all of these changes not only realistic but also in real-time. We demonstrate our results using both clear and cloudy skies, showing how to add, remove, and relight clouds, all the while allowing for advanced effects such as scattering, shadows, light shafts, and lens flares",0
"Моделі generative super-resolution (SR) на основі потоку вчаться створювати різноманітний набір можливих рішень SR, які називають простором SR. Різноманітність розчинів SR збільшується з температурою (τ) прихованих змінних, що вводить випадкові варіації текстури серед розчинів зразків, що призводить до візуальних артефактів і низької точності. У цій статті ми представляємо простий, але ефективний підхід до ансамблювання/злиття зображень для отримання єдиного зображення SR, усуваючи випадкові артефакти та покращуючи точність без значного погіршення якості сприйняття. Ми досягаємо цього, використовуючи переваги різноманітного набору можливих фотореалістичних рішень у просторі SR, охопленому моделями потоку. Ми пропонуємо різні стратегії поєднання зображень і злиття, які пропонують кілька шляхів для переміщення зразків рішень у просторі SR до більш бажаних пунктів призначення в площині сприйняття-спотворення контрольованим способом залежно від вимог щодо точності та якості сприйняття для поточного завдання. Експериментальні результати демонструють, що наша стратегія ансамблювання/злиття зображень досягає більш перспективного компромісу між сприйняттям і спотворенням порівняно з зразками SR-зображень, створених потоковими моделями та конкурентно навченими моделями як з точки зору кількісних показників, так і якості зображення.","Редагування зображень є широко дослідженою проблемою в комп’ютерній графіці. Незважаючи на наявність багатьох розширених інструментів редагування, не існує задовільного рішення для контрольованого оновлення положення сонця за допомогою одного зображення. Ця проблема ускладнюється наявністю хмар, складними ландшафтами та атмосферними впливами, які необхідно враховувати. У цій статті ми вирішуємо цю проблему, починаючи лише з однієї фотографії. Коли користувач натискає початкове положення сонця, наш алгоритм виконує кілька процесів оцінки та сегментації для пошуку горизонту, глибини сцени, хмар і лінії неба. Після цього початкового процесу користувач може робити як дрібні, так і масштабні зміни положення сонця: його можна встановити під гори або перемістити за хмари, практично перетворивши полуденну фотографію на захід сонця (або навпаки). Ми використовуємо попередньо обчислений алгоритм атмосферного розсіювання, щоб зробити всі ці зміни не лише реалістичними, але й у реальному часі. Ми демонструємо наші результати, використовуючи як ясне, так і хмарне небо, показуючи, як додавати, видаляти та повторно освітлювати хмари, водночас допускаючи розширені ефекти, такі як розсіювання, тіні, світлові вали та відблиски від лінз.",0
"Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 444 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI’s GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit “breakthrough” behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting. ","In recent years, deep learning methods have come to the forefront in many areas that require remote sensing, from medicine to agriculture, from defense industry to space research; and these methods have achieved tremendous success as compared to traditional methods. Together with substantial growth in available data with high-quality labels and computational resources, these deep neural network architectures and techniques have seen remarkable developments. The major difference between deep learning and classical recognition methods is that deep learning methods consider an end-to-end learning scheme which gives rise to learning features from raw data. Better regularization techniques and robust optimization algorithms introduced with state-of-the-art deep learning models are other factors leading this difference. In this paper, we discuss the remote sensing problems and how deep learning can be used to solve these problems with a special focus on medical and remote sensing applications. In particular, we briefly review outperforming architectures within the deep learning literature and their use cases. ",0
"Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 444 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI’s GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit “breakthrough” behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting. ","В останні роки методи глибокого навчання вийшли на передній план у багатьох сферах, де потрібне дистанційне зондування, від медицини до сільського господарства, від оборонної промисловості до космічних досліджень; і ці методи досягли величезного успіху в порівнянні з традиційними методами. Разом зі значним зростанням доступних даних із високоякісними мітками та обчислювальними ресурсами ці глибокі архітектури й методи нейронних мереж зазнали значних змін. Основна відмінність між методами глибокого навчання та класичними методами розпізнавання полягає в тому, що методи глибокого навчання розглядають наскрізну схему навчання, яка дає початок функціям навчання з необроблених даних. Кращі методи регулярізації та надійні алгоритми оптимізації, запроваджені з найсучаснішими моделями глибокого навчання, є іншими факторами, що призводять до цієї різниці. У цій статті ми обговорюємо проблеми дистанційного зондування та те, як глибоке навчання можна використовувати для вирішення цих проблем, з особливим акцентом на медичні та дистанційні програми. Зокрема, ми коротко оглядаємо найкращі архітектури в літературі про глибоке навчання та приклади їх використання.",0
"Мовні моделі демонструють як кількісне вдосконалення, так і нові якісні можливості зі збільшенням масштабу. Незважаючи на їхній потенційно трансформаційний вплив, ці нові можливості ще недостатньо охарактеризовані. Для того, щоб інформувати про майбутні дослідження, підготуватися до руйнівних можливостей нових моделей і пом’якшити соціально шкідливі наслідки, життєво важливо, щоб ми розуміли теперішні та найближчі можливості та обмеження мовних моделей. Щоб вирішити цю проблему, ми представляємо тест Beyond the Imitation Game (BIG-bench). BIG-стенд наразі складається з 204 завдань, доданих 444 авторами зі 132 установ. Теми завдань різноманітні, вони охоплюють проблеми з лінгвістики, розвитку дитини, математики, здорового глузду, біології, фізики, соціальних упереджень, розробки програмного забезпечення тощо. BIG-bench зосереджується на завданнях, які, як вважають, виходять за межі можливостей поточних мовних моделей. Ми оцінюємо поведінку моделей GPT OpenAI, внутрішніх архітектур щільних трансформаторів Google і розріджених трансформаторів у стилі Switch на BIG-стенді за розмірами моделей, що охоплюють від мільйонів до сотень мільярдів параметрів. Крім того, команда оцінювачів-людей виконувала всі завдання, щоб забезпечити надійну базову лінію. Висновки включають: продуктивність моделі та калібрування покращуються з масштабом, але є поганими в абсолютному вираженні (і порівняно з продуктивністю оцінювача); продуктивність надзвичайно однакова для всіх класів моделей, хоча й має переваги від розрідженості; завдання, які вдосконалюються поступово і передбачувано, зазвичай включають великий компонент знань або запам’ятовування, тоді як завдання, які демонструють «проривну» поведінку в критичному масштабі, часто включають кілька кроків або компонентів або крихкі показники; соціальне упередження зазвичай зростає з масштабом в умовах з неоднозначним контекстом, але це можна покращити за допомогою підказок.","In recent years, deep learning methods have come to the forefront in many areas that require remote sensing, from medicine to agriculture, from defense industry to space research; and these methods have achieved tremendous success as compared to traditional methods. Together with substantial growth in available data with high-quality labels and computational resources, these deep neural network architectures and techniques have seen remarkable developments. The major difference between deep learning and classical recognition methods is that deep learning methods consider an end-to-end learning scheme which gives rise to learning features from raw data. Better regularization techniques and robust optimization algorithms introduced with state-of-the-art deep learning models are other factors leading this difference. In this paper, we discuss the remote sensing problems and how deep learning can be used to solve these problems with a special focus on medical and remote sensing applications. In particular, we briefly review outperforming architectures within the deep learning literature and their use cases. ",0
"Мовні моделі демонструють як кількісне вдосконалення, так і нові якісні можливості зі збільшенням масштабу. Незважаючи на їхній потенційно трансформаційний вплив, ці нові можливості ще недостатньо охарактеризовані. Для того, щоб інформувати про майбутні дослідження, підготуватися до руйнівних можливостей нових моделей і пом’якшити соціально шкідливі наслідки, життєво важливо, щоб ми розуміли теперішні та найближчі можливості та обмеження мовних моделей. Щоб вирішити цю проблему, ми представляємо тест Beyond the Imitation Game (BIG-bench). BIG-стенд наразі складається з 204 завдань, доданих 444 авторами зі 132 установ. Теми завдань різноманітні, вони охоплюють проблеми з лінгвістики, розвитку дитини, математики, здорового глузду, біології, фізики, соціальних упереджень, розробки програмного забезпечення тощо. BIG-bench зосереджується на завданнях, які, як вважають, виходять за межі можливостей поточних мовних моделей. Ми оцінюємо поведінку моделей GPT OpenAI, внутрішніх архітектур щільних трансформаторів Google і розріджених трансформаторів у стилі Switch на BIG-стенді за розмірами моделей, що охоплюють від мільйонів до сотень мільярдів параметрів. Крім того, команда оцінювачів-людей виконувала всі завдання, щоб забезпечити надійну базову лінію. Висновки включають: продуктивність моделі та калібрування покращуються з масштабом, але є поганими в абсолютному вираженні (і порівняно з продуктивністю оцінювача); продуктивність надзвичайно однакова для всіх класів моделей, хоча й має переваги від розрідженості; завдання, які вдосконалюються поступово і передбачувано, зазвичай включають великий компонент знань або запам’ятовування, тоді як завдання, які демонструють «проривну» поведінку в критичному масштабі, часто включають кілька кроків або компонентів або крихкі показники; соціальне упередження зазвичай зростає з масштабом в умовах з неоднозначним контекстом, але це можна покращити за допомогою підказок.","В останні роки методи глибокого навчання вийшли на передній план у багатьох сферах, де потрібне дистанційне зондування, від медицини до сільського господарства, від оборонної промисловості до космічних досліджень; і ці методи досягли величезного успіху в порівнянні з традиційними методами. Разом зі значним зростанням доступних даних із високоякісними мітками та обчислювальними ресурсами ці глибокі архітектури й методи нейронних мереж зазнали значних змін. Основна відмінність між методами глибокого навчання та класичними методами розпізнавання полягає в тому, що методи глибокого навчання розглядають наскрізну схему навчання, яка дає початок функціям навчання з необроблених даних. Кращі методи регулярізації та надійні алгоритми оптимізації, запроваджені з найсучаснішими моделями глибокого навчання, є іншими факторами, що призводять до цієї різниці. У цій статті ми обговорюємо проблеми дистанційного зондування та те, як глибоке навчання можна використовувати для вирішення цих проблем, з особливим акцентом на медичні та дистанційні програми. Зокрема, ми коротко оглядаємо найкращі архітектури в літературі про глибоке навчання та приклади їх використання.",0
"Magnetic resonance imaging (MRI) is used in many diagnostic applications as it has a high soft-tissue contrast and is a non-invasive medical imaging method. MR signal levels differs according to the parameters T1, T2 and PD that change with respect to the chemical structure of the tissues. However, long scan times might limit acquiring images from multiple contrasts or if the multi-contrasts images are acquired, the contrasts are noisy. To overcome this limitation of MRI, multi-contrast synthesis can be utilized. In this paper, we propose a deep learning method based on Channel-Exchanging-Network (CEN) for multi-contrast image synthesis. Demonstrations are provided on IXI dataset. The proposed model based on CEN is compared against alternative methods based on CNNs and GANs. Our results show that the proposed model achieves superior performance to the competing methods. ","Flow-based generative super-resolution (SR) models learn to produce a diverse set of feasible SR solutions, called the SR space. Diversity of SR solutions increases with the temperature (τ ) of latent variables, which introduces random variations of texture among sample solutions, resulting in visual artifacts and low fidelity. In this paper, we present a simple but effective image ensembling/fusion approach to obtain a single SR image eliminating random artifacts and improving fidelity without significantly compromising perceptual quality. We achieve this by benefiting from a diverse set of feasible photorealistic solutions in the SR space spanned by flow models. We propose different image ensembling and fusion strategies which offer multiple paths to move sample solutions in the SR space to more desired destinations in the perception-distortion plane in a controllable manner depending on the fidelity vs. perceptual quality requirements of the task at hand. Experimental results demonstrate that our image ensembling/fusion strategy achieves more promising perception-distortion tradeoff compared to sample SR images produced by flow models and adversarially trained models in terms of both quantitative metrics and visual quality.",0
"Magnetic resonance imaging (MRI) is used in many diagnostic applications as it has a high soft-tissue contrast and is a non-invasive medical imaging method. MR signal levels differs according to the parameters T1, T2 and PD that change with respect to the chemical structure of the tissues. However, long scan times might limit acquiring images from multiple contrasts or if the multi-contrasts images are acquired, the contrasts are noisy. To overcome this limitation of MRI, multi-contrast synthesis can be utilized. In this paper, we propose a deep learning method based on Channel-Exchanging-Network (CEN) for multi-contrast image synthesis. Demonstrations are provided on IXI dataset. The proposed model based on CEN is compared against alternative methods based on CNNs and GANs. Our results show that the proposed model achieves superior performance to the competing methods. ","Моделі generative super-resolution (SR) на основі потоку вчаться створювати різноманітний набір можливих рішень SR, які називають простором SR. Різноманітність розчинів SR збільшується з температурою (τ) прихованих змінних, що вводить випадкові варіації текстури серед розчинів зразків, що призводить до візуальних артефактів і низької точності. У цій статті ми представляємо простий, але ефективний підхід до ансамблювання/злиття зображень для отримання єдиного зображення SR, усуваючи випадкові артефакти та покращуючи точність без значного погіршення якості сприйняття. Ми досягаємо цього, використовуючи переваги різноманітного набору можливих фотореалістичних рішень у просторі SR, охопленому моделями потоку. Ми пропонуємо різні стратегії поєднання зображень і злиття, які пропонують кілька шляхів для переміщення зразків рішень у просторі SR до більш бажаних пунктів призначення в площині сприйняття-спотворення контрольованим способом залежно від вимог щодо точності та якості сприйняття для поточного завдання. Експериментальні результати демонструють, що наша стратегія ансамблювання/злиття зображень досягає більш перспективного компромісу між сприйняттям і спотворенням порівняно з зразками SR-зображень, створених потоковими моделями та конкурентно навченими моделями як з точки зору кількісних показників, так і якості зображення.",0
"Магнітно-резонансна томографія (МРТ) використовується в багатьох діагностичних програмах, оскільки вона має високий контраст м’яких тканин і є неінвазивним медичним методом візуалізації. Рівень сигналу МР відрізняється відповідно до параметрів T1, T2 і PD, які змінюються залежно від хімічної структури тканин. Однак тривалий час сканування може обмежити отримання зображень із кількох контрастів, або якщо отримані багатоконтрастні зображення, контрасти мають шум. Щоб подолати це обмеження МРТ, можна використовувати мультиконтрастний синтез. У цій статті ми пропонуємо метод глибокого навчання на основі мережі обміну каналами (CEN) для синтезу багатоконтрастного зображення. Демонстрації надаються на базі даних IXI. Запропонована модель на основі CEN порівнюється з альтернативними методами на основі CNN і GAN. Наші результати показують, що запропонована модель досягає кращої продуктивності порівняно з конкурентними методами.","Flow-based generative super-resolution (SR) models learn to produce a diverse set of feasible SR solutions, called the SR space. Diversity of SR solutions increases with the temperature (τ ) of latent variables, which introduces random variations of texture among sample solutions, resulting in visual artifacts and low fidelity. In this paper, we present a simple but effective image ensembling/fusion approach to obtain a single SR image eliminating random artifacts and improving fidelity without significantly compromising perceptual quality. We achieve this by benefiting from a diverse set of feasible photorealistic solutions in the SR space spanned by flow models. We propose different image ensembling and fusion strategies which offer multiple paths to move sample solutions in the SR space to more desired destinations in the perception-distortion plane in a controllable manner depending on the fidelity vs. perceptual quality requirements of the task at hand. Experimental results demonstrate that our image ensembling/fusion strategy achieves more promising perception-distortion tradeoff compared to sample SR images produced by flow models and adversarially trained models in terms of both quantitative metrics and visual quality.",0
"Магнітно-резонансна томографія (МРТ) використовується в багатьох діагностичних програмах, оскільки вона має високий контраст м’яких тканин і є неінвазивним медичним методом візуалізації. Рівень сигналу МР відрізняється відповідно до параметрів T1, T2 і PD, які змінюються залежно від хімічної структури тканин. Однак тривалий час сканування може обмежити отримання зображень із кількох контрастів, або якщо отримані багатоконтрастні зображення, контрасти мають шум. Щоб подолати це обмеження МРТ, можна використовувати мультиконтрастний синтез. У цій статті ми пропонуємо метод глибокого навчання на основі мережі обміну каналами (CEN) для синтезу багатоконтрастного зображення. Демонстрації представлені на наборі даних IXI. Запропонована модель на основі CEN порівнюється з альтернативними методами на основі CNN і GAN. Наші результати показують, що запропонована модель досягає кращої продуктивності порівняно з конкурентними методами.","Моделі generative super-resolution (SR) на основі потоку вчаться створювати різноманітний набір можливих рішень SR, які називають простором SR. Різноманітність розчинів SR збільшується з температурою (τ) прихованих змінних, що вводить випадкові варіації текстури серед розчинів зразків, що призводить до візуальних артефактів і низької точності. У цій статті ми представляємо простий, але ефективний підхід до ансамблювання/злиття зображень для отримання єдиного зображення SR, усуваючи випадкові артефакти та покращуючи точність без значного погіршення якості сприйняття. Ми досягаємо цього, використовуючи переваги різноманітного набору можливих фотореалістичних рішень у просторі SR, охопленому моделями потоку. Ми пропонуємо різні стратегії поєднання зображень і злиття, які пропонують кілька шляхів для переміщення зразків рішень у просторі SR до більш бажаних пунктів призначення в площині сприйняття-спотворення контрольованим способом залежно від вимог щодо точності та якості сприйняття для поточного завдання. Експериментальні результати демонструють, що наша стратегія ансамблювання/злиття зображень досягає більш перспективного компромісу між сприйняттям і спотворенням порівняно з зразками SR-зображень, створених потоковими моделями та конкурентно навченими моделями як з точки зору кількісних показників, так і якості зображення.",0
"The immense amount of videos being uploaded to video sharing platforms makes it impossible for a person to watch all the videos understand what happens in them. Hence, machine learning techniques are now deployed to index videos by recognizing key objects, actions and scenes or places. Summarization is another alternative as it offers to extract only important parts while covering the gist of the video content. Ideally, the user may prefer to analyze a certain action or scene by searching a query term within the video. Current summarization methods generally do not take queries into account or require exhaustive data labeling. In this work, we present a weakly supervised query-focused video summarization method. Our proposed approach makes use of semantic attributes as an indicator of query relevance and semantic attention maps to locate related regions in the frames and utilizes both within a submodular maximization framework. We conducted experiments on the recently introduced RAD dataset and obtained highly competitive results. Moreover, to better evaluate the performance of our approach on longer videos, we collected a new dataset, which consists of 10 videos from YouTube and annotated with shot-level multiple attributes. Our dataset enables much diverse set of queries that can be used to summarize a video from different perspectives with more degrees of freedom.","As recently discussed by Bar, Kiryati, and Sochen in [3], the Ambrosio-Tortorelli approximation of the Mumford-Shah functional defines an extended line process regularization where the regularizer has an additional constraint introduced by the term ρ|∇v| . This term mildly forces some spatial organization by demanding that the edges are smooth. However, it does not force spatial coherence such as edge direction compatibility or edge connectivity, as in the traditional edge detectors such as Canny. Using the connection between regularization and diffusion filters, we incorporate further spatial structure into the regularization process of the Mumford-Shah model. The new model combines smoothing, edge detection and edge linking steps of the traditional approach to boundary detection. Importance of spatial coherence is best observed if the image noise is salt and pepper like. Proposed approach is able to deal with difficult noise cases without using non-smooth cost functions such as  in the data fidelity or regularizer. ",0
"The immense amount of videos being uploaded to video sharing platforms makes it impossible for a person to watch all the videos understand what happens in them. Hence, machine learning techniques are now deployed to index videos by recognizing key objects, actions and scenes or places. Summarization is another alternative as it offers to extract only important parts while covering the gist of the video content. Ideally, the user may prefer to analyze a certain action or scene by searching a query term within the video. Current summarization methods generally do not take queries into account or require exhaustive data labeling. In this work, we present a weakly supervised query-focused video summarization method. Our proposed approach makes use of semantic attributes as an indicator of query relevance and semantic attention maps to locate related regions in the frames and utilizes both within a submodular maximization framework. We conducted experiments on the recently introduced RAD dataset and obtained highly competitive results. Moreover, to better evaluate the performance of our approach on longer videos, we collected a new dataset, which consists of 10 videos from YouTube and annotated with shot-level multiple attributes. Our dataset enables much diverse set of queries that can be used to summarize a video from different perspectives with more degrees of freedom.","Як нещодавно обговорювали Бар, Кіріаті та Сочен у [3], апроксимація Амбросіо-Тортореллі функціоналу Мамфорда-Шаха визначає розширену регулярізацію лінійного процесу, де регулязатор має додаткове обмеження, введене членом ρ|∇v| . Цей термін м’яко вимагає певної просторової організації, вимагаючи гладкості країв. Однак він не забезпечує просторову когерентність, таку як сумісність напрямків країв або з’єднання країв, як у традиційних детекторах країв, таких як Canny. Використовуючи зв’язок між регуляризацією та дифузійними фільтрами, ми включаємо подальшу просторову структуру в процес регуляризації моделі Мамфорда-Шаха. Нова модель поєднує згладжування, визначення країв і з’єднання країв традиційного підходу до виявлення меж. Важливість просторової когерентності найкраще спостерігається, якщо шум зображення схожий на сіль і перець. Запропонований підхід здатний працювати зі складними випадками шуму без використання негладких функцій вартості, таких як у точності даних або регуляризаторі.",0
"Величезна кількість відео, які завантажуються на платформи для обміну відео, не дозволяє людині переглянути всі відео, зрозуміти, що в них відбувається. Таким чином, тепер використовуються методи машинного навчання для індексування відео шляхом розпізнавання ключових об’єктів, дій і сцен або місць. Резюмування є ще однією альтернативою, оскільки воно пропонує виділити лише важливі частини, охоплюючи суть відеовмісту. В ідеалі користувач може віддати перевагу аналізу певної дії чи сцени шляхом пошуку за запитом у відео. Поточні методи підсумовування зазвичай не враховують запити або вимагають вичерпного маркування даних. У цій роботі ми представляємо метод підсумовування відео, орієнтований на запити зі слабким контролем. Запропонований нами підхід використовує семантичні атрибути як індикатор релевантності запиту та семантичні карти уваги для визначення місцезнаходження пов’язаних регіонів у фреймах і використовує обидва в межах субмодульної структури максимізації. Ми провели експерименти з нещодавно представленим набором даних RAD і отримали дуже конкурентоспроможні результати. Крім того, щоб краще оцінити ефективність нашого підходу на довших відео, ми зібрали новий набір даних, який складається з 10 відео з YouTube і анотованих декількома атрибутами на рівні кадру. Наш набір даних дозволяє створювати різноманітні запити, які можна використовувати для узагальнення відео з різних точок зору з більшим ступенем свободи.","As recently discussed by Bar, Kiryati, and Sochen in [3], the Ambrosio-Tortorelli approximation of the Mumford-Shah functional defines an extended line process regularization where the regularizer has an additional constraint introduced by the term ρ|∇v| . This term mildly forces some spatial organization by demanding that the edges are smooth. However, it does not force spatial coherence such as edge direction compatibility or edge connectivity, as in the traditional edge detectors such as Canny. Using the connection between regularization and diffusion filters, we incorporate further spatial structure into the regularization process of the Mumford-Shah model. The new model combines smoothing, edge detection and edge linking steps of the traditional approach to boundary detection. Importance of spatial coherence is best observed if the image noise is salt and pepper like. Proposed approach is able to deal with difficult noise cases without using non-smooth cost functions such as  in the data fidelity or regularizer. ",0
"Величезна кількість відео, які завантажуються на платформи для обміну відео, не дозволяє людині переглянути всі відео, зрозуміти, що в них відбувається. Таким чином, тепер використовуються методи машинного навчання для індексування відео шляхом розпізнавання ключових об’єктів, дій і сцен або місць. Резюмування є ще однією альтернативою, оскільки воно пропонує виділити лише важливі частини, охоплюючи суть відеовмісту. В ідеалі користувач може віддати перевагу аналізу певної дії чи сцени шляхом пошуку за запитом у відео. Поточні методи підсумовування зазвичай не враховують запити або вимагають вичерпного маркування даних. У цій роботі ми представляємо метод підсумовування відео, орієнтований на запити зі слабким контролем. Запропонований нами підхід використовує семантичні атрибути як індикатор релевантності запиту та семантичні карти уваги для визначення місцезнаходження пов’язаних регіонів у фреймах і використовує обидва в межах субмодульної структури максимізації. Ми провели експерименти з нещодавно представленим набором даних RAD і отримали дуже конкурентоспроможні результати. Крім того, щоб краще оцінити ефективність нашого підходу на довших відео, ми зібрали новий набір даних, який складається з 10 відео з YouTube і анотованих декількома атрибутами на рівні кадру. Наш набір даних дозволяє створювати різноманітні запити, які можна використовувати для узагальнення відео з різних точок зору з більшим ступенем свободи.","Як нещодавно обговорювали Бар, Кіріаті та Сочен у [3], апроксимація Амбросіо-Тортореллі функціоналу Мамфорда-Шаха визначає розширену регулярізацію лінійного процесу, де регулязатор має додаткове обмеження, введене членом ρ|∇v| . Цей термін м’яко нав’язує певну просторову організацію, вимагаючи гладкості країв. Однак він не забезпечує просторову когерентність, таку як сумісність напрямків країв або з’єднання країв, як у традиційних детекторах країв, таких як Canny. Використовуючи зв’язок між регуляризацією та дифузійними фільтрами, ми включаємо подальшу просторову структуру в процес регуляризації моделі Мамфорда-Шаха. Нова модель поєднує згладжування, визначення країв і з’єднання країв традиційного підходу до визначення меж. Важливість просторової когерентності найкраще спостерігається, якщо шум зображення схожий на сіль і перець. Запропонований підхід здатний працювати зі складними випадками шуму без використання негладких функцій вартості, таких як у точності даних або регуляризаторі.",0
"Developing artificial learning systems that can understand and generate natural language has been one of the long-standing goals of artificial intelligence. Recent decades have witnessed an impressive progress on both of these problems, giving rise to a new family of approaches. Especially, the advances in deep learning over the past couple of years have led to neural approaches to natural language generation (NLG). These methods combine generative language learning techniques with neural-networks based frameworks. With a wide range of applications in natural language processing, neural NLG (NNLG) is a new and fast growing field of research. In this state-of-the-art report, we investigate the recent developments and applications of NNLG in its full extent from a multidimensional view, covering critical perspectives such as multimodality, multilinguality, controllability and learning strategies. We summarize the fundamental building blocks of NNLG approaches from these aspects and provide detailed reviews of commonly used preprocessing steps and basic neural architectures. This report also focuses on the seminal applications of these NNLG models such as machine translation, description generation, automatic speech recognition, abstractive summarization, text simplification, question answering and generation, and dialogue generation. Finally, we conclude with a thorough discussion of the described frameworks by pointing out some open research directions. ","Robust visual tracking plays a vital role in many areas such as autonomous cars, surveillance and robotics. Recent trackers were shown to achieve adequate results under normal tracking scenarios with clear weather conditions, standard camera setups and lighting conditions. Yet, the performance of these trackers, whether they are corre- lation filter-based or learning-based, degrade under adverse weather conditions. The lack of videos with such weather conditions, in the available visual object tracking datasets, is the prime issue behind the low perfor- mance of the learning-based tracking algorithms. In this work, we provide a new person tracking dataset of real-world sequences (PTAW172Real) captured under foggy, rainy and snowy weather conditions to assess the performance of the current trackers. We also introduce a novel person tracking dataset of synthetic sequences (PTAW217Synth) procedurally generated by our NOVA framework spanning the same weather conditions in varying severity to mitigate the problem of data scarcity. Our experimental results demonstrate that the perfor- mances of the state-of-the-art deep trackers under adverse weather conditions can be boosted when the avail- able real training sequences are complemented with our synthetically generated dataset during training.",0
"Developing artificial learning systems that can understand and generate natural language has been one of the long-standing goals of artificial intelligence. Recent decades have witnessed an impressive progress on both of these problems, giving rise to a new family of approaches. Especially, the advances in deep learning over the past couple of years have led to neural approaches to natural language generation (NLG). These methods combine generative language learning techniques with neural-networks based frameworks. With a wide range of applications in natural language processing, neural NLG (NNLG) is a new and fast growing field of research. In this state-of-the-art report, we investigate the recent developments and applications of NNLG in its full extent from a multidimensional view, covering critical perspectives such as multimodality, multilinguality, controllability and learning strategies. We summarize the fundamental building blocks of NNLG approaches from these aspects and provide detailed reviews of commonly used preprocessing steps and basic neural architectures. This report also focuses on the seminal applications of these NNLG models such as machine translation, description generation, automatic speech recognition, abstractive summarization, text simplification, question answering and generation, and dialogue generation. Finally, we conclude with a thorough discussion of the described frameworks by pointing out some open research directions. ","Надійне візуальне відстеження відіграє життєво важливу роль у багатьох сферах, таких як автономні автомобілі, спостереження та робототехніка. Показано, що останні трекери досягають відповідних результатів за звичайних сценаріїв відстеження за ясних погодних умов, стандартних налаштувань камери та умов освітлення. Тим не менш, продуктивність цих трекерів, незалежно від того, засновані вони на кореляційному фільтрі чи на основі навчання, погіршується за несприятливих погодних умов. Відсутність відео з такими погодними умовами в доступних наборах даних візуального відстеження об’єктів є основною проблемою низької продуктивності алгоритмів відстеження на основі навчання. У цій роботі ми надаємо новий набір даних відстеження людей у реальному світі (PTAW172Real), знятих за умов туманної, дощової та снігової погоди, щоб оцінити ефективність поточних трекерів. Ми також представляємо новий набір даних відстеження людей із синтетичних послідовностей (PTAW217Synth), процедурно згенерованих нашою структурою NOVA, що охоплює однакові погодні умови різного ступеня тяжкості, щоб пом’якшити проблему дефіциту даних. Наші експериментальні результати демонструють, що продуктивність найсучасніших глибинних трекерів за несприятливих погодних умов може бути покращена, якщо доступні реальні навчальні послідовності доповнюються нашим синтетично згенерованим набором даних під час навчання.",0
"Розробка систем штучного навчання, які можуть розуміти та генерувати природну мову, була однією з давніх цілей штучного інтелекту. Останні десятиліття стали свідками вражаючого прогресу в обох цих проблемах, що породило нове сімейство підходів. Зокрема, прогрес у глибокому навчанні за останні пару років призвів до нейронних підходів до створення природної мови (NLG). Ці методи поєднують методи генеративного вивчення мови з нейронними мережами. Завдяки широкому спектру застосувань у обробці природної мови, нейронна NLG (NNLG) є новою галуззю досліджень, яка швидко розвивається. У цьому сучасному звіті ми досліджуємо останні розробки та застосування NNLG у повному обсязі з багатовимірного погляду, охоплюючи важливі перспективи, такі як мультимодальність, багатомовність, керованість і стратегії навчання. Ми підсумовуємо фундаментальні будівельні блоки підходів NNLG з цих аспектів і надаємо детальні огляди поширених етапів попередньої обробки та основних нейронних архітектур. У цьому звіті також зосереджено увагу на основних застосуваннях цих моделей NNLG, таких як машинний переклад, генерація описів, автоматичне розпізнавання мовлення, абстрактне резюмування, спрощення тексту, відповіді на запитання та генерація, а також генерація діалогів. Нарешті, ми закінчуємо детальним обговоренням описаних структур, вказуючи на деякі відкриті напрямки дослідження.","Robust visual tracking plays a vital role in many areas such as autonomous cars, surveillance and robotics. Recent trackers were shown to achieve adequate results under normal tracking scenarios with clear weather conditions, standard camera setups and lighting conditions. Yet, the performance of these trackers, whether they are corre- lation filter-based or learning-based, degrade under adverse weather conditions. The lack of videos with such weather conditions, in the available visual object tracking datasets, is the prime issue behind the low perfor- mance of the learning-based tracking algorithms. In this work, we provide a new person tracking dataset of real-world sequences (PTAW172Real) captured under foggy, rainy and snowy weather conditions to assess the performance of the current trackers. We also introduce a novel person tracking dataset of synthetic sequences (PTAW217Synth) procedurally generated by our NOVA framework spanning the same weather conditions in varying severity to mitigate the problem of data scarcity. Our experimental results demonstrate that the perfor- mances of the state-of-the-art deep trackers under adverse weather conditions can be boosted when the avail- able real training sequences are complemented with our synthetically generated dataset during training.",0
"Розробка систем штучного навчання, які можуть розуміти та генерувати природну мову, була однією з давніх цілей штучного інтелекту. Останні десятиліття стали свідками вражаючого прогресу в обох цих проблемах, що породило нове сімейство підходів. Зокрема, прогрес у глибокому навчанні за останні пару років призвів до нейронних підходів до створення природної мови (NLG). Ці методи поєднують методи генеративного вивчення мови з нейронними мережами. Завдяки широкому спектру застосувань у обробці природної мови, нейронні NLG (NNLG) є новою галуззю досліджень, яка швидко розвивається. У цьому сучасному звіті ми досліджуємо останні розробки та застосування NNLG у повному обсязі з багатовимірного погляду, охоплюючи важливі перспективи, такі як мультимодальність, багатомовність, керованість і стратегії навчання. Ми узагальнюємо фундаментальні будівельні блоки підходів NNLG з цих аспектів і надаємо детальні огляди зазвичай використовуваних етапів попередньої обробки та основних нейронних архітектур. У цьому звіті також зосереджено увагу на основних застосуваннях цих моделей NNLG, таких як машинний переклад, генерація описів, автоматичне розпізнавання мовлення, абстрактне резюмування, спрощення тексту, відповіді на запитання та генерація, а також генерація діалогів. Нарешті, ми закінчуємо детальним обговоренням описаних структур, вказуючи на деякі відкриті напрямки дослідження.","Надійне візуальне відстеження відіграє життєво важливу роль у багатьох сферах, таких як автономні автомобілі, спостереження та робототехніка. Показано, що останні трекери досягають відповідних результатів за звичайних сценаріїв відстеження за ясних погодних умов, стандартних налаштувань камери та умов освітлення. Тим не менш, продуктивність цих трекерів, незалежно від того, засновані вони на кореляційному фільтрі чи на основі навчання, погіршується за несприятливих погодних умов. Відсутність відео з такими погодними умовами в доступних наборах даних візуального відстеження об’єктів є основною проблемою низької продуктивності алгоритмів відстеження на основі навчання. У цій роботі ми надаємо новий набір даних відстеження людей у реальному світі (PTAW172Real), знятих за умов туману, дощу та снігу, щоб оцінити ефективність поточних трекерів. Ми також представляємо новий набір даних відстеження людей із синтетичних послідовностей (PTAW217Synth), процедурно згенерованих нашою структурою NOVA, що охоплює однакові погодні умови різного ступеня тяжкості, щоб пом’якшити проблему дефіциту даних. Наші експериментальні результати демонструють, що продуктивність найсучасніших глибинних трекерів за несприятливих погодних умов може бути покращена, якщо доступні реальні навчальні послідовності доповнюються нашим синтетично згенерованим набором даних під час навчання.",0
"While stochastic video prediction models enable future prediction under uncertainty, they mostly fail to model the complex dynamics of real-world scenes. For example, they cannot provide reliable predictions for scenes with a moving camera and independently moving foreground objects in driving scenarios. The existing methods fail to fully capture the dynamics of the structured world by only focusing on changes in pixels. In this paper, we assume that there is an underlying process creating observations in a video and propose to factorize it into static and dynamic components. We model the static part based on the scene structure and the ego-motion of the vehicle, and the dynamic part based on the remaining motion of the dynamic objects. By learning separate distributions of changes in foreground and background, we can decompose the scene into static and dynamic parts and separately model the change in each. Our experiments demonstrate that disentangling structure and motion helps stochastic video prediction, leading to better future predictions in complex driving scenarios on two real-world driving datasets, KITTI and Cityscapes. ","In this paper, we address the problem of learning to summarize personal photo albums. That is, given a photo album, we aim to select a small set of representative images from the album so that the extracted summary captures most of the story that is being told through the images. More specifically, we extend a recently proposed recurrent neural network based framework by employing a more effective way to represent images and, more importantly, adding a diversity term to the main objective. Our diversity term is based on the idea of jointly training a discriminator network to evaluate the diversity of the selected images. This alleviates the issue of selecting near-duplicate or semantically similar images, which is the primary shortcoming of the base approach. The experimental results show that our improved model produces better or comparable summaries, providing a good balance between quality and diversity. ",0
"While stochastic video prediction models enable future prediction under uncertainty, they mostly fail to model the complex dynamics of real-world scenes. For example, they cannot provide reliable predictions for scenes with a moving camera and independently moving foreground objects in driving scenarios. The existing methods fail to fully capture the dynamics of the structured world by only focusing on changes in pixels. In this paper, we assume that there is an underlying process creating observations in a video and propose to factorize it into static and dynamic components. We model the static part based on the scene structure and the ego-motion of the vehicle, and the dynamic part based on the remaining motion of the dynamic objects. By learning separate distributions of changes in foreground and background, we can decompose the scene into static and dynamic parts and separately model the change in each. Our experiments demonstrate that disentangling structure and motion helps stochastic video prediction, leading to better future predictions in complex driving scenarios on two real-world driving datasets, KITTI and Cityscapes. ","У цій роботі ми торкнемося проблеми навчання узагальненню особистих фотоальбомів. Тобто, враховуючи фотоальбом, ми прагнемо вибрати невеликий набір репрезентативних зображень з альбому, щоб витягнутий підсумок охоплював більшу частину історії, яка розповідається через зображення. Зокрема, ми розширюємо нещодавно запропоновану структуру на основі рекурентної нейронної мережі, використовуючи більш ефективний спосіб представлення зображень і, що більш важливо, додаючи термін різноманітності до основної мети. Наш термін різноманітності базується на ідеї спільного навчання мережі дискримінаторів для оцінки різноманітності вибраних зображень. Це полегшує проблему вибору майже повторюваних або семантично схожих зображень, що є основним недоліком базового підходу. Експериментальні результати показують, що наша вдосконалена модель створює кращі або порівнювані підсумки, забезпечуючи хороший баланс між якістю та різноманітністю.",0
"Хоча стохастичні моделі прогнозування відео дозволяють прогнозувати майбутнє в умовах невизначеності, вони здебільшого не в змоделі складної динаміки сцен реального світу. Наприклад, вони не можуть забезпечити надійні прогнози для сцен із рухомою камерою та незалежно рухомими об’єктами переднього плану в сценаріях водіння. Існуючі методи не можуть повністю охопити динаміку структурованого світу, зосереджуючись лише на змінах у пікселях. У цьому документі ми припускаємо, що існує базовий процес створення спостережень у відео, і пропонуємо розділити його на статичні та динамічні компоненти. Ми моделюємо статичну частину на основі структури сцени та его-руху автомобіля, а динамічну частину – на основі залишкового руху динамічних об’єктів. Вивчаючи окремі розподіли змін переднього та заднього планів, ми можемо розкласти сцену на статичні та динамічні частини та окремо моделювати зміни в кожній. Наші експерименти демонструють, що розмежування структури та руху сприяє стохастичному прогнозуванню відео, що дає змогу краще прогнозувати майбутнє у складних сценаріях водіння на двох наборах даних реального водіння, KITTI та Cityscapes.","In this paper, we address the problem of learning to summarize personal photo albums. That is, given a photo album, we aim to select a small set of representative images from the album so that the extracted summary captures most of the story that is being told through the images. More specifically, we extend a recently proposed recurrent neural network based framework by employing a more effective way to represent images and, more importantly, adding a diversity term to the main objective. Our diversity term is based on the idea of jointly training a discriminator network to evaluate the diversity of the selected images. This alleviates the issue of selecting near-duplicate or semantically similar images, which is the primary shortcoming of the base approach. The experimental results show that our improved model produces better or comparable summaries, providing a good balance between quality and diversity. ",0
"Хоча стохастичні моделі прогнозування відео дозволяють прогнозувати майбутнє в умовах невизначеності, вони здебільшого не можуть моделювати складну динаміку сцен реального світу. Наприклад, вони не можуть забезпечити надійні прогнози для сцен із рухомою камерою та незалежно рухомими об’єктами переднього плану в сценаріях водіння. Існуючі методи не можуть повністю охопити динаміку структурованого світу, зосереджуючись лише на змінах у пікселях. У цьому документі ми припускаємо, що існує базовий процес створення спостережень у відео, і пропонуємо розділити його на статичні та динамічні компоненти. Ми моделюємо статичну частину на основі структури сцени та его-руху автомобіля, а динамічну частину – на основі залишкового руху динамічних об’єктів. Вивчаючи окремі розподіли змін переднього та заднього планів, ми можемо розкласти сцену на статичні та динамічні частини та окремо моделювати зміни в кожній. Наші експерименти демонструють, що розмежування структури та руху сприяє стохастичному прогнозуванню відео, що дає змогу краще прогнозувати майбутнє у складних сценаріях водіння на двох наборах даних реального водіння, KITTI та Cityscapes.","У цій роботі ми торкнемося проблеми навчання узагальненню особистих фотоальбомів. Тобто, враховуючи фотоальбом, ми прагнемо вибрати невеликий набір репрезентативних зображень з альбому, щоб витягнутий підсумок охоплював більшу частину історії, яка розповідається через зображення. Зокрема, ми розширюємо нещодавно запропоновану структуру на основі рекурентної нейронної мережі, використовуючи більш ефективний спосіб представлення зображень і, що більш важливо, додаючи термін різноманітності до основної мети. Наш термін різноманітності базується на ідеї спільного навчання мережі дискримінаторів для оцінки різноманітності вибраних зображень. Це полегшує проблему вибору майже повторюваних або семантично схожих зображень, що є основним недоліком базового підходу. Експериментальні результати показують, що наша вдосконалена модель створює кращі або порівнювані підсумки, забезпечуючи хороший баланс між якістю та різноманітністю.",0
"How to best integrate linguistic and perceptual processing in multi-modal tasks that involve language and vision is an important open problem. In this work, we argue that the common practice of using language in a topdown manner, to direct visual attention over high-level visual features, may not be optimal. We hypothesize that the use of language to also condition the bottom-up processing from pixels to high-level features can provide benefits to the overall performance. To support our claim, we propose a U-Net-based model and perform experiments on two language-vision dense-prediction tasks: referring expression segmentation and language-guided image colorization. We compare results where either one or both of the top-down and bottom-up visual branches are conditioned on language. Our experiments reveal that using language to control the filters for bottom-up visual processing in addition to top-down attention leads to better results on both tasks and achieves competitive performance. Our linguistic analysis suggests that bottom-up conditioning improves segmentation of objects especially when input text refers to low-level visual concepts","Computational saliency models for still images have gained significant popularity in recent years. Saliency prediction from videos, on the other hand, has received relatively little interest from the community. Motivated by this, in this paper, we study the use of deep learning for dynamic saliency prediction and propose the so-called spatio-temporal saliency networks. The key to our models is the architecture of two-stream networks where we investigate different fusion mechanisms to integrate spatial and temporal information. We evaluate our models on the dynamic images and eye movements and University of Central Florida-Sports datasets and present highly competitive results against the existing state-of-the-art models. We also carry out some experiments on a number of still images from the MIT300 dataset by exploiting the optical flow maps predicted from these images. Our results show that considering inherent motion information in this way can be helpful for static saliency estimation.",0
"How to best integrate linguistic and perceptual processing in multi-modal tasks that involve language and vision is an important open problem. In this work, we argue that the common practice of using language in a topdown manner, to direct visual attention over high-level visual features, may not be optimal. We hypothesize that the use of language to also condition the bottom-up processing from pixels to high-level features can provide benefits to the overall performance. To support our claim, we propose a U-Net-based model and perform experiments on two language-vision dense-prediction tasks: referring expression segmentation and language-guided image colorization. We compare results where either one or both of the top-down and bottom-up visual branches are conditioned on language. Our experiments reveal that using language to control the filters for bottom-up visual processing in addition to top-down attention leads to better results on both tasks and achieves competitive performance. Our linguistic analysis suggests that bottom-up conditioning improves segmentation of objects especially when input text refers to low-level visual concepts","Обчислювальні моделі помітності для нерухомих зображень набули значної популярності в останні роки. З іншого боку, передбачення помітності відеозаписів викликало відносно невеликий інтерес у спільноти. Мотивуючись цим, у цій статті ми вивчаємо використання глибокого навчання для динамічного прогнозування помітності та пропонуємо так звані просторово-часові мережі помітності. Ключем до наших моделей є архітектура двопотокових мереж, де ми досліджуємо різні механізми злиття для інтеграції просторової та часової інформації. Ми оцінюємо наші моделі на основі динамічних зображень і рухів очей, а також наборів даних Університету Центральної Флориди-Спорт і представляємо конкурентоспроможні результати порівняно з існуючими найсучаснішими моделями. Ми також проводимо деякі експерименти на кількох нерухомих зображеннях із набору даних MIT300, використовуючи карти оптичного потоку, передбачені на цих зображеннях. Наші результати показують, що врахування інформації про внутрішній рух таким чином може бути корисним для оцінки статичної помітності.",0
"Як найкраще інтегрувати лінгвістичну та перцептивну обробку в мультимодальних завданнях, які включають мову та зір, є важливою відкритою проблемою. У цій роботі ми стверджуємо, що звичайна практика використання мови зверху вниз, щоб спрямувати візуальну увагу на візуальні функції високого рівня, може бути не оптимальною. Ми припускаємо, що використання мови для обумовлення обробки знизу вгору від пікселів до функцій високого рівня може принести переваги загальній продуктивності. Щоб підтвердити наше твердження, ми пропонуємо модель на основі U-Net і проводимо експерименти з двома завданнями щільного прогнозування мовного бачення: сегментація виразу за посиланням і розфарбування зображення під керуванням мови. Ми порівнюємо результати, де одна або обидві візуальні гілки зверху вниз і знизу вгору залежать від мови. Наші експерименти показують, що використання мови для керування фільтрами для візуальної обробки знизу вгору на додаток до уваги зверху вниз призводить до кращих результатів в обох завданнях і досягає конкурентоспроможності. Наш лінгвістичний аналіз показує, що кондиціонування знизу вгору покращує сегментацію об’єктів, особливо коли вхідний текст відноситься до візуальних концепцій низького рівня.","Computational saliency models for still images have gained significant popularity in recent years. Saliency prediction from videos, on the other hand, has received relatively little interest from the community. Motivated by this, in this paper, we study the use of deep learning for dynamic saliency prediction and propose the so-called spatio-temporal saliency networks. The key to our models is the architecture of two-stream networks where we investigate different fusion mechanisms to integrate spatial and temporal information. We evaluate our models on the dynamic images and eye movements and University of Central Florida-Sports datasets and present highly competitive results against the existing state-of-the-art models. We also carry out some experiments on a number of still images from the MIT300 dataset by exploiting the optical flow maps predicted from these images. Our results show that considering inherent motion information in this way can be helpful for static saliency estimation.",0
"Важливою відкритою проблемою є те, як найкраще інтегрувати лінгвістичну обробку та обробку сприйняття в мультимодальних завданнях, які включають мову та зір. У цій роботі ми стверджуємо, що звичайна практика використання мови за принципом «зверху вниз» для спрямування візуальної уваги на візуальні функції високого рівня може бути неоптимальною. Ми припускаємо, що використання мови для обумовлення обробки знизу вгору від пікселів до функцій високого рівня може принести переваги загальній продуктивності. Щоб підтвердити наше твердження, ми пропонуємо модель на основі U-Net і проводимо експерименти з двома завданнями щільного прогнозування мовного бачення: сегментація виразу за посиланням і кольорування зображення під керуванням мови. Ми порівнюємо результати, де одна або обидві візуальні гілки зверху вниз і знизу вгору залежать від мови. Наші експерименти показують, що використання мови для керування фільтрами для візуальної обробки знизу вгору на додаток до уваги зверху вниз призводить до кращих результатів в обох завданнях і досягає конкурентоспроможності. Наш лінгвістичний аналіз показує, що кондиціонування знизу вгору покращує сегментацію об’єктів, особливо коли вхідний текст відноситься до візуальних концепцій низького рівня","Обчислювальні моделі помітності для нерухомих зображень набули значної популярності в останні роки. З іншого боку, передбачення помітності відеозаписів викликало відносно невеликий інтерес у спільноти. Мотивуючись цим, у цій статті ми вивчаємо використання глибокого навчання для динамічного прогнозування помітності та пропонуємо так звані просторово-часові мережі помітності. Ключем до наших моделей є архітектура двопотокових мереж, де ми досліджуємо різні механізми злиття для інтеграції просторової та часової інформації. Ми оцінюємо наші моделі на основі динамічних зображень і рухів очей, а також наборів даних Університету Центральної Флориди-Спорт і представляємо конкурентоспроможні результати порівняно з існуючими найсучаснішими моделями. Ми також проводимо деякі експерименти на кількох нерухомих зображеннях із набору даних MIT300, використовуючи карти оптичного потоку, передбачені на цих зображеннях. Наші результати показують, що врахування інформації про внутрішній рух таким чином може бути корисним для оцінки статичної помітності.",0
"Capturing images under extremely low-light conditions poses significant challenges for the standard camera pipeline. Images become too dark and too noisy, which makes traditional enhancement techniques almost impossible to apply. Recently, learning-based approaches have shown very promising results for this task since they have substantially more expressive capabilities to allow for improved quality. Motivated by these studies, in this paper, we aim to leverage burst photography to boost the performance and obtain much sharper and more accurate RGB images from extremely dark raw images. The backbone of our proposed framework is a novel coarse-to-fine network architecture that generates high-quality outputs progressively. The coarse network predicts a low-resolution, denoised raw image, which is then fed to the fine network to recover fine-scale details and realistic textures. To further reduce the noise level and improve the color accuracy, we extend this network to a permutation invariant structure so that it takes a burst of low-light images as input and merges information from multiple images at the feature-level. Our experiments demonstrate that our approach leads to perceptually more pleasing results than the state-of-the-art methods by producing more detailed and considerably higher quality images.","Predicting saliency in videos is a challenging problem due to complex modeling of interactions between spatial and temporal information, especially when ever-changing, dynamic nature of videos is considered. Recently, researchers have proposed large-scale data sets and models that take advantage of deep learning as a way to understand what is important for video saliency. These approaches, however, learn to combine spatial and temporal features in a static manner and do not adapt themselves much to the changes in the video content. In this article, we introduce the gated fusion network for dynamic saliency (GFSalNet), the first deep saliency model capable of making predictions in a dynamic way via the gated fusion mechanism. Moreover, our model also exploits spatial and channelwise attention within a multiscale architecture that further allows for highly accurate predictions. We evaluate the proposed approach on a number of data sets, and our experimental analysis demonstrates that it outperforms or is highly competitive with the state of the art. Importantly, we show that it has a good generalization ability, and moreover, exploits temporal information more effectively via its adaptive fusion scheme.",0
"Capturing images under extremely low-light conditions poses significant challenges for the standard camera pipeline. Images become too dark and too noisy, which makes traditional enhancement techniques almost impossible to apply. Recently, learning-based approaches have shown very promising results for this task since they have substantially more expressive capabilities to allow for improved quality. Motivated by these studies, in this paper, we aim to leverage burst photography to boost the performance and obtain much sharper and more accurate RGB images from extremely dark raw images. The backbone of our proposed framework is a novel coarse-to-fine network architecture that generates high-quality outputs progressively. The coarse network predicts a low-resolution, denoised raw image, which is then fed to the fine network to recover fine-scale details and realistic textures. To further reduce the noise level and improve the color accuracy, we extend this network to a permutation invariant structure so that it takes a burst of low-light images as input and merges information from multiple images at the feature-level. Our experiments demonstrate that our approach leads to perceptually more pleasing results than the state-of-the-art methods by producing more detailed and considerably higher quality images.","Прогнозування помітності у відео є складною проблемою через складне моделювання взаємодій між просторовою та часовою інформацією, особливо коли враховується постійна мінлива динамічна природа відео. Нещодавно дослідники запропонували масштабні набори даних і моделі, які використовують переваги глибокого навчання як спосіб зрозуміти, що важливо для помітності відео. Ці підходи, однак, вчаться поєднувати просторові та часові характеристики статичним чином і не дуже адаптуються до змін у відеоконтенті. У цій статті ми представляємо мережу закритого синтезу для динамічної помітності (GFSalNet), першу модель глибокої помітності, здатну робити прогнози динамічним способом за допомогою механізму закритого синтезу. Крім того, наша модель також використовує просторову та канальну увагу в рамках багатомасштабної архітектури, що додатково дозволяє робити високоточні прогнози. Ми оцінюємо запропонований підхід на низці наборів даних, і наш експериментальний аналіз показує, що він перевершує сучасні технології або є дуже конкурентоспроможним. Важливо те, що ми показуємо, що він має хорошу здатність до узагальнення та, крім того, ефективніше використовує часову інформацію за допомогою адаптивної схеми синтезу.",0
"Зйомка зображень в умовах надзвичайно слабкого освітлення створює значні труднощі для стандартної камери. Зображення стають надто темними та занадто шумними, що робить практично неможливим застосування традиційних методів покращення. Нещодавно підходи, що ґрунтуються на навчанні, показали багатообіцяючі результати для цього завдання, оскільки вони мають значно більше виразних можливостей для підвищення якості. Керуючись цими дослідженнями, у цій статті ми прагнемо використовувати серійну фотографію, щоб підвищити продуктивність і отримати набагато чіткіші та точніші RGB-зображення з надзвичайно темних необроблених зображень. Основою запропонованої нами структури є нова мережева архітектура від грубої до тонкої, яка поступово генерує високоякісні результати. Груба мережа передбачає необроблене зображення з низькою роздільною здатністю, знешумлене, яке потім передається в тонку мережу для відновлення дрібних деталей і реалістичних текстур. Щоб ще більше знизити рівень шуму та підвищити точність кольору, ми розширюємо цю мережу до структури, інваріантної до перестановок, щоб вона приймала серію зображень із слабким освітленням як вхідні дані та об’єднувала інформацію з кількох зображень на рівні функцій. Наші експерименти демонструють, що наш підхід дає сприятливіші результати, ніж найсучасніші методи, створюючи детальніші та значно якісніші зображення.","Predicting saliency in videos is a challenging problem due to complex modeling of interactions between spatial and temporal information, especially when ever-changing, dynamic nature of videos is considered. Recently, researchers have proposed large-scale data sets and models that take advantage of deep learning as a way to understand what is important for video saliency. These approaches, however, learn to combine spatial and temporal features in a static manner and do not adapt themselves much to the changes in the video content. In this article, we introduce the gated fusion network for dynamic saliency (GFSalNet), the first deep saliency model capable of making predictions in a dynamic way via the gated fusion mechanism. Moreover, our model also exploits spatial and channelwise attention within a multiscale architecture that further allows for highly accurate predictions. We evaluate the proposed approach on a number of data sets, and our experimental analysis demonstrates that it outperforms or is highly competitive with the state of the art. Importantly, we show that it has a good generalization ability, and moreover, exploits temporal information more effectively via its adaptive fusion scheme.",0
"Зйомка зображень в умовах надзвичайно слабкого освітлення створює значні труднощі для стандартної камери. Зображення стають надто темними та занадто шумними, що робить практично неможливим застосування традиційних методів покращення. Нещодавно підходи, засновані на навчанні, показали багатообіцяючі результати для цього завдання, оскільки вони мають значно більше виразних можливостей, що дозволяє покращити якість. Керуючись цими дослідженнями, у цій статті ми прагнемо використовувати серійну фотографію, щоб підвищити продуктивність і отримати набагато чіткіші та точніші RGB-зображення з надзвичайно темних необроблених зображень. Основою запропонованої нами структури є нова мережева архітектура від грубої до тонкої, яка поступово генерує високоякісні результати. Груба мережа передбачає необроблене зображення з низькою роздільною здатністю, знешумлене, яке потім передається в тонку мережу для відновлення дрібних деталей і реалістичних текстур. Щоб ще більше знизити рівень шуму та підвищити точність кольору, ми розширюємо цю мережу до структури, інваріантної до перестановок, щоб вона приймала серію зображень із слабким освітленням як вхідні дані та об’єднувала інформацію з кількох зображень на рівні функцій. Наші експерименти демонструють, що наш підхід дає сприятливіші результати, ніж найсучасніші методи, створюючи детальніші та значно якісніші зображення.","Прогнозування помітності у відео є складною проблемою через складне моделювання взаємодій між просторовою та часовою інформацією, особливо коли враховується постійна мінлива динамічна природа відео. Нещодавно дослідники запропонували масштабні набори даних і моделі, які використовують переваги глибокого навчання як спосіб зрозуміти, що важливо для помітності відео. Ці підходи, однак, вчаться поєднувати просторові та часові характеристики статичним чином і не дуже адаптуються до змін у відеоконтенті. У цій статті ми представляємо мережу закритого синтезу для динамічної помітності (GFSalNet), першу модель глибокої помітності, здатну робити прогнози динамічним способом за допомогою механізму закритого синтезу. Крім того, наша модель також використовує просторову та канальну увагу в рамках багатомасштабної архітектури, що додатково дозволяє робити високоточні прогнози. Ми оцінюємо запропонований підхід на низці наборів даних, і наш експериментальний аналіз показує, що він перевершує сучасні технології або є дуже конкурентоспроможним. Важливо те, що ми показуємо, що він має хорошу здатність до узагальнення та, крім того, ефективніше використовує часову інформацію за допомогою адаптивної схеми синтезу.",0
"Image editing is a commonly studied problem in computer graphics. Despite the presence of many advanced editing tools, there is no satisfactory solution to controllably update the position of the sun using a single image. This problem is made complicated by the presence of clouds, complex landscapes, and the atmospheric effects that must be accounted for. In this paper, we tackle this problem starting with only a single photograph. With the user clicking on the initial position of the sun, our algorithm performs several estimation and segmentation processes for finding the horizon, scene depth, clouds, and the sky line. After this initial process, the user can make both fine- and large-scale changes on the position of the sun: it can be set beneath the mountains or moved behind the clouds practically turning a midday photograph into a sunset (or vice versa). We leverage a precomputed atmospheric scattering algorithm to make all of these changes not only realistic but also in real-time. We demonstrate our results using both clear and cloudy skies, showing how to add, remove, and relight clouds, all the while allowing for advanced effects such as scattering, shadows, light shafts, and lens flares","Reconstructing high dynamic range (HDR) images of a complex scene involving moving objects and dynamic backgrounds is prone to artifacts. A large number of methods have been proposed that attempt to alleviate these artifacts, known as HDR deghosting algorithms. Currently, the quality of these algorithms are judged by subjective evaluations, which are tedious to conduct and get quickly outdated as new algorithms are proposed on a rapid basis. In this paper, we propose an objective metric which aims to simplify this process. Our metric takes a stack of input exposures and the deghosting result and produces a set of artifact maps for different types of artifacts. These artifact maps can be combined to yield a single quality score. We performed a subjective experiment involving 52 subjects and 16 different scenes to validate the agreement of our quality scores with subjective judgements and observed a concordance of almost 80%. Our metric also enables a novel application that we call as hybrid deghosting, in which the output of different deghosting algorithms are combined to obtain a superior deghosting result.",0
"Image editing is a commonly studied problem in computer graphics. Despite the presence of many advanced editing tools, there is no satisfactory solution to controllably update the position of the sun using a single image. This problem is made complicated by the presence of clouds, complex landscapes, and the atmospheric effects that must be accounted for. In this paper, we tackle this problem starting with only a single photograph. With the user clicking on the initial position of the sun, our algorithm performs several estimation and segmentation processes for finding the horizon, scene depth, clouds, and the sky line. After this initial process, the user can make both fine- and large-scale changes on the position of the sun: it can be set beneath the mountains or moved behind the clouds practically turning a midday photograph into a sunset (or vice versa). We leverage a precomputed atmospheric scattering algorithm to make all of these changes not only realistic but also in real-time. We demonstrate our results using both clear and cloudy skies, showing how to add, remove, and relight clouds, all the while allowing for advanced effects such as scattering, shadows, light shafts, and lens flares","Реконструкція зображень із широким динамічним діапазоном (HDR) складної сцени з рухомими об’єктами та динамічним фоном схильна до артефактів. Було запропоновано велику кількість методів, які намагаються зменшити ці артефакти, відомих як алгоритми видалення фантом HDR. Наразі якість цих алгоритмів оцінюється за суб’єктивними оцінками, які нудно проводити та швидко застарівають, оскільки нові алгоритми пропонуються швидко. У цій статті ми пропонуємо об’єктивну метрику, яка має на меті спростити цей процес. Наша метрика бере набір вхідних експозицій і результат видалення фантомних зображень і створює набір карт артефактів для різних типів артефактів. Ці карти артефактів можна комбінувати, щоб отримати єдиний показник якості. Ми провели суб’єктивний експеримент із залученням 52 суб’єктів і 16 різних сцен, щоб перевірити узгодженість наших показників якості з суб’єктивними судженнями та спостерігали узгодженість майже на 80%. Наша метрика також дозволяє нову програму, яку ми називаємо гібридним видаленням фантомних зображень, у якому вихідні дані різних алгоритмів видалення фантомних зображень поєднуються для отримання чудового результату видалення фантомних зображень.",0
"Редагування зображень є широко дослідженою проблемою в комп’ютерній графіці. Незважаючи на наявність багатьох розширених інструментів редагування, не існує задовільного рішення для контрольованого оновлення положення сонця за допомогою одного зображення. Ця проблема ускладнюється наявністю хмар, складними ландшафтами та атмосферними впливами, які необхідно враховувати. У цій статті ми вирішуємо цю проблему, починаючи лише з однієї фотографії. Коли користувач натискає початкове положення сонця, наш алгоритм виконує кілька процесів оцінки та сегментації для пошуку горизонту, глибини сцени, хмар і лінії неба. Після цього початкового процесу користувач може робити як дрібні, так і масштабні зміни положення сонця: його можна встановити під гори або перемістити за хмари, практично перетворивши полуденну фотографію на захід сонця (або навпаки). Ми використовуємо попередньо обчислений алгоритм атмосферного розсіювання, щоб зробити всі ці зміни не лише реалістичними, але й у реальному часі. Ми демонструємо наші результати, використовуючи як ясне, так і хмарне небо, показуючи, як додавати, видаляти та повторно освітлювати хмари, водночас допускаючи розширені ефекти, такі як розсіювання, тіні, світлові вали та відблиски від лінз.","Reconstructing high dynamic range (HDR) images of a complex scene involving moving objects and dynamic backgrounds is prone to artifacts. A large number of methods have been proposed that attempt to alleviate these artifacts, known as HDR deghosting algorithms. Currently, the quality of these algorithms are judged by subjective evaluations, which are tedious to conduct and get quickly outdated as new algorithms are proposed on a rapid basis. In this paper, we propose an objective metric which aims to simplify this process. Our metric takes a stack of input exposures and the deghosting result and produces a set of artifact maps for different types of artifacts. These artifact maps can be combined to yield a single quality score. We performed a subjective experiment involving 52 subjects and 16 different scenes to validate the agreement of our quality scores with subjective judgements and observed a concordance of almost 80%. Our metric also enables a novel application that we call as hybrid deghosting, in which the output of different deghosting algorithms are combined to obtain a superior deghosting result.",0
"Редагування зображень є широко дослідженою проблемою в комп’ютерній графіці. Незважаючи на наявність багатьох розширених інструментів редагування, не існує задовільного рішення для контрольованого оновлення положення сонця за допомогою одного зображення. Ця проблема ускладнюється наявністю хмар, складними ландшафтами та атмосферними впливами, які необхідно враховувати. У цій статті ми вирішуємо цю проблему, починаючи лише з однієї фотографії. Коли користувач натискає початкове положення сонця, наш алгоритм виконує кілька процесів оцінки та сегментації для пошуку горизонту, глибини сцени, хмар і лінії неба. Після цього початкового процесу користувач може робити як дрібні, так і масштабні зміни положення сонця: його можна встановити під гори або перемістити за хмари, практично перетворивши полуденну фотографію на захід сонця (або навпаки). Ми використовуємо попередньо обчислений алгоритм атмосферного розсіювання, щоб зробити всі ці зміни не лише реалістичними, але й у реальному часі. Ми демонструємо наші результати, використовуючи як ясне, так і хмарне небо, показуючи, як додавати, видаляти та повторно освітлювати хмари, водночас допускаючи розширені ефекти, такі як розсіювання, тіні, світлові вали та відблиски від лінз.","Реконструкція зображень із широким динамічним діапазоном (HDR) складної сцени з рухомими об’єктами та динамічним фоном схильна до артефактів. Було запропоновано велику кількість методів, які намагаються зменшити ці артефакти, відомих як алгоритми видалення фантом HDR. Наразі якість цих алгоритмів оцінюється за суб’єктивними оцінками, які нудно проводити та швидко застарівають, оскільки нові алгоритми пропонуються швидко. У цій статті ми пропонуємо об’єктивну метрику, яка має на меті спростити цей процес. Наша метрика бере набір вхідних експозицій і результат видалення фантомних зображень і створює набір карт артефактів для різних типів артефактів. Ці карти артефактів можна комбінувати, щоб отримати єдиний показник якості. Ми провели суб’єктивний експеримент із залученням 52 суб’єктів і 16 різних сцен, щоб перевірити узгодженість наших показників якості з суб’єктивними судженнями та спостерігали узгодженість майже на 80%. Наша метрика також дозволяє нову програму, яку ми називаємо гібридним видаленням фантомних зображень, у якому вихідні дані різних алгоритмів видалення фантомних зображень поєднуються для отримання чудового результату видалення фантомних зображень.",0
"Learning robust representations is critical for the success of person re-identification and attribute recognition systems. However, to achieve this, we must use a large dataset of diverse person images as well as annotations of identity labels and/or a set of different attributes. Apart from the obvious concerns about privacy issues, the manual annotation process is both time consuming and too costly. In this paper, we instead propose to use synthetic person images for addressing these difficulties. Specifically, we first introduce Synthetic18K, a large-scale dataset of over 1 million computer generated person images of 18K unique identities with relevant attributes. Moreover, we demonstrate that pretraining of simple deep architectures on Synthetic18K for person re-identification and attribute recognition and then fine-tuning on real data leads to significant improvements in prediction performances, giving results better than or comparable to state-of-the-art models.","Local symmetry axis based schemes have been used for generic shape recognition as they lead to articulation insensitive representations. Despite their strengths, purely syntactic level of axial representations precludes the possibility of distinguishing a likely articulation from an unlikely one. In order to overcome this weakness, syntax should be combined with pragmatics and/or semantics. As a solution we propose a novel articulation space which enables inferences on the likelihood of possible articulations. Articulation priors can be constructed directly from examples (pragmatics) or set externally (semantics). We incorporate articulation priors to a skeletal matching scheme to arrive at an enriched axial representation which is sensitive to unlikely articulations but insensitive to likely ones",0
"Learning robust representations is critical for the success of person re-identification and attribute recognition systems. However, to achieve this, we must use a large dataset of diverse person images as well as annotations of identity labels and/or a set of different attributes. Apart from the obvious concerns about privacy issues, the manual annotation process is both time consuming and too costly. In this paper, we instead propose to use synthetic person images for addressing these difficulties. Specifically, we first introduce Synthetic18K, a large-scale dataset of over 1 million computer generated person images of 18K unique identities with relevant attributes. Moreover, we demonstrate that pretraining of simple deep architectures on Synthetic18K for person re-identification and attribute recognition and then fine-tuning on real data leads to significant improvements in prediction performances, giving results better than or comparable to state-of-the-art models.","Схеми, засновані на локальній осі симетрії, використовувалися для загального розпізнавання форми, оскільки вони призводили до представлень, нечутливих до артикуляції. Незважаючи на їхні сильні сторони, суто синтаксичний рівень осьових репрезентацій виключає можливість відрізнити ймовірну артикуляцію від малоймовірної. Щоб подолати цю слабкість, синтаксис слід поєднувати з прагматикою та/або семантикою. Як рішення ми пропонуємо новий артикуляційний простір, який дозволяє робити висновки про ймовірність можливих артикуляцій. Пріори артикуляції можуть бути побудовані безпосередньо з прикладів (прагматика) або задані зовні (семантика). Ми включаємо артикуляцію перед скелетною схемою відповідності, щоб досягти збагаченого осьового представлення, чутливого до малоймовірних артикуляцій, але нечутливого до ймовірних",0
"Вивчення надійних представлень має вирішальне значення для успіху систем повторної ідентифікації особи та розпізнавання атрибутів. Однак, щоб досягти цього, ми повинні використовувати великий набір даних різноманітних зображень людей, а також анотації міток ідентифікації та/або набір різних атрибутів. Крім очевидного занепокоєння щодо проблем конфіденційності, процес анотації вручну займає багато часу та надто дорого. У цій статті ми замість цього пропонуємо використовувати зображення синтетичних людей для вирішення цих труднощів. Зокрема, ми вперше представляємо Synthetic18K, великомасштабний набір даних із понад 1 мільйона зображень людей, згенерованих комп’ютером, 18K унікальних ідентифікацій із відповідними атрибутами. Крім того, ми демонструємо, що попереднє навчання простих глибоких архітектур на Synthetic18K для повторної ідентифікації особи та розпізнавання атрибутів, а потім точного налаштування на реальних даних призводить до значного покращення продуктивності прогнозування, даючи результати, кращі або порівнювані з найсучаснішими. моделі.","Local symmetry axis based schemes have been used for generic shape recognition as they lead to articulation insensitive representations. Despite their strengths, purely syntactic level of axial representations precludes the possibility of distinguishing a likely articulation from an unlikely one. In order to overcome this weakness, syntax should be combined with pragmatics and/or semantics. As a solution we propose a novel articulation space which enables inferences on the likelihood of possible articulations. Articulation priors can be constructed directly from examples (pragmatics) or set externally (semantics). We incorporate articulation priors to a skeletal matching scheme to arrive at an enriched axial representation which is sensitive to unlikely articulations but insensitive to likely ones",0
"Вивчення надійних представлень має вирішальне значення для успіху систем повторної ідентифікації особи та розпізнавання атрибутів. Однак, щоб досягти цього, ми повинні використовувати великий набір даних різноманітних зображень людей, а також анотації міток ідентифікації та/або набір різних атрибутів. Крім очевидного занепокоєння щодо проблем конфіденційності, процес анотації вручну займає багато часу та надто дорого. У цій статті ми замість цього пропонуємо використовувати зображення синтетичних людей для вирішення цих труднощів. Зокрема, ми вперше представляємо Synthetic18K, великомасштабний набір даних із понад 1 мільйона зображень людей, згенерованих комп’ютером, 18K унікальних ідентифікацій із відповідними атрибутами. Крім того, ми демонструємо, що попереднє навчання простих глибоких архітектур на Synthetic18K для повторної ідентифікації особи та розпізнавання атрибутів, а потім точного налаштування на реальних даних призводить до значного покращення продуктивності прогнозування, даючи результати, кращі або порівнювані з найсучаснішими. моделі.","Схеми, засновані на локальній осі симетрії, використовувалися для загального розпізнавання форми, оскільки вони призводили до представлень, нечутливих до артикуляції. Незважаючи на їхні сильні сторони, суто синтаксичний рівень осьових репрезентацій виключає можливість відрізнити ймовірну артикуляцію від малоймовірної. Щоб подолати цю слабкість, синтаксис слід поєднувати з прагматикою та/або семантикою. Як рішення ми пропонуємо новий артикуляційний простір, який дозволяє робити висновки про ймовірність можливих артикуляцій. Пріори артикуляції можуть бути побудовані безпосередньо з прикладів (прагматика) або задані зовні (семантика). Ми включаємо артикуляцію перед скелетною схемою відповідності, щоб досягти збагаченого осьового представлення, чутливого до малоймовірних артикуляцій, але нечутливого до ймовірних",0
"Today, the cutting edge of computer vision research greatly depends on the availability of large datasets, which are critical for effectively training and testing new methods. Manually annotating visual data, however, is not only a labor-intensive process but also prone to errors. In this study, we present NOVA, a versatile framework to create realistic-looking 3D rendered worlds containing procedurally generated humans with rich pixel-level ground truth annotations. NOVA can simulate various environmental factors such as weather conditions or different times of day, and bring an exceptionally diverse set of humans to life, each having a distinct body shape, gender and age. To demonstrate NOVA’s capabilities, we generate two synthetic datasets for person tracking. The first one includes 108 sequences, each with different levels of difficulty like tracking in crowded scenes or at nighttime and aims for testing the limits of current state-of-the-art trackers. A second dataset of 97 sequences with normal weather conditions is used to show how our synthetic sequences can be utilized to train and boost the performance of deep-learning based trackers. Our results indicate that the synthetic data generated by NOVA represents a good proxy of the real-world and can be exploited for computer vision tasks.","Predicting where humans look in images has gained significant popularity in recent years. In this work, we present a novel method for learning top-down visual saliency, which is well-suited to locate objects of interest in complex scenes. During training, we jointly learn a superpixel based class-specific dictionary and a Conditional Random Field (CRF). While using such a discriminative dictionary helps to distinguish target objects from the background, performing the computations at the superpixel level allows us to improve accuracy of object localizations. Experimental results on the Graz-02 and PASCAL VOC 2007 datasets show that the proposed approach is able to achieve stateof-the-art results and provides much better saliency maps.",0
"Today, the cutting edge of computer vision research greatly depends on the availability of large datasets, which are critical for effectively training and testing new methods. Manually annotating visual data, however, is not only a labor-intensive process but also prone to errors. In this study, we present NOVA, a versatile framework to create realistic-looking 3D rendered worlds containing procedurally generated humans with rich pixel-level ground truth annotations. NOVA can simulate various environmental factors such as weather conditions or different times of day, and bring an exceptionally diverse set of humans to life, each having a distinct body shape, gender and age. To demonstrate NOVA’s capabilities, we generate two synthetic datasets for person tracking. The first one includes 108 sequences, each with different levels of difficulty like tracking in crowded scenes or at nighttime and aims for testing the limits of current state-of-the-art trackers. A second dataset of 97 sequences with normal weather conditions is used to show how our synthetic sequences can be utilized to train and boost the performance of deep-learning based trackers. Our results indicate that the synthetic data generated by NOVA represents a good proxy of the real-world and can be exploited for computer vision tasks.","Передбачення, куди дивляться люди на зображеннях, набуло значної популярності в останні роки. У цій роботі ми представляємо новий метод вивчення візуальної помітності зверху вниз, який добре підходить для визначення місцезнаходження цікавих об’єктів у складних сценах. Під час навчання ми спільно вивчаємо словник класу на основі суперпікселів і умовне випадкове поле (CRF). Хоча використання такого розрізнювального словника допомагає відрізнити цільові об’єкти від фону, виконання обчислень на рівні суперпікселя дозволяє підвищити точність локалізації об’єктів. Експериментальні результати на наборах даних Graz-02 і PASCAL VOC 2007 показують, що запропонований підхід здатний досягти найсучасніших результатів і забезпечує набагато кращі карти помітності.",0
"Сьогодні передовий край досліджень комп’ютерного зору значною мірою залежить від наявності великих наборів даних, які є критично важливими для ефективного навчання та тестування нових методів. Однак анотування візуальних даних вручну є не лише трудомістким процесом, але й схильним до помилок. У цьому дослідженні ми представляємо NOVA, універсальну структуру для створення реалістично виглядаючих тривимірних світів, що містять процедурно згенерованих людей із багатими анотаціями правдивості на рівні пікселів. NOVA може імітувати різноманітні фактори навколишнього середовища, такі як погодні умови чи різний час доби, і оживляти винятково різноманітну групу людей, кожна з яких має певну форму тіла, стать і вік. Щоб продемонструвати можливості NOVA, ми створили два синтетичних набори даних для відстеження людей. Перший включає 108 послідовностей, кожна з яких має різний рівень складності, як-от відстеження в людних сценах або вночі, і спрямована на перевірку меж поточних найсучасніших трекерів. Другий набір даних із 97 послідовностей із нормальними погодними умовами використовується, щоб показати, як наші синтетичні послідовності можна використовувати для навчання та підвищення продуктивності трекерів на основі глибокого навчання. Наші результати вказують на те, що синтетичні дані, згенеровані NOVA, є хорошим проксі реального світу та можуть використовуватися для завдань комп’ютерного зору.","Predicting where humans look in images has gained significant popularity in recent years. In this work, we present a novel method for learning top-down visual saliency, which is well-suited to locate objects of interest in complex scenes. During training, we jointly learn a superpixel based class-specific dictionary and a Conditional Random Field (CRF). While using such a discriminative dictionary helps to distinguish target objects from the background, performing the computations at the superpixel level allows us to improve accuracy of object localizations. Experimental results on the Graz-02 and PASCAL VOC 2007 datasets show that the proposed approach is able to achieve stateof-the-art results and provides much better saliency maps.",0
"Сьогодні передовий край досліджень комп’ютерного зору значною мірою залежить від наявності великих наборів даних, які є критично важливими для ефективного навчання та тестування нових методів. Однак анотування візуальних даних вручну є не лише трудомістким процесом, але й схильним до помилок. У цьому дослідженні ми представляємо NOVA, універсальну структуру для створення реалістично виглядаючих тривимірних світів, що містять процедурно згенерованих людей із багатими анотаціями правдивості на рівні пікселів. NOVA може імітувати різноманітні фактори навколишнього середовища, такі як погодні умови чи різний час доби, і оживляти винятково різноманітну групу людей, кожна з яких має певну форму тіла, стать і вік. Щоб продемонструвати можливості NOVA, ми створили два синтетичних набори даних для відстеження людей. Перший включає 108 послідовностей, кожна з яких має різний рівень складності, як-от відстеження в людних сценах або вночі, і спрямована на перевірку меж поточних найсучасніших трекерів. Другий набір даних із 97 послідовностей із нормальними погодними умовами використовується, щоб показати, як наші синтетичні послідовності можна використовувати для навчання та підвищення продуктивності трекерів на основі глибокого навчання. Наші результати вказують на те, що синтетичні дані, згенеровані NOVA, є хорошим проксі реального світу та можуть використовуватися для завдань комп’ютерного зору.","Передбачення, куди дивляться люди на зображеннях, набуло значної популярності в останні роки. У цій роботі ми представляємо новий метод вивчення візуальної помітності зверху вниз, який добре підходить для визначення місцезнаходження цікавих об’єктів у складних сценах. Під час навчання ми спільно вивчаємо словник класу на основі суперпікселів і умовне випадкове поле (CRF). Хоча використання такого розрізнювального словника допомагає відрізнити цільові об’єкти від фону, виконання обчислень на рівні суперпікселя дозволяє підвищити точність локалізації об’єктів. Експериментальні результати на наборах даних Graz-02 і PASCAL VOC 2007 показують, що запропонований підхід здатний досягти найсучасніших результатів і забезпечує набагато кращі карти помітності.",0
"Virtual and augmented reality (VR/AR) systems dramatically gained in popularity with various application areas such as gaming, social media, and communication. It is therefore a crucial task to have the knowhow to efficiently utilize, store or deliver 360◦ videos for end-users. Towards this aim, researchers have been developing deep neural network models for 360◦ multimedia processing and computer vision fields. In this line of work, an important research direction is to build models that can learn and predict the observers’ attention on 360◦ videos to obtain so-called saliency maps computationally. Although there are a few saliency models proposed for this purpose, these models generally consider only visual cues in video frames by neglecting audio cues from sound sources. In this study, an unsupervised frequency-based saliency model is presented for predicting the strength and location of saliency in spatial audio. The prediction of salient audio cues is then used as audio bias on the video saliency predictions of state-of-the-art models. Our experiments yield promising results and show that integrating the proposed spatial audio bias into the existing video saliency models consistently improves their performance. ","Abstract  To detect visually salient elements of complex natural scenes, computational bottom-up saliency models commonly examine several feature channels such as color and orientation in parallel. They compute a separate feature map for each channel and then linearly combine these maps to produce a master saliency map. However, only a few studies have investigated how different feature dimensions contribute to the overall visual saliency. We address this integration issue and propose to use covariance matrices of simple image features (known as region covariance descriptors in the computer vision community; Tuzel, Porikli, & Meer, 2006) as meta-features for saliency estimation. As low-dimensional representations of image patches, region covariances capture local image structures better than standard linear filters, but more importantly, they naturally provide nonlinear integration of different features by modeling their correlations. We also show that first-order statistics of features could be easily incorporated to the proposed approach to improve the performance. Our experimental evaluation on several benchmark data sets demonstrate that the proposed approach outperforms the state-of-art models on various tasks including prediction of human eye fixations, salient object detection, and image-retargeting.",0
"Virtual and augmented reality (VR/AR) systems dramatically gained in popularity with various application areas such as gaming, social media, and communication. It is therefore a crucial task to have the knowhow to efficiently utilize, store or deliver 360◦ videos for end-users. Towards this aim, researchers have been developing deep neural network models for 360◦ multimedia processing and computer vision fields. In this line of work, an important research direction is to build models that can learn and predict the observers’ attention on 360◦ videos to obtain so-called saliency maps computationally. Although there are a few saliency models proposed for this purpose, these models generally consider only visual cues in video frames by neglecting audio cues from sound sources. In this study, an unsupervised frequency-based saliency model is presented for predicting the strength and location of saliency in spatial audio. The prediction of salient audio cues is then used as audio bias on the video saliency predictions of state-of-the-art models. Our experiments yield promising results and show that integrating the proposed spatial audio bias into the existing video saliency models consistently improves their performance. ","Анотація Щоб виявити візуально помітні елементи складних природних сцен, обчислювальні моделі помітності знизу вгору зазвичай досліджують кілька каналів ознак, таких як колір і орієнтація паралельно. Вони обчислюють окрему карту функцій для кожного каналу, а потім лінійно об’єднують ці карти, щоб отримати головну карту помітності. Однак лише кілька досліджень досліджували, як різні розміри функцій впливають на загальну візуальну помітність. Ми розглядаємо цю проблему інтеграції та пропонуємо використовувати коваріаційні матриці простих характеристик зображення (відомі як дескриптори коваріації регіонів у спільноті комп’ютерного зору; Tuzel, Porikli та Meer, 2006) як мета-ознаки для оцінки помітності. Будучи низьковимірними представленнями фрагментів зображення, коваріації регіонів фіксують локальні структури зображення краще, ніж стандартні лінійні фільтри, але, що більш важливо, вони природним чином забезпечують нелінійну інтеграцію різних функцій шляхом моделювання їхніх кореляцій. Ми також показуємо, що статистика функцій першого порядку може бути легко включена в запропонований підхід для покращення продуктивності. Наша експериментальна оцінка кількох еталонних наборів даних демонструє, що запропонований підхід перевершує сучасні моделі для різних завдань, включаючи прогнозування фіксації людського ока, виявлення помітних об’єктів і перенацілювання зображень.",0
"Системи віртуальної та доповненої реальності (VR/AR) різко набули популярності в різних сферах застосування, таких як ігри, соціальні мережі та спілкування. Тому дуже важливо мати знання, як ефективно використовувати, зберігати або доставляти 360◦ відео для кінцевих користувачів. З цією метою дослідники розробляють моделі глибокої нейронної мережі для 360◦ обробки мультимедіа та полів комп’ютерного зору. У цьому напрямі роботи важливим напрямком досліджень є побудова моделей, які можуть вивчати та передбачати увагу спостерігачів на 360◦ відео, щоб отримати так звані карти помітності обчислювальним шляхом. Хоча для цієї мети запропоновано декілька моделей помітності, ці моделі зазвичай розглядають лише візуальні підказки у відеокадрах, нехтуючи звуковими підказками від джерел звуку. У цьому дослідженні представлена неконтрольована частотна модель помітності для прогнозування сили та розташування помітності в просторовому аудіо. Прогноз помітних аудіосигналів потім використовується як зміщення аудіо для прогнозів помітності відео найсучасніших моделей. Наші експерименти дають багатообіцяючі результати та показують, що інтеграція запропонованого просторового звукового зміщення в існуючі моделі помітності відео постійно покращує їх продуктивність.","Abstract  To detect visually salient elements of complex natural scenes, computational bottom-up saliency models commonly examine several feature channels such as color and orientation in parallel. They compute a separate feature map for each channel and then linearly combine these maps to produce a master saliency map. However, only a few studies have investigated how different feature dimensions contribute to the overall visual saliency. We address this integration issue and propose to use covariance matrices of simple image features (known as region covariance descriptors in the computer vision community; Tuzel, Porikli, & Meer, 2006) as meta-features for saliency estimation. As low-dimensional representations of image patches, region covariances capture local image structures better than standard linear filters, but more importantly, they naturally provide nonlinear integration of different features by modeling their correlations. We also show that first-order statistics of features could be easily incorporated to the proposed approach to improve the performance. Our experimental evaluation on several benchmark data sets demonstrate that the proposed approach outperforms the state-of-art models on various tasks including prediction of human eye fixations, salient object detection, and image-retargeting.",0
"Системи віртуальної та доповненої реальності (VR/AR) різко набули популярності в різних сферах застосування, таких як ігри, соціальні мережі та спілкування. Тому дуже важливо мати знання, як ефективно використовувати, зберігати або доставляти 360◦ відео для кінцевих користувачів. З цією метою дослідники розробляють моделі глибокої нейронної мережі для 360◦ обробки мультимедіа та полів комп’ютерного зору. У цьому напрямі роботи важливим напрямком досліджень є побудова моделей, які можуть вивчати та передбачати увагу спостерігачів на 360◦ відео, щоб отримати так звані карти помітності обчислювальним шляхом. Хоча для цієї мети запропоновано декілька моделей помітності, ці моделі зазвичай розглядають лише візуальні підказки у відеокадрах, нехтуючи звуковими підказками від джерел звуку. У цьому дослідженні представлена неконтрольована частотна модель помітності для прогнозування сили та розташування помітності в просторовому аудіо. Прогноз помітних аудіосигналів потім використовується як зміщення аудіо для прогнозів помітності відео найсучасніших моделей. Наші експерименти дають багатообіцяючі результати та показують, що інтеграція запропонованого просторового звукового зміщення в існуючі моделі помітності відео постійно покращує їх продуктивність.","Анотація Щоб виявити візуально помітні елементи складних природних сцен, обчислювальні моделі помітності знизу вгору зазвичай досліджують кілька каналів ознак, таких як колір і орієнтація паралельно. Вони обчислюють окрему карту функцій для кожного каналу, а потім лінійно об’єднують ці карти, щоб отримати головну карту помітності. Однак лише кілька досліджень досліджували, як різні розміри функцій впливають на загальну візуальну помітність. Ми розглядаємо цю проблему інтеграції та пропонуємо використовувати коваріаційні матриці простих характеристик зображення (відомі як дескриптори коваріації регіонів у спільноті комп’ютерного зору; Tuzel, Porikli та Meer, 2006) як мета-ознаки для оцінки помітності. Будучи низьковимірними представленнями фрагментів зображення, коваріації регіонів фіксують локальні структури зображення краще, ніж стандартні лінійні фільтри, але, що більш важливо, вони природним чином забезпечують нелінійну інтеграцію різних функцій шляхом моделювання їхніх кореляцій. Ми також показуємо, що статистика функцій першого порядку може бути легко включена в запропонований підхід для покращення продуктивності. Наша експериментальна оцінка кількох еталонних наборів даних демонструє, що запропонований підхід перевершує сучасні моделі для різних завдань, включаючи прогнозування фіксації людського ока, виявлення помітних об’єктів і перенацілювання зображень.",0
"Predicting saliency in videos is a challenging problem due to complex modeling of interactions between spatial and temporal information, especially when ever-changing, dynamic nature of videos is considered. Recently, researchers have proposed large-scale data sets and models that take advantage of deep learning as a way to understand what is important for video saliency. These approaches, however, learn to combine spatial and temporal features in a static manner and do not adapt themselves much to the changes in the video content. In this article, we introduce the gated fusion network for dynamic saliency (GFSalNet), the first deep saliency model capable of making predictions in a dynamic way via the gated fusion mechanism. Moreover, our model also exploits spatial and channelwise attention within a multiscale architecture that further allows for highly accurate predictions. We evaluate the proposed approach on a number of data sets, and our experimental analysis demonstrates that it outperforms or is highly competitive with the state of the art. Importantly, we show that it has a good generalization ability, and moreover, exploits temporal information more effectively via its adaptive fusion scheme.","Moving object detection is an imperative task in computer vision, where it is primarily used for surveillance applications. With the increasing availability of low-altitude aerial vehicles, new challenges for moving object detection have surfaced, both for academia and industry. In this paper, we propose a new approach that can detect moving objects efficiently and handle parallax cases. By introducing sparse flow based parallax handling and downscale processing, we push the boundaries of real-time performance with 16 FPS on limited embedded resources (a five-fold improvement over existing baselines), while managing to perform comparably or even improve the state-of-the-art in two different datasets. We also present a roadmap for extending our approach to exploit multi-modal data in order to mitigate the need for parameter tuning.",0
"Predicting saliency in videos is a challenging problem due to complex modeling of interactions between spatial and temporal information, especially when ever-changing, dynamic nature of videos is considered. Recently, researchers have proposed large-scale data sets and models that take advantage of deep learning as a way to understand what is important for video saliency. These approaches, however, learn to combine spatial and temporal features in a static manner and do not adapt themselves much to the changes in the video content. In this article, we introduce the gated fusion network for dynamic saliency (GFSalNet), the first deep saliency model capable of making predictions in a dynamic way via the gated fusion mechanism. Moreover, our model also exploits spatial and channelwise attention within a multiscale architecture that further allows for highly accurate predictions. We evaluate the proposed approach on a number of data sets, and our experimental analysis demonstrates that it outperforms or is highly competitive with the state of the art. Importantly, we show that it has a good generalization ability, and moreover, exploits temporal information more effectively via its adaptive fusion scheme.","Виявлення рухомих об’єктів є обов’язковим завданням комп’ютерного зору, де воно в основному використовується для програм спостереження. Зі збільшенням доступності літальних апаратів на низькій висоті виникли нові виклики для виявлення рухомих об’єктів як для наукових кіл, так і для промисловості. У цій статті ми пропонуємо новий підхід, який може ефективно виявляти рухомі об’єкти та обробляти випадки паралакса. Запроваджуючи обробку паралакса на основі розрідженого потоку та обробку з низьким масштабом, ми розширюємо межі продуктивності в реальному часі з 16 кадрами в секунду на обмежених вбудованих ресурсах (п’ятикратне покращення порівняно з існуючими базовими показниками), водночас зуміючи досягти порівнянної продуктивності або навіть покращити стан -the-art у двох різних наборах даних. Ми також представляємо дорожню карту для розширення нашого підходу до використання мультимодальних даних, щоб зменшити потребу в налаштуванні параметрів.",0
"Прогнозування помітності у відео є складною проблемою через складне моделювання взаємодії між просторовою та часовою інформацією, особливо коли враховується постійна мінлива динамічна природа відео. Нещодавно дослідники запропонували масштабні набори даних і моделі, які використовують переваги глибокого навчання як спосіб зрозуміти, що важливо для помітності відео. Ці підходи, однак, вчаться поєднувати просторові та часові характеристики статичним чином і не дуже адаптуються до змін у відеоконтенті. У цій статті ми представляємо мережу закритого синтезу для динамічної помітності (GFSalNet), першу модель глибокої помітності, здатну робити прогнози динамічним способом за допомогою механізму закритого синтезу. Крім того, наша модель також використовує просторову та канальну увагу в рамках багатомасштабної архітектури, що додатково дозволяє робити високоточні прогнози. Ми оцінюємо запропонований підхід на низці наборів даних, і наш експериментальний аналіз показує, що він перевершує сучасні технології або є дуже конкурентоспроможним. Важливо те, що ми показуємо, що він має хорошу здатність до узагальнення та, крім того, ефективніше використовує часову інформацію за допомогою адаптивної схеми синтезу.","Moving object detection is an imperative task in computer vision, where it is primarily used for surveillance applications. With the increasing availability of low-altitude aerial vehicles, new challenges for moving object detection have surfaced, both for academia and industry. In this paper, we propose a new approach that can detect moving objects efficiently and handle parallax cases. By introducing sparse flow based parallax handling and downscale processing, we push the boundaries of real-time performance with 16 FPS on limited embedded resources (a five-fold improvement over existing baselines), while managing to perform comparably or even improve the state-of-the-art in two different datasets. We also present a roadmap for extending our approach to exploit multi-modal data in order to mitigate the need for parameter tuning.",0
"Прогнозування помітності у відео є складною проблемою через складне моделювання взаємодій між просторовою та часовою інформацією, особливо коли враховується постійна мінлива динамічна природа відео. Нещодавно дослідники запропонували масштабні набори даних і моделі, які використовують переваги глибокого навчання як спосіб зрозуміти, що важливо для помітності відео. Ці підходи, однак, вчаться поєднувати просторові та часові характеристики статичним чином і не дуже адаптуються до змін у відеоконтенті. У цій статті ми представляємо мережу закритого синтезу для динамічної помітності (GFSalNet), першу модель глибокої помітності, здатну робити прогнози динамічним способом за допомогою механізму закритого синтезу. Крім того, наша модель також використовує просторову та канальну увагу в рамках багатомасштабної архітектури, що додатково дозволяє робити високоточні прогнози. Ми оцінюємо запропонований підхід на низці наборів даних, і наш експериментальний аналіз показує, що він перевершує сучасні технології або є дуже конкурентоспроможним. Важливо те, що ми показуємо, що він має хорошу здатність до узагальнення та, крім того, ефективніше використовує часову інформацію за допомогою адаптивної схеми синтезу.","Виявлення рухомих об&#39;єктів є обов&#39;язковим завданням комп&#39;ютерного зору, де воно в основному використовується для програм спостереження. Зі збільшенням доступності літальних апаратів на низькій висоті виникли нові виклики для виявлення рухомих об’єктів як для наукових кіл, так і для промисловості. У цій статті ми пропонуємо новий підхід, який може ефективно виявляти рухомі об’єкти та обробляти випадки паралакса. Запроваджуючи обробку паралакса на основі розрідженого потоку та обробку з низьким масштабом, ми розширюємо межі продуктивності в реальному часі з 16 кадрами в секунду на обмежених вбудованих ресурсах (п’ятикратне покращення порівняно з існуючими базовими показниками), водночас зуміючи досягти порівнянної продуктивності або навіть покращити стан -the-art у двох різних наборах даних. Ми також представляємо дорожню карту для розширення нашого підходу до використання мультимодальних даних, щоб зменшити потребу в налаштуванні параметрів.",0
"Robust visual tracking plays a vital role in many areas such as autonomous cars, surveillance and robotics. Recent trackers were shown to achieve adequate results under normal tracking scenarios with clear weather conditions, standard camera setups and lighting conditions. Yet, the performance of these trackers, whether they are corre- lation filter-based or learning-based, degrade under adverse weather conditions. The lack of videos with such weather conditions, in the available visual object tracking datasets, is the prime issue behind the low perfor- mance of the learning-based tracking algorithms. In this work, we provide a new person tracking dataset of real-world sequences (PTAW172Real) captured under foggy, rainy and snowy weather conditions to assess the performance of the current trackers. We also introduce a novel person tracking dataset of synthetic sequences (PTAW217Synth) procedurally generated by our NOVA framework spanning the same weather conditions in varying severity to mitigate the problem of data scarcity. Our experimental results demonstrate that the perfor- mances of the state-of-the-art deep trackers under adverse weather conditions can be boosted when the avail- able real training sequences are complemented with our synthetically generated dataset during training.","Recent years have witnessed the emergence of new image smoothing techniques which have provided new insights and raised new questions about the nature of this well-studied problem. Specifically, these models separate a given image into its structure and texture layers by utilizing non-gradient based definitions for edges or special measures that distinguish edges from oscillations. In this study, we propose an alternative yet simple image smoothing approach which depends on covariance matrices of simple image features, aka the region covariances. The use of second order statistics as a patch descriptor allows us to implicitly capture local structure and texture information and makes our approach particularly effective for structure extraction from texture. Our experimental results have shown that the proposed approach leads to better image decompositions as compared to the state-of-the-art methods and preserves prominent edges and shading well. Moreover, we also demonstrate the applicability of our approach on some image editing and manipulation tasks such as image abstraction, texture and detail enhancement, image composition, inverse halftoning and seam carving.",0
"Robust visual tracking plays a vital role in many areas such as autonomous cars, surveillance and robotics. Recent trackers were shown to achieve adequate results under normal tracking scenarios with clear weather conditions, standard camera setups and lighting conditions. Yet, the performance of these trackers, whether they are corre- lation filter-based or learning-based, degrade under adverse weather conditions. The lack of videos with such weather conditions, in the available visual object tracking datasets, is the prime issue behind the low perfor- mance of the learning-based tracking algorithms. In this work, we provide a new person tracking dataset of real-world sequences (PTAW172Real) captured under foggy, rainy and snowy weather conditions to assess the performance of the current trackers. We also introduce a novel person tracking dataset of synthetic sequences (PTAW217Synth) procedurally generated by our NOVA framework spanning the same weather conditions in varying severity to mitigate the problem of data scarcity. Our experimental results demonstrate that the perfor- mances of the state-of-the-art deep trackers under adverse weather conditions can be boosted when the avail- able real training sequences are complemented with our synthetically generated dataset during training.","Останні роки стали свідками появи нових методів згладжування зображень, які дали нові знання та поставили нові питання щодо природи цієї добре вивченої проблеми. Зокрема, ці моделі поділяють дане зображення на шари структури та текстури, використовуючи неградієнтні визначення для країв або спеціальні заходи, які відрізняють краї від коливань. У цьому дослідженні ми пропонуємо альтернативний, але простий підхід до згладжування зображення, який залежить від коваріаційних матриць простих характеристик зображення, також відомих як коваріації регіонів. Використання статистики другого порядку як дескриптора патча дозволяє нам неявно отримувати інформацію про локальну структуру та текстуру та робить наш підхід особливо ефективним для вилучення структури з текстури. Результати наших експериментів показали, що запропонований підхід забезпечує кращу декомпозицію зображення порівняно з сучасними методами та добре зберігає помітні краї та затінення. Крім того, ми також демонструємо застосовність нашого підходу до деяких завдань редагування та маніпулювання зображеннями, таких як абстракція зображення, покращення текстури та деталей, композиція зображення, інверсне півтонування та різьблення швів.",0
"Надійне візуальне відстеження відіграє життєво важливу роль у багатьох сферах, таких як автономні автомобілі, спостереження та робототехніка. Показано, що останні трекери досягають відповідних результатів за звичайних сценаріїв відстеження за ясних погодних умов, стандартних налаштувань камери та умов освітлення. Тим не менш, продуктивність цих трекерів, незалежно від того, засновані вони на кореляційному фільтрі чи на основі навчання, погіршується за несприятливих погодних умов. Відсутність відео з такими погодними умовами в доступних наборах даних візуального відстеження об’єктів є основною проблемою низької продуктивності алгоритмів відстеження на основі навчання. У цій роботі ми надаємо новий набір даних відстеження людей у реальному світі (PTAW172Real), знятих за умов туманної, дощової та снігової погоди, щоб оцінити ефективність поточних трекерів. Ми також представляємо новий набір даних відстеження людей із синтетичних послідовностей (PTAW217Synth), процедурно згенерованих нашою структурою NOVA, що охоплює однакові погодні умови різного ступеня тяжкості, щоб пом’якшити проблему дефіциту даних. Наші експериментальні результати демонструють, що продуктивність найсучасніших глибинних трекерів за несприятливих погодних умов може бути покращена, якщо доступні реальні навчальні послідовності доповнюються нашим синтетично згенерованим набором даних під час навчання.","Recent years have witnessed the emergence of new image smoothing techniques which have provided new insights and raised new questions about the nature of this well-studied problem. Specifically, these models separate a given image into its structure and texture layers by utilizing non-gradient based definitions for edges or special measures that distinguish edges from oscillations. In this study, we propose an alternative yet simple image smoothing approach which depends on covariance matrices of simple image features, aka the region covariances. The use of second order statistics as a patch descriptor allows us to implicitly capture local structure and texture information and makes our approach particularly effective for structure extraction from texture. Our experimental results have shown that the proposed approach leads to better image decompositions as compared to the state-of-the-art methods and preserves prominent edges and shading well. Moreover, we also demonstrate the applicability of our approach on some image editing and manipulation tasks such as image abstraction, texture and detail enhancement, image composition, inverse halftoning and seam carving.",0
"Надійне візуальне відстеження відіграє життєво важливу роль у багатьох сферах, таких як автономні автомобілі, спостереження та робототехніка. Показано, що останні трекери досягають відповідних результатів за звичайних сценаріїв відстеження за ясних погодних умов, стандартних налаштувань камери та умов освітлення. Тим не менш, продуктивність цих трекерів, незалежно від того, засновані вони на кореляційному фільтрі чи на основі навчання, погіршується за несприятливих погодних умов. Відсутність відео з такими погодними умовами в доступних наборах даних візуального відстеження об’єктів є основною проблемою низької продуктивності алгоритмів відстеження на основі навчання. У цій роботі ми надаємо новий набір даних відстеження людей у реальному світі (PTAW172Real), знятих за умов туманної, дощової та снігової погоди, щоб оцінити ефективність поточних трекерів. Ми також представляємо новий набір даних відстеження людей із синтетичних послідовностей (PTAW217Synth), процедурно згенерованих нашою структурою NOVA, що охоплює однакові погодні умови різного ступеня тяжкості, щоб пом’якшити проблему дефіциту даних. Наші експериментальні результати демонструють, що продуктивність найсучасніших глибинних трекерів за несприятливих погодних умов може бути покращена, якщо доступні реальні навчальні послідовності доповнюються нашим синтетично згенерованим набором даних під час навчання.","Останні роки стали свідками появи нових методів згладжування зображень, які дали нове розуміння та поставили нові питання щодо природи цієї добре вивченої проблеми. Зокрема, ці моделі поділяють дане зображення на шари структури та текстури, використовуючи неградієнтні визначення для країв або спеціальні заходи, які відрізняють краї від коливань. У цьому дослідженні ми пропонуємо альтернативний, але простий підхід до згладжування зображення, який залежить від коваріаційних матриць простих характеристик зображення, також відомих як коваріації регіонів. Використання статистики другого порядку як дескриптора патча дозволяє нам неявно отримувати інформацію про локальну структуру та текстуру та робить наш підхід особливо ефективним для вилучення структури з текстури. Результати наших експериментів показали, що запропонований підхід забезпечує кращу декомпозицію зображення порівняно з сучасними методами та добре зберігає помітні краї та затінення. Крім того, ми також демонструємо застосовність нашого підходу до деяких завдань редагування та маніпулювання зображеннями, таких як абстракція зображення, покращення текстури та деталей, композиція зображення, зворотне півтонування та різьблення швів.",0
"Automatic generation of video descriptions in natural language, also called video captioning, aims to understand the visual content of the video and produce a natural language sentence depicting the objects and actions in the scene. This challenging integrated vision and language problem, however, has been predominantly addressed for English. The lack of data and the linguistic properties of other languages limit the success of existing approaches for such languages. In this paper we target Turkish, a morphologically rich and agglutinative language that has very different properties compared to English. To do so, we create the frst large-scale video captioning dataset for this language by carefully translating the English descriptions of the videos in the MSVD (Microsoft Research Video Description Corpus) dataset into Turkish. In addition to enabling research in video captioning in Turkish, the parallel English–Turkish descriptions also enable the study of the role of video context in (multimodal) machine translation. In our experiments, we build models for both video captioning and multimodal machine translation and investigate the efect of diferent word segmentation approaches and diferent neural architectures to better address the properties of Turkish. We hope that the MSVD-Turkish dataset and the results reported in this work will lead to better video captioning and multimodal machine translation models for Turkish and other morphology rich and agglutinative languages.","Virtual and augmented reality (VR/AR) systems dramatically gained in popularity with various application areas such as gaming, social media, and communication. It is therefore a crucial task to have the knowhow to efficiently utilize, store or deliver 360◦ videos for end-users. Towards this aim, researchers have been developing deep neural network models for 360◦ multimedia processing and computer vision fields. In this line of work, an important research direction is to build models that can learn and predict the observers’ attention on 360◦ videos to obtain so-called saliency maps computationally. Although there are a few saliency models proposed for this purpose, these models generally consider only visual cues in video frames by neglecting audio cues from sound sources. In this study, an unsupervised frequency-based saliency model is presented for predicting the strength and location of saliency in spatial audio. The prediction of salient audio cues is then used as audio bias on the video saliency predictions of state-of-the-art models. Our experiments yield promising results and show that integrating the proposed spatial audio bias into the existing video saliency models consistently improves their performance. ",0
"Automatic generation of video descriptions in natural language, also called video captioning, aims to understand the visual content of the video and produce a natural language sentence depicting the objects and actions in the scene. This challenging integrated vision and language problem, however, has been predominantly addressed for English. The lack of data and the linguistic properties of other languages limit the success of existing approaches for such languages. In this paper we target Turkish, a morphologically rich and agglutinative language that has very different properties compared to English. To do so, we create the frst large-scale video captioning dataset for this language by carefully translating the English descriptions of the videos in the MSVD (Microsoft Research Video Description Corpus) dataset into Turkish. In addition to enabling research in video captioning in Turkish, the parallel English–Turkish descriptions also enable the study of the role of video context in (multimodal) machine translation. In our experiments, we build models for both video captioning and multimodal machine translation and investigate the efect of diferent word segmentation approaches and diferent neural architectures to better address the properties of Turkish. We hope that the MSVD-Turkish dataset and the results reported in this work will lead to better video captioning and multimodal machine translation models for Turkish and other morphology rich and agglutinative languages.","Системи віртуальної та доповненої реальності (VR/AR) різко набули популярності в різних сферах застосування, таких як ігри, соціальні мережі та спілкування. Тому дуже важливо мати знання, як ефективно використовувати, зберігати або доставляти 360◦ відео для кінцевих користувачів. З цією метою дослідники розробляють моделі глибокої нейронної мережі для 360◦ обробки мультимедіа та полів комп’ютерного зору. У цьому напрямі роботи важливим напрямком досліджень є побудова моделей, які можуть вивчати та передбачати увагу спостерігачів на 360◦ відео, щоб отримати так звані карти помітності обчислювальним шляхом. Хоча для цієї мети запропоновано декілька моделей помітності, ці моделі зазвичай розглядають лише візуальні підказки у відеокадрах, нехтуючи звуковими підказками від джерел звуку. У цьому дослідженні представлена неконтрольована частотна модель помітності для прогнозування сили та розташування помітності в просторовому аудіо. Прогноз помітних аудіосигналів потім використовується як зміщення аудіо для прогнозів помітності відео найсучасніших моделей. Наші експерименти дають багатообіцяючі результати та показують, що інтеграція запропонованого просторового звукового зміщення в існуючі моделі помітності відео постійно покращує їх продуктивність.",0
"Автоматична генерація відеоописів природною мовою, яка також називається відеотитрами, має на меті зрозуміти візуальний вміст відео та створити речення природною мовою, що зображує об’єкти та дії в сцені. Ця складна інтегрована проблема бачення та мови, однак, була переважно розглянута для англійської мови. Брак даних і лінгвістичні властивості інших мов обмежують успіх існуючих підходів для таких мов. У цій статті ми орієнтуємося на турецьку, морфологічно багату та аглютинативну мову, яка має дуже різні властивості порівняно з англійською. Для цього ми створюємо перший великомасштабний набір даних субтитрів до відео для цієї мови, ретельно перекладаючи англійські описи відео в наборі даних MSVD (Microsoft Research Video Description Corpus) на турецьку мову. Окрім того, що вони дозволяють досліджувати субтитри до відео турецькою мовою, паралельні англо-турецькі описи також дозволяють вивчати роль відеоконтексту в (мультимодальному) машинному перекладі. У наших експериментах ми створюємо моделі як для субтитрів до відео, так і для мультимодального машинного перекладу та досліджуємо вплив різних підходів до сегментації слів і різних нейронних архітектур для кращого врахування властивостей турецької мови. Ми сподіваємося, що набір даних MSVD-Turkish і результати, наведені в цій роботі, призведуть до кращих відеосубтитрів і мультимодальних моделей машинного перекладу для турецької та інших морфологічних і аглютинативних мов.","Virtual and augmented reality (VR/AR) systems dramatically gained in popularity with various application areas such as gaming, social media, and communication. It is therefore a crucial task to have the knowhow to efficiently utilize, store or deliver 360◦ videos for end-users. Towards this aim, researchers have been developing deep neural network models for 360◦ multimedia processing and computer vision fields. In this line of work, an important research direction is to build models that can learn and predict the observers’ attention on 360◦ videos to obtain so-called saliency maps computationally. Although there are a few saliency models proposed for this purpose, these models generally consider only visual cues in video frames by neglecting audio cues from sound sources. In this study, an unsupervised frequency-based saliency model is presented for predicting the strength and location of saliency in spatial audio. The prediction of salient audio cues is then used as audio bias on the video saliency predictions of state-of-the-art models. Our experiments yield promising results and show that integrating the proposed spatial audio bias into the existing video saliency models consistently improves their performance. ",0
"Автоматична генерація відеоописів природною мовою, яка також називається відеотитрами, має на меті зрозуміти візуальний вміст відео та створити речення природною мовою, що зображує об’єкти та дії в сцені. Ця складна інтегрована проблема бачення та мови, однак, була переважно розглянута для англійської мови. Відсутність даних і лінгвістичні властивості інших мов обмежують успіх існуючих підходів для таких мов. У цій статті ми орієнтуємося на турецьку, морфологічно багату та аглютинативну мову, яка має дуже різні властивості порівняно з англійською. Для цього ми створюємо перший великомасштабний набір даних субтитрів до відео для цієї мови, ретельно перекладаючи англійські описи відео в наборі даних MSVD (Microsoft Research Video Description Corpus) на турецьку мову. Окрім того, що вони дозволяють досліджувати субтитри до відео турецькою мовою, паралельні англо-турецькі описи також дозволяють вивчати роль відеоконтексту в (мультимодальному) машинному перекладі. У наших експериментах ми створюємо моделі як для субтитрів до відео, так і для мультимодального машинного перекладу та досліджуємо вплив різних підходів до сегментації слів і різних нейронних архітектур для кращого врахування властивостей турецької мови. Ми сподіваємося, що набір даних MSVD-Turkish і результати, наведені в цій роботі, призведуть до кращих відеосубтитрів і мультимодальних моделей машинного перекладу для турецької та інших морфологічних і аглютинативних мов.","Системи віртуальної та доповненої реальності (VR/AR) різко набули популярності в різних сферах застосування, таких як ігри, соціальні мережі та спілкування. Тому дуже важливо мати знання, як ефективно використовувати, зберігати або доставляти 360◦ відео для кінцевих користувачів. З цією метою дослідники розробляють моделі глибокої нейронної мережі для 360◦ обробки мультимедіа та полів комп’ютерного зору. У цьому напрямі роботи важливим напрямком досліджень є побудова моделей, які можуть вивчати та передбачати увагу спостерігачів на 360◦ відео, щоб отримати так звані карти помітності обчислювальним шляхом. Хоча для цієї мети запропоновано декілька моделей помітності, ці моделі зазвичай розглядають лише візуальні підказки у відеокадрах, нехтуючи звуковими підказками від джерел звуку. У цьому дослідженні представлена неконтрольована частотна модель помітності для прогнозування сили та розташування помітності в просторовому аудіо. Прогноз помітних аудіосигналів потім використовується як зміщення аудіо для прогнозів помітності відео найсучасніших моделей. Наші експерименти дають багатообіцяючі результати та показують, що інтеграція запропонованого просторового звукового зміщення в існуючі моделі помітності відео постійно покращує їх продуктивність.",0
"Collecting textual descriptions is an especially costly task for dense video captioning, since each event in the video needs to be annotated separately and a long descriptive paragraph needs to be provided. In this paper, we investigate a way to mitigate this heavy burden and propose to leverage captions of visually similar images as auxiliary context. Our model successfully fetches visually relevant images and combines noun and verb phrases from their captions to generating coherent descriptions. To this end, we use a generator and discriminator design, together with an attention-based fusion technique, to incorporate image captions as context in the video caption generation process. The experiments on the challenging ActivityNet Captions dataset demonstrate that our proposed approach achieves more accurate and more diverse video descriptions compared to the strong baseline using METEOR, BLEU and CIDEr-D metrics and qualitative evaluations.","In recent years, graph neural networks have been successfully applied for learning the dynamics of complex and partially observable physical systems. However, their use in the robotics domain is, to date, still limited. In this paper, we introduce Belief Regulated Dual Propagation Networks (BRDPN), a general-purpose learnable physics engine, which enables a robot to predict the effects of its actions in scenes containing groups of articulated multi-part objects. Specifically, our framework extends recently proposed propagation networks (PropNets) and consists of two complementary components, a physics predictor and a belief regulator. While the former predicts the future states of the object(s) manipulated by the robot, the latter constantly corrects the robot’s knowledge regarding the objects and their relations. Our results showed that after training in a simulator, the robot can reliably predict the consequences of its actions in object trajectory level and exploit its own interaction experience to correct its belief about the state of the environment, enabling better predictions in partially observable environments. Furthermore, the trained model was transferred to the real world and verified in predicting trajectories of pushed interacting objects whose joint relations were initially unknown. We compared BRDPN against PropNets, and showed that BRDPN performs consistently well. Moreover, BRDPN can adapt its physic predictions, since the relations can be predicted online. ",0
"Collecting textual descriptions is an especially costly task for dense video captioning, since each event in the video needs to be annotated separately and a long descriptive paragraph needs to be provided. In this paper, we investigate a way to mitigate this heavy burden and propose to leverage captions of visually similar images as auxiliary context. Our model successfully fetches visually relevant images and combines noun and verb phrases from their captions to generating coherent descriptions. To this end, we use a generator and discriminator design, together with an attention-based fusion technique, to incorporate image captions as context in the video caption generation process. The experiments on the challenging ActivityNet Captions dataset demonstrate that our proposed approach achieves more accurate and more diverse video descriptions compared to the strong baseline using METEOR, BLEU and CIDEr-D metrics and qualitative evaluations.","В останні роки графові нейронні мережі успішно застосовуються для вивчення динаміки складних і частково спостережуваних фізичних систем. Однак їх використання в області робототехніки на сьогоднішній день все ще обмежене. У цьому документі ми представляємо мережі подвійного розповсюдження, регульовані переконаннями (BRDPN), універсальний навчальний фізичний механізм, який дозволяє роботу передбачати наслідки його дій у сценах, що містять групи шарнірних багатокомпонентних об’єктів. Зокрема, наша структура розширює нещодавно запропоновані мережі розповсюдження (PropNets) і складається з двох додаткових компонентів, фізичного предиктора та регулятора віри. Тоді як перший передбачає майбутні стани об’єктів, якими маніпулює робот, другий постійно коригує знання робота щодо об’єктів та їхніх зв’язків. Наші результати показали, що після навчання на симуляторі робот може надійно передбачати наслідки своїх дій на рівні траєкторії об’єкта та використовувати власний досвід взаємодії, щоб виправити свої переконання щодо стану навколишнього середовища, забезпечуючи кращі прогнози в частково доступних для спостереження середовищах. Крім того, навчена модель була перенесена в реальний світ і перевірена у прогнозуванні траєкторій штовхаючих взаємодіючих об’єктів, спільні зв’язки яких спочатку були невідомі. Ми порівняли BRDPN із PropNets і показали, що BRDPN стабільно добре працює. Крім того, BRDPN може адаптувати свої фізичні прогнози, оскільки співвідношення можна передбачити онлайн.",0
"Збір текстових описів є особливо дорогим завданням для щільних субтитрів до відео, оскільки кожна подія у відео має бути анотована окремо та має бути наданий довгий абзац з описом. У цій статті ми досліджуємо спосіб полегшити цей важкий тягар і пропонуємо використовувати підписи до візуально подібних зображень як допоміжний контекст. Наша модель успішно вибирає візуально релевантні зображення та комбінує фрази іменників і дієслів із їхніх підписів для створення зв’язних описів. З цією метою ми використовуємо дизайн генератора та дискримінатора разом із технікою злиття на основі уваги, щоб включити підписи до зображень як контекст у процес генерації субтитрів до відео. Експерименти зі складним набором даних ActivityNet Captions демонструють, що запропонований нами підхід забезпечує більш точні та різноманітні описи відео порівняно з сильним базовим сценарієм із використанням показників METEOR, BLEU і CIDer-D і якісних оцінок.","In recent years, graph neural networks have been successfully applied for learning the dynamics of complex and partially observable physical systems. However, their use in the robotics domain is, to date, still limited. In this paper, we introduce Belief Regulated Dual Propagation Networks (BRDPN), a general-purpose learnable physics engine, which enables a robot to predict the effects of its actions in scenes containing groups of articulated multi-part objects. Specifically, our framework extends recently proposed propagation networks (PropNets) and consists of two complementary components, a physics predictor and a belief regulator. While the former predicts the future states of the object(s) manipulated by the robot, the latter constantly corrects the robot’s knowledge regarding the objects and their relations. Our results showed that after training in a simulator, the robot can reliably predict the consequences of its actions in object trajectory level and exploit its own interaction experience to correct its belief about the state of the environment, enabling better predictions in partially observable environments. Furthermore, the trained model was transferred to the real world and verified in predicting trajectories of pushed interacting objects whose joint relations were initially unknown. We compared BRDPN against PropNets, and showed that BRDPN performs consistently well. Moreover, BRDPN can adapt its physic predictions, since the relations can be predicted online. ",0
"Збір текстових описів є особливо дорогим завданням для щільних субтитрів до відео, оскільки кожна подія у відео має бути анотована окремо та має бути наданий довгий абзац з описом. У цій статті ми досліджуємо спосіб полегшити цей важкий тягар і пропонуємо використовувати підписи до візуально подібних зображень як допоміжний контекст. Наша модель успішно вибирає візуально релевантні зображення та комбінує фрази іменників і дієслів із їхніх підписів для створення зв’язних описів. З цією метою ми використовуємо дизайн генератора та дискримінатора разом із технікою злиття на основі уваги, щоб включити підписи до зображень як контекст у процес генерації субтитрів до відео. Експерименти зі складним набором даних ActivityNet Captions демонструють, що запропонований нами підхід забезпечує більш точні та різноманітні описи відео порівняно з сильним базовим сценарієм із використанням показників METEOR, BLEU і CIDer-D і якісних оцінок.","В останні роки графові нейронні мережі успішно застосовуються для вивчення динаміки складних і частково спостережуваних фізичних систем. Однак їх використання в області робототехніки на сьогоднішній день все ще обмежене. У цьому документі ми представляємо мережі подвійного розповсюдження, регульовані переконаннями (BRDPN), універсальний навчальний фізичний механізм, який дозволяє роботу передбачати наслідки його дій у сценах, що містять групи шарнірних багатокомпонентних об’єктів. Зокрема, наша структура розширює нещодавно запропоновані мережі розповсюдження (PropNets) і складається з двох додаткових компонентів, фізичного предиктора та регулятора віри. Тоді як перший передбачає майбутні стани об’єктів, якими маніпулює робот, другий постійно коригує знання робота щодо об’єктів та їхніх зв’язків. Наші результати показали, що після навчання на симуляторі робот може надійно передбачати наслідки своїх дій на рівні траєкторії об’єкта та використовувати власний досвід взаємодії, щоб виправити свої переконання щодо стану навколишнього середовища, забезпечуючи кращі прогнози в частково доступних для спостереження середовищах. Крім того, навчена модель була перенесена в реальний світ і перевірена у прогнозуванні траєкторій штовхаючих взаємодіючих об’єктів, спільні зв’язки яких спочатку були невідомі. Ми порівняли BRDPN із PropNets і показали, що BRDPN стабільно добре працює. Крім того, BRDPN може адаптувати свої фізичні прогнози, оскільки співвідношення можна передбачити онлайн.",0
"Multi-contrast MRI protocols increase the level of morphological information available for diagnosis. Yet, the number and quality of contrasts are limited in practice by various factors including scan time and patient motion. Synthesis of missing or corrupted contrasts from other high-quality ones can alleviate this limitation. When a single target contrast is of interest, common approaches for multi-contrast MRI involve either one-to-one or many-to-one synthesis methods depending on their input. One-to-one meth- ods take as input a single source contrast, and they learn a latent representation sensitive to unique fea- tures of the source. Meanwhile, many-to-one methods receive multiple distinct sources, and they learn a shared latent representation more sensitive to common features across sources. For enhanced image synthesis, we propose a multi-stream approach that aggregates information across multiple source im- ages via a mixture of multiple one-to-one streams and a joint many-to-one stream. The complementary feature maps generated in the one-to-one streams and the shared feature maps generated in the many- to-one stream are combined with a fusion block. The location of the fusion block is adaptively modified to maximize task-specific performance. Quantitative and radiological assessments on T1 ,- T2-, PD-weighted, and FLAIR images clearly demonstrate the superior performance of the proposed method compared to previous state-of-the-art one-to-one and many-to-one methods.","With the growing interest in computational models of visual attention, saliency prediction has become an important research topic in computer vision. Over the past years, many different successful saliency models have been proposed especially for image saliency prediction. However, these models generally do not consider the dynamic nature of the scenes, and hence, they work better on static images. To date, there has been relatively little work on dynamic saliency that deals with predicting where humans look at videos. In addition, previous studies showed that how the feature integration is carried out is very crucial for more accurate results. Yet, many dynamic saliency models follow a similar simple design and extract separate spatial and temporal saliency maps which are then integrated together to obtain the final saliency map. In this paper, we present a comparative study for different feature integration strategies in dynamic saliency estimation. We employ a number of low and high-level visual features such as static saliency, motion, faces, humans and text, some of which have not been previously used in dynamic saliency estimation. In order to explore the strength of feature integration strategies, we investigate four learning-based (SVM, Gradient Boosting, NNLS, Random Forest) and two transformation- based (Mean, Max) fusion methods, resulting in six new dynamic saliency models. Our experimental analysis on two different dynamic saliency benchmark datasets reveal that our models achieve better performance than the individual features. In addition, our learning-based models outperform the state-of-the-art dynamic saliency models.",0
"Multi-contrast MRI protocols increase the level of morphological information available for diagnosis. Yet, the number and quality of contrasts are limited in practice by various factors including scan time and patient motion. Synthesis of missing or corrupted contrasts from other high-quality ones can alleviate this limitation. When a single target contrast is of interest, common approaches for multi-contrast MRI involve either one-to-one or many-to-one synthesis methods depending on their input. One-to-one meth- ods take as input a single source contrast, and they learn a latent representation sensitive to unique fea- tures of the source. Meanwhile, many-to-one methods receive multiple distinct sources, and they learn a shared latent representation more sensitive to common features across sources. For enhanced image synthesis, we propose a multi-stream approach that aggregates information across multiple source im- ages via a mixture of multiple one-to-one streams and a joint many-to-one stream. The complementary feature maps generated in the one-to-one streams and the shared feature maps generated in the many- to-one stream are combined with a fusion block. The location of the fusion block is adaptively modified to maximize task-specific performance. Quantitative and radiological assessments on T1 ,- T2-, PD-weighted, and FLAIR images clearly demonstrate the superior performance of the proposed method compared to previous state-of-the-art one-to-one and many-to-one methods.","Зі зростанням інтересу до обчислювальних моделей зорової уваги прогнозування помітності стало важливою темою дослідження комп’ютерного зору. За останні роки було запропоновано багато різних успішних моделей помітності, особливо для прогнозування помітності зображення. Однак ці моделі зазвичай не враховують динамічну природу сцен, а отже, вони краще працюють на статичних зображеннях. На сьогоднішній день було проведено відносно мало робіт щодо динамічної помітності, яка стосується передбачення того, де люди дивляться відео. Крім того, попередні дослідження показали, що для більш точних результатів дуже важливо те, як здійснюється інтеграція функцій. Тим не менш, багато динамічних моделей помітності дотримуються схожої простої конструкції та виділяють окремі просторові та часові карти помітності, які потім інтегруються разом, щоб отримати остаточну карту помітності. У цій статті ми представляємо порівняльне дослідження для різних стратегій інтеграції функцій у динамічній оцінці помітності. Ми використовуємо ряд візуальних функцій низького та високого рівня, таких як статична помітність, рух, обличчя, люди та текст, деякі з яких раніше не використовувалися в динамічній оцінці помітності. Щоб дослідити силу стратегій інтеграції функцій, ми досліджуємо чотири методи злиття на основі навчання (SVM, Gradient Boosting, NNLS, Random Forest) і два на основі трансформації (Mean, Max), у результаті чого створено шість нових динамічних моделей помітності. Наш експериментальний аналіз двох різних наборів даних динамічного порівняння показує, що наші моделі досягають кращої продуктивності, ніж окремі функції. Крім того, наші моделі, засновані на навчанні, перевершують найсучасніші моделі динамічної помітності.",0
"Мультиконтрастні протоколи МРТ підвищують рівень доступної для діагностики морфологічної інформації. Проте на практиці кількість і якість контрастів обмежена різними факторами, включаючи час сканування та рух пацієнта. Синтез відсутніх або пошкоджених контрастів з інших високоякісних може пом’якшити це обмеження. Коли цікавить одне цільове контрастування, загальні підходи до багатоконтрастної МРТ включають методи синтезу «один до одного» або «багато до одного» залежно від їх вхідних даних. Методи «один-до-одного» приймають як вхідні дані контраст одного джерела, і вони вивчають приховане представлення, чутливе до унікальних особливостей джерела. Тим часом методи «багато-до-одного» отримують кілька різних джерел, і вони вивчають спільне приховане представлення, більш чутливе до загальних функцій у джерелах. Для покращеного синтезу зображень ми пропонуємо багатопотоковий підхід, який агрегує інформацію з кількох вихідних зображень за допомогою суміші кількох потоків один до одного та спільного потоку багато до одного. Додаткові карти функцій, згенеровані в потоках «один-до-одного», і спільні карти функцій, згенеровані в потоці «багато-до-одного», поєднуються з блоком злиття. Розташування блоку злиття адаптивно змінюється, щоб максимально підвищити продуктивність для конкретного завдання. Кількісні та радіологічні оцінки T1, T2-, PD-зважених зображень і зображень FLAIR чітко демонструють чудову продуктивність запропонованого методу порівняно з попередніми сучасними методами один до одного та багато до одного.","With the growing interest in computational models of visual attention, saliency prediction has become an important research topic in computer vision. Over the past years, many different successful saliency models have been proposed especially for image saliency prediction. However, these models generally do not consider the dynamic nature of the scenes, and hence, they work better on static images. To date, there has been relatively little work on dynamic saliency that deals with predicting where humans look at videos. In addition, previous studies showed that how the feature integration is carried out is very crucial for more accurate results. Yet, many dynamic saliency models follow a similar simple design and extract separate spatial and temporal saliency maps which are then integrated together to obtain the final saliency map. In this paper, we present a comparative study for different feature integration strategies in dynamic saliency estimation. We employ a number of low and high-level visual features such as static saliency, motion, faces, humans and text, some of which have not been previously used in dynamic saliency estimation. In order to explore the strength of feature integration strategies, we investigate four learning-based (SVM, Gradient Boosting, NNLS, Random Forest) and two transformation- based (Mean, Max) fusion methods, resulting in six new dynamic saliency models. Our experimental analysis on two different dynamic saliency benchmark datasets reveal that our models achieve better performance than the individual features. In addition, our learning-based models outperform the state-of-the-art dynamic saliency models.",0
"Мультиконтрастні протоколи МРТ підвищують рівень доступної для діагностики морфологічної інформації. Проте на практиці кількість і якість контрастів обмежена різними факторами, включаючи час сканування та рух пацієнта. Синтез відсутніх або пошкоджених контрастів з інших високоякісних може пом’якшити це обмеження. Коли цікавить одне цільове контрастування, загальні підходи до багатоконтрастної МРТ включають методи синтезу «один до одного» або «багато до одного» залежно від їх вхідних даних. Методи «один-до-одного» приймають як вхідні дані контраст одного джерела, і вони вивчають приховане представлення, чутливе до унікальних особливостей джерела. Тим часом методи «багато-до-одного» отримують кілька різних джерел, і вони вивчають спільне приховане представлення, більш чутливе до загальних функцій у джерелах. Для покращеного синтезу зображень ми пропонуємо багатопотоковий підхід, який агрегує інформацію з кількох вихідних зображень за допомогою суміші кількох потоків один до одного та спільного потоку багато до одного. Додаткові карти функцій, згенеровані в потоках «один-до-одного», і спільні карти функцій, згенеровані в потоці «багато-до-одного», поєднуються з блоком злиття. Розташування блоку злиття адаптивно змінюється, щоб максимально підвищити продуктивність для конкретного завдання. Кількісні та радіологічні оцінки T1, T2-, PD-зважених зображень і зображень FLAIR чітко демонструють чудову продуктивність запропонованого методу порівняно з попередніми сучасними методами один до одного та багато до одного.","Зі зростанням інтересу до обчислювальних моделей зорової уваги прогнозування помітності стало важливою темою дослідження комп’ютерного зору. За останні роки було запропоновано багато різних успішних моделей помітності, особливо для прогнозування помітності зображення. Однак ці моделі зазвичай не враховують динамічну природу сцен, а отже, вони краще працюють на статичних зображеннях. На сьогоднішній день було проведено відносно мало робіт щодо динамічної помітності, яка стосується передбачення того, де люди дивляться відео. Крім того, попередні дослідження показали, що для більш точних результатів дуже важливо, як здійснюється інтеграція функцій. Тим не менш, багато динамічних моделей помітності дотримуються схожої простої конструкції та виділяють окремі просторові та часові карти помітності, які потім об’єднують разом, щоб отримати остаточну карту помітності. У цій статті ми представляємо порівняльне дослідження для різних стратегій інтеграції функцій у динамічній оцінці помітності. Ми використовуємо ряд візуальних функцій низького та високого рівня, таких як статична помітність, рух, обличчя, люди та текст, деякі з яких раніше не використовувалися в динамічній оцінці помітності. Щоб дослідити силу стратегій інтеграції функцій, ми досліджуємо чотири методи злиття на основі навчання (SVM, Gradient Boosting, NNLS, Random Forest) і два на основі трансформації (Mean, Max), у результаті чого створено шість нових динамічних моделей помітності. Наш експериментальний аналіз двох різних наборів даних динамічного порівняння показує, що наші моделі досягають кращої продуктивності, ніж окремі функції. Крім того, наші моделі, засновані на навчанні, перевершують найсучасніші моделі динамічної помітності.",0
"Pushing is an essential non-prehensile manipulation skill used for tasks ranging from pre-grasp manipulation to scene rearrangement, reasoning about object relations in the scene, and thus pushing actions have been widely studied in robotics. The effective use of pushing actions often requires an understanding of the dynamics of the manipulated objects and adaptation to the discrepancies between prediction and reality. For this reason, effect prediction and parameter estimation with pushing actions have been heavily investigated in the literature. However, current approaches are limited because they either model systems with a fixed number of objects or use image-based representations whose outputs are not very interpretable and quickly accumulate errors. In this paper, we propose a graph neural network based framework for effect prediction and parameter estimation of pushing actions by modeling object relations based on contacts or articulations. Our framework is validated both in real and simulated environments containing different shaped multi-part objects connected via different types of joints and objects with different masses. Our approach enables the robot to predict and adapt the effect of a pushing action as it observes the scene. Further, we demonstrate 6D effect prediction in the lever-up action in the context of robot-based hard-disk disassembly","Motion is an important cue for video prediction and often utilized by separating video content into static and dynamic components. Most of the previous work utilizing motion is deterministic but there are stochastic methods that can model the inherent uncertainty of the future. Existing stochastic models either do not reason about motion explicitly or make limiting assumptions about the static part. In this paper, we reason about appearance and motion in the video stochastically by predicting the future based on the motion history. Explicit reasoning about motion without history already reaches the performance of current stochastic models. The motion history further improves the results by allowing to predict consistent dynamics several frames into the future. Our model performs comparably to the stateof-the-art models on the generic video prediction datasets, however, significantly outperforms them on two challenging real-world autonomous driving datasets with complex motion and dynamic background.",0
"Pushing is an essential non-prehensile manipulation skill used for tasks ranging from pre-grasp manipulation to scene rearrangement, reasoning about object relations in the scene, and thus pushing actions have been widely studied in robotics. The effective use of pushing actions often requires an understanding of the dynamics of the manipulated objects and adaptation to the discrepancies between prediction and reality. For this reason, effect prediction and parameter estimation with pushing actions have been heavily investigated in the literature. However, current approaches are limited because they either model systems with a fixed number of objects or use image-based representations whose outputs are not very interpretable and quickly accumulate errors. In this paper, we propose a graph neural network based framework for effect prediction and parameter estimation of pushing actions by modeling object relations based on contacts or articulations. Our framework is validated both in real and simulated environments containing different shaped multi-part objects connected via different types of joints and objects with different masses. Our approach enables the robot to predict and adapt the effect of a pushing action as it observes the scene. Further, we demonstrate 6D effect prediction in the lever-up action in the context of robot-based hard-disk disassembly","Рух є важливою підказкою для передбачення відео, і часто використовується для поділу відеовмісту на статичні та динамічні компоненти. Більшість попередніх робіт із використанням руху є детермінованими, але існують стохастичні методи, які можуть моделювати притаманну невизначеність майбутнього. Існуючі стохастичні моделі або не міркують про рух явно, або роблять обмежувальні припущення щодо статичної частини. У цій статті ми розглядаємо зовнішній вигляд і рух у відео стохастично, передбачаючи майбутнє на основі історії руху. Явне міркування про рух без історії вже досягає продуктивності поточних стохастичних моделей. Історія руху ще більше покращує результати, дозволяючи передбачити узгоджену динаміку на кілька кадрів у майбутньому. Наша модель працює порівнянно з найсучаснішими моделями на загальних наборах даних прогнозування відео, однак значно перевершує їх на двох складних реальних наборах даних автономного водіння зі складним рухом і динамічним фоном.",0
"Поштовх — це важлива навичка маніпуляції, яка не хватає, і використовується для виконання завдань, починаючи від маніпуляцій перед захопленням до перестановки сцени, міркувань про об’єктні відносини в сцені, і, отже, дії поштовху широко вивчаються в робототехніці. Ефективне використання штовхаючих дій часто вимагає розуміння динаміки об&#39;єктів, якими маніпулюють, і адаптації до розбіжностей між прогнозом і реальністю. З цієї причини прогнозування ефекту та оцінка параметрів за допомогою виштовхувальних дій були ретельно досліджені в літературі. Однак поточні підходи обмежені, оскільки вони або моделюють системи з фіксованою кількістю об’єктів, або використовують представлення на основі зображень, результати яких не дуже добре інтерпретуються та швидко накопичують помилки. У цій статті ми пропонуємо структуру на основі графової нейронної мережі для прогнозування ефекту та оцінки параметрів штовхаючих дій шляхом моделювання об’єктних відносин на основі контактів або артикуляцій. Наша структура перевірена як у реальному, так і в симульованому середовищі, що містить багатокомпонентні об’єкти різної форми, з’єднані за допомогою різних типів з’єднань, і об’єкти з різною масою. Наш підхід дозволяє роботу передбачити та адаптувати ефект штовхання, коли він спостерігає за сценою. Крім того, ми демонструємо прогнозування 6D-ефекту під час дії важеля вгору в контексті роботизованого розбирання жорсткого диска","Motion is an important cue for video prediction and often utilized by separating video content into static and dynamic components. Most of the previous work utilizing motion is deterministic but there are stochastic methods that can model the inherent uncertainty of the future. Existing stochastic models either do not reason about motion explicitly or make limiting assumptions about the static part. In this paper, we reason about appearance and motion in the video stochastically by predicting the future based on the motion history. Explicit reasoning about motion without history already reaches the performance of current stochastic models. The motion history further improves the results by allowing to predict consistent dynamics several frames into the future. Our model performs comparably to the stateof-the-art models on the generic video prediction datasets, however, significantly outperforms them on two challenging real-world autonomous driving datasets with complex motion and dynamic background.",0
"Поштовх — це важлива навичка маніпуляції, яка не хватає, і використовується для виконання завдань, починаючи від маніпуляцій перед захопленням до перестановки сцени, міркувань про об’єктні відносини в сцені, і, отже, дії поштовху широко вивчаються в робототехніці. Ефективне використання штовхаючих дій часто вимагає розуміння динаміки об&#39;єктів, якими маніпулюють, і адаптації до розбіжностей між прогнозом і реальністю. З цієї причини прогнозування ефекту та оцінка параметрів за допомогою виштовхувальних дій були ретельно досліджені в літературі. Однак поточні підходи обмежені, оскільки вони або моделюють системи з фіксованою кількістю об’єктів, або використовують представлення на основі зображень, результати яких не дуже добре інтерпретуються та швидко накопичують помилки. У цій статті ми пропонуємо структуру на основі графової нейронної мережі для прогнозування ефекту та оцінки параметрів штовхаючих дій шляхом моделювання об’єктних відносин на основі контактів або артикуляцій. Наша структура перевірена як у реальному, так і в симульованому середовищі, що містить багатокомпонентні об’єкти різної форми, з’єднані за допомогою різних типів з’єднань, і об’єкти з різною масою. Наш підхід дозволяє роботу передбачити та адаптувати ефект штовхання, коли він спостерігає за сценою. Крім того, ми демонструємо прогнозування 6D-ефекту під час дії важеля вгору в контексті роботизованого розбирання жорсткого диска","Рух є важливою підказкою для передбачення відео, і часто використовується для поділу відеовмісту на статичні та динамічні компоненти. Більшість попередніх робіт із використанням руху є детермінованими, але існують стохастичні методи, які можуть моделювати притаманну невизначеність майбутнього. Існуючі стохастичні моделі або не міркують про рух явно, або роблять обмежувальні припущення щодо статичної частини. У цій статті ми розглядаємо зовнішній вигляд і рух у відео стохастично, передбачаючи майбутнє на основі історії руху. Явне міркування про рух без історії вже досягає продуктивності поточних стохастичних моделей. Історія руху ще більше покращує результати, дозволяючи передбачити узгоджену динаміку на кілька кадрів у майбутньому. Наша модель працює порівнянно з найсучаснішими моделями на загальних наборах даних прогнозування відео, однак значно перевершує їх на двох складних реальних наборах даних автономного водіння зі складним рухом і динамічним фоном.",0
"Pre-trained language models have been shown to improve performance in many natural language tasks substantially. Although the early focus of such models was single language pre-training, recent advances have resulted in cross-lingual and visual pre-training methods. In this paper, we combine these two approaches to learn visually-grounded crosslingual representations. Specifically, we extend the translation language modelling (Lample and Conneau, 2019) with masked region classification and perform pre-training with three-way parallel vision & language corpora. We show that when fine-tuned for multimodal machine translation, these models obtain stateof-the-art performance. We also provide qualitative insights into the usefulness of the learned grounded representations.","Today, the cutting edge of computer vision research greatly depends on the availability of large datasets, which are critical for effectively training and testing new methods. Manually annotating visual data, however, is not only a labor-intensive process but also prone to errors. In this study, we present NOVA, a versatile framework to create realistic-looking 3D rendered worlds containing procedurally generated humans with rich pixel-level ground truth annotations. NOVA can simulate various environmental factors such as weather conditions or different times of day, and bring an exceptionally diverse set of humans to life, each having a distinct body shape, gender and age. To demonstrate NOVA’s capabilities, we generate two synthetic datasets for person tracking. The first one includes 108 sequences, each with different levels of difficulty like tracking in crowded scenes or at nighttime and aims for testing the limits of current state-of-the-art trackers. A second dataset of 97 sequences with normal weather conditions is used to show how our synthetic sequences can be utilized to train and boost the performance of deep-learning based trackers. Our results indicate that the synthetic data generated by NOVA represents a good proxy of the real-world and can be exploited for computer vision tasks.",0
"Pre-trained language models have been shown to improve performance in many natural language tasks substantially. Although the early focus of such models was single language pre-training, recent advances have resulted in cross-lingual and visual pre-training methods. In this paper, we combine these two approaches to learn visually-grounded crosslingual representations. Specifically, we extend the translation language modelling (Lample and Conneau, 2019) with masked region classification and perform pre-training with three-way parallel vision & language corpora. We show that when fine-tuned for multimodal machine translation, these models obtain stateof-the-art performance. We also provide qualitative insights into the usefulness of the learned grounded representations.","Сьогодні передовий край досліджень комп’ютерного зору значною мірою залежить від наявності великих наборів даних, які є критично важливими для ефективного навчання та тестування нових методів. Однак анотування візуальних даних вручну є не лише трудомістким процесом, але й схильним до помилок. У цьому дослідженні ми представляємо NOVA, універсальну структуру для створення реалістично виглядаючих тривимірних світів, що містять процедурно згенерованих людей із багатими анотаціями правдивості на рівні пікселів. NOVA може імітувати різноманітні фактори навколишнього середовища, такі як погодні умови чи різний час доби, і оживляти винятково різноманітну групу людей, кожна з яких має певну форму тіла, стать і вік. Щоб продемонструвати можливості NOVA, ми створили два синтетичних набори даних для відстеження людей. Перший включає 108 послідовностей, кожна з яких має різний рівень складності, як-от відстеження в людних сценах або вночі, і спрямована на перевірку меж поточних найсучасніших трекерів. Другий набір даних із 97 послідовностей із нормальними погодними умовами використовується, щоб показати, як наші синтетичні послідовності можна використовувати для навчання та підвищення продуктивності трекерів на основі глибокого навчання. Наші результати вказують на те, що синтетичні дані, згенеровані NOVA, є хорошим проксі реального світу та можуть використовуватися для завдань комп’ютерного зору.",0
"Було показано, що попередньо підготовлені мовні моделі суттєво покращують продуктивність у багатьох завданнях природної мови. Незважаючи на те, що на ранніх етапах такі моделі зосереджувалися на одній мовній підготовці, останні досягнення призвели до міжмовних і візуальних методів попередньої підготовки. У цій статті ми поєднуємо ці два підходи, щоб вивчити візуально обґрунтовані міжмовні репрезентації. Зокрема, ми розширюємо моделювання мови перекладу (Lample and Conneau, 2019) за допомогою класифікації замаскованих регіонів і виконуємо попереднє навчання за допомогою тристороннього паралельного бачення та мовних корпусів. Ми показуємо, що після точного налаштування для мультимодального машинного перекладу ці моделі отримують найсучаснішу продуктивність. Ми також надаємо якісне розуміння корисності вивчених обґрунтованих уявлень.","Today, the cutting edge of computer vision research greatly depends on the availability of large datasets, which are critical for effectively training and testing new methods. Manually annotating visual data, however, is not only a labor-intensive process but also prone to errors. In this study, we present NOVA, a versatile framework to create realistic-looking 3D rendered worlds containing procedurally generated humans with rich pixel-level ground truth annotations. NOVA can simulate various environmental factors such as weather conditions or different times of day, and bring an exceptionally diverse set of humans to life, each having a distinct body shape, gender and age. To demonstrate NOVA’s capabilities, we generate two synthetic datasets for person tracking. The first one includes 108 sequences, each with different levels of difficulty like tracking in crowded scenes or at nighttime and aims for testing the limits of current state-of-the-art trackers. A second dataset of 97 sequences with normal weather conditions is used to show how our synthetic sequences can be utilized to train and boost the performance of deep-learning based trackers. Our results indicate that the synthetic data generated by NOVA represents a good proxy of the real-world and can be exploited for computer vision tasks.",0
"Було показано, що попередньо підготовлені мовні моделі суттєво покращують продуктивність у багатьох завданнях природної мови. Незважаючи на те, що на ранніх етапах такі моделі зосереджувалися на одній мовній підготовці, останні досягнення призвели до міжмовних і візуальних методів попередньої підготовки. У цій статті ми поєднуємо ці два підходи, щоб вивчити візуально обґрунтовані міжмовні репрезентації. Зокрема, ми розширюємо моделювання мови перекладу (Lample and Conneau, 2019) за допомогою класифікації замаскованих регіонів і проводимо попереднє навчання за допомогою тристороннього паралельного бачення та мовних корпусів. Ми показуємо, що після точного налаштування для мультимодального машинного перекладу ці моделі отримують найсучаснішу продуктивність. Ми також надаємо якісне розуміння корисності вивчених обґрунтованих уявлень.","Сьогодні передовий край досліджень комп’ютерного зору значною мірою залежить від наявності великих наборів даних, які є критично важливими для ефективного навчання та тестування нових методів. Однак анотування візуальних даних вручну є не лише трудомістким процесом, але й схильним до помилок. У цьому дослідженні ми представляємо NOVA, універсальну структуру для створення реалістично виглядаючих тривимірних світів, що містять процедурно згенерованих людей із багатими анотаціями правдивості на рівні пікселів. NOVA може імітувати різноманітні фактори навколишнього середовища, такі як погодні умови чи різний час доби, і оживляти винятково різноманітну групу людей, кожна з яких має певну форму тіла, стать і вік. Щоб продемонструвати можливості NOVA, ми створили два синтетичних набори даних для відстеження людей. Перший включає 108 послідовностей, кожна з яких має різний рівень складності, як-от відстеження в людних сценах або вночі, і спрямована на перевірку меж поточних найсучасніших трекерів. Другий набір даних із 97 послідовностей із нормальними погодними умовами використовується, щоб показати, як наші синтетичні послідовності можна використовувати для навчання та підвищення продуктивності трекерів на основі глибокого навчання. Наші результати вказують на те, що синтетичні дані, згенеровані NOVA, є хорошим проксі реального світу та можуть використовуватися для завдань комп’ютерного зору.",0
"Motion is an important cue for video prediction and often utilized by separating video content into static and dynamic components. Most of the previous work utilizing motion is deterministic but there are stochastic methods that can model the inherent uncertainty of the future. Existing stochastic models either do not reason about motion explicitly or make limiting assumptions about the static part. In this paper, we reason about appearance and motion in the video stochastically by predicting the future based on the motion history. Explicit reasoning about motion without history already reaches the performance of current stochastic models. The motion history further improves the results by allowing to predict consistent dynamics several frames into the future. Our model performs comparably to the stateof-the-art models on the generic video prediction datasets, however, significantly outperforms them on two challenging real-world autonomous driving datasets with complex motion and dynamic background.","Many researchers argue that fusing multiple cues increases the reliability and robustness of visual tracking. However, how the multi-cue integration is realized during tracking is still an open issue. In this work, we present a novel data fusion approach for multi-cue tracking using particle filter. Our method differs from previous approaches in a number of ways. First, we carry out the integration of cues both in making predictions about the target object and in verifying them through observations. Our second and more significant contribution is that both stages of integration directly depend on the dynamically changing reliabilities of visual cues. These two aspects of our method allow the tracker to easily adapt itself to the changes in the context, and accordingly improve the tracking accuracy by resolving the ambiguities",0
"Motion is an important cue for video prediction and often utilized by separating video content into static and dynamic components. Most of the previous work utilizing motion is deterministic but there are stochastic methods that can model the inherent uncertainty of the future. Existing stochastic models either do not reason about motion explicitly or make limiting assumptions about the static part. In this paper, we reason about appearance and motion in the video stochastically by predicting the future based on the motion history. Explicit reasoning about motion without history already reaches the performance of current stochastic models. The motion history further improves the results by allowing to predict consistent dynamics several frames into the future. Our model performs comparably to the stateof-the-art models on the generic video prediction datasets, however, significantly outperforms them on two challenging real-world autonomous driving datasets with complex motion and dynamic background.","Багато дослідників стверджують, що об’єднання кількох підказок підвищує надійність і стійкість візуального відстеження. Проте те, як реалізується інтеграція кількох підказок під час відстеження, залишається відкритим питанням. У цій роботі ми представляємо новий підхід об’єднання даних для відстеження кількох сигналів за допомогою фільтра частинок. Наш метод відрізняється від попередніх підходів кількома параметрами. По-перше, ми здійснюємо інтеграцію підказок як для створення прогнозів щодо цільового об’єкта, так і для їх перевірки за допомогою спостережень. Наш другий і більш значний внесок полягає в тому, що обидва етапи інтеграції безпосередньо залежать від надійності візуальних підказок, що динамічно змінюється. Ці два аспекти нашого методу дозволяють трекеру легко адаптуватися до змін у контексті та, відповідно, підвищити точність відстеження шляхом вирішення неоднозначностей.",0
"Рух є важливою підказкою для передбачення відео, і часто використовується для поділу відеовмісту на статичні та динамічні компоненти. Більшість попередніх робіт із використанням руху є детермінованими, але існують стохастичні методи, які можуть моделювати притаманну невизначеність майбутнього. Існуючі стохастичні моделі або не міркують про рух явно, або роблять обмежувальні припущення щодо статичної частини. У цій статті ми розглядаємо зовнішній вигляд і рух у відео стохастично, передбачаючи майбутнє на основі історії руху. Явне міркування про рух без історії вже досягає продуктивності поточних стохастичних моделей. Історія руху ще більше покращує результати, дозволяючи передбачити послідовну динаміку на кілька кадрів у майбутньому. Наша модель працює порівнянно з найсучаснішими моделями на загальних наборах даних прогнозування відео, однак значно перевершує їх на двох складних реальних наборах даних автономного водіння зі складним рухом і динамічним фоном.","Many researchers argue that fusing multiple cues increases the reliability and robustness of visual tracking. However, how the multi-cue integration is realized during tracking is still an open issue. In this work, we present a novel data fusion approach for multi-cue tracking using particle filter. Our method differs from previous approaches in a number of ways. First, we carry out the integration of cues both in making predictions about the target object and in verifying them through observations. Our second and more significant contribution is that both stages of integration directly depend on the dynamically changing reliabilities of visual cues. These two aspects of our method allow the tracker to easily adapt itself to the changes in the context, and accordingly improve the tracking accuracy by resolving the ambiguities",0
"Рух є важливою підказкою для передбачення відео, і часто використовується для поділу відеовмісту на статичні та динамічні компоненти. Більшість попередніх робіт із використанням руху є детермінованими, але існують стохастичні методи, які можуть моделювати притаманну невизначеність майбутнього. Існуючі стохастичні моделі або не міркують про рух явно, або роблять обмежувальні припущення щодо статичної частини. У цій статті ми розглядаємо зовнішній вигляд і рух у відео стохастично, передбачаючи майбутнє на основі історії руху. Явне міркування про рух без історії вже досягає продуктивності поточних стохастичних моделей. Історія руху ще більше покращує результати, дозволяючи передбачити послідовну динаміку на кілька кадрів у майбутньому. Наша модель працює порівнянно з найсучаснішими моделями на загальних наборах даних прогнозування відео, однак значно перевершує їх на двох складних реальних наборах даних автономного водіння зі складним рухом і динамічним фоном.","Багато дослідників стверджують, що об’єднання кількох підказок підвищує надійність і стійкість візуального відстеження. Проте те, як реалізується інтеграція кількох підказок під час відстеження, залишається відкритим питанням. У цій роботі ми представляємо новий підхід об’єднання даних для відстеження кількох сигналів за допомогою фільтра частинок. Наш метод відрізняється від попередніх підходів кількома параметрами. По-перше, ми здійснюємо інтеграцію підказок як для створення прогнозів щодо цільового об’єкта, так і для їх перевірки за допомогою спостережень. Наш другий і більш значний внесок полягає в тому, що обидва етапи інтеграції безпосередньо залежать від надійності візуальних підказок, що динамічно змінюється. Ці два аспекти нашого методу дозволяють трекеру легко адаптуватися до змін у контексті та, відповідно, підвищити точність відстеження шляхом вирішення неоднозначностей.",0
"Making sense of ever-growing amount of visual data available on the web is difficult, especially when considered in an unsupervised manner. As a step towards this goal, this study tackles a relatively less explored topic of generating structured summaries of large photo collections. Our framework relies on the notion of a story graph which captures the main narratives in the data and their relationships based on their visual, textual and spatio-temporal features. Its output is a directed graph with a set of possibly intersecting paths. Our proposed approach identifies coherent visual storylines and exploits sub-modularity to select a subset of these lines which covers the general narrative at most. Our experimental analysis reveals that extracted story graphs allow for obtaining better results when utilized as priors for photo album summarization. Moreover, our user studies show that our approach delivers better performance on next image prediction and coverage tasks than the state-of-the-art.","Automatic description generation from natural images is a challenging problem thathas recently received a large amount of interest from the computer vision and natural lan-guage processing communities.  In this survey,  we classify the existing approaches basedon how they conceptualize this problem, viz., models that cast description as either gen-eration  problem  or  as  a  retrieval  problem  over  a  visual  or  multimodal  representationalspace.  We provide a detailed review of existing models, highlighting their advantages anddisadvantages.  Moreover, we give an overview of the benchmark image datasets and theevaluation measures that have been developed to assess the quality of machine-generatedimage descriptions.  Finally we extrapolate future directions in the area of automatic imagedescription generation.",0
"Making sense of ever-growing amount of visual data available on the web is difficult, especially when considered in an unsupervised manner. As a step towards this goal, this study tackles a relatively less explored topic of generating structured summaries of large photo collections. Our framework relies on the notion of a story graph which captures the main narratives in the data and their relationships based on their visual, textual and spatio-temporal features. Its output is a directed graph with a set of possibly intersecting paths. Our proposed approach identifies coherent visual storylines and exploits sub-modularity to select a subset of these lines which covers the general narrative at most. Our experimental analysis reveals that extracted story graphs allow for obtaining better results when utilized as priors for photo album summarization. Moreover, our user studies show that our approach delivers better performance on next image prediction and coverage tasks than the state-of-the-art.","Автоматичне створення опису з природних зображень є складною проблемою, яка нещодавно привернула великий інтерес у спільнот комп’ютерного зору та обробки природної мови. У цьому дослідженні ми класифікуємо існуючі підходи на основі того, як вони концептуалізують цю проблему, а саме моделі, які перетворюють опис як проблему генерації або як проблему пошуку у візуальному чи мультимодальному репрезентативному просторі. Ми надаємо детальний огляд існуючих моделей, висвітлюючи їх переваги та недоліки. Крім того, ми надаємо огляд еталонних наборів даних зображень і заходів оцінки, які були розроблені для оцінки якості описів зображень, створених машиною. Нарешті ми екстраполюємо майбутні напрямки в області автоматичного створення опису зображення.",0
"Важко розібратися в постійно зростаючій кількості візуальних даних, доступних в Інтернеті, особливо якщо розглядати їх без нагляду. Як крок до цієї мети, це дослідження стосується відносно менш дослідженої теми створення структурованих підсумків великих колекцій фотографій. Наша структура спирається на поняття графіка історії, який фіксує основні наративи в даних та їхні зв’язки на основі їхніх візуальних, текстових і просторово-часових особливостей. Його виходом є орієнтований граф із набором можливо пересічних шляхів. Запропонований нами підхід визначає послідовні візуальні сюжетні лінії та використовує субмодульність, щоб вибрати підмножину цих ліній, яка максимально охоплює загальну розповідь. Наш експериментальний аналіз показує, що витягнуті графіки історій дозволяють отримати кращі результати, якщо їх використовувати як попередні для підсумовування фотоальбому. Крім того, наші дослідження користувачів показують, що наш підхід забезпечує кращу продуктивність у наступних задачах прогнозування зображення та покриття, ніж найсучасніший.","Automatic description generation from natural images is a challenging problem thathas recently received a large amount of interest from the computer vision and natural lan-guage processing communities.  In this survey,  we classify the existing approaches basedon how they conceptualize this problem, viz., models that cast description as either gen-eration  problem  or  as  a  retrieval  problem  over  a  visual  or  multimodal  representationalspace.  We provide a detailed review of existing models, highlighting their advantages anddisadvantages.  Moreover, we give an overview of the benchmark image datasets and theevaluation measures that have been developed to assess the quality of machine-generatedimage descriptions.  Finally we extrapolate future directions in the area of automatic imagedescription generation.",0
"Важко розібратися в постійно зростаючій кількості візуальних даних, доступних в Інтернеті, особливо якщо розглядати їх без нагляду. Як крок до цієї мети це дослідження стосується відносно менш дослідженої теми створення структурованих підсумків великих колекцій фотографій. Наша структура спирається на поняття графіка історії, який фіксує основні наративи в даних та їхні зв’язки на основі їхніх візуальних, текстових і просторово-часових особливостей. Його виходом є орієнтований граф із набором можливо пересічних шляхів. Запропонований нами підхід визначає послідовні візуальні сюжетні лінії та використовує субмодульність, щоб вибрати підмножину цих ліній, яка максимально охоплює загальну розповідь. Наш експериментальний аналіз показує, що витягнуті графіки історій дозволяють отримати кращі результати, якщо їх використовувати як попередні для підсумовування фотоальбому. Крім того, наші дослідження користувачів показують, що наш підхід забезпечує кращу продуктивність у наступних задачах прогнозування зображення та покриття, ніж найсучасніший.","Автоматичне створення опису з природних зображень є складною проблемою, яка нещодавно привернула великий інтерес у спільнот комп’ютерного зору та обробки природної мови. У цьому дослідженні ми класифікуємо існуючі підходи на основі того, як вони концептуалізують цю проблему, а саме моделі, які перетворюють опис як проблему генерації або як проблему пошуку у візуальному чи мультимодальному репрезентативному просторі. Ми надаємо детальний огляд існуючих моделей, висвітлюючи їх переваги та недоліки. Крім того, ми надаємо огляд еталонних наборів даних зображень і заходів оцінки, які були розроблені для оцінки якості описів зображень, створених машиною. Нарешті ми екстраполюємо майбутні напрямки в області автоматичного створення опису зображення.",0
"Automatic generation of video descriptions in natural language, also called video captioning, aims to understand the visual content of the video and produce a natural language sentence depicting the objects and actions in the scene. This challenging integrated vision and language problem, however, has been predominantly addressed for English. The lack of data and the linguistic properties of other languages limit the success of existing approaches for such languages. In this paper we target Turkish, a morphologically rich and agglutinative language that has very different properties compared to English. To do so, we create the first large scale video captioning dataset for this language by carefully translating the English descriptions of the videos in the MSVD (Microsoft Research Video Description Corpus) dataset into Turkish. In addition to enabling research in video captioning in Turkish, the parallel English-Turkish descriptions also enables the study of the role of video context in (multimodal) machine translation. In our experiments, we build models for both video captioning and multimodal machine translation and investigate the effect of different word segmentation approaches and different neural architectures to better address the properties of Turkish. We hope that the MSVDTurkish dataset and the results reported in this work will lead to better video captioning and multimodal machine translation models for Turkish and other morphology rich and agglutinative languages.","Previous sampling-based image matting methods typically rely on certain heuristics in collecting representative samples from known regions, and thus their performance deteriorates if the underlying assumptions are not satisfied. To alleviate this, in this paper we take an entirely new approach and formulate sampling as a sparse subset selection problem where we propose to pick a small set of candidate samples that best explains the unknown pixels. Moreover, we describe a new distance measure for comparing two samples which is based on KL-divergence between the distributions of features extracted in the vicinity of the samples. Using a standard benchmark dataset for image matting, we demonstrate that our approach provides more accurate results compared with the state-of-the-art methods. ",0
"Automatic generation of video descriptions in natural language, also called video captioning, aims to understand the visual content of the video and produce a natural language sentence depicting the objects and actions in the scene. This challenging integrated vision and language problem, however, has been predominantly addressed for English. The lack of data and the linguistic properties of other languages limit the success of existing approaches for such languages. In this paper we target Turkish, a morphologically rich and agglutinative language that has very different properties compared to English. To do so, we create the first large scale video captioning dataset for this language by carefully translating the English descriptions of the videos in the MSVD (Microsoft Research Video Description Corpus) dataset into Turkish. In addition to enabling research in video captioning in Turkish, the parallel English-Turkish descriptions also enables the study of the role of video context in (multimodal) machine translation. In our experiments, we build models for both video captioning and multimodal machine translation and investigate the effect of different word segmentation approaches and different neural architectures to better address the properties of Turkish. We hope that the MSVDTurkish dataset and the results reported in this work will lead to better video captioning and multimodal machine translation models for Turkish and other morphology rich and agglutinative languages.","Попередні методи матування зображень на основі вибірки зазвичай покладаються на певні евристики при зборі репрезентативних зразків із відомих регіонів, і, отже, їх продуктивність погіршується, якщо базові припущення не задовольняються. Щоб полегшити це, у цій статті ми застосували абсолютно новий підхід і сформулювали вибірку як проблему вибору розрідженої підмножини, де ми пропонуємо вибрати невеликий набір вибірок-кандидатів, які найкраще пояснюють невідомі пікселі. Крім того, ми описуємо нову міру відстані для порівняння двох зразків, яка базується на KL-розбіжності між розподілами ознак, виділених поблизу зразків. Використовуючи стандартний контрольний набір даних для матування зображення, ми демонструємо, що наш підхід забезпечує більш точні результати порівняно з найсучаснішими методами.",0
"Автоматична генерація відеоописів природною мовою, яка також називається відеотитрами, має на меті зрозуміти візуальний вміст відео та створити речення природною мовою, що зображує об’єкти та дії в сцені. Ця складна інтегрована проблема бачення та мови, однак, була переважно розглянута для англійської мови. Брак даних і лінгвістичні властивості інших мов обмежують успіх існуючих підходів для таких мов. У цій статті ми орієнтуємося на турецьку, морфологічно багату та аглютинативну мову, яка має дуже різні властивості порівняно з англійською. Для цього ми створюємо перший великомасштабний набір даних субтитрів до відео для цієї мови, ретельно перекладаючи англійські описи відео в наборі даних MSVD (Microsoft Research Video Description Corpus) на турецьку мову. На додаток до дослідження субтитрів до відео турецькою мовою, паралельні англо-турецькі описи також дозволяють вивчати роль відеоконтексту в (мультимодальному) машинному перекладі. У наших експериментах ми створюємо моделі як для субтитрів до відео, так і для мультимодального машинного перекладу та досліджуємо вплив різних підходів до сегментації слів і різних нейронних архітектур для кращого врахування властивостей турецької мови. Ми сподіваємося, що набір даних MSVDTurkish і результати, наведені в цій роботі, призведуть до кращих відеосубтитрів і мультимодальних моделей машинного перекладу для турецької та інших морфологічних і аглютинативних мов.","Previous sampling-based image matting methods typically rely on certain heuristics in collecting representative samples from known regions, and thus their performance deteriorates if the underlying assumptions are not satisfied. To alleviate this, in this paper we take an entirely new approach and formulate sampling as a sparse subset selection problem where we propose to pick a small set of candidate samples that best explains the unknown pixels. Moreover, we describe a new distance measure for comparing two samples which is based on KL-divergence between the distributions of features extracted in the vicinity of the samples. Using a standard benchmark dataset for image matting, we demonstrate that our approach provides more accurate results compared with the state-of-the-art methods. ",0
"Автоматична генерація відеоописів природною мовою, яка також називається відеотитрами, має на меті зрозуміти візуальний вміст відео та створити речення природною мовою, що зображує об’єкти та дії в сцені. Ця складна інтегрована проблема бачення та мови, однак, була переважно розглянута для англійської мови. Брак даних і лінгвістичні властивості інших мов обмежують успіх існуючих підходів для таких мов. У цій статті ми орієнтуємося на турецьку, морфологічно багату та аглютинативну мову, яка має дуже різні властивості порівняно з англійською. Для цього ми створюємо перший великомасштабний набір даних субтитрів до відео для цієї мови, ретельно перекладаючи англійські описи відео в наборі даних MSVD (Microsoft Research Video Description Corpus) на турецьку мову. На додаток до дослідження субтитрів до відео турецькою мовою, паралельні англо-турецькі описи також дозволяють вивчати роль відеоконтексту в (мультимодальному) машинному перекладі. У наших експериментах ми створюємо моделі як для субтитрів до відео, так і для мультимодального машинного перекладу та досліджуємо вплив різних підходів до сегментації слів і різних нейронних архітектур для кращого врахування властивостей турецької мови. Ми сподіваємося, що набір даних MSVDTurkish і результати, наведені в цій роботі, призведуть до кращих відеосубтитрів і мультимодальних моделей машинного перекладу для турецької та інших морфологічних і аглютинативних мов.","Попередні методи матування зображень на основі вибірки зазвичай покладаються на певні евристики при зборі репрезентативних зразків із відомих регіонів, і, отже, їх продуктивність погіршується, якщо базові припущення не задовольняються. Щоб полегшити це, у цій статті ми застосували абсолютно новий підхід і сформулювали вибірку як проблему вибору розрідженої підмножини, де ми пропонуємо вибрати невеликий набір вибірок-кандидатів, які найкраще пояснюють невідомі пікселі. Крім того, ми описуємо нову міру відстані для порівняння двох зразків, яка базується на KL-розбіжності між розподілами ознак, виділених поблизу зразків. Використовуючи стандартний контрольний набір даних для матування зображення, ми демонструємо, що наш підхід забезпечує більш точні результати порівняно з найсучаснішими методами.",0
"Humans are able to perceive, understand and reason about causal events. Developing models with similar physical and causal understanding capabilities is a long-standing goal of artificial intelligence. As a step towards this direction, we introduce CRAFT1 , a new video question answering dataset that requires causal reasoning about physical forces and object interactions. It contains 58K video and question pairs that are generated from 10K videos from 20 different virtual environments, containing various objects in motion that interact with each other and the scene. Two question categories in CRAFT include previously studied descriptive and counterfactual questions. Additionally, inspired by the Force Dynamics Theory in cognitive linguistics, we introduce a new causal question category that involves understanding the causal interactions between objects through notions like cause, enable, and prevent. Our results show that even though the questions in CRAFT are easy for humans, the tested baseline models, including existing state-of-the-art methods, do not yet deal with the challenges posed in our benchmark.","In this study, we explore building a two-stage framework for enabling users to directly manipulate high-level attributes of a natural scene. The key to our approach is a deep generative network that can hallucinate images of a scene as if they were taken in a different season (e.g., during winter), weather condition (e.g., on a cloudy day), or at a different time of the day (e.g., at sunset). Once the scene is hallucinated with the given attributes, the corresponding look is then transferred to the input image while preserving the semantic details intact, giving a photo-realistic manipulation result. As the proposed framework hallucinates what the scene will look like, it does not require any reference style image as commonly utilized in most of the appearance or style transfer approaches. Moreover, it allows to simultaneously manipulate a given scene according to a diverse set of transient attributes within a single model, eliminating the need of training multiple networks per each translation task. Our comprehensive set of qualitative and quantitative results demonstrates the effectiveness of our approach against the competing methods.",0
"Humans are able to perceive, understand and reason about causal events. Developing models with similar physical and causal understanding capabilities is a long-standing goal of artificial intelligence. As a step towards this direction, we introduce CRAFT1 , a new video question answering dataset that requires causal reasoning about physical forces and object interactions. It contains 58K video and question pairs that are generated from 10K videos from 20 different virtual environments, containing various objects in motion that interact with each other and the scene. Two question categories in CRAFT include previously studied descriptive and counterfactual questions. Additionally, inspired by the Force Dynamics Theory in cognitive linguistics, we introduce a new causal question category that involves understanding the causal interactions between objects through notions like cause, enable, and prevent. Our results show that even though the questions in CRAFT are easy for humans, the tested baseline models, including existing state-of-the-art methods, do not yet deal with the challenges posed in our benchmark.","У цьому дослідженні ми досліджуємо побудову двоетапної структури, яка дозволяє користувачам безпосередньо маніпулювати високорівневими атрибутами природної сцени. Ключем до нашого підходу є глибока генеративна мережа, яка може галюцинувати зображення сцени, ніби вони були зроблені в інший сезон (наприклад, взимку), погодні умови (наприклад, у похмурий день) або в інший час день (наприклад, на заході сонця). Після галюцинації сцени з заданими атрибутами відповідний вигляд переноситься на вхідне зображення, зберігаючи семантичні деталі недоторканими, що дає фотореалістичний результат маніпуляції. Оскільки запропонований фреймворк показує, як буде виглядати сцена, він не потребує жодного еталонного стилю зображення, яке зазвичай використовується в більшості підходів до зовнішнього вигляду чи передачі стилю. Крім того, це дозволяє одночасно маніпулювати певною сценою відповідно до різноманітного набору тимчасових атрибутів у межах однієї моделі, усуваючи потребу в навчанні кількох мереж для кожного завдання перекладу. Наш комплексний набір якісних і кількісних результатів демонструє ефективність нашого підходу проти конкурентних методів.",0
"Люди здатні сприймати, розуміти та міркувати про причинно-наслідкові події. Розробка моделей із подібними фізичними та причинно-наслідковими можливостями розуміння є давньою метою штучного інтелекту. Як крок у цьому напрямку ми представляємо CRAFT1, новий набір даних із відповідями на відеозапитання, який вимагає причинно-наслідкових міркувань про фізичні сили та взаємодію об’єктів. Він містить 58 тис. відео та пар запитань, створених із 10 тис. відео з 20 різних віртуальних середовищ, що містять різні рухомі об’єкти, які взаємодіють один з одним і сценою. Дві категорії запитань у CRAFT включають раніше вивчені описові та контрфактичні запитання. Крім того, натхненний теорією динаміки сил у когнітивній лінгвістиці, ми вводимо нову категорію причинно-наслідкових питань, яка передбачає розуміння причинно-наслідкових взаємодій між об’єктами через такі поняття, як причина, можливість і запобігання. Наші результати показують, що, незважаючи на те, що запитання в CRAFT легкі для людей, перевірені базові моделі, включаючи існуючі найсучасніші методи, ще не справляються з проблемами, поставленими в нашому тесті.","In this study, we explore building a two-stage framework for enabling users to directly manipulate high-level attributes of a natural scene. The key to our approach is a deep generative network that can hallucinate images of a scene as if they were taken in a different season (e.g., during winter), weather condition (e.g., on a cloudy day), or at a different time of the day (e.g., at sunset). Once the scene is hallucinated with the given attributes, the corresponding look is then transferred to the input image while preserving the semantic details intact, giving a photo-realistic manipulation result. As the proposed framework hallucinates what the scene will look like, it does not require any reference style image as commonly utilized in most of the appearance or style transfer approaches. Moreover, it allows to simultaneously manipulate a given scene according to a diverse set of transient attributes within a single model, eliminating the need of training multiple networks per each translation task. Our comprehensive set of qualitative and quantitative results demonstrates the effectiveness of our approach against the competing methods.",0
"Люди здатні сприймати, розуміти та міркувати про причинно-наслідкові події. Розробка моделей із подібними фізичними та причинно-наслідковими можливостями розуміння є давньою метою штучного інтелекту. Як крок у цьому напрямку ми представляємо CRAFT1, новий набір даних із відповідями на відеозапитання, який вимагає причинно-наслідкових міркувань про фізичні сили та взаємодію об’єктів. Він містить 58 тис. відео та пар запитань, створених із 10 тис. відео з 20 різних віртуальних середовищ, що містять різні рухомі об’єкти, які взаємодіють один з одним і сценою. Дві категорії запитань у CRAFT включають раніше вивчені описові та контрфактичні запитання. Крім того, натхненний теорією динаміки сил у когнітивній лінгвістиці, ми вводимо нову категорію причинно-наслідкових питань, яка передбачає розуміння причинно-наслідкових взаємодій між об’єктами через такі поняття, як причина, можливість і запобігання. Наші результати показують, що, незважаючи на те, що запитання в CRAFT легкі для людей, перевірені базові моделі, включаючи існуючі найсучасніші методи, ще не справляються з проблемами, поставленими в нашому тесті.","У цьому дослідженні ми досліджуємо побудову двоетапної структури, яка дозволяє користувачам безпосередньо маніпулювати високорівневими атрибутами природної сцени. Ключем до нашого підходу є глибока генеративна мережа, яка може галюцинувати зображення сцени, ніби вони були зроблені в інший сезон (наприклад, взимку), погодні умови (наприклад, у похмурий день) або в інший час день (наприклад, на заході сонця). Після галюцинації сцени з заданими атрибутами відповідний вигляд переноситься на вхідне зображення, зберігаючи семантичні деталі недоторканими, що дає фотореалістичний результат маніпуляції. Оскільки запропонований фреймворк показує, як буде виглядати сцена, він не потребує жодного еталонного стилю зображення, яке зазвичай використовується в більшості підходів до зовнішнього вигляду чи передачі стилю. Крім того, це дозволяє одночасно маніпулювати певною сценою відповідно до різноманітного набору тимчасових атрибутів у межах однієї моделі, усуваючи потребу в навчанні кількох мереж для кожного завдання перекладу. Наш комплексний набір якісних і кількісних результатів демонструє ефективність нашого підходу проти конкурентних методів.",0
"In recent years, graph neural networks have been successfully applied for learning the dynamics of complex and partially observable physical systems. However, their use in the robotics domain is, to date, still limited. In this paper, we introduce Belief Regulated Dual Propagation Networks (BRDPN), a general-purpose learnable physics engine, which enables a robot to predict the effects of its actions in scenes containing groups of articulated multi-part objects. Specifically, our framework extends recently proposed propagation networks (PropNets) and consists of two complementary components, a physics predictor and a belief regulator. While the former predicts the future states of the object(s) manipulated by the robot, the latter constantly corrects the robot’s knowledge regarding the objects and their relations. Our results showed that after training in a simulator, the robot can reliably predict the consequences of its actions in object trajectory level and exploit its own interaction experience to correct its belief about the state of the environment, enabling better predictions in partially observable environments. Furthermore, the trained model was transferred to the real world and verified in predicting trajectories of pushed interacting objects whose joint relations were initially unknown. We compared BRDPN against PropNets, and showed that BRDPN performs consistently well. Moreover, BRDPN can adapt its physic predictions, since the relations can be predicted online. ","Correlation filters have recently attracted attention in visual tracking due to their efficiency and high per- formance. However, their application to long-term tracking is somewhat limited since these trackers are not equipped with mechanisms to cope with challenging cases like partial occlusion, deformation or scale changes. In this paper, we propose a deformable part-based correlation filter tracking approach which depends on coupled interactions between a global filter and several part filters. Specifically, local filters provide an initial estimate, which is then used by the global filter as a reference to determine the final result. Then, the global filter provides a feedback to the part filters regarding their updates and the related deformation parameters. In this way, our proposed collaborative model handles not only partial occlusion but also scale changes. Experiments on two large public benchmark datasets demonstrate that our approach gives significantly better results compared with the state-of-the-art trackers.",0
"In recent years, graph neural networks have been successfully applied for learning the dynamics of complex and partially observable physical systems. However, their use in the robotics domain is, to date, still limited. In this paper, we introduce Belief Regulated Dual Propagation Networks (BRDPN), a general-purpose learnable physics engine, which enables a robot to predict the effects of its actions in scenes containing groups of articulated multi-part objects. Specifically, our framework extends recently proposed propagation networks (PropNets) and consists of two complementary components, a physics predictor and a belief regulator. While the former predicts the future states of the object(s) manipulated by the robot, the latter constantly corrects the robot’s knowledge regarding the objects and their relations. Our results showed that after training in a simulator, the robot can reliably predict the consequences of its actions in object trajectory level and exploit its own interaction experience to correct its belief about the state of the environment, enabling better predictions in partially observable environments. Furthermore, the trained model was transferred to the real world and verified in predicting trajectories of pushed interacting objects whose joint relations were initially unknown. We compared BRDPN against PropNets, and showed that BRDPN performs consistently well. Moreover, BRDPN can adapt its physic predictions, since the relations can be predicted online. ","Кореляційні фільтри нещодавно привернули увагу у візуальному відстеженні завдяки своїй ефективності та високій продуктивності. Однак їхнє застосування для тривалого відстеження дещо обмежене, оскільки ці трекери не оснащені механізмами, щоб впоратися зі складними випадками, такими як часткова оклюзія, деформація або зміни масштабу. У цій статті ми пропонуємо підхід відстеження кореляційного фільтра на основі деформованих частин, який залежить від зв’язаних взаємодій між глобальним фільтром і декількома фільтрами частин. Зокрема, локальні фільтри забезпечують початкову оцінку, яка потім використовується глобальним фільтром як еталон для визначення кінцевого результату. Потім глобальний фільтр надає зворотний зв’язок фільтрам частин щодо їх оновлень і відповідних параметрів деформації. Таким чином, наша запропонована спільна модель обробляє не лише часткову оклюзію, але й зміни масштабу. Експерименти на двох великих загальнодоступних наборах даних показують, що наш підхід дає значно кращі результати порівняно з найсучаснішими трекерами.",0
"В останні роки графові нейронні мережі успішно застосовуються для вивчення динаміки складних і частково спостережуваних фізичних систем. Однак їх використання в області робототехніки на сьогоднішній день все ще обмежене. У цій статті ми представляємо мережу подвійного розповсюдження, регульовану переконаннями (BRDPN), універсальний навчальний фізичний механізм, який дозволяє роботу передбачати наслідки його дій у сценах, що містять групи шарнірних багатокомпонентних об’єктів. Зокрема, наша структура розширює нещодавно запропоновані мережі розповсюдження (PropNets) і складається з двох взаємодоповнюючих компонентів, фізичного предиктора та регулятора переконань. У той час як перший передбачає майбутні стани об’єктів, якими маніпулює робот, другий постійно коригує знання робота щодо об’єктів та їхніх зв’язків. Наші результати показали, що після навчання на симуляторі робот може надійно передбачати наслідки своїх дій на рівні траєкторії об’єкта та використовувати власний досвід взаємодії, щоб виправити свої переконання щодо стану навколишнього середовища, забезпечуючи кращі прогнози в частково доступних для спостереження середовищах. Крім того, навчена модель була перенесена в реальний світ і перевірена у передбаченні траєкторій штовхаючих взаємодіючих об’єктів, спільні зв’язки яких спочатку були невідомі. Ми порівняли BRDPN із PropNets і показали, що BRDPN стабільно добре працює. Крім того, BRDPN може адаптувати свої фізичні прогнози, оскільки співвідношення можна передбачити онлайн.","Correlation filters have recently attracted attention in visual tracking due to their efficiency and high per- formance. However, their application to long-term tracking is somewhat limited since these trackers are not equipped with mechanisms to cope with challenging cases like partial occlusion, deformation or scale changes. In this paper, we propose a deformable part-based correlation filter tracking approach which depends on coupled interactions between a global filter and several part filters. Specifically, local filters provide an initial estimate, which is then used by the global filter as a reference to determine the final result. Then, the global filter provides a feedback to the part filters regarding their updates and the related deformation parameters. In this way, our proposed collaborative model handles not only partial occlusion but also scale changes. Experiments on two large public benchmark datasets demonstrate that our approach gives significantly better results compared with the state-of-the-art trackers.",0
"В останні роки графові нейронні мережі успішно застосовуються для вивчення динаміки складних і частково спостережуваних фізичних систем. Однак їх використання в області робототехніки на сьогоднішній день все ще обмежене. У цьому документі ми представляємо мережі подвійного розповсюдження, регульовані переконаннями (BRDPN), універсальний навчальний фізичний механізм, який дозволяє роботу передбачати наслідки його дій у сценах, що містять групи шарнірних багатокомпонентних об’єктів. Зокрема, наша структура розширює нещодавно запропоновані мережі розповсюдження (PropNets) і складається з двох додаткових компонентів, фізичного предиктора та регулятора віри. Тоді як перший передбачає майбутні стани об’єктів, якими маніпулює робот, другий постійно коригує знання робота щодо об’єктів та їхніх зв’язків. Наші результати показали, що після навчання на симуляторі робот може надійно передбачати наслідки своїх дій на рівні траєкторії об’єкта та використовувати власний досвід взаємодії, щоб виправити свої переконання щодо стану навколишнього середовища, забезпечуючи кращі прогнози в частково доступних для спостереження середовищах. Крім того, навчена модель була перенесена в реальний світ і перевірена у прогнозуванні траєкторій штовхаючих взаємодіючих об’єктів, спільні зв’язки яких спочатку були невідомі. Ми порівняли BRDPN із PropNets і показали, що BRDPN стабільно добре працює. Крім того, BRDPN може адаптувати свої фізичні прогнози, оскільки співвідношення можна передбачити онлайн.","Кореляційні фільтри нещодавно привернули увагу у візуальному відстеженні завдяки своїй ефективності та високій продуктивності. Однак їхнє застосування для тривалого відстеження дещо обмежене, оскільки ці трекери не оснащені механізмами, щоб впоратися зі складними випадками, такими як часткова оклюзія, деформація або зміни масштабу. У цій статті ми пропонуємо підхід відстеження кореляційного фільтра на основі деформованих частин, який залежить від зв’язаних взаємодій між глобальним фільтром і декількома фільтрами частин. Зокрема, локальні фільтри забезпечують початкову оцінку, яка потім використовується глобальним фільтром як еталон для визначення кінцевого результату. Потім глобальний фільтр надає зворотний зв’язок фільтрам частин щодо їх оновлень і відповідних параметрів деформації. Таким чином, наша запропонована спільна модель обробляє не лише часткову оклюзію, але й зміни масштабу. Експерименти на двох великих загальнодоступних наборах даних показують, що наш підхід дає значно кращі результати порівняно з найсучаснішими трекерами.",0
"In recent years, many computational models for saliency prediction have been introduced. For dynamic scenes, the existing models typically combine different feature maps extracted from spatial and temporal domains either by following generic integration strategies such as averaging or winners take all or using machine learning techniques to set each feature’s importance. Rather than resorting to these fixed feature integration schemes, in this paper, we propose a novel weakly supervised dynamic saliency model called HedgeSal, which is based on a decision-theoretic online learning scheme. Our framework uses two pretrained deep static saliency models as experts to extract individual saliency maps from appearance and motion streams, and then generates the final saliency map by weighted decisions of all these models. As visual characteristics of dynamic scenes constantly vary, the models providing consistently good predictions in the past are automatically assigned higher weights, allowing each expert to adjust itself to the current conditions. We demonstrate the effectiveness of our model on the CRCNS, UCFSports and CITIUS datasets.","This paper presents a new image segmentation framework which employs a shape prior in the form of an edge strength function to introduce a higher-level influence on the segmentation process. We formulate segmentation as the minimization of three coupled functionals, respectively, defining three processes: prior-guided segmentation, shape feature extraction and local deformation estimation. Particularly, the shape feature extraction process is in charge of estimating an edge strength function from the evolving object region. The local deformation estimation process uses this function to determine a meaningful correspondence between a given prior and the evolving object region, and the deformation map estimated in return supervises the segmentation by enforcing the evolving object boundary towards the prior shape.",0
"In recent years, many computational models for saliency prediction have been introduced. For dynamic scenes, the existing models typically combine different feature maps extracted from spatial and temporal domains either by following generic integration strategies such as averaging or winners take all or using machine learning techniques to set each feature’s importance. Rather than resorting to these fixed feature integration schemes, in this paper, we propose a novel weakly supervised dynamic saliency model called HedgeSal, which is based on a decision-theoretic online learning scheme. Our framework uses two pretrained deep static saliency models as experts to extract individual saliency maps from appearance and motion streams, and then generates the final saliency map by weighted decisions of all these models. As visual characteristics of dynamic scenes constantly vary, the models providing consistently good predictions in the past are automatically assigned higher weights, allowing each expert to adjust itself to the current conditions. We demonstrate the effectiveness of our model on the CRCNS, UCFSports and CITIUS datasets.","У цьому документі представлено нову структуру сегментації зображення, яка використовує попередні форми у формі функції міцності країв, щоб запровадити вплив вищого рівня на процес сегментації. Ми формулюємо сегментацію як мінімізацію трьох пов’язаних функціоналів відповідно, визначаючи три процеси: попередньо керовану сегментацію, виділення ознак форми та локальну оцінку деформації. Зокрема, процес виділення ознак форми відповідає за оцінку функції міцності країв з області об’єкта, що розвивається. Процес локальної оцінки деформації використовує цю функцію для визначення значущої відповідності між даним попереднім і еволюційною областю об’єкта, а карта деформації, оцінена у свою чергу, контролює сегментацію, примушуючи еволюцію межі об’єкта до попередньої форми.",0
"В останні роки було введено багато обчислювальних моделей для прогнозування помітності. Для динамічних сцен існуючі моделі зазвичай поєднують різні карти функцій, отримані з просторових і часових доменів, дотримуючись загальних стратегій інтеграції, таких як усереднення або переможці отримують усе, або використовуючи методи машинного навчання для встановлення важливості кожної функції. Замість того, щоб вдаватися до цих схем інтеграції фіксованих функцій, у цій статті ми пропонуємо нову слабко контрольовану динамічну модель помітності під назвою HedgeSal, яка базується на схемі онлайн-навчання, що базується на теоретичному прийнятті рішень. Наш фреймворк використовує дві попередньо підготовлені глибокі статичні моделі помітності як експертів для отримання індивідуальних карт помітності з потоків зовнішнього вигляду та руху, а потім генерує остаточну карту помітності шляхом зважених рішень усіх цих моделей. Оскільки візуальні характеристики динамічних сцен постійно змінюються, моделям, що забезпечують незмінно хороші прогнози в минулому, автоматично призначаються вищі ваги, що дозволяє кожному експерту пристосуватися до поточних умов. Ми демонструємо ефективність нашої моделі на наборах даних CRCNS, UCFSports і CITIUS.","This paper presents a new image segmentation framework which employs a shape prior in the form of an edge strength function to introduce a higher-level influence on the segmentation process. We formulate segmentation as the minimization of three coupled functionals, respectively, defining three processes: prior-guided segmentation, shape feature extraction and local deformation estimation. Particularly, the shape feature extraction process is in charge of estimating an edge strength function from the evolving object region. The local deformation estimation process uses this function to determine a meaningful correspondence between a given prior and the evolving object region, and the deformation map estimated in return supervises the segmentation by enforcing the evolving object boundary towards the prior shape.",0
"В останні роки було введено багато обчислювальних моделей для прогнозування помітності. Для динамічних сцен існуючі моделі зазвичай поєднують різні карти функцій, отримані з просторових і часових доменів, дотримуючись загальних стратегій інтеграції, таких як усереднення або переможці отримують усе, або використовуючи методи машинного навчання для встановлення важливості кожної функції. Замість того, щоб вдаватися до цих схем інтеграції фіксованих функцій, у цій статті ми пропонуємо нову слабко контрольовану динамічну модель помітності під назвою HedgeSal, яка базується на схемі онлайн-навчання, що базується на теоретичному прийнятті рішень. Наш фреймворк використовує дві попередньо підготовлені глибокі статичні моделі помітності як експертів для отримання індивідуальних карт помітності з потоків зовнішнього вигляду та руху, а потім генерує остаточну карту помітності шляхом зважених рішень усіх цих моделей. Оскільки візуальні характеристики динамічних сцен постійно змінюються, моделям, що забезпечують незмінно хороші прогнози в минулому, автоматично призначаються вищі ваги, що дозволяє кожному експерту пристосуватися до поточних умов. Ми демонструємо ефективність нашої моделі на наборах даних CRCNS, UCFSports і CITIUS.","У цьому документі представлено нову структуру сегментації зображення, яка використовує попередні форми у формі функції міцності країв, щоб запровадити вплив вищого рівня на процес сегментації. Ми формулюємо сегментацію як мінімізацію трьох пов’язаних функціоналів відповідно, визначаючи три процеси: попередньо керовану сегментацію, виділення ознак форми та локальну оцінку деформації. Зокрема, процес виділення ознак форми відповідає за оцінку функції міцності країв з області об’єкта, що розвивається. Процес локальної оцінки деформації використовує цю функцію для визначення значущої відповідності між даним попереднім і еволюційною областю об’єкта, а карта деформації, оцінена у свою чергу, контролює сегментацію, примушуючи еволюцію межі об’єкта до попередньої форми.",0
"In recent years, deep learning methods have come to the forefront in many areas that require remote sensing, from medicine to agriculture, from defense industry to space research; and these methods have achieved tremendous success as compared to traditional methods. Together with substantial growth in available data with high-quality labels and computational resources, these deep neural network architectures and techniques have seen remarkable developments. The major difference between deep learning and classical recognition methods is that deep learning methods consider an end-to-end learning scheme which gives rise to learning features from raw data. Better regularization techniques and robust optimization algorithms introduced with state-of-the-art deep learning models are other factors leading this difference. In this paper, we discuss the remote sensing problems and how deep learning can be used to solve these problems with a special focus on medical and remote sensing applications. In particular, we briefly review outperforming architectures within the deep learning literature and their use cases. ","This paper describes our two-stage system1 for the Euphemism Detection shared task hosted by the 3rd Workshop on Figurative Language Processing in conjunction with EMNLP 2022. Euphemisms tone down expressions about sensitive or unpleasant issues like addiction and death. The ambiguous nature of euphemistic words or expressions makes it challenging to detect their actual meaning within a context. In the first stage, we seek to mitigate this ambiguity by incorporating literal descriptions into input text prompts to our baseline model. It turns out that this kind of direct supervision yields remarkable performance improvement. In the second stage, we integrate visual supervision into our system using visual imageries, two sets of images generated by a text-to-image model by taking terms and descriptions as input. Our experiments demonstrate that visual supervision also gives a statistically significant performance boost. Our system achieved the second place with an F1 score of 87.2%, only about 0.9% worse than the best submission.",0
"In recent years, deep learning methods have come to the forefront in many areas that require remote sensing, from medicine to agriculture, from defense industry to space research; and these methods have achieved tremendous success as compared to traditional methods. Together with substantial growth in available data with high-quality labels and computational resources, these deep neural network architectures and techniques have seen remarkable developments. The major difference between deep learning and classical recognition methods is that deep learning methods consider an end-to-end learning scheme which gives rise to learning features from raw data. Better regularization techniques and robust optimization algorithms introduced with state-of-the-art deep learning models are other factors leading this difference. In this paper, we discuss the remote sensing problems and how deep learning can be used to solve these problems with a special focus on medical and remote sensing applications. In particular, we briefly review outperforming architectures within the deep learning literature and their use cases. ","У цьому документі описано нашу двоетапну систему1 для спільного завдання «Виявлення евфемізмів», організованого 3-м семінаром з обробки образної мови спільно з EMNLP 2022. Евфемізми пом’якшують вирази про чутливі або неприємні питання, такі як залежність і смерть. Неоднозначний характер евфемістичних слів або виразів ускладнює виявлення їх справжнього значення в контексті. На першому етапі ми намагаємося пом’якшити цю неоднозначність, включивши літеральні описи у вхідні текстові підказки нашої базової моделі. Виявляється, такий вид прямого контролю дає значне покращення продуктивності. На другому етапі ми інтегруємо візуальний нагляд у нашу систему за допомогою візуальних зображень, двох наборів зображень, створених за допомогою моделі тексту в зображення, використовуючи терміни та описи як вхідні дані. Наші експерименти демонструють, що візуальний контроль також дає статистично значуще підвищення продуктивності. Наша система посіла друге місце з результатом F1 87,2%, що лише приблизно на 0,9% гірше, ніж найкраще подання.",0
"В останні роки методи глибокого навчання вийшли на передній план у багатьох сферах, де потрібне дистанційне зондування, від медицини до сільського господарства, від оборонної промисловості до космічних досліджень; і ці методи досягли величезного успіху в порівнянні з традиційними методами. Разом зі значним зростанням доступних даних із високоякісними мітками та обчислювальними ресурсами ці глибокі архітектури й методи нейронних мереж зазнали значних змін. Основна відмінність між методами глибокого навчання та класичними методами розпізнавання полягає в тому, що методи глибокого навчання розглядають наскрізну схему навчання, яка дає початок функціям навчання з необроблених даних. Кращі методи регулярізації та надійні алгоритми оптимізації, запроваджені з найсучаснішими моделями глибокого навчання, є іншими факторами, що призводять до цієї різниці. У цій статті ми обговорюємо проблеми дистанційного зондування та те, як глибоке навчання можна використовувати для вирішення цих проблем, з особливим акцентом на медичні та дистанційні програми. Зокрема, ми коротко оглядаємо найкращі архітектури в літературі про глибоке навчання та приклади їх використання.","This paper describes our two-stage system1 for the Euphemism Detection shared task hosted by the 3rd Workshop on Figurative Language Processing in conjunction with EMNLP 2022. Euphemisms tone down expressions about sensitive or unpleasant issues like addiction and death. The ambiguous nature of euphemistic words or expressions makes it challenging to detect their actual meaning within a context. In the first stage, we seek to mitigate this ambiguity by incorporating literal descriptions into input text prompts to our baseline model. It turns out that this kind of direct supervision yields remarkable performance improvement. In the second stage, we integrate visual supervision into our system using visual imageries, two sets of images generated by a text-to-image model by taking terms and descriptions as input. Our experiments demonstrate that visual supervision also gives a statistically significant performance boost. Our system achieved the second place with an F1 score of 87.2%, only about 0.9% worse than the best submission.",0
"В останні роки методи глибокого навчання вийшли на передній план у багатьох сферах, де потрібне дистанційне зондування, від медицини до сільського господарства, від оборонної промисловості до космічних досліджень; і ці методи досягли величезного успіху в порівнянні з традиційними методами. Разом зі значним зростанням доступних даних із високоякісними мітками та обчислювальними ресурсами ці глибокі архітектури й методи нейронних мереж зазнали значних змін. Основна відмінність між методами глибокого навчання та класичними методами розпізнавання полягає в тому, що методи глибокого навчання розглядають наскрізну схему навчання, яка дає початок функціям навчання з необроблених даних. Кращі методи регулярізації та надійні алгоритми оптимізації, запроваджені з найсучаснішими моделями глибокого навчання, є іншими факторами, що призводять до цієї різниці. У цій статті ми обговорюємо проблеми дистанційного зондування та те, як глибоке навчання можна використовувати для вирішення цих проблем, з особливим акцентом на медичні та дистанційні програми. Зокрема, ми коротко оглядаємо найкращі архітектури в літературі про глибоке навчання та приклади їх використання.","У цьому документі описано нашу двоетапну систему1 для спільного завдання «Виявлення евфемізмів», організованого 3-м семінаром з обробки образної мови спільно з EMNLP 2022. Евфемізми пом’якшують вирази про чутливі або неприємні питання, такі як залежність і смерть. Неоднозначний характер евфемістичних слів або виразів ускладнює виявлення їх справжнього значення в контексті. На першому етапі ми намагаємося пом’якшити цю неоднозначність, включивши літеральні описи до вхідних текстових підказок нашої базової моделі. Виявляється, такий вид прямого контролю дає значне покращення продуктивності. На другому етапі ми інтегруємо візуальний нагляд у нашу систему за допомогою візуальних зображень, двох наборів зображень, створених за допомогою моделі тексту в зображення, використовуючи терміни та описи як вхідні дані. Наші експерименти демонструють, що візуальний контроль також дає статистично значуще підвищення продуктивності. Наша система посіла друге місце з результатом F1 87,2%, що лише на 0,9% гірше, ніж найкраще подання.",0
"In this study, we explore building a two-stage framework for enabling users to directly manipulate high-level attributes of a natural scene. The key to our approach is a deep generative network that can hallucinate images of a scene as if they were taken in a different season (e.g., during winter), weather condition (e.g., on a cloudy day), or at a different time of the day (e.g., at sunset). Once the scene is hallucinated with the given attributes, the corresponding look is then transferred to the input image while preserving the semantic details intact, giving a photo-realistic manipulation result. As the proposed framework hallucinates what the scene will look like, it does not require any reference style image as commonly utilized in most of the appearance or style transfer approaches. Moreover, it allows to simultaneously manipulate a given scene according to a diverse set of transient attributes within a single model, eliminating the need of training multiple networks per each translation task. Our comprehensive set of qualitative and quantitative results demonstrates the effectiveness of our approach against the competing methods.","In MediaEval 2016, we focus on the image interestingness subtask which involves predicting interesting key frames of a video in the form of a movie trailer. We specifically propose three different deep models for this subtask. The first two models are based on fine-tuning two pretrained models, namely AlexNet and MemNet, where we cast the interestingness prediction as a regression problem. Our third deep model, on the other hand, depends on a triplet network which is comprised of three instances of the same feedforward network with shared weights, and trained according to a triplet ranking loss. Our experiments demonstrate that all these models provide relatively similar and promising results on the image interestingness subtask.",0
"In this study, we explore building a two-stage framework for enabling users to directly manipulate high-level attributes of a natural scene. The key to our approach is a deep generative network that can hallucinate images of a scene as if they were taken in a different season (e.g., during winter), weather condition (e.g., on a cloudy day), or at a different time of the day (e.g., at sunset). Once the scene is hallucinated with the given attributes, the corresponding look is then transferred to the input image while preserving the semantic details intact, giving a photo-realistic manipulation result. As the proposed framework hallucinates what the scene will look like, it does not require any reference style image as commonly utilized in most of the appearance or style transfer approaches. Moreover, it allows to simultaneously manipulate a given scene according to a diverse set of transient attributes within a single model, eliminating the need of training multiple networks per each translation task. Our comprehensive set of qualitative and quantitative results demonstrates the effectiveness of our approach against the competing methods.","У MediaEval 2016 ми зосереджуємося на підзавданні щодо цікавості зображення, яке передбачає прогнозування цікавих ключових кадрів відео у формі трейлера фільму. Ми спеціально пропонуємо три різні глибокі моделі для цього підзавдання. Перші дві моделі базуються на тонкому налаштуванні двох попередньо навчених моделей, а саме AlexNet і MemNet, де ми використовуємо прогноз цікавості як проблему регресії. Наша третя глибока модель, з іншого боку, залежить від триплетної мережі, яка складається з трьох екземплярів тієї самої прямої мережі зі спільними вагами та навчена відповідно до втрати рейтингу триплетів. Наші експерименти демонструють, що всі ці моделі дають відносно схожі та багатообіцяючі результати щодо підзавдання щодо цікавості зображення.",0
"У цьому дослідженні ми досліджуємо побудову двоетапної структури, яка дозволяє користувачам безпосередньо маніпулювати високорівневими атрибутами природної сцени. Ключем до нашого підходу є глибока генеративна мережа, яка може галюцинувати зображення сцени, ніби вони були зроблені в інший сезон (наприклад, взимку), погодні умови (наприклад, у похмурий день) або в інший час день (наприклад, на заході сонця). Після галюцинації сцени з заданими атрибутами відповідний вигляд переноситься на вхідне зображення, зберігаючи семантичні деталі недоторканими, що дає фотореалістичний результат маніпуляції. Оскільки запропонований фреймворк показує, як буде виглядати сцена, він не потребує жодного еталонного стилю зображення, яке зазвичай використовується в більшості підходів до зовнішнього вигляду чи передачі стилю. Крім того, це дозволяє одночасно маніпулювати певною сценою відповідно до різноманітного набору тимчасових атрибутів у межах однієї моделі, усуваючи потребу в навчанні кількох мереж для кожного завдання перекладу. Наш комплексний набір якісних і кількісних результатів демонструє ефективність нашого підходу проти конкурентних методів.","In MediaEval 2016, we focus on the image interestingness subtask which involves predicting interesting key frames of a video in the form of a movie trailer. We specifically propose three different deep models for this subtask. The first two models are based on fine-tuning two pretrained models, namely AlexNet and MemNet, where we cast the interestingness prediction as a regression problem. Our third deep model, on the other hand, depends on a triplet network which is comprised of three instances of the same feedforward network with shared weights, and trained according to a triplet ranking loss. Our experiments demonstrate that all these models provide relatively similar and promising results on the image interestingness subtask.",0
"У цьому дослідженні ми досліджуємо побудову двоетапної структури, яка дозволяє користувачам безпосередньо маніпулювати високорівневими атрибутами природної сцени. Ключем до нашого підходу є глибока генеративна мережа, яка може галюцинувати зображення сцени, ніби вони були зроблені в інший сезон (наприклад, взимку), погодні умови (наприклад, у похмурий день) або в інший час день (наприклад, на заході сонця). Після галюцинації сцени з заданими атрибутами відповідний вигляд переноситься на вхідне зображення, зберігаючи семантичні деталі недоторканими, що дає фотореалістичний результат маніпуляції. Оскільки запропонований фреймворк показує, як буде виглядати сцена, він не потребує жодного еталонного стилю зображення, яке зазвичай використовується в більшості підходів до зовнішнього вигляду чи передачі стилю. Крім того, це дозволяє одночасно маніпулювати певною сценою відповідно до різноманітного набору тимчасових атрибутів у межах однієї моделі, усуваючи потребу в навчанні кількох мереж для кожного завдання перекладу. Наш комплексний набір якісних і кількісних результатів демонструє ефективність нашого підходу проти конкурентних методів.","У MediaEval 2016 ми зосереджуємося на підзавданні щодо цікавості зображення, яке передбачає прогнозування цікавих ключових кадрів відео у формі трейлера фільму. Ми спеціально пропонуємо три різні глибокі моделі для цього підзавдання. Перші дві моделі базуються на тонкому налаштуванні двох попередньо навчених моделей, а саме AlexNet і MemNet, де ми використовуємо прогноз цікавості як проблему регресії. Наша третя глибока модель, з іншого боку, залежить від триплетної мережі, яка складається з трьох екземплярів тієї самої прямої мережі зі спільними вагами та навчена відповідно до втрати рейтингу триплетів. Наші експерименти демонструють, що всі ці моделі дають відносно подібні та багатообіцяючі результати щодо підзавдання щодо цікавості зображення.",0
"In this paper, we address the problem of learning to summarize personal photo albums. That is, given a photo album, we aim to select a small set of representative images from the album so that the extracted summary captures most of the story that is being told through the images. More specifically, we extend a recently proposed recurrent neural network based framework by employing a more effective way to represent images and, more importantly, adding a diversity term to the main objective. Our diversity term is based on the idea of jointly training a discriminator network to evaluate the diversity of the selected images. This alleviates the issue of selecting near-duplicate or semantically similar images, which is the primary shortcoming of the base approach. The experimental results show that our improved model produces better or comparable summaries, providing a good balance between quality and diversity. "," In the past few years, automatically generating descriptions for images has attracted a lot of attention in computer vision and natural language processing research. Among the existing approaches, data-driven methods have been proven to be highly effective. These methods compare the given image against a large set of training images to determine a set of relevant images, then generate a description using the associated captions. In this study, the authors propose to integrate an objectbased semantic image representation into a deep features-based retrieval framework to select the relevant images. Moreover, they present a novel phrase selection paradigm and a sentence generation model which depends on a joint analysis of salient regions in the input and retrieved images within a clustering framework. The authors demonstrate the effectiveness of their proposed approach on Flickr8K and Flickr30K benchmark datasets and show that their model gives highly competitive results compared with the state-of-the-art models",0
"In this paper, we address the problem of learning to summarize personal photo albums. That is, given a photo album, we aim to select a small set of representative images from the album so that the extracted summary captures most of the story that is being told through the images. More specifically, we extend a recently proposed recurrent neural network based framework by employing a more effective way to represent images and, more importantly, adding a diversity term to the main objective. Our diversity term is based on the idea of jointly training a discriminator network to evaluate the diversity of the selected images. This alleviates the issue of selecting near-duplicate or semantically similar images, which is the primary shortcoming of the base approach. The experimental results show that our improved model produces better or comparable summaries, providing a good balance between quality and diversity. ","За останні кілька років автоматичне генерування описів для зображень привернуло багато уваги в дослідженнях комп’ютерного зору та обробки природної мови. Серед існуючих підходів методи, керовані даними, виявилися високоефективними. Ці методи порівнюють задане зображення з великим набором навчальних зображень, щоб визначити набір релевантних зображень, а потім генерують опис, використовуючи відповідні підписи. У цьому дослідженні автори пропонують інтегрувати об’єктно-орієнтоване представлення семантичного зображення в глибоку структуру пошуку на основі ознак, щоб вибрати відповідні зображення. Крім того, вони представляють нову парадигму вибору фраз і модель генерації речень, яка залежить від спільного аналізу помітних областей у вхідних і отриманих зображеннях у рамках кластеризації. Автори демонструють ефективність запропонованого ними підходу на базах даних Flickr8K і Flickr30K і показують, що їх модель дає висококонкурентні результати порівняно з найсучаснішими моделями",0
"У цій роботі ми торкнемося проблеми навчання узагальненню особистих фотоальбомів. Тобто, враховуючи фотоальбом, ми прагнемо вибрати невеликий набір репрезентативних зображень з альбому, щоб витягнутий підсумок охоплював більшу частину історії, яка розповідається через зображення. Зокрема, ми розширюємо нещодавно запропоновану структуру на основі рекурентної нейронної мережі, використовуючи більш ефективний спосіб представлення зображень і, що більш важливо, додаючи термін різноманітності до основної мети. Наш термін різноманітності базується на ідеї спільного навчання мережі дискримінаторів для оцінки різноманітності вибраних зображень. Це полегшує проблему вибору майже повторюваних або семантично схожих зображень, що є основним недоліком базового підходу. Експериментальні результати показують, що наша вдосконалена модель створює кращі або порівнювані підсумки, забезпечуючи хороший баланс між якістю та різноманітністю."," In the past few years, automatically generating descriptions for images has attracted a lot of attention in computer vision and natural language processing research. Among the existing approaches, data-driven methods have been proven to be highly effective. These methods compare the given image against a large set of training images to determine a set of relevant images, then generate a description using the associated captions. In this study, the authors propose to integrate an objectbased semantic image representation into a deep features-based retrieval framework to select the relevant images. Moreover, they present a novel phrase selection paradigm and a sentence generation model which depends on a joint analysis of salient regions in the input and retrieved images within a clustering framework. The authors demonstrate the effectiveness of their proposed approach on Flickr8K and Flickr30K benchmark datasets and show that their model gives highly competitive results compared with the state-of-the-art models",0
"У цій роботі ми торкнемося проблеми навчання узагальненню особистих фотоальбомів. Тобто, враховуючи фотоальбом, ми прагнемо вибрати невеликий набір репрезентативних зображень з альбому, щоб витягнутий підсумок охоплював більшу частину історії, яка розповідається через зображення. Зокрема, ми розширюємо нещодавно запропоновану структуру на основі рекурентної нейронної мережі, використовуючи більш ефективний спосіб представлення зображень і, що більш важливо, додаючи термін різноманітності до основної мети. Наш термін різноманітності базується на ідеї спільного навчання мережі дискримінаторів для оцінки різноманітності вибраних зображень. Це полегшує проблему вибору майже повторюваних або семантично схожих зображень, що є основним недоліком базового підходу. Експериментальні результати показують, що наша вдосконалена модель створює кращі або порівнювані підсумки, забезпечуючи хороший баланс між якістю та різноманітністю.","За останні кілька років автоматичне генерування описів для зображень привернуло багато уваги в дослідженнях комп’ютерного зору та обробки природної мови. Серед існуючих підходів методи, керовані даними, виявилися високоефективними. Ці методи порівнюють задане зображення з великим набором навчальних зображень, щоб визначити набір релевантних зображень, а потім генерують опис, використовуючи відповідні підписи. У цьому дослідженні автори пропонують інтегрувати об’єктно-орієнтоване представлення семантичного зображення в глибоку структуру пошуку на основі ознак, щоб вибрати відповідні зображення. Крім того, вони представляють нову парадигму вибору фраз і модель генерації речень, яка залежить від спільного аналізу помітних областей у вхідних і отриманих зображеннях у рамках кластеризації. Автори демонструють ефективність запропонованого ними підходу на базах даних Flickr8K і Flickr30K і показують, що їх модель дає висококонкурентні результати порівняно з найсучаснішими моделями",0
"This paper addresses the problem of comprehending procedural commonsense knowledge. This is a challenging task as it requires identifying key entities, keeping track of their state changes, and understanding temporal and causal relations. Contrary to most of the previous work, in this study, we do not rely on strong inductive bias and explore the question of how multimodality can be exploited to provide a complementary semantic signal. Towards this end, we introduce a new entity-aware neural comprehension model augmented with external relational memory units. Our model learns to dynamically update entity states in relation to each other while reading the text instructions. Our experimental analysis on the visual reasoning tasks in the recently proposed RecipeQA dataset reveals that our approach improves the accuracy of the previously reported models by a large margin. Moreover, we find that our model learns effective dynamic representations of entities even though we do not use any supervision at the level of entity states.1","Developing artificial learning systems that can understand and generate natural language has been one of the long-standing goals of artificial intelligence. Recent decades have witnessed an impressive progress on both of these problems, giving rise to a new family of approaches. Especially, the advances in deep learning over the past couple of years have led to neural approaches to natural language generation (NLG). These methods combine generative language learning techniques with neural-networks based frameworks. With a wide range of applications in natural language processing, neural NLG (NNLG) is a new and fast growing field of research. In this state-of-the-art report, we investigate the recent developments and applications of NNLG in its full extent from a multidimensional view, covering critical perspectives such as multimodality, multilinguality, controllability and learning strategies. We summarize the fundamental building blocks of NNLG approaches from these aspects and provide detailed reviews of commonly used preprocessing steps and basic neural architectures. This report also focuses on the seminal applications of these NNLG models such as machine translation, description generation, automatic speech recognition, abstractive summarization, text simplification, question answering and generation, and dialogue generation. Finally, we conclude with a thorough discussion of the described frameworks by pointing out some open research directions. ",0
"This paper addresses the problem of comprehending procedural commonsense knowledge. This is a challenging task as it requires identifying key entities, keeping track of their state changes, and understanding temporal and causal relations. Contrary to most of the previous work, in this study, we do not rely on strong inductive bias and explore the question of how multimodality can be exploited to provide a complementary semantic signal. Towards this end, we introduce a new entity-aware neural comprehension model augmented with external relational memory units. Our model learns to dynamically update entity states in relation to each other while reading the text instructions. Our experimental analysis on the visual reasoning tasks in the recently proposed RecipeQA dataset reveals that our approach improves the accuracy of the previously reported models by a large margin. Moreover, we find that our model learns effective dynamic representations of entities even though we do not use any supervision at the level of entity states.1","Розробка систем штучного навчання, які можуть розуміти та генерувати природну мову, була однією з давніх цілей штучного інтелекту. Останні десятиліття стали свідками вражаючого прогресу в обох цих проблемах, що породило нове сімейство підходів. Зокрема, прогрес у глибокому навчанні за останні пару років призвів до нейронних підходів до створення природної мови (NLG). Ці методи поєднують методи генеративного вивчення мови з нейронними мережами. Завдяки широкому спектру застосувань у обробці природної мови, нейронна NLG (NNLG) є новою галуззю досліджень, яка швидко розвивається. У цьому сучасному звіті ми досліджуємо останні розробки та застосування NNLG у повному обсязі з багатовимірного погляду, охоплюючи важливі перспективи, такі як мультимодальність, багатомовність, керованість і стратегії навчання. Ми підсумовуємо фундаментальні будівельні блоки підходів NNLG з цих аспектів і надаємо детальні огляди поширених етапів попередньої обробки та основних нейронних архітектур. У цьому звіті також зосереджено увагу на основних застосуваннях цих моделей NNLG, таких як машинний переклад, генерація описів, автоматичне розпізнавання мовлення, абстрактне резюмування, спрощення тексту, відповіді на запитання та генерація, а також генерація діалогів. Нарешті, ми закінчуємо детальним обговоренням описаних структур, вказуючи на деякі відкриті напрямки дослідження.",0
"У цій статті розглядається проблема розуміння процедурних знань здорового глузду. Це складне завдання, оскільки вимагає визначення ключових об’єктів, відстеження змін їхнього стану та розуміння часових і причинно-наслідкових зв’язків. На відміну від більшості попередніх робіт, у цьому дослідженні ми не покладаємося на сильне індуктивне упередження та досліджуємо питання про те, як мультимодальність можна використовувати для надання додаткового семантичного сигналу. З цією метою ми представляємо нову модель нейронного розуміння з урахуванням сутностей, доповнену блоками зовнішньої реляційної пам’яті. Наша модель вчиться динамічно оновлювати стани сутностей по відношенню один до одного під час читання текстових інструкцій. Наш експериментальний аналіз завдань візуального мислення в нещодавно запропонованому наборі даних RecipeQA показує, що наш підхід значно покращує точність раніше повідомлених моделей. Крім того, ми виявили, що наша модель вивчає ефективні динамічні представлення сутностей, навіть якщо ми не використовуємо жодного контролю на рівні станів сутності.1","Developing artificial learning systems that can understand and generate natural language has been one of the long-standing goals of artificial intelligence. Recent decades have witnessed an impressive progress on both of these problems, giving rise to a new family of approaches. Especially, the advances in deep learning over the past couple of years have led to neural approaches to natural language generation (NLG). These methods combine generative language learning techniques with neural-networks based frameworks. With a wide range of applications in natural language processing, neural NLG (NNLG) is a new and fast growing field of research. In this state-of-the-art report, we investigate the recent developments and applications of NNLG in its full extent from a multidimensional view, covering critical perspectives such as multimodality, multilinguality, controllability and learning strategies. We summarize the fundamental building blocks of NNLG approaches from these aspects and provide detailed reviews of commonly used preprocessing steps and basic neural architectures. This report also focuses on the seminal applications of these NNLG models such as machine translation, description generation, automatic speech recognition, abstractive summarization, text simplification, question answering and generation, and dialogue generation. Finally, we conclude with a thorough discussion of the described frameworks by pointing out some open research directions. ",0
"У цій статті розглядається проблема розуміння процедурних знань здорового глузду. Це складне завдання, оскільки вимагає визначення ключових об’єктів, відстеження змін їхнього стану та розуміння часових і причинно-наслідкових зв’язків. На відміну від більшості попередніх робіт, у цьому дослідженні ми не покладаємося на сильне індуктивне упередження та досліджуємо питання про те, як мультимодальність можна використовувати для надання додаткового семантичного сигналу. З цією метою ми представляємо нову модель нейронного розуміння з урахуванням сутностей, доповнену блоками зовнішньої реляційної пам’яті. Наша модель вчиться динамічно оновлювати стани сутностей по відношенню один до одного під час читання текстових інструкцій. Наш експериментальний аналіз завдань візуального мислення в нещодавно запропонованому наборі даних RecipeQA показує, що наш підхід значно покращує точність раніше повідомлених моделей. Крім того, ми виявили, що наша модель вивчає ефективні динамічні представлення сутностей, навіть якщо ми не використовуємо жодного контролю на рівні станів сутності.1","Розробка систем штучного навчання, які можуть розуміти та генерувати природну мову, була однією з давніх цілей штучного інтелекту. Останні десятиліття стали свідками вражаючого прогресу в обох цих проблемах, що породило нове сімейство підходів. Зокрема, прогрес у глибокому навчанні за останні пару років призвів до нейронних підходів до створення природної мови (NLG). Ці методи поєднують методи генеративного вивчення мови з нейронними мережами. Завдяки широкому спектру застосувань у обробці природної мови, нейронні NLG (NNLG) є новою галуззю досліджень, яка швидко розвивається. У цьому сучасному звіті ми досліджуємо останні розробки та застосування NNLG у повному обсязі з багатовимірного погляду, охоплюючи важливі перспективи, такі як мультимодальність, багатомовність, керованість і стратегії навчання. Ми підсумовуємо фундаментальні будівельні блоки підходів NNLG з цих аспектів і надаємо детальні огляди поширених етапів попередньої обробки та основних нейронних архітектур. У цьому звіті також зосереджено увагу на основних застосуваннях цих моделей NNLG, таких як машинний переклад, генерація описів, автоматичне розпізнавання мовлення, абстрактне резюмування, спрощення тексту, відповіді на запитання та генерація, а також генерація діалогів. Нарешті, ми закінчуємо детальним обговоренням описаних структур, вказуючи на деякі відкриті напрямки дослідження.",0
"Acquiring images of the same anatomy with multiple different contrasts increases the diversity of diagnostic information available in an MR exam. Yet, the scan time limitations may prohibit the acquisition of certain contrasts, and some contrasts may be corrupted by noise and artifacts. In such cases, the ability to synthesize unacquired or corrupted contrasts can improve diagnostic utility. For multi-contrast synthesis, the current methods learn a nonlinear intensity transformation between the source and target images, either via nonlinear regression or deterministic neural networks. These methods can, in turn, suffer from the loss of structural details in synthesized images. Here, in this paper, we propose a new approach for multi-contrast MRI synthesis based on conditional generative adversarial networks. The proposed approach preserves intermediate-to-high frequency details via an adversarial loss, and it offers enhanced synthesis performance via pixel-wise and perceptual losses for registered multi-contrast images and a cycle-consistency loss for unregistered images. Information from neighboring cross-sections are utilized to further improve synthesis quality. Demonstrations on T1 - and T2- weighted images from healthy subjects and patients clearly indicate the superior performance of the proposed approach compared to the previous state-of-the-art methods. Our synthesis approach can help improve the quality and versatility of the multi-contrast MRI exams without the need for prolonged or repeated examinations.","While stochastic video prediction models enable future prediction under uncertainty, they mostly fail to model the complex dynamics of real-world scenes. For example, they cannot provide reliable predictions for scenes with a moving camera and independently moving foreground objects in driving scenarios. The existing methods fail to fully capture the dynamics of the structured world by only focusing on changes in pixels. In this paper, we assume that there is an underlying process creating observations in a video and propose to factorize it into static and dynamic components. We model the static part based on the scene structure and the ego-motion of the vehicle, and the dynamic part based on the remaining motion of the dynamic objects. By learning separate distributions of changes in foreground and background, we can decompose the scene into static and dynamic parts and separately model the change in each. Our experiments demonstrate that disentangling structure and motion helps stochastic video prediction, leading to better future predictions in complex driving scenarios on two real-world driving datasets, KITTI and Cityscapes. ",0
"Acquiring images of the same anatomy with multiple different contrasts increases the diversity of diagnostic information available in an MR exam. Yet, the scan time limitations may prohibit the acquisition of certain contrasts, and some contrasts may be corrupted by noise and artifacts. In such cases, the ability to synthesize unacquired or corrupted contrasts can improve diagnostic utility. For multi-contrast synthesis, the current methods learn a nonlinear intensity transformation between the source and target images, either via nonlinear regression or deterministic neural networks. These methods can, in turn, suffer from the loss of structural details in synthesized images. Here, in this paper, we propose a new approach for multi-contrast MRI synthesis based on conditional generative adversarial networks. The proposed approach preserves intermediate-to-high frequency details via an adversarial loss, and it offers enhanced synthesis performance via pixel-wise and perceptual losses for registered multi-contrast images and a cycle-consistency loss for unregistered images. Information from neighboring cross-sections are utilized to further improve synthesis quality. Demonstrations on T1 - and T2- weighted images from healthy subjects and patients clearly indicate the superior performance of the proposed approach compared to the previous state-of-the-art methods. Our synthesis approach can help improve the quality and versatility of the multi-contrast MRI exams without the need for prolonged or repeated examinations.","Хоча стохастичні моделі прогнозування відео дозволяють прогнозувати майбутнє в умовах невизначеності, вони здебільшого не в змоделі складної динаміки сцен реального світу. Наприклад, вони не можуть забезпечити надійні прогнози для сцен із рухомою камерою та незалежно рухомими об’єктами переднього плану в сценаріях водіння. Існуючі методи не можуть повністю охопити динаміку структурованого світу, зосереджуючись лише на змінах у пікселях. У цьому документі ми припускаємо, що існує базовий процес створення спостережень у відео, і пропонуємо розділити його на статичні та динамічні компоненти. Ми моделюємо статичну частину на основі структури сцени та его-руху автомобіля, а динамічну частину – на основі залишкового руху динамічних об’єктів. Вивчаючи окремі розподіли змін переднього та заднього планів, ми можемо розкласти сцену на статичні та динамічні частини та окремо моделювати зміни в кожній. Наші експерименти демонструють, що розмежування структури та руху сприяє стохастичному прогнозуванню відео, що дає змогу краще прогнозувати майбутнє у складних сценаріях водіння на двох наборах даних реального водіння, KITTI та Cityscapes.",0
"Отримання зображень однієї анатомії з кількома різними контрастами збільшує різноманітність діагностичної інформації, доступної під час МРТ-дослідження. Проте обмеження часу сканування можуть забороняти отримання певних контрастів, а деякі контрасти можуть бути пошкоджені шумом і артефактами. У таких випадках здатність синтезувати неотримані або пошкоджені контрасти може покращити діагностичну корисність. Для багатоконтрастного синтезу поточні методи вивчають нелінійне перетворення інтенсивності між вихідним і цільовим зображеннями за допомогою нелінійної регресії або детермінованих нейронних мереж. Ці методи, у свою чергу, можуть страждати від втрати структурних деталей у синтезованих зображеннях. Тут, у цій статті, ми пропонуємо новий підхід для синтезу мультиконтрастної МРТ на основі умовних генеративних змагальних мереж. Запропонований підхід зберігає проміжні та високі частотні деталі через суперечливі втрати та пропонує покращену продуктивність синтезу через піксельні та перцептивні втрати для зареєстрованих багатоконтрастних зображень і втрату узгодженості циклу для незареєстрованих зображень. Інформація з сусідніх перерізів використовується для подальшого покращення якості синтезу. Демонстрації на T1- і T2-зважених зображеннях здорових суб&#39;єктів і пацієнтів чітко вказують на кращу продуктивність запропонованого підходу порівняно з попередніми сучасними методами. Наш підхід до синтезу може допомогти покращити якість і універсальність багатоконтрастних МРТ-досліджень без необхідності тривалих або повторних обстежень.","While stochastic video prediction models enable future prediction under uncertainty, they mostly fail to model the complex dynamics of real-world scenes. For example, they cannot provide reliable predictions for scenes with a moving camera and independently moving foreground objects in driving scenarios. The existing methods fail to fully capture the dynamics of the structured world by only focusing on changes in pixels. In this paper, we assume that there is an underlying process creating observations in a video and propose to factorize it into static and dynamic components. We model the static part based on the scene structure and the ego-motion of the vehicle, and the dynamic part based on the remaining motion of the dynamic objects. By learning separate distributions of changes in foreground and background, we can decompose the scene into static and dynamic parts and separately model the change in each. Our experiments demonstrate that disentangling structure and motion helps stochastic video prediction, leading to better future predictions in complex driving scenarios on two real-world driving datasets, KITTI and Cityscapes. ",0
"Отримання зображень однієї анатомії з кількома різними контрастами збільшує різноманітність діагностичної інформації, доступної під час МРТ-дослідження. Проте обмеження часу сканування можуть забороняти отримання певних контрастів, а деякі контрасти можуть бути зіпсовані шумом і артефактами. У таких випадках здатність синтезувати неотримані або пошкоджені контрасти може покращити діагностичну корисність. Для багатоконтрастного синтезу поточні методи вивчають нелінійне перетворення інтенсивності між вихідним і цільовим зображеннями за допомогою нелінійної регресії або детермінованих нейронних мереж. Ці методи, у свою чергу, можуть страждати від втрати структурних деталей у синтезованих зображеннях. Тут, у цій статті, ми пропонуємо новий підхід для синтезу мультиконтрастної МРТ на основі умовних генеративних змагальних мереж. Запропонований підхід зберігає проміжні та високі частотні деталі через суперечливі втрати та пропонує покращену продуктивність синтезу через піксельні та перцептивні втрати для зареєстрованих багатоконтрастних зображень і втрату узгодженості циклу для незареєстрованих зображень. Інформація з сусідніх перерізів використовується для подальшого покращення якості синтезу. Демонстрації на T1- і T2-зважених зображеннях здорових суб&#39;єктів і пацієнтів чітко вказують на кращу продуктивність запропонованого підходу порівняно з попередніми сучасними методами. Наш підхід до синтезу може допомогти покращити якість і універсальність багатоконтрастних МРТ-досліджень без необхідності тривалих або повторних обстежень.","Хоча стохастичні моделі прогнозування відео дозволяють прогнозувати майбутнє в умовах невизначеності, вони здебільшого не можуть моделювати складну динаміку сцен реального світу. Наприклад, вони не можуть забезпечити надійні прогнози для сцен із рухомою камерою та незалежно рухомими об’єктами переднього плану в сценаріях водіння. Існуючі методи не можуть повністю охопити динаміку структурованого світу, зосереджуючись лише на змінах у пікселях. У цьому документі ми припускаємо, що існує базовий процес створення спостережень у відео, і пропонуємо розділити його на статичні та динамічні компоненти. Ми моделюємо статичну частину на основі структури сцени та его-руху автомобіля, а динамічну частину – на основі залишкового руху динамічних об’єктів. Вивчаючи окремі розподіли змін переднього та заднього планів, ми можемо розкласти сцену на статичні та динамічні частини та окремо моделювати зміни в кожній. Наші експерименти демонструють, що розмежування структури та руху сприяє стохастичному прогнозуванню відео, що дає змогу краще прогнозувати майбутнє у складних сценаріях водіння на двох наборах даних реального водіння, KITTI та Cityscapes.",0
"Understanding and reasoning about cooking recipes is a fruitful research direction towards enabling machines to interpret procedural text. In this work, we introduce RecipeQA, a dataset for multimodal comprehension of cooking recipes. It comprises of approximately 20K instructional recipes with multiple modalities such as titles, descriptions and aligned set of images. With over 36K automatically generated question-answer pairs, we design a set of comprehension and reasoning tasks that require joint understanding of images and text, capturing the temporal flow of events and making sense of procedural knowledge. Our preliminary results indicate that RecipeQA will serve as a challenging test bed and an ideal benchmark for evaluating machine comprehension systems.","We present a simple and robust feature preserving image regularization by letting local region measures modulate the diffusivity. The purpose of this modulation is to disambiguate low level cues in early vision. We interpret the Ambrosio-Tortorelli approximation of the Mumford-Shah model as a system with modulatory feedback and utilize this interpretation to integrate high level information into the regularization process. The method does not require any prior model or learning; the high level information is extracted from local regions and fed back to the regularization step. An important characteristic of the method is that both negative and positive feedback can be simultaneously used without creating oscillations. Experiments performed with both gray and color natural images demonstrate the potential of the method under difficult noise types, non-uniform contrast, existence of multi-scale patterns and textures.",0
"Understanding and reasoning about cooking recipes is a fruitful research direction towards enabling machines to interpret procedural text. In this work, we introduce RecipeQA, a dataset for multimodal comprehension of cooking recipes. It comprises of approximately 20K instructional recipes with multiple modalities such as titles, descriptions and aligned set of images. With over 36K automatically generated question-answer pairs, we design a set of comprehension and reasoning tasks that require joint understanding of images and text, capturing the temporal flow of events and making sense of procedural knowledge. Our preliminary results indicate that RecipeQA will serve as a challenging test bed and an ideal benchmark for evaluating machine comprehension systems.","Ми представляємо просту та надійну функцію, яка зберігає регулярізацію зображення, дозволяючи вимірюванням локальних регіонів модулювати дифузію. Метою цієї модуляції є усунення неоднозначності сигналів низького рівня в ранньому баченні. Ми інтерпретуємо наближення Амбросіо-Тортореллі моделі Мамфорда-Шаха як систему з модулюючим зворотним зв&#39;язком і використовуємо цю інтерпретацію для інтеграції інформації високого рівня в процес регулярізації. Метод не потребує попередньої моделі чи навчання; інформація високого рівня витягується з локальних регіонів і повертається на етап регулярізації. Важливою характеристикою методу є те, що як негативний, так і позитивний зворотний зв&#39;язок можуть використовуватися одночасно без створення коливань. Експерименти, проведені як з сірими, так і з кольоровими природними зображеннями, демонструють потенціал методу в умовах складних типів шуму, нерівномірного контрасту, наявності багатомасштабних візерунків і текстур.",0
"Розуміння та обґрунтування кулінарних рецептів є плідним напрямком дослідження, спрямованим на те, щоб дозволити машинам інтерпретувати процедурний текст. У цій роботі ми представляємо RecipeQA, набір даних для мультимодального розуміння кулінарних рецептів. Він складається з приблизно 20 тисяч навчальних рецептів із різними модальностями, такими як заголовки, описи та вирівняний набір зображень. Маючи понад 36 тисяч автоматично згенерованих пар запитань-відповідей, ми розробляємо набір завдань на розуміння й міркування, які вимагають спільного розуміння зображень і тексту, фіксації тимчасового потоку подій і осмислення процедурних знань. Наші попередні результати вказують на те, що RecipeQA слугуватиме складним випробувальним майданчиком і ідеальним еталоном для оцінки систем машинного розуміння.","We present a simple and robust feature preserving image regularization by letting local region measures modulate the diffusivity. The purpose of this modulation is to disambiguate low level cues in early vision. We interpret the Ambrosio-Tortorelli approximation of the Mumford-Shah model as a system with modulatory feedback and utilize this interpretation to integrate high level information into the regularization process. The method does not require any prior model or learning; the high level information is extracted from local regions and fed back to the regularization step. An important characteristic of the method is that both negative and positive feedback can be simultaneously used without creating oscillations. Experiments performed with both gray and color natural images demonstrate the potential of the method under difficult noise types, non-uniform contrast, existence of multi-scale patterns and textures.",0
"Розуміння та обґрунтування кулінарних рецептів є плідним напрямком дослідження, спрямованим на те, щоб дозволити машинам інтерпретувати процедурний текст. У цій роботі ми представляємо RecipeQA, набір даних для мультимодального розуміння кулінарних рецептів. Він складається з приблизно 20 тисяч навчальних рецептів із різними модальностями, такими як заголовки, описи та вирівняний набір зображень. Маючи понад 36 тисяч автоматично згенерованих пар запитань-відповідей, ми розробляємо набір завдань на розуміння й міркування, які вимагають спільного розуміння зображень і тексту, фіксації тимчасового потоку подій і осмислення процедурних знань. Наші попередні результати вказують на те, що RecipeQA слугуватиме складним випробувальним майданчиком і ідеальним еталоном для оцінки систем машинного розуміння.","Ми представляємо просту та надійну функцію, яка зберігає регулярність зображення, дозволяючи вимірюванням локальних регіонів модулювати дифузію. Метою цієї модуляції є усунення неоднозначності сигналів низького рівня в ранньому баченні. Ми інтерпретуємо наближення Амбросіо-Тортореллі моделі Мамфорда-Шаха як систему з модулюючим зворотним зв&#39;язком і використовуємо цю інтерпретацію для інтеграції інформації високого рівня в процес регулярізації. Метод не потребує попередньої моделі чи навчання; інформація високого рівня витягується з локальних регіонів і повертається на етап регулярізації. Важливою характеристикою методу є те, що як негативний, так і позитивний зворотний зв&#39;язок можуть використовуватися одночасно без створення коливань. Експерименти, проведені як з сірими, так і з кольоровими природними зображеннями, демонструють потенціал методу в умовах складних типів шуму, нерівномірного контрасту, наявності багатомасштабних візерунків і текстур.",0
"Developing techniques for editing an outfit image through natural sentences and accordingly generating new outfits has promising applications for art, fashion and design. However, it is considered as a certainly challenging task since image manipulation should be carried out only on the relevant parts of the image while keeping the remaining sections untouched. Moreover, this manipulation process should generate an image that is as realistic as possible. In this work, we propose FiLMedGAN, which leverages feature-wise linear modulation (FiLM) to relate and transform visual features with natural language representations without using extra spatial information. Our experiments demonstrate that this approach, when combined with skip connections and total variation regularization, produces more plausible results than the baseline work, and has a better localization capability when generating new outfits consistent with the target description.","Pre-trained language models have been shown to improve performance in many natural language tasks substantially. Although the early focus of such models was single language pre-training, recent advances have resulted in cross-lingual and visual pre-training methods. In this paper, we combine these two approaches to learn visually-grounded crosslingual representations. Specifically, we extend the translation language modelling (Lample and Conneau, 2019) with masked region classification and perform pre-training with three-way parallel vision & language corpora. We show that when fine-tuned for multimodal machine translation, these models obtain stateof-the-art performance. We also provide qualitative insights into the usefulness of the learned grounded representations.",0
"Developing techniques for editing an outfit image through natural sentences and accordingly generating new outfits has promising applications for art, fashion and design. However, it is considered as a certainly challenging task since image manipulation should be carried out only on the relevant parts of the image while keeping the remaining sections untouched. Moreover, this manipulation process should generate an image that is as realistic as possible. In this work, we propose FiLMedGAN, which leverages feature-wise linear modulation (FiLM) to relate and transform visual features with natural language representations without using extra spatial information. Our experiments demonstrate that this approach, when combined with skip connections and total variation regularization, produces more plausible results than the baseline work, and has a better localization capability when generating new outfits consistent with the target description.","Було показано, що попередньо підготовлені мовні моделі суттєво покращують продуктивність у багатьох завданнях природної мови. Незважаючи на те, що на ранніх етапах такі моделі зосереджувалися на одній мовній підготовці, останні досягнення призвели до міжмовних і візуальних методів попередньої підготовки. У цій статті ми поєднуємо ці два підходи, щоб вивчити візуально обґрунтовані міжмовні уявлення. Зокрема, ми розширюємо моделювання мови перекладу (Lample and Conneau, 2019) за допомогою класифікації замаскованих регіонів і виконуємо попереднє навчання за допомогою тристороннього паралельного бачення та мовних корпусів. Ми показуємо, що після точного налаштування для мультимодального машинного перекладу ці моделі отримують найсучаснішу продуктивність. Ми також надаємо якісне уявлення про корисність вивчених обґрунтованих уявлень.",0
"Розробка методів редагування зображення вбрання за допомогою природних речень і відповідно генерування нових вбрань має перспективні застосування для мистецтва, моди та дизайну. Однак це вважається, безумовно, складним завданням, оскільки маніпуляції із зображенням слід виконувати лише на відповідних частинах зображення, залишаючи решту ділянок недоторканими. Більше того, цей процес маніпуляції має створити зображення, яке є максимально реалістичним. У цій роботі ми пропонуємо FiLMedGAN, який використовує функціональну лінійну модуляцію (FiLM), щоб зв’язати та трансформувати візуальні функції з представленнями природної мови без використання додаткової просторової інформації. Наші експерименти демонструють, що цей підхід у поєднанні з пропускаючими з’єднаннями та регуляризацією повної варіації дає більш вірогідні результати, ніж базова робота, і має кращу здатність до локалізації під час створення нових комплектів, що відповідають цільовому опису.","Pre-trained language models have been shown to improve performance in many natural language tasks substantially. Although the early focus of such models was single language pre-training, recent advances have resulted in cross-lingual and visual pre-training methods. In this paper, we combine these two approaches to learn visually-grounded crosslingual representations. Specifically, we extend the translation language modelling (Lample and Conneau, 2019) with masked region classification and perform pre-training with three-way parallel vision & language corpora. We show that when fine-tuned for multimodal machine translation, these models obtain stateof-the-art performance. We also provide qualitative insights into the usefulness of the learned grounded representations.",0
"Розробка методів редагування зображення вбрання за допомогою природних речень і відповідно генерування нових вбрань має перспективні застосування для мистецтва, моди та дизайну. Однак це вважається, безумовно, складним завданням, оскільки маніпуляції із зображенням слід виконувати лише на відповідних частинах зображення, залишаючи решту ділянок недоторканими. Більше того, цей процес маніпуляції має створити зображення, яке є максимально реалістичним. У цій роботі ми пропонуємо FiLMedGAN, який використовує функціональну лінійну модуляцію (FiLM), щоб зв’язати та трансформувати візуальні функції з представленнями природної мови без використання додаткової просторової інформації. Наші експерименти демонструють, що цей підхід у поєднанні з пропускаючими з’єднаннями та регуляризацією повної варіації дає більш вірогідні результати, ніж базова робота, і має кращу здатність до локалізації під час створення нових комплектів, що відповідають цільовому опису.","Було показано, що попередньо підготовлені мовні моделі суттєво покращують продуктивність у багатьох завданнях природної мови. Незважаючи на те, що на ранніх етапах такі моделі зосереджувалися на одній мовній підготовці, останні досягнення призвели до міжмовних і візуальних методів попередньої підготовки. У цій статті ми поєднуємо ці два підходи, щоб вивчити візуально обґрунтовані міжмовні репрезентації. Зокрема, ми розширюємо моделювання мови перекладу (Lample and Conneau, 2019) за допомогою класифікації замаскованих регіонів і проводимо попереднє навчання за допомогою тристороннього паралельного бачення та мовних корпусів. Ми показуємо, що після точного налаштування для мультимодального машинного перекладу ці моделі отримують найсучаснішу продуктивність. Ми також надаємо якісне розуміння корисності вивчених обґрунтованих уявлень.",0
"Moving object detection is an imperative task in computer vision, where it is primarily used for surveillance applications. With the increasing availability of low-altitude aerial vehicles, new challenges for moving object detection have surfaced, both for academia and industry. In this paper, we propose a new approach that can detect moving objects efficiently and handle parallax cases. By introducing sparse flow based parallax handling and downscale processing, we push the boundaries of real-time performance with 16 FPS on limited embedded resources (a five-fold improvement over existing baselines), while managing to perform comparably or even improve the state-of-the-art in two different datasets. We also present a roadmap for extending our approach to exploit multi-modal data in order to mitigate the need for parameter tuning.","Moving Object Detection is one of the integral tasks for aerial reconnaissance and surveillance applications. Despite the problem’s rising potential due to increasing availability of unmanned aerial vehicles, moving object detection suffers from a lack of widely-accepted, correctly labelled dataset that would facilitate a robust evaluation of the techniques published by the community. Towards this end, we compile a new dataset by manually annotating several sequences from VIVID and UAV123 datasets for moving object detection. We also propose a feature-based, efficient pipeline that is optimized for near real-time performance on GPU-based embedded SoMs (system on module). We evaluate our pipeline on this extended dataset for low altitude moving object detection. Ground-truth annotations are made publicly available to the community to foster further research in moving object detection field. ",0
"Moving object detection is an imperative task in computer vision, where it is primarily used for surveillance applications. With the increasing availability of low-altitude aerial vehicles, new challenges for moving object detection have surfaced, both for academia and industry. In this paper, we propose a new approach that can detect moving objects efficiently and handle parallax cases. By introducing sparse flow based parallax handling and downscale processing, we push the boundaries of real-time performance with 16 FPS on limited embedded resources (a five-fold improvement over existing baselines), while managing to perform comparably or even improve the state-of-the-art in two different datasets. We also present a roadmap for extending our approach to exploit multi-modal data in order to mitigate the need for parameter tuning.","Виявлення рухомих об&#39;єктів є одним із невід&#39;ємних завдань для програм повітряної розвідки та спостереження. Незважаючи на зростаючий потенціал проблеми через збільшення доступності безпілотних літальних апаратів, виявлення рухомих об’єктів страждає від відсутності загальноприйнятого, правильно позначеного набору даних, який би полегшив надійну оцінку методів, опублікованих спільнотою. З цією метою ми збираємо новий набір даних, вручну анотуючи кілька послідовностей із наборів даних VIVID і UAV123 для виявлення рухомих об’єктів. Ми також пропонуємо ефективний конвеєр на основі функцій, який оптимізований для роботи майже в реальному часі на вбудованих SoM на основі GPU (система на модулі). Ми оцінюємо наш конвеєр на цьому розширеному наборі даних для виявлення рухомих об’єктів на малій висоті. Наземні правдиві анотації стають загальнодоступними для спільноти, щоб сприяти подальшим дослідженням у сфері виявлення рухомих об’єктів.",0
"Виявлення рухомих об&#39;єктів є обов&#39;язковим завданням комп&#39;ютерного зору, де воно в основному використовується для програм спостереження. Зі збільшенням доступності літальних апаратів на низькій висоті виникли нові виклики для виявлення рухомих об’єктів як для наукових кіл, так і для промисловості. У цій статті ми пропонуємо новий підхід, який може ефективно виявляти рухомі об’єкти та обробляти випадки паралакса. Запроваджуючи обробку паралакса на основі розрідженого потоку та обробку з низьким масштабом, ми розширюємо межі продуктивності в реальному часі з 16 кадрами в секунду на обмежених вбудованих ресурсах (п’ятикратне покращення порівняно з існуючими базовими показниками), водночас зуміючи досягти порівнянної продуктивності або навіть покращити стан -the-art у двох різних наборах даних. Ми також представляємо дорожню карту для розширення нашого підходу до використання мультимодальних даних, щоб зменшити потребу в налаштуванні параметрів.","Moving Object Detection is one of the integral tasks for aerial reconnaissance and surveillance applications. Despite the problem’s rising potential due to increasing availability of unmanned aerial vehicles, moving object detection suffers from a lack of widely-accepted, correctly labelled dataset that would facilitate a robust evaluation of the techniques published by the community. Towards this end, we compile a new dataset by manually annotating several sequences from VIVID and UAV123 datasets for moving object detection. We also propose a feature-based, efficient pipeline that is optimized for near real-time performance on GPU-based embedded SoMs (system on module). We evaluate our pipeline on this extended dataset for low altitude moving object detection. Ground-truth annotations are made publicly available to the community to foster further research in moving object detection field. ",0
"Виявлення рухомих об’єктів є обов’язковим завданням комп’ютерного зору, де воно в основному використовується для програм спостереження. Зі збільшенням доступності літальних апаратів на низькій висоті виникли нові виклики для виявлення рухомих об’єктів як для наукових кіл, так і для промисловості. У цій статті ми пропонуємо новий підхід, який може ефективно виявляти рухомі об’єкти та обробляти випадки паралакса. Запроваджуючи обробку паралакса на основі розрідженого потоку та обробку з низьким масштабом, ми розширюємо межі продуктивності в реальному часі з 16 кадрами в секунду на обмежених вбудованих ресурсах (п’ятикратне покращення порівняно з існуючими базовими показниками), водночас зуміючи досягти порівнянної продуктивності або навіть покращити стан -the-art у двох різних наборах даних. Ми також представляємо дорожню карту для розширення нашого підходу до використання мультимодальних даних, щоб зменшити потребу в налаштуванні параметрів.","Виявлення рухомих об&#39;єктів є одним із невід&#39;ємних завдань для програм повітряної розвідки та спостереження. Незважаючи на зростаючий потенціал проблеми через збільшення доступності безпілотних літальних апаратів, виявлення рухомих об’єктів страждає від відсутності загальноприйнятого, правильно позначеного набору даних, який би полегшив надійну оцінку методів, опублікованих спільнотою. З цією метою ми збираємо новий набір даних, вручну анотуючи кілька послідовностей із наборів даних VIVID і UAV123 для виявлення рухомих об’єктів. Ми також пропонуємо ефективний конвеєр на основі функцій, який оптимізований для роботи майже в реальному часі на вбудованих SoM на основі GPU (система на модулі). Ми оцінюємо наш конвеєр на цьому розширеному наборі даних для виявлення рухомих об’єктів на малій висоті. Наземні правдиві анотації стають загальнодоступними для спільноти, щоб сприяти подальшим дослідженням у сфері виявлення рухомих об’єктів.",0
"Computational saliency models for still images have gained significant popularity in recent years. Saliency prediction from videos, on the other hand, has received relatively little interest from the community. Motivated by this, in this paper, we study the use of deep learning for dynamic saliency prediction and propose the so-called spatio-temporal saliency networks. The key to our models is the architecture of two-stream networks where we investigate different fusion mechanisms to integrate spatial and temporal information. We evaluate our models on the dynamic images and eye movements and University of Central Florida-Sports datasets and present highly competitive results against the existing state-of-the-art models. We also carry out some experiments on a number of still images from the MIT300 dataset by exploiting the optical flow maps predicted from these images. Our results show that considering inherent motion information in this way can be helpful for static saliency estimation.","We present a new skeletal representation along with a matching framework to address the deformable shape recognition problem. The disconnectedness arises as a result of excessive regularization that we use to describe a shape at an attainably coarse scale. Our motivation is to rely on the stable properties of the shape instead of inaccurately measured secondary details. The new representation does not suffer from the common instability problems of traditional connected skeletons and the matching process gives quite successful results on a diverse database of 2D shapes. An important difference of our approach from the conventional use of the skeleton is that we replace the local coordinate frame with a global euclidean frame supported by additional mechanisms to handle articulations and local boundary deformations. As a result, we can produce descriptions that are sensitive to any combination of changes in scale, position, orientation, and articulation, as well as invariant ones.",0
"Computational saliency models for still images have gained significant popularity in recent years. Saliency prediction from videos, on the other hand, has received relatively little interest from the community. Motivated by this, in this paper, we study the use of deep learning for dynamic saliency prediction and propose the so-called spatio-temporal saliency networks. The key to our models is the architecture of two-stream networks where we investigate different fusion mechanisms to integrate spatial and temporal information. We evaluate our models on the dynamic images and eye movements and University of Central Florida-Sports datasets and present highly competitive results against the existing state-of-the-art models. We also carry out some experiments on a number of still images from the MIT300 dataset by exploiting the optical flow maps predicted from these images. Our results show that considering inherent motion information in this way can be helpful for static saliency estimation.","Ми представляємо нове скелетне представлення разом із відповідним каркасом для вирішення проблеми розпізнавання форми, що деформується. Роз’єднаність виникає в результаті надмірної регуляризації, яку ми використовуємо для опису форми в досяжно грубому масштабі. Наша мотивація полягає в тому, щоб покладатися на стабільні властивості форми замість неточно виміряних вторинних деталей. Нове представлення не страждає від поширених проблем нестабільності традиційних з’єднаних скелетів, і процес зіставлення дає досить успішні результати на різноманітній базі даних двовимірних форм. Важлива відмінність нашого підходу від звичайного використання скелета полягає в тому, що ми замінюємо локальну систему координат на глобальну евклідову систему, яка підтримується додатковими механізмами для обробки артикуляцій і локальних граничних деформацій. У результаті ми можемо створювати описи, чутливі до будь-якої комбінації змін у масштабі, положенні, орієнтації та артикуляції, а також інваріантні.",0
"Обчислювальні моделі помітності для нерухомих зображень набули значної популярності в останні роки. З іншого боку, передбачення помітності відеозаписів викликало відносно невеликий інтерес у спільноти. Мотивуючись цим, у цій статті ми вивчаємо використання глибокого навчання для динамічного прогнозування помітності та пропонуємо так звані просторово-часові мережі помітності. Ключем до наших моделей є архітектура двопотокових мереж, де ми досліджуємо різні механізми злиття для інтеграції просторової та часової інформації. Ми оцінюємо наші моделі на основі динамічних зображень і рухів очей, а також наборів даних Університету Центральної Флориди-Спорт і представляємо конкурентоспроможні результати порівняно з існуючими найсучаснішими моделями. Ми також проводимо деякі експерименти на кількох нерухомих зображеннях із набору даних MIT300, використовуючи карти оптичного потоку, передбачені на цих зображеннях. Наші результати показують, що врахування інформації про внутрішній рух таким чином може бути корисним для оцінки статичної помітності.","We present a new skeletal representation along with a matching framework to address the deformable shape recognition problem. The disconnectedness arises as a result of excessive regularization that we use to describe a shape at an attainably coarse scale. Our motivation is to rely on the stable properties of the shape instead of inaccurately measured secondary details. The new representation does not suffer from the common instability problems of traditional connected skeletons and the matching process gives quite successful results on a diverse database of 2D shapes. An important difference of our approach from the conventional use of the skeleton is that we replace the local coordinate frame with a global euclidean frame supported by additional mechanisms to handle articulations and local boundary deformations. As a result, we can produce descriptions that are sensitive to any combination of changes in scale, position, orientation, and articulation, as well as invariant ones.",0
"Обчислювальні моделі помітності для нерухомих зображень набули значної популярності в останні роки. З іншого боку, передбачення помітності відеозаписів викликало відносно невеликий інтерес у спільноти. Мотивуючись цим, у цій статті ми вивчаємо використання глибокого навчання для динамічного прогнозування помітності та пропонуємо так звані просторово-часові мережі помітності. Ключем до наших моделей є архітектура двопотокових мереж, де ми досліджуємо різні механізми злиття для інтеграції просторової та часової інформації. Ми оцінюємо наші моделі на основі динамічних зображень і рухів очей, а також наборів даних Університету Центральної Флориди-Спорт і представляємо конкурентоспроможні результати порівняно з існуючими найсучаснішими моделями. Ми також проводимо деякі експерименти на кількох нерухомих зображеннях із набору даних MIT300, використовуючи карти оптичного потоку, передбачені на цих зображеннях. Наші результати показують, що врахування інформації про внутрішній рух таким чином може бути корисним для оцінки статичної помітності.","Ми представляємо нове скелетне представлення разом із відповідним каркасом для вирішення проблеми розпізнавання форми, що деформується. Роз’єднаність виникає в результаті надмірної регуляризації, яку ми використовуємо для опису форми в досяжно грубому масштабі. Наша мотивація полягає в тому, щоб покладатися на стабільні властивості форми замість неточно виміряних вторинних деталей. Нове представлення не страждає від поширених проблем нестабільності традиційних з’єднаних скелетів, і процес зіставлення дає досить успішні результати на різноманітній базі даних двовимірних форм. Важлива відмінність нашого підходу від звичайного використання скелета полягає в тому, що ми замінюємо локальну систему координат на глобальну евклідову систему, яка підтримується додатковими механізмами для обробки артикуляцій і локальних граничних деформацій. У результаті ми можемо створювати описи, чутливі до будь-якої комбінації змін у масштабі, положенні, орієнтації та артикуляції, а також інваріантні.",0
" In the past few years, automatically generating descriptions for images has attracted a lot of attention in computer vision and natural language processing research. Among the existing approaches, data-driven methods have been proven to be highly effective. These methods compare the given image against a large set of training images to determine a set of relevant images, then generate a description using the associated captions. In this study, the authors propose to integrate an objectbased semantic image representation into a deep features-based retrieval framework to select the relevant images. Moreover, they present a novel phrase selection paradigm and a sentence generation model which depends on a joint analysis of salient regions in the input and retrieved images within a clustering framework. The authors demonstrate the effectiveness of their proposed approach on Flickr8K and Flickr30K benchmark datasets and show that their model gives highly competitive results compared with the state-of-the-art models","This paper addresses the problem of comprehending procedural commonsense knowledge. This is a challenging task as it requires identifying key entities, keeping track of their state changes, and understanding temporal and causal relations. Contrary to most of the previous work, in this study, we do not rely on strong inductive bias and explore the question of how multimodality can be exploited to provide a complementary semantic signal. Towards this end, we introduce a new entity-aware neural comprehension model augmented with external relational memory units. Our model learns to dynamically update entity states in relation to each other while reading the text instructions. Our experimental analysis on the visual reasoning tasks in the recently proposed RecipeQA dataset reveals that our approach improves the accuracy of the previously reported models by a large margin. Moreover, we find that our model learns effective dynamic representations of entities even though we do not use any supervision at the level of entity states.1",0
" In the past few years, automatically generating descriptions for images has attracted a lot of attention in computer vision and natural language processing research. Among the existing approaches, data-driven methods have been proven to be highly effective. These methods compare the given image against a large set of training images to determine a set of relevant images, then generate a description using the associated captions. In this study, the authors propose to integrate an objectbased semantic image representation into a deep features-based retrieval framework to select the relevant images. Moreover, they present a novel phrase selection paradigm and a sentence generation model which depends on a joint analysis of salient regions in the input and retrieved images within a clustering framework. The authors demonstrate the effectiveness of their proposed approach on Flickr8K and Flickr30K benchmark datasets and show that their model gives highly competitive results compared with the state-of-the-art models","У цій статті розглядається проблема розуміння процедурних знань здорового глузду. Це складне завдання, оскільки вимагає визначення ключових об’єктів, відстеження змін їхнього стану та розуміння часових і причинно-наслідкових зв’язків. На відміну від більшості попередніх робіт, у цьому дослідженні ми не покладаємося на сильне індуктивне упередження та досліджуємо питання про те, як мультимодальність можна використовувати для надання додаткового семантичного сигналу. З цією метою ми представляємо нову модель нейронного розуміння з урахуванням сутностей, доповнену блоками зовнішньої реляційної пам’яті. Наша модель вчиться динамічно оновлювати стани сутностей по відношенню один до одного під час читання текстових інструкцій. Наш експериментальний аналіз завдань візуального мислення в нещодавно запропонованому наборі даних RecipeQA показує, що наш підхід значно покращує точність раніше повідомлених моделей. Крім того, ми виявили, що наша модель вивчає ефективні динамічні представлення сутностей, навіть якщо ми не використовуємо жодного контролю на рівні станів сутності.1",0
"За останні кілька років автоматичне генерування описів для зображень привернуло багато уваги в дослідженнях комп’ютерного зору та обробки природної мови. Серед існуючих підходів методи, керовані даними, виявилися високоефективними. Ці методи порівнюють задане зображення з великим набором навчальних зображень, щоб визначити набір релевантних зображень, а потім генерують опис, використовуючи відповідні підписи. У цьому дослідженні автори пропонують інтегрувати об’єктно-орієнтоване представлення семантичного зображення в глибоку структуру пошуку на основі ознак, щоб вибрати відповідні зображення. Крім того, вони представляють нову парадигму вибору фраз і модель генерації речень, яка залежить від спільного аналізу помітних областей у вхідних і отриманих зображеннях у рамках кластеризації. Автори демонструють ефективність запропонованого ними підходу на базах даних Flickr8K і Flickr30K і показують, що їх модель дає висококонкурентні результати порівняно з найсучаснішими моделями","This paper addresses the problem of comprehending procedural commonsense knowledge. This is a challenging task as it requires identifying key entities, keeping track of their state changes, and understanding temporal and causal relations. Contrary to most of the previous work, in this study, we do not rely on strong inductive bias and explore the question of how multimodality can be exploited to provide a complementary semantic signal. Towards this end, we introduce a new entity-aware neural comprehension model augmented with external relational memory units. Our model learns to dynamically update entity states in relation to each other while reading the text instructions. Our experimental analysis on the visual reasoning tasks in the recently proposed RecipeQA dataset reveals that our approach improves the accuracy of the previously reported models by a large margin. Moreover, we find that our model learns effective dynamic representations of entities even though we do not use any supervision at the level of entity states.1",0
"За останні кілька років автоматичне генерування описів для зображень привернуло багато уваги в дослідженнях комп’ютерного зору та обробки природної мови. Серед існуючих підходів методи, керовані даними, виявилися високоефективними. Ці методи порівнюють задане зображення з великим набором навчальних зображень, щоб визначити набір релевантних зображень, а потім генерують опис, використовуючи відповідні підписи. У цьому дослідженні автори пропонують інтегрувати об’єктно-орієнтоване представлення семантичного зображення в глибоку структуру пошуку на основі ознак, щоб вибрати відповідні зображення. Крім того, вони представляють нову парадигму вибору фраз і модель генерації речень, яка залежить від спільного аналізу помітних областей у вхідних і отриманих зображеннях у рамках кластеризації. Автори демонструють ефективність запропонованого ними підходу на базах даних Flickr8K і Flickr30K і показують, що їх модель дає висококонкурентні результати порівняно з найсучаснішими моделями","У цій статті розглядається проблема розуміння процедурних знань здорового глузду. Це складне завдання, оскільки вимагає визначення ключових об’єктів, відстеження змін їхнього стану та розуміння часових і причинно-наслідкових зв’язків. На відміну від більшості попередніх робіт, у цьому дослідженні ми не покладаємося на сильне індуктивне упередження та досліджуємо питання про те, як мультимодальність можна використовувати для надання додаткового семантичного сигналу. З цією метою ми представляємо нову модель нейронного розуміння з урахуванням сутностей, доповнену блоками зовнішньої реляційної пам’яті. Наша модель вчиться динамічно оновлювати стани сутностей по відношенню один до одного під час читання текстових інструкцій. Наш експериментальний аналіз завдань візуального мислення в нещодавно запропонованому наборі даних RecipeQA показує, що наш підхід значно покращує точність раніше повідомлених моделей. Крім того, ми виявили, що наша модель вивчає ефективні динамічні представлення сутностей, навіть якщо ми не використовуємо жодного контролю на рівні станів сутності.1",0
"In this paper, we present a new sampling-based alpha matting approach for the accurate estimation of foreground and background layers of an image. Previous samplingbased methods typically rely on certain heuristics in collecting representative samples from known regions, and thus their performance deteriorates if the underlying assumptions are not satisfied. To alleviate this, we take an entirely new approach and formulate sampling as a sparse subset selection problem where we propose to pick a small set of candidate samples that best explains the unknown pixels. Moreover, we describe a new dissimilarity measure for comparing two samples which is based on KL-divergence between the distributions of features extracted in the vicinity of the samples. The proposed framework is general and could be easily extended to video matting by additionally taking temporal information into account in the sampling process. Evaluation on standard benchmark data sets for image and video matting demonstrates that our approach provides more accurate results compared with the state-of-the-art methods.","Collecting textual descriptions is an especially costly task for dense video captioning, since each event in the video needs to be annotated separately and a long descriptive paragraph needs to be provided. In this paper, we investigate a way to mitigate this heavy burden and propose to leverage captions of visually similar images as auxiliary context. Our model successfully fetches visually relevant images and combines noun and verb phrases from their captions to generating coherent descriptions. To this end, we use a generator and discriminator design, together with an attention-based fusion technique, to incorporate image captions as context in the video caption generation process. The experiments on the challenging ActivityNet Captions dataset demonstrate that our proposed approach achieves more accurate and more diverse video descriptions compared to the strong baseline using METEOR, BLEU and CIDEr-D metrics and qualitative evaluations.",0
"In this paper, we present a new sampling-based alpha matting approach for the accurate estimation of foreground and background layers of an image. Previous samplingbased methods typically rely on certain heuristics in collecting representative samples from known regions, and thus their performance deteriorates if the underlying assumptions are not satisfied. To alleviate this, we take an entirely new approach and formulate sampling as a sparse subset selection problem where we propose to pick a small set of candidate samples that best explains the unknown pixels. Moreover, we describe a new dissimilarity measure for comparing two samples which is based on KL-divergence between the distributions of features extracted in the vicinity of the samples. The proposed framework is general and could be easily extended to video matting by additionally taking temporal information into account in the sampling process. Evaluation on standard benchmark data sets for image and video matting demonstrates that our approach provides more accurate results compared with the state-of-the-art methods.","Збір текстових описів є особливо дорогим завданням для щільних субтитрів до відео, оскільки кожна подія у відео має бути анотована окремо та має бути наданий довгий абзац з описом. У цій статті ми досліджуємо спосіб полегшити цей важкий тягар і пропонуємо використовувати підписи до візуально подібних зображень як допоміжний контекст. Наша модель успішно вибирає візуально релевантні зображення та комбінує фрази іменників і дієслів із їхніх підписів для створення зв’язних описів. З цією метою ми використовуємо дизайн генератора та дискримінатора разом із технікою злиття на основі уваги, щоб включити підписи до зображень як контекст у процес генерації субтитрів до відео. Експерименти зі складним набором даних ActivityNet Captions демонструють, що запропонований нами підхід забезпечує більш точні та різноманітні описи відео порівняно з сильним базовим сценарієм із використанням показників METEOR, BLEU і CIDer-D і якісних оцінок.",0
"У цій статті ми представляємо новий підхід альфа-матування на основі вибірки для точної оцінки переднього та фонового шарів зображення. Попередні методи, засновані на вибірці, зазвичай покладаються на певні евристики при зборі репрезентативних вибірок із відомих регіонів, і тому їх продуктивність погіршується, якщо базові припущення не задовольняються. Щоб полегшити це, ми застосували абсолютно новий підхід і сформулювали вибірку як проблему відбору розрідженої підмножини, де ми пропонуємо вибрати невеликий набір вибірок-кандидатів, які найкраще пояснюють невідомі пікселі. Крім того, ми описуємо нову міру відмінності для порівняння двох зразків, яка базується на KL-розбіжності між розподілами ознак, виділених поблизу зразків. Запропонована структура є загальною і може бути легко розширена до відеоматінгу шляхом додаткового врахування тимчасової інформації в процесі вибірки. Оцінка стандартних контрольних наборів даних для матування зображень і відео демонструє, що наш підхід забезпечує більш точні результати порівняно з найсучаснішими методами.","Collecting textual descriptions is an especially costly task for dense video captioning, since each event in the video needs to be annotated separately and a long descriptive paragraph needs to be provided. In this paper, we investigate a way to mitigate this heavy burden and propose to leverage captions of visually similar images as auxiliary context. Our model successfully fetches visually relevant images and combines noun and verb phrases from their captions to generating coherent descriptions. To this end, we use a generator and discriminator design, together with an attention-based fusion technique, to incorporate image captions as context in the video caption generation process. The experiments on the challenging ActivityNet Captions dataset demonstrate that our proposed approach achieves more accurate and more diverse video descriptions compared to the strong baseline using METEOR, BLEU and CIDEr-D metrics and qualitative evaluations.",0
"У цій статті ми представляємо новий підхід альфа-матування на основі вибірки для точної оцінки переднього та фонового шарів зображення. Попередні методи, засновані на вибірці, зазвичай покладаються на певні евристики при зборі репрезентативних вибірок із відомих регіонів, і тому їх продуктивність погіршується, якщо базові припущення не задовольняються. Щоб полегшити це, ми застосували абсолютно новий підхід і сформулювали вибірку як проблему відбору розрідженої підмножини, де ми пропонуємо вибрати невеликий набір вибірок-кандидатів, які найкраще пояснюють невідомі пікселі. Крім того, ми описуємо нову міру відмінності для порівняння двох зразків, яка базується на KL-розбіжності між розподілами ознак, виділених поблизу зразків. Запропонована структура є загальною і може бути легко розширена до відеоматінгу шляхом додаткового врахування тимчасової інформації в процесі вибірки. Оцінка стандартних контрольних наборів даних для матування зображень і відео демонструє, що наш підхід забезпечує більш точні результати порівняно з найсучаснішими методами.","Збір текстових описів є особливо дорогим завданням для щільних субтитрів до відео, оскільки кожна подія у відео має бути анотована окремо та має бути наданий довгий абзац з описом. У цій статті ми досліджуємо спосіб полегшити цей важкий тягар і пропонуємо використовувати підписи до візуально подібних зображень як допоміжний контекст. Наша модель успішно вибирає візуально релевантні зображення та комбінує фрази іменників і дієслів із їхніх підписів для створення зв’язних описів. З цією метою ми використовуємо дизайн генератора та дискримінатора разом із технікою злиття на основі уваги, щоб включити підписи до зображень як контекст у процес генерації субтитрів до відео. Експерименти зі складним набором даних ActivityNet Captions демонструють, що запропонований нами підхід забезпечує більш точні та різноманітні описи відео порівняно з сильним базовим сценарієм із використанням показників METEOR, BLEU і CIDer-D і якісних оцінок.",0
"With the growing interest in computational models of visual attention, saliency prediction has become an important research topic in computer vision. Over the past years, many different successful saliency models have been proposed especially for image saliency prediction. However, these models generally do not consider the dynamic nature of the scenes, and hence, they work better on static images. To date, there has been relatively little work on dynamic saliency that deals with predicting where humans look at videos. In addition, previous studies showed that how the feature integration is carried out is very crucial for more accurate results. Yet, many dynamic saliency models follow a similar simple design and extract separate spatial and temporal saliency maps which are then integrated together to obtain the final saliency map. In this paper, we present a comparative study for different feature integration strategies in dynamic saliency estimation. We employ a number of low and high-level visual features such as static saliency, motion, faces, humans and text, some of which have not been previously used in dynamic saliency estimation. In order to explore the strength of feature integration strategies, we investigate four learning-based (SVM, Gradient Boosting, NNLS, Random Forest) and two transformation- based (Mean, Max) fusion methods, resulting in six new dynamic saliency models. Our experimental analysis on two different dynamic saliency benchmark datasets reveal that our models achieve better performance than the individual features. In addition, our learning-based models outperform the state-of-the-art dynamic saliency models.","Learning robust representations is critical for the success of person re-identification and attribute recognition systems. However, to achieve this, we must use a large dataset of diverse person images as well as annotations of identity labels and/or a set of different attributes. Apart from the obvious concerns about privacy issues, the manual annotation process is both time consuming and too costly. In this paper, we instead propose to use synthetic person images for addressing these difficulties. Specifically, we first introduce Synthetic18K, a large-scale dataset of over 1 million computer generated person images of 18K unique identities with relevant attributes. Moreover, we demonstrate that pretraining of simple deep architectures on Synthetic18K for person re-identification and attribute recognition and then fine-tuning on real data leads to significant improvements in prediction performances, giving results better than or comparable to state-of-the-art models.",0
"With the growing interest in computational models of visual attention, saliency prediction has become an important research topic in computer vision. Over the past years, many different successful saliency models have been proposed especially for image saliency prediction. However, these models generally do not consider the dynamic nature of the scenes, and hence, they work better on static images. To date, there has been relatively little work on dynamic saliency that deals with predicting where humans look at videos. In addition, previous studies showed that how the feature integration is carried out is very crucial for more accurate results. Yet, many dynamic saliency models follow a similar simple design and extract separate spatial and temporal saliency maps which are then integrated together to obtain the final saliency map. In this paper, we present a comparative study for different feature integration strategies in dynamic saliency estimation. We employ a number of low and high-level visual features such as static saliency, motion, faces, humans and text, some of which have not been previously used in dynamic saliency estimation. In order to explore the strength of feature integration strategies, we investigate four learning-based (SVM, Gradient Boosting, NNLS, Random Forest) and two transformation- based (Mean, Max) fusion methods, resulting in six new dynamic saliency models. Our experimental analysis on two different dynamic saliency benchmark datasets reveal that our models achieve better performance than the individual features. In addition, our learning-based models outperform the state-of-the-art dynamic saliency models.","Вивчення надійних представлень має вирішальне значення для успіху систем повторної ідентифікації особи та розпізнавання атрибутів. Однак, щоб досягти цього, ми повинні використовувати великий набір даних різноманітних зображень людей, а також анотації міток ідентифікації та/або набір різних атрибутів. Крім очевидного занепокоєння щодо проблем конфіденційності, процес анотації вручну займає багато часу та надто дорого. У цій статті ми замість цього пропонуємо використовувати зображення синтетичних людей для вирішення цих труднощів. Зокрема, ми вперше представляємо Synthetic18K, великомасштабний набір даних із понад 1 мільйона зображень людей, згенерованих комп’ютером, 18K унікальних ідентифікацій із відповідними атрибутами. Крім того, ми демонструємо, що попереднє навчання простих глибоких архітектур на Synthetic18K для повторної ідентифікації особи та розпізнавання атрибутів, а потім точного налаштування на реальних даних призводить до значного покращення продуктивності прогнозування, даючи результати, кращі або порівнювані з найсучаснішими. моделі.",0
"Зі зростанням інтересу до обчислювальних моделей зорової уваги прогнозування помітності стало важливою темою дослідження комп’ютерного зору. За останні роки було запропоновано багато різних успішних моделей помітності, особливо для прогнозування помітності зображення. Однак ці моделі зазвичай не враховують динамічну природу сцен, а отже, вони краще працюють на статичних зображеннях. На сьогоднішній день було проведено відносно мало робіт щодо динамічної помітності, яка стосується передбачення того, де люди дивляться відео. Крім того, попередні дослідження показали, що для більш точних результатів дуже важливо те, як здійснюється інтеграція функцій. Тим не менш, багато динамічних моделей помітності дотримуються схожої простої конструкції та виділяють окремі просторові та часові карти помітності, які потім інтегруються разом, щоб отримати остаточну карту помітності. У цій статті ми представляємо порівняльне дослідження для різних стратегій інтеграції функцій у динамічній оцінці помітності. Ми використовуємо ряд візуальних функцій низького та високого рівня, таких як статична помітність, рух, обличчя, люди та текст, деякі з яких раніше не використовувалися в динамічній оцінці помітності. Щоб дослідити силу стратегій інтеграції функцій, ми досліджуємо чотири методи злиття на основі навчання (SVM, Gradient Boosting, NNLS, Random Forest) і два на основі трансформації (Mean, Max), у результаті чого створено шість нових динамічних моделей помітності. Наш експериментальний аналіз двох різних наборів даних динамічного порівняння показує, що наші моделі досягають кращої продуктивності, ніж окремі функції. Крім того, наші моделі, засновані на навчанні, перевершують найсучасніші моделі динамічної помітності.","Learning robust representations is critical for the success of person re-identification and attribute recognition systems. However, to achieve this, we must use a large dataset of diverse person images as well as annotations of identity labels and/or a set of different attributes. Apart from the obvious concerns about privacy issues, the manual annotation process is both time consuming and too costly. In this paper, we instead propose to use synthetic person images for addressing these difficulties. Specifically, we first introduce Synthetic18K, a large-scale dataset of over 1 million computer generated person images of 18K unique identities with relevant attributes. Moreover, we demonstrate that pretraining of simple deep architectures on Synthetic18K for person re-identification and attribute recognition and then fine-tuning on real data leads to significant improvements in prediction performances, giving results better than or comparable to state-of-the-art models.",0
"Зі зростанням інтересу до обчислювальних моделей зорової уваги прогнозування помітності стало важливою темою дослідження комп’ютерного зору. За останні роки було запропоновано багато різних успішних моделей помітності, особливо для прогнозування помітності зображення. Однак ці моделі зазвичай не враховують динамічну природу сцен, а отже, вони краще працюють на статичних зображеннях. На сьогоднішній день було проведено відносно мало робіт щодо динамічної помітності, яка стосується передбачення того, де люди дивляться відео. Крім того, попередні дослідження показали, що для більш точних результатів дуже важливо, як здійснюється інтеграція функцій. Тим не менш, багато динамічних моделей помітності дотримуються схожої простої конструкції та виділяють окремі просторові та часові карти помітності, які потім інтегруються разом, щоб отримати остаточну карту помітності. У цій статті ми представляємо порівняльне дослідження для різних стратегій інтеграції функцій у динамічній оцінці помітності. Ми використовуємо ряд візуальних функцій низького та високого рівня, таких як статична помітність, рух, обличчя, люди та текст, деякі з яких раніше не використовувалися в динамічній оцінці помітності. Щоб дослідити силу стратегій інтеграції функцій, ми досліджуємо чотири методи злиття на основі навчання (SVM, Gradient Boosting, NNLS, Random Forest) і два на основі трансформації (Mean, Max), у результаті чого створено шість нових динамічних моделей помітності. Наш експериментальний аналіз двох різних наборів даних динамічного порівняння показує, що наші моделі досягають кращої продуктивності, ніж окремі функції. Крім того, наші моделі, засновані на навчанні, перевершують найсучасніші моделі динамічної помітності.","Вивчення надійних представлень має вирішальне значення для успіху систем повторної ідентифікації особи та розпізнавання атрибутів. Однак, щоб досягти цього, ми повинні використовувати великий набір даних різноманітних зображень людей, а також анотації міток ідентифікації та/або набір різних атрибутів. Крім очевидного занепокоєння щодо проблем конфіденційності, процес анотації вручну займає багато часу та надто дорого. У цій статті ми замість цього пропонуємо використовувати зображення синтетичних людей для вирішення цих труднощів. Зокрема, ми вперше представляємо Synthetic18K, великомасштабний набір даних із понад 1 мільйона зображень людей, згенерованих комп’ютером, 18K унікальних ідентифікацій із відповідними атрибутами. Крім того, ми демонструємо, що попереднє навчання простих глибоких архітектур на Synthetic18K для повторної ідентифікації особи та розпізнавання атрибутів, а потім точного налаштування на реальних даних призводить до значного покращення продуктивності прогнозування, даючи результати, кращі або порівнювані з найсучаснішими. моделі.",0
"Moving Object Detection is one of the integral tasks for aerial reconnaissance and surveillance applications. Despite the problem’s rising potential due to increasing availability of unmanned aerial vehicles, moving object detection suffers from a lack of widely-accepted, correctly labelled dataset that would facilitate a robust evaluation of the techniques published by the community. Towards this end, we compile a new dataset by manually annotating several sequences from VIVID and UAV123 datasets for moving object detection. We also propose a feature-based, efficient pipeline that is optimized for near real-time performance on GPU-based embedded SoMs (system on module). We evaluate our pipeline on this extended dataset for low altitude moving object detection. Ground-truth annotations are made publicly available to the community to foster further research in moving object detection field. ","Developing techniques for editing an outfit image through natural sentences and accordingly generating new outfits has promising applications for art, fashion and design. However, it is considered as a certainly challenging task since image manipulation should be carried out only on the relevant parts of the image while keeping the remaining sections untouched. Moreover, this manipulation process should generate an image that is as realistic as possible. In this work, we propose FiLMedGAN, which leverages feature-wise linear modulation (FiLM) to relate and transform visual features with natural language representations without using extra spatial information. Our experiments demonstrate that this approach, when combined with skip connections and total variation regularization, produces more plausible results than the baseline work, and has a better localization capability when generating new outfits consistent with the target description.",0
"Moving Object Detection is one of the integral tasks for aerial reconnaissance and surveillance applications. Despite the problem’s rising potential due to increasing availability of unmanned aerial vehicles, moving object detection suffers from a lack of widely-accepted, correctly labelled dataset that would facilitate a robust evaluation of the techniques published by the community. Towards this end, we compile a new dataset by manually annotating several sequences from VIVID and UAV123 datasets for moving object detection. We also propose a feature-based, efficient pipeline that is optimized for near real-time performance on GPU-based embedded SoMs (system on module). We evaluate our pipeline on this extended dataset for low altitude moving object detection. Ground-truth annotations are made publicly available to the community to foster further research in moving object detection field. ","Розробка методів редагування зображення вбрання за допомогою природних речень і відповідно генерування нових вбрань має перспективні застосування для мистецтва, моди та дизайну. Однак це вважається, безумовно, складним завданням, оскільки маніпуляції із зображенням слід виконувати лише на відповідних частинах зображення, залишаючи решту ділянок недоторканими. Більше того, цей процес маніпуляції має створити зображення, яке є максимально реалістичним. У цій роботі ми пропонуємо FiLMedGAN, який використовує функціональну лінійну модуляцію (FiLM), щоб зв’язати та трансформувати візуальні функції з представленнями природної мови без використання додаткової просторової інформації. Наші експерименти демонструють, що цей підхід у поєднанні з пропускаючими з’єднаннями та регуляризацією повної варіації дає більш вірогідні результати, ніж базова робота, і має кращу здатність до локалізації під час створення нових комплектів, що відповідають цільовому опису.",0
"Виявлення рухомих об&#39;єктів є одним із невід&#39;ємних завдань для програм повітряної розвідки та спостереження. Незважаючи на зростаючий потенціал проблеми через збільшення доступності безпілотних літальних апаратів, виявлення рухомих об’єктів страждає від відсутності загальноприйнятого, правильно позначеного набору даних, який би полегшив надійну оцінку методів, опублікованих спільнотою. З цією метою ми збираємо новий набір даних, вручну анотуючи кілька послідовностей із наборів даних VIVID і UAV123 для виявлення рухомих об’єктів. Ми також пропонуємо ефективний конвеєр на основі функцій, який оптимізований для роботи майже в реальному часі на вбудованих SoM на основі GPU (система на модулі). Ми оцінюємо наш конвеєр на цьому розширеному наборі даних для виявлення рухомих об’єктів на малій висоті. Наземні правдиві анотації стають загальнодоступними для спільноти, щоб сприяти подальшим дослідженням у сфері виявлення рухомих об’єктів.","Developing techniques for editing an outfit image through natural sentences and accordingly generating new outfits has promising applications for art, fashion and design. However, it is considered as a certainly challenging task since image manipulation should be carried out only on the relevant parts of the image while keeping the remaining sections untouched. Moreover, this manipulation process should generate an image that is as realistic as possible. In this work, we propose FiLMedGAN, which leverages feature-wise linear modulation (FiLM) to relate and transform visual features with natural language representations without using extra spatial information. Our experiments demonstrate that this approach, when combined with skip connections and total variation regularization, produces more plausible results than the baseline work, and has a better localization capability when generating new outfits consistent with the target description.",0
"Виявлення рухомих об&#39;єктів є одним із невід&#39;ємних завдань для програм повітряної розвідки та спостереження. Незважаючи на зростаючий потенціал проблеми через збільшення доступності безпілотних літальних апаратів, виявлення рухомих об’єктів страждає від відсутності загальноприйнятого, правильно позначеного набору даних, який би полегшив надійну оцінку методів, опублікованих спільнотою. З цією метою ми збираємо новий набір даних, вручну анотуючи кілька послідовностей із наборів даних VIVID і UAV123 для виявлення рухомих об’єктів. Ми також пропонуємо ефективний конвеєр на основі функцій, який оптимізований для роботи майже в реальному часі на вбудованих SoM на основі GPU (система на модулі). Ми оцінюємо наш конвеєр на цьому розширеному наборі даних для виявлення рухомих об’єктів на малій висоті. Наземні правдиві анотації стають загальнодоступними для спільноти, щоб сприяти подальшим дослідженням у сфері виявлення рухомих об’єктів.","Розробка методів редагування зображення вбрання за допомогою природних речень і відповідно генерування нових вбрань має перспективні застосування для мистецтва, моди та дизайну. Однак це вважається, безумовно, складним завданням, оскільки маніпуляції із зображенням слід виконувати лише на відповідних частинах зображення, залишаючи решту ділянок недоторканими. Більше того, цей процес маніпуляції має створити зображення, яке є максимально реалістичним. У цій роботі ми пропонуємо FiLMedGAN, який використовує функціональну лінійну модуляцію (FiLM), щоб зв’язати та трансформувати візуальні функції з представленнями природної мови без використання додаткової просторової інформації. Наші експерименти демонструють, що цей підхід у поєднанні з пропускаючими з’єднаннями та регуляризацією повної варіації дає більш вірогідні результати, ніж базова робота, і має кращу здатність до локалізації під час створення нових комплектів, що відповідають цільовому опису.",0
"The task of generating natural language descriptions from images has received a lot of attention in recent years. Consequently, it is becoming increasingly important to evaluate such image captioning approaches in an automatic manner. In this paper, we provide an in-depth evaluation of the existing image captioning metrics through a series of carefully designed experiments. Moreover, we explore the utilization of the recently proposed Word Mover’s Distance (WMD) document metric for the purpose of image captioning. Our findings outline the differences and/or similarities between metrics and their relative robustness by means of extensive correlation, accuracy and distraction based evaluations. Our results also demonstrate that WMD provides strong advantages over other metrics.","Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 444 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI’s GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit “breakthrough” behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting. ",0
"The task of generating natural language descriptions from images has received a lot of attention in recent years. Consequently, it is becoming increasingly important to evaluate such image captioning approaches in an automatic manner. In this paper, we provide an in-depth evaluation of the existing image captioning metrics through a series of carefully designed experiments. Moreover, we explore the utilization of the recently proposed Word Mover’s Distance (WMD) document metric for the purpose of image captioning. Our findings outline the differences and/or similarities between metrics and their relative robustness by means of extensive correlation, accuracy and distraction based evaluations. Our results also demonstrate that WMD provides strong advantages over other metrics.","Мовні моделі демонструють як кількісне вдосконалення, так і нові якісні можливості зі збільшенням масштабу. Незважаючи на їхній потенційно трансформаційний вплив, ці нові можливості ще недостатньо охарактеризовані. Для того, щоб інформувати про майбутні дослідження, підготуватися до руйнівних можливостей нових моделей і пом’якшити соціально шкідливі наслідки, життєво важливо, щоб ми розуміли теперішні та найближчі можливості та обмеження мовних моделей. Щоб вирішити цю проблему, ми представляємо тест Beyond the Imitation Game (BIG-bench). BIG-стенд наразі складається з 204 завдань, доданих 444 авторами зі 132 установ. Теми завдань різноманітні, вони охоплюють проблеми з лінгвістики, розвитку дитини, математики, здорового глузду, біології, фізики, соціальних упереджень, розробки програмного забезпечення тощо. BIG-bench зосереджується на завданнях, які, як вважають, виходять за межі можливостей поточних мовних моделей. Ми оцінюємо поведінку моделей GPT OpenAI, внутрішніх архітектур щільних трансформаторів Google і розріджених трансформаторів у стилі Switch на BIG-стенді за розмірами моделей, що охоплюють від мільйонів до сотень мільярдів параметрів. Крім того, команда оцінювачів-людей виконувала всі завдання, щоб забезпечити надійну базову лінію. Висновки включають: продуктивність моделі та калібрування покращуються з масштабом, але є поганими в абсолютному вираженні (і порівняно з продуктивністю оцінювача); продуктивність надзвичайно однакова для всіх класів моделей, хоча й має переваги від розрідженості; завдання, які вдосконалюються поступово і передбачувано, зазвичай включають великий компонент знань або запам’ятовування, тоді як завдання, які демонструють «проривну» поведінку в критичному масштабі, часто включають кілька кроків або компонентів або крихкі показники; соціальне упередження зазвичай зростає з масштабом в умовах з неоднозначним контекстом, але це можна покращити за допомогою підказок.",0
"Завдання створення описів природною мовою з зображень привернуло багато уваги в останні роки. Отже, стає все більш важливим автоматично оцінювати такі підходи до підписів до зображень. У цій статті ми надаємо поглиблену оцінку існуючих показників субтитрів до зображень за допомогою серії ретельно розроблених експериментів. Крім того, ми вивчаємо використання нещодавно запропонованої метрики документа Word Mover&#39;s Distance (WMD) для підписів до зображень. Наші висновки окреслюють відмінності та/або схожість між показниками та їх відносною надійністю за допомогою обширних оцінок на основі кореляції, точності та відволікання. Наші результати також демонструють, що ЗМЗ має значні переваги перед іншими показниками.","Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 444 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI’s GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit “breakthrough” behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting. ",0
"Завдання генерації описів природною мовою з зображень привернуло багато уваги в останні роки. Отже, стає все більш важливим автоматично оцінювати такі підходи до підписів до зображень. У цій статті ми надаємо поглиблену оцінку існуючих показників субтитрів до зображень за допомогою серії ретельно розроблених експериментів. Крім того, ми досліджуємо використання нещодавно запропонованої метрики документа Word Mover Distance (WMD) для підписів до зображень. Наші висновки окреслюють відмінності та/або схожість між показниками та їх відносною надійністю за допомогою обширних оцінок на основі кореляції, точності та відволікання. Наші результати також демонструють, що ЗМЗ має значні переваги перед іншими показниками.","Мовні моделі демонструють як кількісне вдосконалення, так і нові якісні можливості зі збільшенням масштабу. Незважаючи на їхній потенційно трансформаційний вплив, ці нові можливості ще недостатньо охарактеризовані. Для того, щоб інформувати про майбутні дослідження, підготуватися до руйнівних можливостей нових моделей і пом’якшити соціально шкідливі наслідки, життєво важливо, щоб ми розуміли теперішні та найближчі можливості та обмеження мовних моделей. Щоб вирішити цю проблему, ми представляємо тест Beyond the Imitation Game (BIG-bench). BIG-стенд наразі складається з 204 завдань, доданих 444 авторами зі 132 установ. Теми завдань різноманітні, вони охоплюють проблеми з лінгвістики, розвитку дитини, математики, здорового глузду, біології, фізики, соціальних упереджень, розробки програмного забезпечення тощо. BIG-bench зосереджується на завданнях, які, як вважають, виходять за межі можливостей поточних мовних моделей. Ми оцінюємо поведінку моделей GPT OpenAI, внутрішніх архітектур щільних трансформаторів Google і розріджених трансформаторів у стилі Switch на BIG-стенді за розмірами моделей, що охоплюють від мільйонів до сотень мільярдів параметрів. Крім того, команда оцінювачів-людей виконувала всі завдання, щоб забезпечити надійну базову лінію. Висновки включають: продуктивність моделі та калібрування покращуються з масштабом, але є поганими в абсолютному вираженні (і порівняно з продуктивністю оцінювача); продуктивність надзвичайно однакова для всіх класів моделей, хоча й має переваги від розрідженості; завдання, які вдосконалюються поступово і передбачувано, зазвичай включають великий компонент знань або запам’ятовування, тоді як завдання, які демонструють «проривну» поведінку в критичному масштабі, часто включають кілька кроків або компонентів або крихкі показники; соціальне упередження зазвичай зростає з масштабом в умовах з неоднозначним контекстом, але це можна покращити за допомогою підказок.",0
"Existing computational models of visual attention generally employ simple image features such as color, intensity or orientation to generate a saliency map which highlights the image parts that attract human attention. Interestingly, most of these models do not process any depth information and operate only on standard two-dimensional RGB images. On the other hand, depth processing through stereo vision is a key characteristics of the human visual system. In line with this observation, in this study, we propose to extend two state-of-the-art static saliency models that depend on region covariances to process additional depth information available in RGB-D images. We evaluate our proposed models on NUS-3D benchmark dataset by taking into account different evaluation metrics. Our results reveal that using the additional depth information improves the saliency prediction in a statistically significant manner, giving more accurate saliency maps.  ","Automatic generation of video descriptions in natural language, also called video captioning, aims to understand the visual content of the video and produce a natural language sentence depicting the objects and actions in the scene. This challenging integrated vision and language problem, however, has been predominantly addressed for English. The lack of data and the linguistic properties of other languages limit the success of existing approaches for such languages. In this paper we target Turkish, a morphologically rich and agglutinative language that has very different properties compared to English. To do so, we create the frst large-scale video captioning dataset for this language by carefully translating the English descriptions of the videos in the MSVD (Microsoft Research Video Description Corpus) dataset into Turkish. In addition to enabling research in video captioning in Turkish, the parallel English–Turkish descriptions also enable the study of the role of video context in (multimodal) machine translation. In our experiments, we build models for both video captioning and multimodal machine translation and investigate the efect of diferent word segmentation approaches and diferent neural architectures to better address the properties of Turkish. We hope that the MSVD-Turkish dataset and the results reported in this work will lead to better video captioning and multimodal machine translation models for Turkish and other morphology rich and agglutinative languages.",0
"Existing computational models of visual attention generally employ simple image features such as color, intensity or orientation to generate a saliency map which highlights the image parts that attract human attention. Interestingly, most of these models do not process any depth information and operate only on standard two-dimensional RGB images. On the other hand, depth processing through stereo vision is a key characteristics of the human visual system. In line with this observation, in this study, we propose to extend two state-of-the-art static saliency models that depend on region covariances to process additional depth information available in RGB-D images. We evaluate our proposed models on NUS-3D benchmark dataset by taking into account different evaluation metrics. Our results reveal that using the additional depth information improves the saliency prediction in a statistically significant manner, giving more accurate saliency maps.  ","Автоматична генерація відеоописів природною мовою, яка також називається відеотитрами, має на меті зрозуміти візуальний вміст відео та створити речення природною мовою, що зображує об’єкти та дії в сцені. Ця складна інтегрована проблема бачення та мови, однак, була переважно розглянута для англійської мови. Брак даних і лінгвістичні властивості інших мов обмежують успіх існуючих підходів для таких мов. У цій статті ми орієнтуємося на турецьку, морфологічно багату та аглютинативну мову, яка має дуже різні властивості порівняно з англійською. Для цього ми створюємо перший великомасштабний набір даних субтитрів до відео для цієї мови, ретельно перекладаючи англійські описи відео в наборі даних MSVD (Microsoft Research Video Description Corpus) на турецьку мову. Окрім того, що вони дозволяють досліджувати субтитри до відео турецькою мовою, паралельні англо-турецькі описи також дозволяють вивчати роль відеоконтексту в (мультимодальному) машинному перекладі. У наших експериментах ми створюємо моделі як для субтитрів до відео, так і для мультимодального машинного перекладу та досліджуємо вплив різних підходів до сегментації слів і різних нейронних архітектур для кращого врахування властивостей турецької мови. Ми сподіваємося, що набір даних MSVD-Turkish і результати, наведені в цій роботі, призведуть до кращих відеосубтитрів і мультимодальних моделей машинного перекладу для турецької та інших морфологічних і аглютинативних мов.",0
"Існуючі обчислювальні моделі зорової уваги зазвичай використовують прості характеристики зображення, такі як колір, інтенсивність або орієнтація, щоб створити карту помітності, яка виділяє частини зображення, які привертають увагу людини. Цікаво, що більшість із цих моделей не обробляють інформацію про глибину й працюють лише зі стандартними двовимірними зображеннями RGB. З іншого боку, обробка глибини за допомогою стереозору є ключовою характеристикою зорової системи людини. Згідно з цим спостереженням, у цьому дослідженні ми пропонуємо розширити дві найсучасніші статичні моделі помітності, які залежать від коваріацій регіону, для обробки додаткової інформації про глибину, доступної в зображеннях RGB-D. Ми оцінюємо запропоновані нами моделі на основі порівняльного набору даних NUS-3D, беручи до уваги різні метрики оцінювання. Наші результати показують, що використання додаткової інформації про глибину покращує прогнозування помітності статистично значущим чином, надаючи точніші карти помітності.","Automatic generation of video descriptions in natural language, also called video captioning, aims to understand the visual content of the video and produce a natural language sentence depicting the objects and actions in the scene. This challenging integrated vision and language problem, however, has been predominantly addressed for English. The lack of data and the linguistic properties of other languages limit the success of existing approaches for such languages. In this paper we target Turkish, a morphologically rich and agglutinative language that has very different properties compared to English. To do so, we create the frst large-scale video captioning dataset for this language by carefully translating the English descriptions of the videos in the MSVD (Microsoft Research Video Description Corpus) dataset into Turkish. In addition to enabling research in video captioning in Turkish, the parallel English–Turkish descriptions also enable the study of the role of video context in (multimodal) machine translation. In our experiments, we build models for both video captioning and multimodal machine translation and investigate the efect of diferent word segmentation approaches and diferent neural architectures to better address the properties of Turkish. We hope that the MSVD-Turkish dataset and the results reported in this work will lead to better video captioning and multimodal machine translation models for Turkish and other morphology rich and agglutinative languages.",0
"Існуючі обчислювальні моделі зорової уваги зазвичай використовують прості характеристики зображення, такі як колір, інтенсивність або орієнтація, щоб створити карту помітності, яка виділяє частини зображення, які привертають увагу людини. Цікаво, що більшість із цих моделей не обробляють жодної інформації про глибину й працюють лише зі стандартними двовимірними зображеннями RGB. З іншого боку, обробка глибини за допомогою стереозору є ключовою характеристикою зорової системи людини. Згідно з цим спостереженням, у цьому дослідженні ми пропонуємо розширити дві найсучасніші статичні моделі помітності, які залежать від коваріацій регіону, для обробки додаткової інформації про глибину, доступної в зображеннях RGB-D. Ми оцінюємо запропоновані нами моделі на основі порівняльного набору даних NUS-3D, беручи до уваги різні метрики оцінювання. Наші результати показують, що використання додаткової інформації про глибину покращує прогнозування помітності статистично значущим чином, надаючи точніші карти помітності.","Автоматична генерація відеоописів природною мовою, яка також називається відеотитрами, має на меті зрозуміти візуальний вміст відео та створити речення природною мовою, що зображує об’єкти та дії в сцені. Ця складна інтегрована проблема бачення та мови, однак, була переважно розглянута для англійської мови. Брак даних і лінгвістичні властивості інших мов обмежують успіх існуючих підходів для таких мов. У цій статті ми орієнтуємося на турецьку, морфологічно багату та аглютинативну мову, яка має дуже різні властивості порівняно з англійською. Для цього ми створюємо перший великомасштабний набір даних субтитрів до відео для цієї мови, ретельно перекладаючи англійські описи відео в наборі даних MSVD (Microsoft Research Video Description Corpus) на турецьку мову. Окрім того, що вони дозволяють досліджувати субтитри до відео турецькою мовою, паралельні англо-турецькі описи також дозволяють вивчати роль відеоконтексту в (мультимодальному) машинному перекладі. У наших експериментах ми створюємо моделі як для субтитрів до відео, так і для мультимодального машинного перекладу та досліджуємо вплив різних підходів до сегментації слів і різних нейронних архітектур для кращого врахування властивостей турецької мови. Ми сподіваємося, що набір даних MSVD-Turkish і результати, наведені в цій роботі, призведуть до кращих відеосубтитрів і мультимодальних моделей машинного перекладу для турецької та інших морфологічних і аглютинативних мов.",0
"Automatic image synthesis research has been rapidly growing with deep networks getting more and more expressive. In the last couple of years, we have observed images of digits, indoor scenes, birds, chairs, etc. being automatically generated. The expressive power of image generators have also been enhanced by introducing several forms of conditioning variables such as object names, sentences, bounding box and key-point locations. In this work, we propose a novel deep conditional generative adversarial network architecture that takes its strength from the semantic layout and scene attributes integrated as conditioning variables. We show that our architecture is able to generate realistic outdoor scene images under different conditions, e.g. daynight, sunny-foggy, with clear object boundaries. ","In this paper, we propose a novel query expansion approach for improving transferbased automatic image captioning. The core idea of our method is to translate the given visual query into a distributional semantics based form, which is generated by the average of the sentence vectors extracted from the captions of images visually similar to the input image. Using three image captioning benchmark datasets, we show that our approach provides more accurate results compared to the state-of-theart data-driven methods in terms of both automatic metrics and subjective evaluation.",0
"Automatic image synthesis research has been rapidly growing with deep networks getting more and more expressive. In the last couple of years, we have observed images of digits, indoor scenes, birds, chairs, etc. being automatically generated. The expressive power of image generators have also been enhanced by introducing several forms of conditioning variables such as object names, sentences, bounding box and key-point locations. In this work, we propose a novel deep conditional generative adversarial network architecture that takes its strength from the semantic layout and scene attributes integrated as conditioning variables. We show that our architecture is able to generate realistic outdoor scene images under different conditions, e.g. daynight, sunny-foggy, with clear object boundaries. ","У цій статті ми пропонуємо новий підхід до розширення запиту для покращення автоматичного створення підписів до зображень на основі передачі. Основна ідея нашого методу полягає в тому, щоб перевести заданий візуальний запит у форму, засновану на семантиці розподілу, яка генерується середнім значенням векторів речень, витягнутих із підписів зображень, візуально схожих на вхідне зображення. Використовуючи три контрольних набори даних субтитрів до зображень, ми показуємо, що наш підхід дає точніші результати порівняно з сучасними методами, що керуються даними, з точки зору як автоматичних показників, так і суб’єктивної оцінки.",0
"Дослідження автоматичного синтезу зображень стрімко розвиваються, а глибокі мережі стають дедалі виразнішими. За останні пару років ми спостерігали автоматичне генерування зображень цифр, сцен у приміщенні, птахів, стільців тощо. Виражальну силу генераторів зображень також було покращено завдяки введенню кількох форм обумовлюючих змінних, таких як назви об’єктів, речення, обмежувальна рамка та розташування ключових точок. У цій роботі ми пропонуємо нову глибоку умовну генеруючу змагальну архітектуру мережі, яка бере свою силу від семантичного макету та атрибутів сцени, інтегрованих як змінні умови. Ми показуємо, що наша архітектура здатна генерувати реалістичні зображення зовнішньої сцени за різних умов, наприклад, вдень, у сонячну погоду з туманом, із чіткими межами об’єктів.","In this paper, we propose a novel query expansion approach for improving transferbased automatic image captioning. The core idea of our method is to translate the given visual query into a distributional semantics based form, which is generated by the average of the sentence vectors extracted from the captions of images visually similar to the input image. Using three image captioning benchmark datasets, we show that our approach provides more accurate results compared to the state-of-theart data-driven methods in terms of both automatic metrics and subjective evaluation.",0
"Дослідження автоматичного синтезу зображень стрімко розвиваються, а глибокі мережі стають дедалі виразнішими. За останні пару років ми спостерігали автоматичне генерування зображень цифр, сцен у приміщенні, птахів, стільців тощо. Виражальну силу генераторів зображень також було покращено завдяки введенню кількох форм обумовлюючих змінних, таких як назви об’єктів, речення, обмежувальна рамка та розташування ключових точок. У цій роботі ми пропонуємо нову глибоку умовну генеруючу змагальну архітектуру мережі, яка бере свою силу від семантичного макету та атрибутів сцени, інтегрованих як змінні умови. Ми показуємо, що наша архітектура здатна генерувати реалістичні зображення зовнішньої сцени за різних умов, наприклад, вдень, у сонячну погоду з туманом, із чіткими межами об’єктів.","У цій статті ми пропонуємо новий підхід до розширення запиту для покращення автоматичного створення підписів до зображень на основі передачі. Основна ідея нашого методу полягає в тому, щоб перевести заданий візуальний запит у форму, засновану на семантиці розподілу, яка генерується середнім значенням векторів речень, витягнутих із підписів зображень, візуально схожих на вхідне зображення. Використовуючи три контрольних набори даних субтитрів до зображень, ми показуємо, що наш підхід дає точніші результати порівняно з сучасними методами, що керуються даними, з точки зору як автоматичних показників, так і суб’єктивної оцінки.",0
"In this study, we explore whether the captions in the wild can boost the performance of object detection in images. Captions that accompany images usually provide significant information about the visual content of the image, making them an important resource for image understanding. However, captions in the wild are likely to include numerous types of noises which can hurt visual estimation. In this paper, we propose data-driven methods to deal with the noisy captions and utilize them to improve object detection. We show how a pre-trained state-of-theart object detector can take advantage of noisy captions. Our experiments demonstrate that captions provide promising cues about the visual content of the images and can aid in improving object detection.","Automatic generation of video descriptions in natural language, also called video captioning, aims to understand the visual content of the video and produce a natural language sentence depicting the objects and actions in the scene. This challenging integrated vision and language problem, however, has been predominantly addressed for English. The lack of data and the linguistic properties of other languages limit the success of existing approaches for such languages. In this paper we target Turkish, a morphologically rich and agglutinative language that has very different properties compared to English. To do so, we create the first large scale video captioning dataset for this language by carefully translating the English descriptions of the videos in the MSVD (Microsoft Research Video Description Corpus) dataset into Turkish. In addition to enabling research in video captioning in Turkish, the parallel English-Turkish descriptions also enables the study of the role of video context in (multimodal) machine translation. In our experiments, we build models for both video captioning and multimodal machine translation and investigate the effect of different word segmentation approaches and different neural architectures to better address the properties of Turkish. We hope that the MSVDTurkish dataset and the results reported in this work will lead to better video captioning and multimodal machine translation models for Turkish and other morphology rich and agglutinative languages.",0
"In this study, we explore whether the captions in the wild can boost the performance of object detection in images. Captions that accompany images usually provide significant information about the visual content of the image, making them an important resource for image understanding. However, captions in the wild are likely to include numerous types of noises which can hurt visual estimation. In this paper, we propose data-driven methods to deal with the noisy captions and utilize them to improve object detection. We show how a pre-trained state-of-theart object detector can take advantage of noisy captions. Our experiments demonstrate that captions provide promising cues about the visual content of the images and can aid in improving object detection.","Автоматична генерація відеоописів природною мовою, яка також називається відеотитрами, має на меті зрозуміти візуальний вміст відео та створити речення природною мовою, що зображує об’єкти та дії в сцені. Ця складна інтегрована проблема бачення та мови, однак, була переважно розглянута для англійської мови. Брак даних і лінгвістичні властивості інших мов обмежують успіх існуючих підходів для таких мов. У цій статті ми орієнтуємося на турецьку, морфологічно багату та аглютинативну мову, яка має дуже різні властивості порівняно з англійською. Для цього ми створюємо перший великомасштабний набір даних субтитрів до відео для цієї мови, ретельно перекладаючи англійські описи відео в наборі даних MSVD (Microsoft Research Video Description Corpus) на турецьку мову. На додаток до дослідження субтитрів до відео турецькою мовою, паралельні англо-турецькі описи також дозволяють вивчати роль відеоконтексту в (мультимодальному) машинному перекладі. У наших експериментах ми створюємо моделі як для субтитрів до відео, так і для мультимодального машинного перекладу та досліджуємо вплив різних підходів до сегментації слів і різних нейронних архітектур для кращого врахування властивостей турецької мови. Ми сподіваємося, що набір даних MSVDTurkish і результати, наведені в цій роботі, призведуть до кращих відеосубтитрів і мультимодальних моделей машинного перекладу для турецької та інших морфологічних і аглютинативних мов.",0
"У цьому дослідженні ми досліджуємо, чи можуть підписи в дикій природі підвищити ефективність виявлення об’єктів на зображеннях. Підписи, які супроводжують зображення, зазвичай надають значну інформацію про візуальний вміст зображення, що робить їх важливим ресурсом для розуміння зображення. Проте підписи в дикій природі, ймовірно, містять численні типи шумів, які можуть зашкодити візуальній оцінці. У цій статті ми пропонуємо керовані даними методи боротьби з зашумленими титрами та використовуємо їх для покращення виявлення об’єктів. Ми показуємо, як попередньо навчений сучасний детектор об’єктів може використовувати шумні титри. Наші експерименти демонструють, що підписи дають багатообіцяючі підказки щодо візуального вмісту зображень і можуть допомогти покращити виявлення об’єктів.","Automatic generation of video descriptions in natural language, also called video captioning, aims to understand the visual content of the video and produce a natural language sentence depicting the objects and actions in the scene. This challenging integrated vision and language problem, however, has been predominantly addressed for English. The lack of data and the linguistic properties of other languages limit the success of existing approaches for such languages. In this paper we target Turkish, a morphologically rich and agglutinative language that has very different properties compared to English. To do so, we create the first large scale video captioning dataset for this language by carefully translating the English descriptions of the videos in the MSVD (Microsoft Research Video Description Corpus) dataset into Turkish. In addition to enabling research in video captioning in Turkish, the parallel English-Turkish descriptions also enables the study of the role of video context in (multimodal) machine translation. In our experiments, we build models for both video captioning and multimodal machine translation and investigate the effect of different word segmentation approaches and different neural architectures to better address the properties of Turkish. We hope that the MSVDTurkish dataset and the results reported in this work will lead to better video captioning and multimodal machine translation models for Turkish and other morphology rich and agglutinative languages.",0
"У цьому дослідженні ми досліджуємо, чи можуть підписи в дикій природі підвищити ефективність виявлення об’єктів на зображеннях. Підписи, які супроводжують зображення, зазвичай надають значну інформацію про візуальний вміст зображення, що робить їх важливим ресурсом для розуміння зображення. Проте підписи в дикій природі, ймовірно, містять численні типи шумів, які можуть зашкодити візуальній оцінці. У цій статті ми пропонуємо керовані даними методи боротьби з зашумленими титрами та використовуємо їх для покращення виявлення об’єктів. Ми показуємо, як попередньо навчений сучасний детектор об’єктів може використовувати шумні титри. Наші експерименти демонструють, що підписи дають багатообіцяючі підказки щодо візуального вмісту зображень і можуть допомогти покращити виявлення об’єктів.","Автоматична генерація відеоописів природною мовою, яка також називається відеотитрами, має на меті зрозуміти візуальний вміст відео та створити речення природною мовою, що зображує об’єкти та дії в сцені. Ця складна інтегрована проблема бачення та мови, однак, була переважно розглянута для англійської мови. Брак даних і лінгвістичні властивості інших мов обмежують успіх існуючих підходів для таких мов. У цій статті ми орієнтуємося на турецьку, морфологічно багату та аглютинативну мову, яка має дуже різні властивості порівняно з англійською. Для цього ми створюємо перший великомасштабний набір даних субтитрів до відео для цієї мови, ретельно перекладаючи англійські описи відео в наборі даних MSVD (Microsoft Research Video Description Corpus) на турецьку мову. На додаток до дослідження субтитрів до відео турецькою мовою, паралельні англо-турецькі описи також дозволяють вивчати роль відеоконтексту в (мультимодальному) машинному перекладі. У наших експериментах ми створюємо моделі як для субтитрів до відео, так і для мультимодального машинного перекладу та досліджуємо вплив різних підходів до сегментації слів і різних нейронних архітектур для кращого врахування властивостей турецької мови. Ми сподіваємося, що набір даних MSVDTurkish і результати, наведені в цій роботі, призведуть до кращих відеосубтитрів і мультимодальних моделей машинного перекладу для турецької та інших морфологічних і аглютинативних мов.",0
"In recent years, visual saliency estimation in images has attracted much attention in the computer vision community. However, predicting saliency in videos has received relatively little attention. Inspired by the recent success of deep convolutional neural networks based static saliency models, in this work, we study two different two-stream convolutional networks for dynamic saliency prediction. To improve the generalization capability of our models, we also introduce a novel, empirically grounded data augmentation technique for this task. We test our models on DIEM dataset and report superior results against the existing models. Moreover, we perform transfer learning experiments on SALICON, a recently proposed static saliency dataset, by finetuning our models on the optical flows estimated from static images. Our experiments show that taking motion into account in this way can be helpful for static saliency estimation. ","Predicting where a photo was taken is quite important and yet a challenging task for computer vision algorithms. Our motivation is to solve this difficult problem in a cityscale setting by employing a data-driven approach. In order to pursue this goal, we developed a fast and robust scene matching method that follows a coarse-to-fine strategy. In particular, we combine scene retrieval via global features and dense scene alignment and use a large set of geo-tagged images of downtown San Francisco in our evaluation. The experimental results show that the proposed approach, despite its simplicity, is surprisingly effective and achieves comparable results with the state-of-the-art.",0
"In recent years, visual saliency estimation in images has attracted much attention in the computer vision community. However, predicting saliency in videos has received relatively little attention. Inspired by the recent success of deep convolutional neural networks based static saliency models, in this work, we study two different two-stream convolutional networks for dynamic saliency prediction. To improve the generalization capability of our models, we also introduce a novel, empirically grounded data augmentation technique for this task. We test our models on DIEM dataset and report superior results against the existing models. Moreover, we perform transfer learning experiments on SALICON, a recently proposed static saliency dataset, by finetuning our models on the optical flows estimated from static images. Our experiments show that taking motion into account in this way can be helpful for static saliency estimation. ","Передбачити, де було зроблено фотографію, є досить важливим і водночас складним завданням для алгоритмів комп’ютерного зору. Наша мотивація полягає в тому, щоб вирішити цю складну проблему в умовах міста за допомогою підходу, керованого даними. Щоб досягти цієї мети, ми розробили швидкий і надійний метод зіставлення сцен, який дотримується стратегії від грубого до точного. Зокрема, ми поєднуємо пошук сцени за допомогою глобальних функцій і щільного вирівнювання сцени та використовуємо великий набір зображень центру Сан-Франциско з геотегами в нашій оцінці. Експериментальні результати показують, що запропонований підхід, незважаючи на його простоту, є напрочуд ефективним і досягає результатів, порівнянних із сучасними.",0
"В останні роки оцінка візуальної помітності зображень привернула велику увагу в спільноті комп’ютерного зору. Однак прогнозуванню помітності у відео приділено відносно мало уваги. Натхненні нещодавнім успіхом глибоких згорткових нейронних мереж на основі статичних моделей помітності, у цій роботі ми вивчаємо дві різні двопотокові згорткові мережі для динамічного прогнозування помітності. Щоб покращити можливості узагальнення наших моделей, ми також запровадили нову, емпірично обґрунтовану техніку збільшення даних для цього завдання. Ми перевіряємо наші моделі на наборі даних DIEM і повідомляємо про кращі результати порівняно з існуючими моделями. Крім того, ми проводимо експерименти з навчанням переносу на SALICON, нещодавно запропонованому наборі даних статичної помітності, шляхом точного налаштування наших моделей на оптичних потоках, оцінених за статичними зображеннями. Наші експерименти показують, що врахування руху таким чином може бути корисним для оцінки статичної помітності.","Predicting where a photo was taken is quite important and yet a challenging task for computer vision algorithms. Our motivation is to solve this difficult problem in a cityscale setting by employing a data-driven approach. In order to pursue this goal, we developed a fast and robust scene matching method that follows a coarse-to-fine strategy. In particular, we combine scene retrieval via global features and dense scene alignment and use a large set of geo-tagged images of downtown San Francisco in our evaluation. The experimental results show that the proposed approach, despite its simplicity, is surprisingly effective and achieves comparable results with the state-of-the-art.",0
"В останні роки оцінка візуальної помітності зображень привернула велику увагу в спільноті комп’ютерного зору. Однак прогнозуванню помітності у відео приділено відносно мало уваги. Натхненні нещодавнім успіхом глибоких згорткових нейронних мереж на основі статичних моделей помітності, у цій роботі ми вивчаємо дві різні двопотокові згорткові мережі для динамічного прогнозування помітності. Щоб покращити можливості узагальнення наших моделей, ми також запровадили нову, емпірично обґрунтовану техніку збільшення даних для цього завдання. Ми перевіряємо наші моделі на наборі даних DIEM і повідомляємо про кращі результати порівняно з існуючими моделями. Крім того, ми проводимо експерименти з навчанням переносу на SALICON, нещодавно запропонованому наборі даних статичної помітності, шляхом точного налаштування наших моделей на оптичних потоках, оцінених за статичними зображеннями. Наші експерименти показують, що врахування руху таким чином може бути корисним для оцінки статичної помітності.","Передбачити, де було зроблено фотографію, є досить важливим і водночас складним завданням для алгоритмів комп’ютерного зору. Наша мотивація полягає в тому, щоб вирішити цю складну проблему в умовах міста за допомогою підходу, керованого даними. Щоб досягти цієї мети, ми розробили швидкий і надійний метод зіставлення сцен, який дотримується стратегії від грубого до точного. Зокрема, ми поєднуємо пошук сцени за допомогою глобальних функцій і щільного вирівнювання сцени та використовуємо великий набір зображень центру Сан-Франциско з геотегами в нашій оцінці. Експериментальні результати показують, що запропонований підхід, незважаючи на його простоту, є напрочуд ефективним і досягає результатів, порівнянних із сучасними.",0
"Correlation filters have recently attracted attention in visual tracking due to their efficiency and high per- formance. However, their application to long-term tracking is somewhat limited since these trackers are not equipped with mechanisms to cope with challenging cases like partial occlusion, deformation or scale changes. In this paper, we propose a deformable part-based correlation filter tracking approach which depends on coupled interactions between a global filter and several part filters. Specifically, local filters provide an initial estimate, which is then used by the global filter as a reference to determine the final result. Then, the global filter provides a feedback to the part filters regarding their updates and the related deformation parameters. In this way, our proposed collaborative model handles not only partial occlusion but also scale changes. Experiments on two large public benchmark datasets demonstrate that our approach gives significantly better results compared with the state-of-the-art trackers.","Pushing is an essential non-prehensile manipulation skill used for tasks ranging from pre-grasp manipulation to scene rearrangement, reasoning about object relations in the scene, and thus pushing actions have been widely studied in robotics. The effective use of pushing actions often requires an understanding of the dynamics of the manipulated objects and adaptation to the discrepancies between prediction and reality. For this reason, effect prediction and parameter estimation with pushing actions have been heavily investigated in the literature. However, current approaches are limited because they either model systems with a fixed number of objects or use image-based representations whose outputs are not very interpretable and quickly accumulate errors. In this paper, we propose a graph neural network based framework for effect prediction and parameter estimation of pushing actions by modeling object relations based on contacts or articulations. Our framework is validated both in real and simulated environments containing different shaped multi-part objects connected via different types of joints and objects with different masses. Our approach enables the robot to predict and adapt the effect of a pushing action as it observes the scene. Further, we demonstrate 6D effect prediction in the lever-up action in the context of robot-based hard-disk disassembly",0
"Correlation filters have recently attracted attention in visual tracking due to their efficiency and high per- formance. However, their application to long-term tracking is somewhat limited since these trackers are not equipped with mechanisms to cope with challenging cases like partial occlusion, deformation or scale changes. In this paper, we propose a deformable part-based correlation filter tracking approach which depends on coupled interactions between a global filter and several part filters. Specifically, local filters provide an initial estimate, which is then used by the global filter as a reference to determine the final result. Then, the global filter provides a feedback to the part filters regarding their updates and the related deformation parameters. In this way, our proposed collaborative model handles not only partial occlusion but also scale changes. Experiments on two large public benchmark datasets demonstrate that our approach gives significantly better results compared with the state-of-the-art trackers.","Поштовх — це важлива навичка маніпуляції, яка не хватає, і використовується для виконання завдань, починаючи від маніпуляцій перед захопленням до перестановки сцени, міркувань про об’єктні відносини в сцені, і, отже, дії поштовху широко вивчаються в робототехніці. Ефективне використання штовхаючих дій часто вимагає розуміння динаміки об&#39;єктів, якими маніпулюють, і адаптації до розбіжностей між прогнозом і реальністю. З цієї причини прогнозування ефекту та оцінка параметрів за допомогою виштовхувальних дій були ретельно досліджені в літературі. Однак поточні підходи обмежені, оскільки вони або моделюють системи з фіксованою кількістю об’єктів, або використовують представлення на основі зображень, результати яких не дуже добре інтерпретуються та швидко накопичують помилки. У цій статті ми пропонуємо структуру на основі графової нейронної мережі для прогнозування ефекту та оцінки параметрів штовхаючих дій шляхом моделювання об’єктних відносин на основі контактів або артикуляцій. Наша структура перевірена як у реальному, так і в симульованому середовищі, що містить багатокомпонентні об’єкти різної форми, з’єднані за допомогою різних типів з’єднань, і об’єкти з різною масою. Наш підхід дозволяє роботу передбачити та адаптувати ефект штовхання, коли він спостерігає за сценою. Крім того, ми демонструємо прогнозування 6D-ефекту під час дії важеля вгору в контексті роботизованого розбирання жорсткого диска",0
"Кореляційні фільтри нещодавно привернули увагу у візуальному відстеженні завдяки своїй ефективності та високій продуктивності. Однак їхнє застосування для тривалого відстеження дещо обмежене, оскільки ці трекери не оснащені механізмами, щоб впоратися зі складними випадками, такими як часткова оклюзія, деформація або зміни масштабу. У цій статті ми пропонуємо підхід відстеження кореляційного фільтра на основі деформованих частин, який залежить від зв’язаних взаємодій між глобальним фільтром і декількома фільтрами частин. Зокрема, локальні фільтри забезпечують початкову оцінку, яка потім використовується глобальним фільтром як еталон для визначення кінцевого результату. Потім глобальний фільтр надає зворотний зв’язок фільтрам частин щодо їх оновлень і відповідних параметрів деформації. Таким чином, наша запропонована спільна модель обробляє не лише часткову оклюзію, але й зміни масштабу. Експерименти на двох великих загальнодоступних наборах даних показують, що наш підхід дає значно кращі результати порівняно з найсучаснішими трекерами.","Pushing is an essential non-prehensile manipulation skill used for tasks ranging from pre-grasp manipulation to scene rearrangement, reasoning about object relations in the scene, and thus pushing actions have been widely studied in robotics. The effective use of pushing actions often requires an understanding of the dynamics of the manipulated objects and adaptation to the discrepancies between prediction and reality. For this reason, effect prediction and parameter estimation with pushing actions have been heavily investigated in the literature. However, current approaches are limited because they either model systems with a fixed number of objects or use image-based representations whose outputs are not very interpretable and quickly accumulate errors. In this paper, we propose a graph neural network based framework for effect prediction and parameter estimation of pushing actions by modeling object relations based on contacts or articulations. Our framework is validated both in real and simulated environments containing different shaped multi-part objects connected via different types of joints and objects with different masses. Our approach enables the robot to predict and adapt the effect of a pushing action as it observes the scene. Further, we demonstrate 6D effect prediction in the lever-up action in the context of robot-based hard-disk disassembly",0
"Кореляційні фільтри нещодавно привернули увагу у візуальному відстеженні завдяки своїй ефективності та високій продуктивності. Однак їхнє застосування для тривалого відстеження дещо обмежене, оскільки ці трекери не оснащені механізмами, щоб впоратися зі складними випадками, такими як часткова оклюзія, деформація або зміни масштабу. У цій статті ми пропонуємо підхід відстеження кореляційного фільтра на основі деформованих частин, який залежить від зв’язаних взаємодій між глобальним фільтром і декількома фільтрами частин. Зокрема, локальні фільтри забезпечують початкову оцінку, яка потім використовується глобальним фільтром як еталон для визначення кінцевого результату. Потім глобальний фільтр надає зворотний зв’язок фільтрам частин щодо їх оновлень і відповідних параметрів деформації. Таким чином, наша запропонована спільна модель обробляє не лише часткову оклюзію, але й зміни масштабу. Експерименти на двох великих загальнодоступних наборах даних показують, що наш підхід дає значно кращі результати порівняно з найсучаснішими трекерами.","Поштовх — це важлива навичка маніпуляції, яка не хватає, і використовується для виконання завдань, починаючи від маніпуляцій перед захопленням до перестановки сцени, міркувань про об’єктні відносини в сцені, і, отже, дії поштовху широко вивчаються в робототехніці. Ефективне використання штовхаючих дій часто вимагає розуміння динаміки об&#39;єктів, якими маніпулюють, і адаптації до розбіжностей між прогнозом і реальністю. З цієї причини прогнозування ефекту та оцінка параметрів за допомогою виштовхувальних дій були ретельно досліджені в літературі. Однак поточні підходи обмежені, оскільки вони або моделюють системи з фіксованою кількістю об’єктів, або використовують представлення на основі зображень, результати яких не дуже добре інтерпретуються та швидко накопичують помилки. У цій статті ми пропонуємо структуру на основі графової нейронної мережі для прогнозування ефекту та оцінки параметрів штовхаючих дій шляхом моделювання об’єктних відносин на основі контактів або артикуляцій. Наша структура перевірена як у реальному, так і в симульованому середовищі, що містить багатокомпонентні об’єкти різної форми, з’єднані за допомогою різних типів з’єднань, і об’єкти з різною масою. Наш підхід дозволяє роботу передбачити та адаптувати ефект штовхання, коли він спостерігає за сценою. Крім того, ми демонструємо прогнозування 6D-ефекту під час дії важеля вгору в контексті роботизованого розбирання жорсткого диска",0
"Reconstructing high dynamic range (HDR) images of a complex scene involving moving objects and dynamic backgrounds is prone to artifacts. A large number of methods have been proposed that attempt to alleviate these artifacts, known as HDR deghosting algorithms. Currently, the quality of these algorithms are judged by subjective evaluations, which are tedious to conduct and get quickly outdated as new algorithms are proposed on a rapid basis. In this paper, we propose an objective metric which aims to simplify this process. Our metric takes a stack of input exposures and the deghosting result and produces a set of artifact maps for different types of artifacts. These artifact maps can be combined to yield a single quality score. We performed a subjective experiment involving 52 subjects and 16 different scenes to validate the agreement of our quality scores with subjective judgements and observed a concordance of almost 80%. Our metric also enables a novel application that we call as hybrid deghosting, in which the output of different deghosting algorithms are combined to obtain a superior deghosting result.","Existing computational models of visual attention generally employ simple image features such as color, intensity or orientation to generate a saliency map which highlights the image parts that attract human attention. Interestingly, most of these models do not process any depth information and operate only on standard two-dimensional RGB images. On the other hand, depth processing through stereo vision is a key characteristics of the human visual system. In line with this observation, in this study, we propose to extend two state-of-the-art static saliency models that depend on region covariances to process additional depth information available in RGB-D images. We evaluate our proposed models on NUS-3D benchmark dataset by taking into account different evaluation metrics. Our results reveal that using the additional depth information improves the saliency prediction in a statistically significant manner, giving more accurate saliency maps.  ",0
"Reconstructing high dynamic range (HDR) images of a complex scene involving moving objects and dynamic backgrounds is prone to artifacts. A large number of methods have been proposed that attempt to alleviate these artifacts, known as HDR deghosting algorithms. Currently, the quality of these algorithms are judged by subjective evaluations, which are tedious to conduct and get quickly outdated as new algorithms are proposed on a rapid basis. In this paper, we propose an objective metric which aims to simplify this process. Our metric takes a stack of input exposures and the deghosting result and produces a set of artifact maps for different types of artifacts. These artifact maps can be combined to yield a single quality score. We performed a subjective experiment involving 52 subjects and 16 different scenes to validate the agreement of our quality scores with subjective judgements and observed a concordance of almost 80%. Our metric also enables a novel application that we call as hybrid deghosting, in which the output of different deghosting algorithms are combined to obtain a superior deghosting result.","Існуючі обчислювальні моделі зорової уваги зазвичай використовують прості характеристики зображення, такі як колір, інтенсивність або орієнтація, щоб створити карту помітності, яка виділяє частини зображення, які привертають увагу людини. Цікаво, що більшість із цих моделей не обробляють інформацію про глибину й працюють лише зі стандартними двовимірними зображеннями RGB. З іншого боку, обробка глибини за допомогою стереозору є ключовою характеристикою зорової системи людини. Згідно з цим спостереженням, у цьому дослідженні ми пропонуємо розширити дві найсучасніші статичні моделі помітності, які залежать від коваріацій регіону, для обробки додаткової інформації про глибину, доступної в зображеннях RGB-D. Ми оцінюємо запропоновані нами моделі на основі порівняльного набору даних NUS-3D, беручи до уваги різні метрики оцінювання. Наші результати показують, що використання додаткової інформації про глибину покращує прогнозування помітності статистично значущим чином, надаючи точніші карти помітності.",0
"Реконструкція зображень із широким динамічним діапазоном (HDR) складної сцени з рухомими об’єктами та динамічним фоном схильна до артефактів. Було запропоновано велику кількість методів, які намагаються усунути ці артефакти, відомих як алгоритми видалення фантом HDR. Наразі якість цих алгоритмів оцінюється за суб’єктивними оцінками, які нудно проводити та швидко застарівають, оскільки нові алгоритми пропонуються швидко. У цій статті ми пропонуємо об’єктивну метрику, яка має на меті спростити цей процес. Наша метрика бере набір вхідних експозицій і результат видалення фантомних зображень і створює набір карт артефактів для різних типів артефактів. Ці карти артефактів можна поєднувати, щоб отримати єдиний показник якості. Ми провели суб’єктивний експеримент із залученням 52 суб’єктів і 16 різних сцен, щоб перевірити узгодженість наших показників якості з суб’єктивними судженнями та спостерігали узгодженість майже на 80%. Наша метрика також дозволяє нову програму, яку ми називаємо гібридним видаленням фантомних зображень, у якому вихідні дані різних алгоритмів видалення фантомних зображень поєднуються для отримання чудового результату видалення фантомних зображень.","Existing computational models of visual attention generally employ simple image features such as color, intensity or orientation to generate a saliency map which highlights the image parts that attract human attention. Interestingly, most of these models do not process any depth information and operate only on standard two-dimensional RGB images. On the other hand, depth processing through stereo vision is a key characteristics of the human visual system. In line with this observation, in this study, we propose to extend two state-of-the-art static saliency models that depend on region covariances to process additional depth information available in RGB-D images. We evaluate our proposed models on NUS-3D benchmark dataset by taking into account different evaluation metrics. Our results reveal that using the additional depth information improves the saliency prediction in a statistically significant manner, giving more accurate saliency maps.  ",0
"Реконструкція зображень із широким динамічним діапазоном (HDR) складної сцени з рухомими об’єктами та динамічним фоном схильна до артефактів. Було запропоновано велику кількість методів, які намагаються зменшити ці артефакти, відомих як алгоритми видалення фантом HDR. Наразі якість цих алгоритмів оцінюється за суб’єктивними оцінками, які нудно проводити та швидко застарівають, оскільки нові алгоритми пропонуються швидко. У цій статті ми пропонуємо об’єктивну метрику, яка має на меті спростити цей процес. Наша метрика бере набір вхідних експозицій і результат видалення фантомних зображень і створює набір карт артефактів для різних типів артефактів. Ці карти артефактів можна комбінувати, щоб отримати єдиний показник якості. Ми провели суб’єктивний експеримент із залученням 52 суб’єктів і 16 різних сцен, щоб перевірити узгодженість наших показників якості з суб’єктивними судженнями та спостерігали узгодженість майже на 80%. Наша метрика також дозволяє нову програму, яку ми називаємо гібридним видаленням фантомних зображень, у якому вихідні дані різних алгоритмів видалення фантомних зображень поєднуються для отримання чудового результату видалення фантомних зображень.","Існуючі обчислювальні моделі зорової уваги зазвичай використовують прості характеристики зображення, такі як колір, інтенсивність або орієнтація, щоб створити карту помітності, яка виділяє частини зображення, які привертають увагу людини. Цікаво, що більшість із цих моделей не обробляють інформацію про глибину й працюють лише зі стандартними двовимірними зображеннями RGB. З іншого боку, обробка глибини за допомогою стереозору є ключовою характеристикою зорової системи людини. Згідно з цим спостереженням, у цьому дослідженні ми пропонуємо розширити дві найсучасніші статичні моделі помітності, які залежать від коваріацій регіону, для обробки додаткової інформації про глибину, доступної в зображеннях RGB-D. Ми оцінюємо запропоновані нами моделі на основі порівняльного набору даних NUS-3D, беручи до уваги різні метрики оцінювання. Наші результати показують, що використання додаткової інформації про глибину покращує прогнозування помітності статистично значущим чином, надаючи точніші карти помітності.",0
"Automatic description generation from natural images is a challenging problem thathas recently received a large amount of interest from the computer vision and natural lan-guage processing communities.  In this survey,  we classify the existing approaches basedon how they conceptualize this problem, viz., models that cast description as either gen-eration  problem  or  as  a  retrieval  problem  over  a  visual  or  multimodal  representationalspace.  We provide a detailed review of existing models, highlighting their advantages anddisadvantages.  Moreover, we give an overview of the benchmark image datasets and theevaluation measures that have been developed to assess the quality of machine-generatedimage descriptions.  Finally we extrapolate future directions in the area of automatic imagedescription generation.","Obtaining a high quality high dynamic range (HDR) image in the presence of camera and object movement has been a long-standing challenge. Many methods, known as HDR deghosting algorithms, have been developed over the past ten years to undertake this challenge. Each of these algorithms approaches the deghosting problem from a different perspective, providing solutions with different degrees of complexity, solutions that range from rudimentary heuristics to advanced computer vision techniques. The proposed solutions generally differ in two ways: (1) how to detect ghost regions and (2) what to do to eliminate ghosts. Some algorithms choose to completely discard moving objects giving rise to HDR images which only contain the static regions. Some other algorithms try to find the best image to use for each dynamic region. Yet others try to register moving objects from different images in the spirit of maximizing dynamic range in dynamic regions. Furthermore, each algorithm may introduce different types of artifacts as they aim to eliminate ghosts. These artifacts may come in the form of noise, broken objects, under- and over-exposed regions, and residual ghosting. Given the high volume of studies conducted in this field over the recent years, a comprehensive survey of the state of the art is required. Thus, the first goal of this paper is to provide this survey. Secondly, the large number of algorithms brings about the need to classify them. Thus the second goal of this paper is to propose a taxonomy of deghosting algorithms which can be used to group existing and future algorithms into meaningful classes. Thirdly, the existence of a large number of algorithms brings about the need to evaluate their effectiveness, as each new algorithm claims to outperform its precedents. Therefore, the last goal of this paper is to share the results of a subjective experiment which aims to evaluate various state-of-the-art deghosting algorithms.",0
"Automatic description generation from natural images is a challenging problem thathas recently received a large amount of interest from the computer vision and natural lan-guage processing communities.  In this survey,  we classify the existing approaches basedon how they conceptualize this problem, viz., models that cast description as either gen-eration  problem  or  as  a  retrieval  problem  over  a  visual  or  multimodal  representationalspace.  We provide a detailed review of existing models, highlighting their advantages anddisadvantages.  Moreover, we give an overview of the benchmark image datasets and theevaluation measures that have been developed to assess the quality of machine-generatedimage descriptions.  Finally we extrapolate future directions in the area of automatic imagedescription generation.","Отримання високоякісного зображення з широким динамічним діапазоном (HDR) за наявності камери та руху об’єктів було давньою проблемою. За останні десять років було розроблено багато методів, відомих як алгоритми видалення фантом HDR, щоб вирішити цю проблему. Кожен із цих алгоритмів підходить до проблеми видалення фантом з іншої точки зору, надаючи рішення з різним ступенем складності, рішення, що варіюються від рудиментарної евристики до вдосконалених методів комп’ютерного зору. Пропоновані рішення зазвичай відрізняються двома способами: (1) як виявити привиди та (2) що робити, щоб усунути привиди. Деякі алгоритми вирішують повністю відкидати рухомі об’єкти, створюючи зображення HDR, які містять лише статичні області. Деякі інші алгоритми намагаються знайти найкраще зображення для використання для кожної динамічної області. Треті намагаються зареєструвати рухомі об’єкти з різних зображень у дусі максимізації динамічного діапазону в динамічних областях. Крім того, кожен алгоритм може вводити різні типи артефактів, оскільки вони спрямовані на усунення привидів. Ці артефакти можуть проявлятися у вигляді шуму, зламаних об’єктів, недо- та переекспонованих областей, а також залишкових ореолів. Враховуючи великий обсяг досліджень, проведених у цій галузі за останні роки, необхідний комплексний огляд сучасного стану. Таким чином, першою метою цієї статті є проведення цього опитування. По-друге, велика кількість алгоритмів викликає необхідність їх класифікації. Таким чином, друга мета цієї статті полягає в тому, щоб запропонувати таксономію алгоритмів видалення фантомних зображень, яка може бути використана для групування існуючих і майбутніх алгоритмів у значущі класи. По-третє, існування великої кількості алгоритмів викликає необхідність оцінки їх ефективності, оскільки кожен новий алгоритм претендує на перевершення своїх прецедентів. Тому останньою метою цієї статті є поділитися результатами суб’єктивного експерименту, який має на меті оцінити різні найсучасніші алгоритми видалення фантомних зображень.",0
"Автоматичне створення опису з природних зображень є складною проблемою, яка нещодавно привернула великий інтерес у спільнот комп’ютерного зору та обробки природної мови. У цьому дослідженні ми класифікуємо існуючі підходи на основі того, як вони концептуалізують цю проблему, а саме моделі, які перетворюють опис як проблему генерації або як проблему пошуку у візуальному чи мультимодальному репрезентативному просторі. Ми надаємо детальний огляд існуючих моделей, висвітлюючи їх переваги та недоліки. Крім того, ми надаємо огляд еталонних наборів даних зображень і заходів оцінки, які були розроблені для оцінки якості описів зображень, створених машиною. Нарешті ми екстраполюємо майбутні напрямки в області автоматичного створення опису зображення.","Obtaining a high quality high dynamic range (HDR) image in the presence of camera and object movement has been a long-standing challenge. Many methods, known as HDR deghosting algorithms, have been developed over the past ten years to undertake this challenge. Each of these algorithms approaches the deghosting problem from a different perspective, providing solutions with different degrees of complexity, solutions that range from rudimentary heuristics to advanced computer vision techniques. The proposed solutions generally differ in two ways: (1) how to detect ghost regions and (2) what to do to eliminate ghosts. Some algorithms choose to completely discard moving objects giving rise to HDR images which only contain the static regions. Some other algorithms try to find the best image to use for each dynamic region. Yet others try to register moving objects from different images in the spirit of maximizing dynamic range in dynamic regions. Furthermore, each algorithm may introduce different types of artifacts as they aim to eliminate ghosts. These artifacts may come in the form of noise, broken objects, under- and over-exposed regions, and residual ghosting. Given the high volume of studies conducted in this field over the recent years, a comprehensive survey of the state of the art is required. Thus, the first goal of this paper is to provide this survey. Secondly, the large number of algorithms brings about the need to classify them. Thus the second goal of this paper is to propose a taxonomy of deghosting algorithms which can be used to group existing and future algorithms into meaningful classes. Thirdly, the existence of a large number of algorithms brings about the need to evaluate their effectiveness, as each new algorithm claims to outperform its precedents. Therefore, the last goal of this paper is to share the results of a subjective experiment which aims to evaluate various state-of-the-art deghosting algorithms.",0
"Автоматичне створення опису з природних зображень є складною проблемою, яка нещодавно привернула великий інтерес у спільнот комп’ютерного зору та обробки природної мови. У цьому дослідженні ми класифікуємо існуючі підходи на основі того, як вони концептуалізують цю проблему, а саме моделі, які перетворюють опис як проблему генерації або як проблему пошуку у візуальному чи мультимодальному репрезентативному просторі. Ми надаємо детальний огляд існуючих моделей, висвітлюючи їх переваги та недоліки. Крім того, ми надаємо огляд еталонних наборів даних зображень і заходів оцінки, які були розроблені для оцінки якості описів зображень, створених машиною. Нарешті ми екстраполюємо майбутні напрямки в області автоматичного створення опису зображення.","Отримання високоякісного зображення з широким динамічним діапазоном (HDR) за наявності камери та руху об’єктів було давньою проблемою. За останні десять років для вирішення цього завдання було розроблено багато методів, відомих як алгоритми видалення фантом HDR. Кожен із цих алгоритмів підходить до проблеми видалення фантом з іншої точки зору, надаючи рішення з різним ступенем складності, рішення, що варіюються від рудиментарної евристики до вдосконалених методів комп’ютерного зору. Пропоновані рішення зазвичай відрізняються двома способами: (1) як виявити привиди та (2) що робити, щоб усунути привиди. Деякі алгоритми вирішують повністю відкидати рухомі об’єкти, створюючи зображення HDR, які містять лише статичні області. Деякі інші алгоритми намагаються знайти найкраще зображення для використання для кожної динамічної області. Треті намагаються зареєструвати рухомі об’єкти з різних зображень у дусі максимізації динамічного діапазону в динамічних областях. Крім того, кожен алгоритм може вводити різні типи артефактів, оскільки вони спрямовані на усунення привидів. Ці артефакти можуть проявлятися у вигляді шуму, зламаних об’єктів, недо- та переекспонованих областей, а також залишкових ореолів. Враховуючи великий обсяг досліджень, проведених у цій галузі за останні роки, необхідний комплексний огляд сучасного стану. Таким чином, першою метою цієї статті є проведення цього опитування. По-друге, велика кількість алгоритмів викликає необхідність їх класифікації. Таким чином, друга мета цієї статті полягає в тому, щоб запропонувати таксономію алгоритмів видалення фантомних зображень, яка може бути використана для групування існуючих і майбутніх алгоритмів у значущі класи. По-третє, існування великої кількості алгоритмів викликає необхідність оцінки їх ефективності, оскільки кожен новий алгоритм претендує на перевершення своїх прецедентів. Тому останньою метою цієї статті є поділитися результатами суб’єктивного експерименту, який має на меті оцінити різні найсучасніші алгоритми видалення фантомних зображень.",0
"In daily life, humans demonstrate an amazing ability to remember images they see on magazines, commercials, TV, web pages, etc. but automatic prediction of intrinsic memorability of images using computer vision and machine learning techniques has only been investigated very recently. Our goal in this article is to explore the role of visual attention and image semantics in understanding image memorability. In particular, we present an attention-driven spatial pooling strategy and show that considering image features from the salient parts of images improves the results of the previous models. We also investigate different semantic properties of images by carrying out an analysis of a diverse set of recently proposed semantic features which encode meta-level object categories, scene attributes, and invoked feelings. We show that these features which are automatically extracted from images provide memorability predictions as nearly accurate as those derived from human anno- tations. Moreover, our combined model yields results superior to those of state-of-the art fully automatic models.","In daily life, humans demonstrate astounding ability to remember images they see on magazines, commercials, TV, the web and so on, but automatic prediction of intrinsic memorability of images using computer vision and machine learning techniques was not investigated until a few years ago. However, despite these recent advances, none of the available approaches makes use of any attentional mechanism, a fundamental aspect of human vision, which selects relevant image regions for higher-level processing. Our goal in this paper is to explore the role of visual attention in understanding memorability of images. In particular, we present an attention-driven spatial pooling strategy for image memorability and show that the regions estimated by bottom-up and object-level saliency maps are more effective in predicting memorability than considering a fixed spatial pyramid structure as in the previous studies.",0
"In daily life, humans demonstrate an amazing ability to remember images they see on magazines, commercials, TV, web pages, etc. but automatic prediction of intrinsic memorability of images using computer vision and machine learning techniques has only been investigated very recently. Our goal in this article is to explore the role of visual attention and image semantics in understanding image memorability. In particular, we present an attention-driven spatial pooling strategy and show that considering image features from the salient parts of images improves the results of the previous models. We also investigate different semantic properties of images by carrying out an analysis of a diverse set of recently proposed semantic features which encode meta-level object categories, scene attributes, and invoked feelings. We show that these features which are automatically extracted from images provide memorability predictions as nearly accurate as those derived from human anno- tations. Moreover, our combined model yields results superior to those of state-of-the art fully automatic models.","У повсякденному житті люди демонструють приголомшливу здатність запам’ятовувати зображення, які вони бачать у журналах, рекламі, на телебаченні, в Інтернеті тощо, але автоматичне передбачення внутрішньої запам’ятовуваності зображень за допомогою комп’ютерного зору та методів машинного навчання досліджувалося лише кілька років тому. Однак, незважаючи на ці нещодавні досягнення, жоден із доступних підходів не використовує жодного механізму уваги, фундаментального аспекту людського зору, який вибирає відповідні області зображення для обробки на вищому рівні. Наша мета в цій статті — дослідити роль візуальної уваги в розумінні запам’ятовуваності зображень. Зокрема, ми представляємо орієнтовану на увагу стратегію просторового об’єднання для запам’ятовуваності зображення та показуємо, що регіони, оцінені за допомогою карт помітності знизу вгору та на рівні об’єктів, є більш ефективними для прогнозування запам’ятовуваності, ніж розгляд фіксованої просторової пірамідальної структури, як у попередніх дослідженнях.",0
"У повсякденному житті люди демонструють дивовижну здатність запам’ятовувати зображення, які вони бачать у журналах, рекламі, на телебаченні, на веб-сторінках тощо, але автоматичне передбачення внутрішньої запам’ятовуваності зображень за допомогою комп’ютерного зору та методів машинного навчання було досліджено зовсім недавно. Наша мета в цій статті — дослідити роль візуальної уваги та семантики зображення в розумінні запам’ятовуваності зображення. Зокрема, ми представляємо орієнтовану на увагу стратегію просторового об’єднання та показуємо, що врахування особливостей зображень із помітних частин зображень покращує результати попередніх моделей. Ми також досліджуємо різні семантичні властивості зображень, проводячи аналіз різноманітного набору нещодавно запропонованих семантичних особливостей, які кодують категорії об’єктів метарівня, атрибути сцени та викликані почуття. Ми показуємо, що ці функції, які автоматично витягуються із зображень, забезпечують прогнози запам’ятовуваності настільки ж точні, як і ті, що отримані з анотацій людини. Крім того, наша комбінована модель дає результати, які перевершують результати сучасних повністю автоматичних моделей.","In daily life, humans demonstrate astounding ability to remember images they see on magazines, commercials, TV, the web and so on, but automatic prediction of intrinsic memorability of images using computer vision and machine learning techniques was not investigated until a few years ago. However, despite these recent advances, none of the available approaches makes use of any attentional mechanism, a fundamental aspect of human vision, which selects relevant image regions for higher-level processing. Our goal in this paper is to explore the role of visual attention in understanding memorability of images. In particular, we present an attention-driven spatial pooling strategy for image memorability and show that the regions estimated by bottom-up and object-level saliency maps are more effective in predicting memorability than considering a fixed spatial pyramid structure as in the previous studies.",0
"У повсякденному житті люди демонструють дивовижну здатність запам’ятовувати зображення, які вони бачать у журналах, рекламі, на телебаченні, на веб-сторінках тощо, але автоматичне передбачення внутрішньої запам’ятовуваності зображень за допомогою комп’ютерного зору та методів машинного навчання було досліджено зовсім недавно. Наша мета в цій статті — дослідити роль візуальної уваги та семантики зображення в розумінні запам’ятовуваності зображення. Зокрема, ми представляємо орієнтовану на увагу стратегію просторового об’єднання та показуємо, що врахування особливостей зображень із помітних частин зображень покращує результати попередніх моделей. Ми також досліджуємо різні семантичні властивості зображень, проводячи аналіз різноманітного набору нещодавно запропонованих семантичних особливостей, які кодують категорії об’єктів метарівня, атрибути сцени та викликані почуття. Ми показуємо, що ці функції, які автоматично витягуються із зображень, забезпечують прогнози запам’ятовуваності настільки ж точні, як і ті, що отримані з анотацій людини. Крім того, наша комбінована модель дає результати, які перевершують результати сучасних повністю автоматичних моделей.","У повсякденному житті люди демонструють приголомшливу здатність запам’ятовувати зображення, які вони бачать у журналах, рекламі, на телебаченні, в Інтернеті тощо, але автоматичне передбачення внутрішньої запам’ятовуваності зображень за допомогою комп’ютерного зору та методів машинного навчання досліджувалося лише кілька років тому. Однак, незважаючи на ці нещодавні досягнення, жоден із доступних підходів не використовує жодного механізму уваги, фундаментального аспекту людського зору, який вибирає відповідні області зображення для обробки на вищому рівні. Наша мета в цій статті — дослідити роль візуальної уваги в розумінні запам’ятовуваності зображень. Зокрема, ми представляємо орієнтовану на увагу стратегію просторового об’єднання для запам’ятовуваності зображення та показуємо, що регіони, оцінені за допомогою карт помітності знизу вгору та на рівні об’єктів, є більш ефективними для прогнозування запам’ятовуваності, ніж розгляд фіксованої просторової пірамідальної структури, як у попередніх дослідженнях.",0
"In this paper, we propose a novel query expansion approach for improving transferbased automatic image captioning. The core idea of our method is to translate the given visual query into a distributional semantics based form, which is generated by the average of the sentence vectors extracted from the captions of images visually similar to the input image. Using three image captioning benchmark datasets, we show that our approach provides more accurate results compared to the state-of-theart data-driven methods in terms of both automatic metrics and subjective evaluation.","Acquiring images of the same anatomy with multiple different contrasts increases the diversity of diagnostic information available in an MR exam. Yet, the scan time limitations may prohibit the acquisition of certain contrasts, and some contrasts may be corrupted by noise and artifacts. In such cases, the ability to synthesize unacquired or corrupted contrasts can improve diagnostic utility. For multi-contrast synthesis, the current methods learn a nonlinear intensity transformation between the source and target images, either via nonlinear regression or deterministic neural networks. These methods can, in turn, suffer from the loss of structural details in synthesized images. Here, in this paper, we propose a new approach for multi-contrast MRI synthesis based on conditional generative adversarial networks. The proposed approach preserves intermediate-to-high frequency details via an adversarial loss, and it offers enhanced synthesis performance via pixel-wise and perceptual losses for registered multi-contrast images and a cycle-consistency loss for unregistered images. Information from neighboring cross-sections are utilized to further improve synthesis quality. Demonstrations on T1 - and T2- weighted images from healthy subjects and patients clearly indicate the superior performance of the proposed approach compared to the previous state-of-the-art methods. Our synthesis approach can help improve the quality and versatility of the multi-contrast MRI exams without the need for prolonged or repeated examinations.",0
"In this paper, we propose a novel query expansion approach for improving transferbased automatic image captioning. The core idea of our method is to translate the given visual query into a distributional semantics based form, which is generated by the average of the sentence vectors extracted from the captions of images visually similar to the input image. Using three image captioning benchmark datasets, we show that our approach provides more accurate results compared to the state-of-theart data-driven methods in terms of both automatic metrics and subjective evaluation.","Отримання зображень однієї анатомії з кількома різними контрастами збільшує різноманітність діагностичної інформації, доступної під час МРТ-дослідження. Проте обмеження часу сканування можуть забороняти отримання певних контрастів, а деякі контрасти можуть бути пошкоджені шумом і артефактами. У таких випадках здатність синтезувати неотримані або пошкоджені контрасти може покращити діагностичну корисність. Для багатоконтрастного синтезу поточні методи вивчають нелінійне перетворення інтенсивності між вихідним і цільовим зображеннями за допомогою нелінійної регресії або детермінованих нейронних мереж. Ці методи, у свою чергу, можуть страждати від втрати структурних деталей у синтезованих зображеннях. Тут, у цій статті, ми пропонуємо новий підхід для синтезу мультиконтрастної МРТ на основі умовних генеративних змагальних мереж. Запропонований підхід зберігає проміжні та високі частотні деталі через суперечливі втрати та пропонує покращену продуктивність синтезу через піксельні та перцептивні втрати для зареєстрованих багатоконтрастних зображень і втрату узгодженості циклу для незареєстрованих зображень. Інформація з сусідніх перерізів використовується для подальшого покращення якості синтезу. Демонстрації на T1- і T2-зважених зображеннях здорових суб&#39;єктів і пацієнтів чітко вказують на кращу продуктивність запропонованого підходу порівняно з попередніми сучасними методами. Наш підхід до синтезу може допомогти покращити якість і універсальність багатоконтрастних МРТ-досліджень без необхідності тривалих або повторних обстежень.",0
"У цьому документі ми пропонуємо новий підхід до розширення запиту для покращення автоматичного створення підписів до зображень на основі передачі. Основна ідея нашого методу полягає в тому, щоб перевести заданий візуальний запит у форму, засновану на семантиці розподілу, яка генерується середнім значенням векторів речень, витягнутих із підписів зображень, візуально схожих на вхідне зображення. Використовуючи три контрольних набори даних субтитрів до зображень, ми показуємо, що наш підхід дає точніші результати порівняно з сучасними методами, що керуються даними, з точки зору як автоматичних показників, так і суб’єктивної оцінки.","Acquiring images of the same anatomy with multiple different contrasts increases the diversity of diagnostic information available in an MR exam. Yet, the scan time limitations may prohibit the acquisition of certain contrasts, and some contrasts may be corrupted by noise and artifacts. In such cases, the ability to synthesize unacquired or corrupted contrasts can improve diagnostic utility. For multi-contrast synthesis, the current methods learn a nonlinear intensity transformation between the source and target images, either via nonlinear regression or deterministic neural networks. These methods can, in turn, suffer from the loss of structural details in synthesized images. Here, in this paper, we propose a new approach for multi-contrast MRI synthesis based on conditional generative adversarial networks. The proposed approach preserves intermediate-to-high frequency details via an adversarial loss, and it offers enhanced synthesis performance via pixel-wise and perceptual losses for registered multi-contrast images and a cycle-consistency loss for unregistered images. Information from neighboring cross-sections are utilized to further improve synthesis quality. Demonstrations on T1 - and T2- weighted images from healthy subjects and patients clearly indicate the superior performance of the proposed approach compared to the previous state-of-the-art methods. Our synthesis approach can help improve the quality and versatility of the multi-contrast MRI exams without the need for prolonged or repeated examinations.",0
"У цьому документі ми пропонуємо новий підхід розширення запиту для покращення автоматичного створення підписів до зображень на основі передачі. Основна ідея нашого методу полягає в тому, щоб перевести заданий візуальний запит у форму, засновану на семантиці розподілу, яка генерується середнім значенням векторів речень, витягнутих із підписів зображень, візуально схожих на вхідне зображення. Використовуючи три контрольних набори даних субтитрів до зображень, ми показуємо, що наш підхід дає точніші результати порівняно з сучасними методами, що керуються даними, з точки зору як автоматичних показників, так і суб’єктивної оцінки.","Отримання зображень однієї анатомії з кількома різними контрастами збільшує різноманітність діагностичної інформації, доступної під час МРТ-дослідження. Проте обмеження часу сканування можуть забороняти отримання певних контрастів, а деякі контрасти можуть бути зіпсовані шумом і артефактами. У таких випадках здатність синтезувати неотримані або пошкоджені контрасти може покращити діагностичну корисність. Для багатоконтрастного синтезу поточні методи вивчають нелінійне перетворення інтенсивності між вихідним і цільовим зображеннями за допомогою нелінійної регресії або детермінованих нейронних мереж. Ці методи, у свою чергу, можуть страждати від втрати структурних деталей у синтезованих зображеннях. Тут, у цій статті, ми пропонуємо новий підхід для синтезу мультиконтрастної МРТ на основі умовних генеративних змагальних мереж. Запропонований підхід зберігає проміжні та високі частотні деталі через суперечливі втрати та пропонує покращену продуктивність синтезу через піксельні та перцептивні втрати для зареєстрованих багатоконтрастних зображень і втрату узгодженості циклу для незареєстрованих зображень. Інформація з сусідніх перерізів використовується для подальшого покращення якості синтезу. Демонстрації на T1- і T2-зважених зображеннях здорових суб&#39;єктів і пацієнтів чітко вказують на кращу продуктивність запропонованого підходу порівняно з попередніми сучасними методами. Наш підхід до синтезу може допомогти покращити якість і універсальність багатоконтрастних МРТ-досліджень без необхідності тривалих або повторних обстежень.",0
"Obtaining a high quality high dynamic range (HDR) image in the presence of camera and object movement has been a long-standing challenge. Many methods, known as HDR deghosting algorithms, have been developed over the past ten years to undertake this challenge. Each of these algorithms approaches the deghosting problem from a different perspective, providing solutions with different degrees of complexity, solutions that range from rudimentary heuristics to advanced computer vision techniques. The proposed solutions generally differ in two ways: (1) how to detect ghost regions and (2) what to do to eliminate ghosts. Some algorithms choose to completely discard moving objects giving rise to HDR images which only contain the static regions. Some other algorithms try to find the best image to use for each dynamic region. Yet others try to register moving objects from different images in the spirit of maximizing dynamic range in dynamic regions. Furthermore, each algorithm may introduce different types of artifacts as they aim to eliminate ghosts. These artifacts may come in the form of noise, broken objects, under- and over-exposed regions, and residual ghosting. Given the high volume of studies conducted in this field over the recent years, a comprehensive survey of the state of the art is required. Thus, the first goal of this paper is to provide this survey. Secondly, the large number of algorithms brings about the need to classify them. Thus the second goal of this paper is to propose a taxonomy of deghosting algorithms which can be used to group existing and future algorithms into meaningful classes. Thirdly, the existence of a large number of algorithms brings about the need to evaluate their effectiveness, as each new algorithm claims to outperform its precedents. Therefore, the last goal of this paper is to share the results of a subjective experiment which aims to evaluate various state-of-the-art deghosting algorithms.","The immense amount of videos being uploaded to video sharing platforms makes it impossible for a person to watch all the videos understand what happens in them. Hence, machine learning techniques are now deployed to index videos by recognizing key objects, actions and scenes or places. Summarization is another alternative as it offers to extract only important parts while covering the gist of the video content. Ideally, the user may prefer to analyze a certain action or scene by searching a query term within the video. Current summarization methods generally do not take queries into account or require exhaustive data labeling. In this work, we present a weakly supervised query-focused video summarization method. Our proposed approach makes use of semantic attributes as an indicator of query relevance and semantic attention maps to locate related regions in the frames and utilizes both within a submodular maximization framework. We conducted experiments on the recently introduced RAD dataset and obtained highly competitive results. Moreover, to better evaluate the performance of our approach on longer videos, we collected a new dataset, which consists of 10 videos from YouTube and annotated with shot-level multiple attributes. Our dataset enables much diverse set of queries that can be used to summarize a video from different perspectives with more degrees of freedom.",0
"Obtaining a high quality high dynamic range (HDR) image in the presence of camera and object movement has been a long-standing challenge. Many methods, known as HDR deghosting algorithms, have been developed over the past ten years to undertake this challenge. Each of these algorithms approaches the deghosting problem from a different perspective, providing solutions with different degrees of complexity, solutions that range from rudimentary heuristics to advanced computer vision techniques. The proposed solutions generally differ in two ways: (1) how to detect ghost regions and (2) what to do to eliminate ghosts. Some algorithms choose to completely discard moving objects giving rise to HDR images which only contain the static regions. Some other algorithms try to find the best image to use for each dynamic region. Yet others try to register moving objects from different images in the spirit of maximizing dynamic range in dynamic regions. Furthermore, each algorithm may introduce different types of artifacts as they aim to eliminate ghosts. These artifacts may come in the form of noise, broken objects, under- and over-exposed regions, and residual ghosting. Given the high volume of studies conducted in this field over the recent years, a comprehensive survey of the state of the art is required. Thus, the first goal of this paper is to provide this survey. Secondly, the large number of algorithms brings about the need to classify them. Thus the second goal of this paper is to propose a taxonomy of deghosting algorithms which can be used to group existing and future algorithms into meaningful classes. Thirdly, the existence of a large number of algorithms brings about the need to evaluate their effectiveness, as each new algorithm claims to outperform its precedents. Therefore, the last goal of this paper is to share the results of a subjective experiment which aims to evaluate various state-of-the-art deghosting algorithms.","Величезна кількість відео, які завантажуються на платформи для обміну відео, не дозволяє людині переглянути всі відео, зрозуміти, що в них відбувається. Таким чином, тепер використовуються методи машинного навчання для індексування відео шляхом розпізнавання ключових об’єктів, дій і сцен або місць. Резюмування є ще однією альтернативою, оскільки воно пропонує виділити лише важливі частини, охоплюючи суть відеовмісту. В ідеалі користувач може віддати перевагу аналізу певної дії чи сцени шляхом пошуку за запитом у відео. Поточні методи підсумовування зазвичай не враховують запити або вимагають вичерпного маркування даних. У цій роботі ми представляємо метод підсумовування відео, орієнтований на запити зі слабким контролем. Запропонований нами підхід використовує семантичні атрибути як індикатор релевантності запиту та семантичні карти уваги для визначення місцезнаходження пов’язаних регіонів у фреймах і використовує обидва в межах субмодульної структури максимізації. Ми провели експерименти з нещодавно представленим набором даних RAD і отримали дуже конкурентоспроможні результати. Крім того, щоб краще оцінити ефективність нашого підходу на довших відео, ми зібрали новий набір даних, який складається з 10 відео з YouTube і анотованих декількома атрибутами на рівні кадру. Наш набір даних дозволяє створювати різноманітні запити, які можна використовувати для узагальнення відео з різних точок зору з більшим ступенем свободи.",0
"Отримання високоякісного зображення з широким динамічним діапазоном (HDR) за наявності камери та руху об’єктів було давньою проблемою. За останні десять років було розроблено багато методів, відомих як алгоритми видалення фантом HDR, щоб вирішити цю проблему. Кожен із цих алгоритмів підходить до проблеми видалення фантом з іншої точки зору, надаючи рішення з різним ступенем складності, рішення, що варіюються від рудиментарної евристики до вдосконалених методів комп’ютерного зору. Пропоновані рішення зазвичай відрізняються двома способами: (1) як виявити привиди та (2) що робити, щоб усунути привиди. Деякі алгоритми вирішують повністю відкидати рухомі об’єкти, створюючи зображення HDR, які містять лише статичні області. Деякі інші алгоритми намагаються знайти найкраще зображення для використання для кожної динамічної області. Треті намагаються зареєструвати рухомі об’єкти з різних зображень у дусі максимізації динамічного діапазону в динамічних областях. Крім того, кожен алгоритм може вводити різні типи артефактів, оскільки вони спрямовані на усунення привидів. Ці артефакти можуть проявлятися у вигляді шуму, зламаних об’єктів, недо- та переекспонованих областей, а також залишкових ореолів. Враховуючи великий обсяг досліджень, проведених у цій галузі за останні роки, необхідний комплексний огляд сучасного стану. Таким чином, першою метою цієї статті є проведення цього опитування. По-друге, велика кількість алгоритмів викликає необхідність їх класифікації. Таким чином, друга мета цієї статті полягає в тому, щоб запропонувати таксономію алгоритмів видалення фантомних зображень, яка може бути використана для групування існуючих і майбутніх алгоритмів у значущі класи. По-третє, існування великої кількості алгоритмів викликає необхідність оцінки їх ефективності, оскільки кожен новий алгоритм претендує на перевершення своїх прецедентів. Тому останньою метою цієї статті є поділитися результатами суб’єктивного експерименту, який має на меті оцінити різні найсучасніші алгоритми видалення фантомних зображень.","The immense amount of videos being uploaded to video sharing platforms makes it impossible for a person to watch all the videos understand what happens in them. Hence, machine learning techniques are now deployed to index videos by recognizing key objects, actions and scenes or places. Summarization is another alternative as it offers to extract only important parts while covering the gist of the video content. Ideally, the user may prefer to analyze a certain action or scene by searching a query term within the video. Current summarization methods generally do not take queries into account or require exhaustive data labeling. In this work, we present a weakly supervised query-focused video summarization method. Our proposed approach makes use of semantic attributes as an indicator of query relevance and semantic attention maps to locate related regions in the frames and utilizes both within a submodular maximization framework. We conducted experiments on the recently introduced RAD dataset and obtained highly competitive results. Moreover, to better evaluate the performance of our approach on longer videos, we collected a new dataset, which consists of 10 videos from YouTube and annotated with shot-level multiple attributes. Our dataset enables much diverse set of queries that can be used to summarize a video from different perspectives with more degrees of freedom.",0
"Отримання високоякісного зображення з широким динамічним діапазоном (HDR) за наявності камери та руху об’єктів було давньою проблемою. За останні десять років було розроблено багато методів, відомих як алгоритми видалення фантом HDR, щоб вирішити цю проблему. Кожен із цих алгоритмів підходить до проблеми видалення фантом з іншої точки зору, надаючи рішення з різним ступенем складності, рішення, що варіюються від рудиментарної евристики до вдосконалених методів комп’ютерного зору. Пропоновані рішення зазвичай відрізняються двома способами: (1) як виявити привиди та (2) що робити, щоб усунути привиди. Деякі алгоритми вирішують повністю відкидати рухомі об’єкти, створюючи зображення HDR, які містять лише статичні області. Деякі інші алгоритми намагаються знайти найкраще зображення для використання для кожної динамічної області. Треті намагаються зареєструвати рухомі об’єкти з різних зображень у дусі максимізації динамічного діапазону в динамічних областях. Крім того, кожен алгоритм може вводити різні типи артефактів, оскільки вони спрямовані на усунення привидів. Ці артефакти можуть проявлятися у вигляді шуму, зламаних об’єктів, недо- та переекспонованих областей, а також залишкових ореолів. Враховуючи великий обсяг досліджень, проведених у цій галузі за останні роки, необхідний комплексний огляд сучасного стану. Таким чином, першою метою цієї статті є проведення цього опитування. По-друге, велика кількість алгоритмів викликає необхідність їх класифікації. Таким чином, друга мета цієї статті полягає в тому, щоб запропонувати таксономію алгоритмів видалення фантомних зображень, яка може бути використана для групування існуючих і майбутніх алгоритмів у значущі класи. По-третє, існування великої кількості алгоритмів викликає необхідність оцінки їх ефективності, оскільки кожен новий алгоритм претендує на перевершення своїх прецедентів. Тому останньою метою цієї статті є поділитися результатами суб’єктивного експерименту, який має на меті оцінити різні найсучасніші алгоритми видалення фантомних зображень.","Величезна кількість відео, які завантажуються на платформи для обміну відео, не дозволяє людині переглянути всі відео, зрозуміти, що в них відбувається. Таким чином, тепер застосовуються методи машинного навчання для індексування відео шляхом розпізнавання ключових об’єктів, дій і сцен або місць. Резюмування є ще однією альтернативою, оскільки воно пропонує виділити лише важливі частини, охоплюючи суть відеоконтенту. В ідеалі користувач може віддати перевагу аналізу певної дії чи сцени шляхом пошуку за запитом у відео. Поточні методи підсумовування зазвичай не враховують запити або вимагають вичерпного маркування даних. У цій роботі ми представляємо метод підсумовування відео, орієнтований на запити зі слабким контролем. Запропонований нами підхід використовує семантичні атрибути як індикатор релевантності запиту та семантичні карти уваги для визначення місцезнаходження пов’язаних регіонів у фреймах і використовує обидва в межах субмодульної структури максимізації. Ми провели експерименти з нещодавно представленим набором даних RAD і отримали дуже конкурентоспроможні результати. Крім того, щоб краще оцінити ефективність нашого підходу на довших відео, ми зібрали новий набір даних, який складається з 10 відео з YouTube і анотованих декількома атрибутами на рівні кадру. Наш набір даних дозволяє створювати різноманітні запити, які можна використовувати для узагальнення відео з різних точок зору з більшим ступенем свободи.",0
"Predicting where a photo was taken is quite important and yet a challenging task for computer vision algorithms. Our motivation is to solve this difficult problem in a cityscale setting by employing a data-driven approach. In order to pursue this goal, we developed a fast and robust scene matching method that follows a coarse-to-fine strategy. In particular, we combine scene retrieval via global features and dense scene alignment and use a large set of geo-tagged images of downtown San Francisco in our evaluation. The experimental results show that the proposed approach, despite its simplicity, is surprisingly effective and achieves comparable results with the state-of-the-art.","In this study, we explore whether the captions in the wild can boost the performance of object detection in images. Captions that accompany images usually provide significant information about the visual content of the image, making them an important resource for image understanding. However, captions in the wild are likely to include numerous types of noises which can hurt visual estimation. In this paper, we propose data-driven methods to deal with the noisy captions and utilize them to improve object detection. We show how a pre-trained state-of-theart object detector can take advantage of noisy captions. Our experiments demonstrate that captions provide promising cues about the visual content of the images and can aid in improving object detection.",0
"Predicting where a photo was taken is quite important and yet a challenging task for computer vision algorithms. Our motivation is to solve this difficult problem in a cityscale setting by employing a data-driven approach. In order to pursue this goal, we developed a fast and robust scene matching method that follows a coarse-to-fine strategy. In particular, we combine scene retrieval via global features and dense scene alignment and use a large set of geo-tagged images of downtown San Francisco in our evaluation. The experimental results show that the proposed approach, despite its simplicity, is surprisingly effective and achieves comparable results with the state-of-the-art.","У цьому дослідженні ми досліджуємо, чи можуть підписи в дикій природі підвищити ефективність виявлення об’єктів на зображеннях. Підписи, які супроводжують зображення, зазвичай надають значну інформацію про візуальний вміст зображення, що робить їх важливим ресурсом для розуміння зображення. Проте підписи в дикій природі, ймовірно, містять численні типи шумів, які можуть зашкодити візуальній оцінці. У цій статті ми пропонуємо керовані даними методи боротьби з зашумленими титрами та використовуємо їх для покращення виявлення об’єктів. Ми показуємо, як попередньо навчений сучасний детектор об’єктів може використовувати шумні титри. Наші експерименти демонструють, що підписи дають багатообіцяючі підказки щодо візуального вмісту зображень і можуть допомогти покращити виявлення об’єктів.",0
"Передбачити, де було зроблено фотографію, є досить важливим і водночас складним завданням для алгоритмів комп’ютерного зору. Наша мотивація полягає в тому, щоб вирішити цю складну проблему в умовах міста за допомогою підходу, керованого даними. Щоб досягти цієї мети, ми розробили швидкий і надійний метод зіставлення сцен, який дотримується стратегії від грубого до точного. Зокрема, ми поєднуємо пошук сцени за допомогою глобальних функцій і щільного вирівнювання сцени та використовуємо великий набір зображень центру Сан-Франциско з геотегами в нашій оцінці. Експериментальні результати показують, що запропонований підхід, незважаючи на його простоту, є напрочуд ефективним і досягає результатів, порівнянних із сучасними.","In this study, we explore whether the captions in the wild can boost the performance of object detection in images. Captions that accompany images usually provide significant information about the visual content of the image, making them an important resource for image understanding. However, captions in the wild are likely to include numerous types of noises which can hurt visual estimation. In this paper, we propose data-driven methods to deal with the noisy captions and utilize them to improve object detection. We show how a pre-trained state-of-theart object detector can take advantage of noisy captions. Our experiments demonstrate that captions provide promising cues about the visual content of the images and can aid in improving object detection.",0
"Передбачити, де було зроблено фотографію, є досить важливим і водночас складним завданням для алгоритмів комп’ютерного зору. Наша мотивація полягає в тому, щоб вирішити цю складну проблему в умовах міста за допомогою підходу, керованого даними. Щоб досягти цієї мети, ми розробили швидкий і надійний метод зіставлення сцен, який дотримується стратегії від грубого до точного. Зокрема, ми поєднуємо пошук сцени за допомогою глобальних функцій і щільного вирівнювання сцени та використовуємо великий набір зображень центру Сан-Франциско з геотегами в нашій оцінці. Експериментальні результати показують, що запропонований підхід, незважаючи на його простоту, є напрочуд ефективним і досягає результатів, порівнянних із сучасними.","У цьому дослідженні ми досліджуємо, чи можуть підписи в дикій природі підвищити ефективність виявлення об’єктів на зображеннях. Підписи, які супроводжують зображення, зазвичай надають значну інформацію про візуальний вміст зображення, що робить їх важливим ресурсом для розуміння зображення. Проте підписи в дикій природі, ймовірно, містять численні типи шумів, які можуть зашкодити візуальній оцінці. У цій статті ми пропонуємо керовані даними методи боротьби з зашумленими титрами та використовуємо їх для покращення виявлення об’єктів. Ми показуємо, як попередньо навчений сучасний детектор об’єктів може використовувати шумні титри. Наші експерименти демонструють, що підписи дають багатообіцяючі підказки щодо візуального вмісту зображень і можуть допомогти покращити виявлення об’єктів.",0
"Previous sampling-based image matting methods typically rely on certain heuristics in collecting representative samples from known regions, and thus their performance deteriorates if the underlying assumptions are not satisfied. To alleviate this, in this paper we take an entirely new approach and formulate sampling as a sparse subset selection problem where we propose to pick a small set of candidate samples that best explains the unknown pixels. Moreover, we describe a new distance measure for comparing two samples which is based on KL-divergence between the distributions of features extracted in the vicinity of the samples. Using a standard benchmark dataset for image matting, we demonstrate that our approach provides more accurate results compared with the state-of-the-art methods. ","Many applications of computer vision requires segmenting out of an object of interest from a given image. Motivated by unlevel-sets formulation of Raviv, Kiryati and Sochen [8] and statistical formulation of Leventon, Grimson and Faugeras [6], we present a new image segmentation method which accounts for prior shape information. Our method depends on Ambrosio-Tortorelli approximation of Mumford-Shah functional. The prior shape is represented by a by-product of this functional, a smooth edge indicator function, known as the “edge strength function”, which provides a distance-like surface for the shape boundary. Our method can handle arbitrary deformations due to shape variability as well as plane Euclidean transformations. The method is also robust with respect to noise and missing parts. Furthermore, this formulation does not require simple closed curves as in a typical level set formulation.",0
"Previous sampling-based image matting methods typically rely on certain heuristics in collecting representative samples from known regions, and thus their performance deteriorates if the underlying assumptions are not satisfied. To alleviate this, in this paper we take an entirely new approach and formulate sampling as a sparse subset selection problem where we propose to pick a small set of candidate samples that best explains the unknown pixels. Moreover, we describe a new distance measure for comparing two samples which is based on KL-divergence between the distributions of features extracted in the vicinity of the samples. Using a standard benchmark dataset for image matting, we demonstrate that our approach provides more accurate results compared with the state-of-the-art methods. ","Багато застосувань комп’ютерного зору вимагають сегментування об’єкта інтересу з даного зображення. Спираючись на формулювання нерівневих наборів Равіва, Кір’яті та Сочена [8] та статистичне формулювання Левентона, Грімсона та Фогера [6], ми представляємо новий метод сегментації зображення, який враховує попередню інформацію про форму. Наш метод залежить від апроксимації Амбросіо-Тортореллі функціонала Мамфорда-Шаха. Попередня форма представлена побічним продуктом цього функціоналу, функцією індикатора гладких країв, відомою як «функція сили краю», яка забезпечує поверхню, подібну до відстані, для межі форми. Наш метод може обробляти довільні деформації через мінливість форми, а також плоскі евклідові перетворення. Метод також надійний щодо шуму та відсутніх частин. Крім того, це формулювання не вимагає простих замкнутих кривих, як у типовому формулюванні набору рівнів.",0
"Попередні методи матування зображень на основі вибірки зазвичай покладаються на певні евристики при зборі репрезентативних зразків із відомих регіонів, і, отже, їх продуктивність погіршується, якщо базові припущення не задовольняються. Щоб полегшити це, у цій статті ми застосували абсолютно новий підхід і сформулювали вибірку як проблему вибору розрідженої підмножини, де ми пропонуємо вибрати невеликий набір вибірок-кандидатів, які найкраще пояснюють невідомі пікселі. Крім того, ми описуємо нову міру відстані для порівняння двох зразків, яка базується на KL-розбіжності між розподілами ознак, виділених поблизу зразків. Використовуючи стандартний контрольний набір даних для матування зображення, ми демонструємо, що наш підхід забезпечує більш точні результати порівняно з найсучаснішими методами.","Many applications of computer vision requires segmenting out of an object of interest from a given image. Motivated by unlevel-sets formulation of Raviv, Kiryati and Sochen [8] and statistical formulation of Leventon, Grimson and Faugeras [6], we present a new image segmentation method which accounts for prior shape information. Our method depends on Ambrosio-Tortorelli approximation of Mumford-Shah functional. The prior shape is represented by a by-product of this functional, a smooth edge indicator function, known as the “edge strength function”, which provides a distance-like surface for the shape boundary. Our method can handle arbitrary deformations due to shape variability as well as plane Euclidean transformations. The method is also robust with respect to noise and missing parts. Furthermore, this formulation does not require simple closed curves as in a typical level set formulation.",0
"Попередні методи матування зображень на основі вибірки зазвичай покладаються на певні евристики при зборі репрезентативних зразків із відомих регіонів, і, отже, їх продуктивність погіршується, якщо базові припущення не задовольняються. Щоб полегшити це, у цій статті ми застосували абсолютно новий підхід і сформулювали вибірку як проблему вибору розрідженої підмножини, де ми пропонуємо вибрати невеликий набір вибірок-кандидатів, які найкраще пояснюють невідомі пікселі. Крім того, ми описуємо нову міру відстані для порівняння двох зразків, яка базується на KL-розбіжності між розподілами ознак, виділених поблизу зразків. Використовуючи стандартний контрольний набір даних для матування зображення, ми демонструємо, що наш підхід забезпечує більш точні результати порівняно з найсучаснішими методами.","Багато застосувань комп’ютерного зору вимагають сегментування об’єкта інтересу з даного зображення. Спираючись на формулювання нерівневих наборів Равіва, Кір’яті та Сочена [8] та статистичне формулювання Левентона, Грімсона та Фогера [6], ми представляємо новий метод сегментації зображення, який враховує попередню інформацію про форму. Наш метод залежить від апроксимації Амбросіо-Тортореллі функціоналу Мамфорда-Шаха. Попередня форма представлена побічним продуктом цього функціоналу, функцією індикатора гладких країв, відомою як «функція сили краю», яка забезпечує поверхню, подібну до відстані, для межі форми. Наш метод може обробляти довільні деформації через мінливість форми, а також плоскі евклідові перетворення. Метод також надійний щодо шуму та відсутніх частин. Крім того, це формулювання не вимагає простих замкнутих кривих, як у типовому формулюванні набору рівнів.",0
"Predicting where humans look in images has gained significant popularity in recent years. In this work, we present a novel method for learning top-down visual saliency, which is well-suited to locate objects of interest in complex scenes. During training, we jointly learn a superpixel based class-specific dictionary and a Conditional Random Field (CRF). While using such a discriminative dictionary helps to distinguish target objects from the background, performing the computations at the superpixel level allows us to improve accuracy of object localizations. Experimental results on the Graz-02 and PASCAL VOC 2007 datasets show that the proposed approach is able to achieve stateof-the-art results and provides much better saliency maps.","In daily life, humans demonstrate an amazing ability to remember images they see on magazines, commercials, TV, web pages, etc. but automatic prediction of intrinsic memorability of images using computer vision and machine learning techniques has only been investigated very recently. Our goal in this article is to explore the role of visual attention and image semantics in understanding image memorability. In particular, we present an attention-driven spatial pooling strategy and show that considering image features from the salient parts of images improves the results of the previous models. We also investigate different semantic properties of images by carrying out an analysis of a diverse set of recently proposed semantic features which encode meta-level object categories, scene attributes, and invoked feelings. We show that these features which are automatically extracted from images provide memorability predictions as nearly accurate as those derived from human anno- tations. Moreover, our combined model yields results superior to those of state-of-the art fully automatic models.",0
"Predicting where humans look in images has gained significant popularity in recent years. In this work, we present a novel method for learning top-down visual saliency, which is well-suited to locate objects of interest in complex scenes. During training, we jointly learn a superpixel based class-specific dictionary and a Conditional Random Field (CRF). While using such a discriminative dictionary helps to distinguish target objects from the background, performing the computations at the superpixel level allows us to improve accuracy of object localizations. Experimental results on the Graz-02 and PASCAL VOC 2007 datasets show that the proposed approach is able to achieve stateof-the-art results and provides much better saliency maps.","У повсякденному житті люди демонструють дивовижну здатність запам’ятовувати зображення, які вони бачать у журналах, рекламі, на телебаченні, на веб-сторінках тощо, але автоматичне передбачення внутрішньої запам’ятовуваності зображень за допомогою комп’ютерного зору та методів машинного навчання було досліджено зовсім недавно. Наша мета в цій статті — дослідити роль візуальної уваги та семантики зображення в розумінні запам’ятовуваності зображення. Зокрема, ми представляємо орієнтовану на увагу стратегію просторового об’єднання та показуємо, що врахування особливостей зображень із помітних частин зображень покращує результати попередніх моделей. Ми також досліджуємо різні семантичні властивості зображень, проводячи аналіз різноманітного набору нещодавно запропонованих семантичних особливостей, які кодують категорії об’єктів метарівня, атрибути сцени та викликані почуття. Ми показуємо, що ці функції, які автоматично витягуються із зображень, забезпечують прогнози запам’ятовуваності настільки ж точні, як і ті, що отримані з анотацій людини. Крім того, наша комбінована модель дає результати, які перевершують результати сучасних повністю автоматичних моделей.",0
"Передбачення, куди дивляться люди на зображеннях, набуло значної популярності в останні роки. У цій роботі ми представляємо новий метод вивчення візуальної помітності зверху вниз, який добре підходить для визначення місцезнаходження цікавих об’єктів у складних сценах. Під час навчання ми спільно вивчаємо словник класу на основі суперпікселів і умовне випадкове поле (CRF). Хоча використання такого розрізнювального словника допомагає відрізнити цільові об’єкти від фону, виконання обчислень на рівні суперпікселя дозволяє підвищити точність локалізації об’єктів. Експериментальні результати на наборах даних Graz-02 і PASCAL VOC 2007 показують, що запропонований підхід здатний досягти найсучасніших результатів і забезпечує набагато кращі карти помітності.","In daily life, humans demonstrate an amazing ability to remember images they see on magazines, commercials, TV, web pages, etc. but automatic prediction of intrinsic memorability of images using computer vision and machine learning techniques has only been investigated very recently. Our goal in this article is to explore the role of visual attention and image semantics in understanding image memorability. In particular, we present an attention-driven spatial pooling strategy and show that considering image features from the salient parts of images improves the results of the previous models. We also investigate different semantic properties of images by carrying out an analysis of a diverse set of recently proposed semantic features which encode meta-level object categories, scene attributes, and invoked feelings. We show that these features which are automatically extracted from images provide memorability predictions as nearly accurate as those derived from human anno- tations. Moreover, our combined model yields results superior to those of state-of-the art fully automatic models.",0
"Передбачення, куди дивляться люди на зображеннях, набуло значної популярності в останні роки. У цій роботі ми представляємо новий метод вивчення візуальної помітності зверху вниз, який добре підходить для визначення місцезнаходження цікавих об’єктів у складних сценах. Під час навчання ми спільно вивчаємо словник класу на основі суперпікселів і умовне випадкове поле (CRF). Хоча використання такого розрізнювального словника допомагає відрізнити цільові об’єкти від фону, виконання обчислень на рівні суперпікселя дозволяє підвищити точність локалізації об’єктів. Експериментальні результати на наборах даних Graz-02 і PASCAL VOC 2007 показують, що запропонований підхід здатний досягти найсучасніших результатів і забезпечує набагато кращі карти помітності.","У повсякденному житті люди демонструють дивовижну здатність запам’ятовувати зображення, які вони бачать у журналах, рекламі, на телебаченні, на веб-сторінках тощо, але автоматичне передбачення внутрішньої запам’ятовуваності зображень за допомогою комп’ютерного зору та методів машинного навчання було досліджено зовсім недавно. Наша мета в цій статті — дослідити роль візуальної уваги та семантики зображення в розумінні запам’ятовуваності зображення. Зокрема, ми представляємо орієнтовану на увагу стратегію просторового об’єднання та показуємо, що врахування особливостей зображень із помітних частин зображень покращує результати попередніх моделей. Ми також досліджуємо різні семантичні властивості зображень, проводячи аналіз різноманітного набору нещодавно запропонованих семантичних особливостей, які кодують категорії об’єктів метарівня, атрибути сцени та викликані почуття. Ми показуємо, що ці функції, які автоматично витягуються із зображень, забезпечують прогнози запам’ятовуваності настільки ж точні, як і ті, що отримані з анотацій людини. Крім того, наша комбінована модель дає результати, які перевершують результати сучасних повністю автоматичних моделей.",0
"Recent years have witnessed the emergence of new image smoothing techniques which have provided new insights and raised new questions about the nature of this well-studied problem. Specifically, these models separate a given image into its structure and texture layers by utilizing non-gradient based definitions for edges or special measures that distinguish edges from oscillations. In this study, we propose an alternative yet simple image smoothing approach which depends on covariance matrices of simple image features, aka the region covariances. The use of second order statistics as a patch descriptor allows us to implicitly capture local structure and texture information and makes our approach particularly effective for structure extraction from texture. Our experimental results have shown that the proposed approach leads to better image decompositions as compared to the state-of-the-art methods and preserves prominent edges and shading well. Moreover, we also demonstrate the applicability of our approach on some image editing and manipulation tasks such as image abstraction, texture and detail enhancement, image composition, inverse halftoning and seam carving.","In recent years, visual saliency estimation in images has attracted much attention in the computer vision community. However, predicting saliency in videos has received relatively little attention. Inspired by the recent success of deep convolutional neural networks based static saliency models, in this work, we study two different two-stream convolutional networks for dynamic saliency prediction. To improve the generalization capability of our models, we also introduce a novel, empirically grounded data augmentation technique for this task. We test our models on DIEM dataset and report superior results against the existing models. Moreover, we perform transfer learning experiments on SALICON, a recently proposed static saliency dataset, by finetuning our models on the optical flows estimated from static images. Our experiments show that taking motion into account in this way can be helpful for static saliency estimation. ",0
"Recent years have witnessed the emergence of new image smoothing techniques which have provided new insights and raised new questions about the nature of this well-studied problem. Specifically, these models separate a given image into its structure and texture layers by utilizing non-gradient based definitions for edges or special measures that distinguish edges from oscillations. In this study, we propose an alternative yet simple image smoothing approach which depends on covariance matrices of simple image features, aka the region covariances. The use of second order statistics as a patch descriptor allows us to implicitly capture local structure and texture information and makes our approach particularly effective for structure extraction from texture. Our experimental results have shown that the proposed approach leads to better image decompositions as compared to the state-of-the-art methods and preserves prominent edges and shading well. Moreover, we also demonstrate the applicability of our approach on some image editing and manipulation tasks such as image abstraction, texture and detail enhancement, image composition, inverse halftoning and seam carving.","В останні роки оцінка візуальної помітності зображень привернула велику увагу в спільноті комп’ютерного зору. Однак прогнозуванню помітності у відео приділено відносно мало уваги. Натхненні нещодавнім успіхом глибоких згорткових нейронних мереж на основі статичних моделей помітності, у цій роботі ми вивчаємо дві різні двопотокові згорткові мережі для динамічного прогнозування помітності. Щоб покращити можливості узагальнення наших моделей, ми також запровадили нову, емпірично обґрунтовану техніку збільшення даних для цього завдання. Ми перевіряємо наші моделі на наборі даних DIEM і повідомляємо про кращі результати порівняно з існуючими моделями. Крім того, ми проводимо експерименти з навчанням переносу на SALICON, нещодавно запропонованому наборі даних статичної помітності, шляхом точного налаштування наших моделей на оптичних потоках, оцінених за статичними зображеннями. Наші експерименти показують, що врахування руху таким чином може бути корисним для оцінки статичної помітності.",0
"Останні роки стали свідками появи нових методів згладжування зображень, які дали нові знання та поставили нові питання щодо природи цієї добре вивченої проблеми. Зокрема, ці моделі поділяють дане зображення на шари структури та текстури, використовуючи неградієнтні визначення для країв або спеціальні заходи, які відрізняють краї від коливань. У цьому дослідженні ми пропонуємо альтернативний, але простий підхід до згладжування зображення, який залежить від коваріаційних матриць простих характеристик зображення, також відомих як коваріації регіонів. Використання статистики другого порядку як дескриптора патча дозволяє нам неявно отримувати інформацію про локальну структуру та текстуру та робить наш підхід особливо ефективним для вилучення структури з текстури. Результати наших експериментів показали, що запропонований підхід забезпечує кращу декомпозицію зображення порівняно з сучасними методами та добре зберігає помітні краї та затінення. Крім того, ми також демонструємо застосовність нашого підходу до деяких завдань редагування та маніпулювання зображеннями, таких як абстракція зображення, покращення текстури та деталей, композиція зображення, інверсне півтонування та різьблення швів.","In recent years, visual saliency estimation in images has attracted much attention in the computer vision community. However, predicting saliency in videos has received relatively little attention. Inspired by the recent success of deep convolutional neural networks based static saliency models, in this work, we study two different two-stream convolutional networks for dynamic saliency prediction. To improve the generalization capability of our models, we also introduce a novel, empirically grounded data augmentation technique for this task. We test our models on DIEM dataset and report superior results against the existing models. Moreover, we perform transfer learning experiments on SALICON, a recently proposed static saliency dataset, by finetuning our models on the optical flows estimated from static images. Our experiments show that taking motion into account in this way can be helpful for static saliency estimation. ",0
"Останні роки стали свідками появи нових методів згладжування зображень, які дали нові знання та поставили нові питання щодо природи цієї добре вивченої проблеми. Зокрема, ці моделі поділяють дане зображення на шари структури та текстури, використовуючи неградієнтні визначення для країв або спеціальні заходи, які відрізняють краї від коливань. У цьому дослідженні ми пропонуємо альтернативний, але простий підхід до згладжування зображення, який залежить від коваріаційних матриць простих характеристик зображення, також відомих як коваріації регіонів. Використання статистики другого порядку як дескриптора патча дозволяє нам неявно отримувати інформацію про локальну структуру та текстуру та робить наш підхід особливо ефективним для вилучення структури з текстури. Результати наших експериментів показали, що запропонований підхід забезпечує кращу декомпозицію зображення порівняно з сучасними методами та добре зберігає помітні краї та затінення. Крім того, ми також демонструємо застосовність нашого підходу до деяких завдань редагування та маніпулювання зображеннями, таких як абстракція зображення, покращення текстури та деталей, композиція зображення, інверсне півтонування та різьблення швів.","В останні роки оцінка візуальної помітності зображень привернула велику увагу в спільноті комп’ютерного зору. Однак прогнозуванню помітності у відео приділено відносно мало уваги. Натхненні нещодавнім успіхом глибоких згорткових нейронних мереж на основі статичних моделей помітності, у цій роботі ми вивчаємо дві різні двопотокові згорткові мережі для динамічного прогнозування помітності. Щоб покращити можливості узагальнення наших моделей, ми також запровадили нову, емпірично обґрунтовану техніку збільшення даних для цього завдання. Ми перевіряємо наші моделі на наборі даних DIEM і повідомляємо про кращі результати порівняно з існуючими моделями. Крім того, ми проводимо експерименти з навчанням переносу на SALICON, нещодавно запропонованому наборі даних статичної помітності, шляхом точного налаштування наших моделей на оптичних потоках, оцінених за статичними зображеннями. Наші експерименти показують, що врахування руху таким чином може бути корисним для оцінки статичної помітності.",0
"In the last few decades, significant achievements have been attained in predicting where humans look at images through different computational models. However, how to determine contributions of different visual features to overall saliency still remains an open problem. To overcome this issue, a recent class of models formulates saliency estimation as a supervised learning problem and accordingly apply machine learning techniques. In this paper, we also address this challenging problem and propose to use multiple kernel learning (MKL) to combine information coming from different feature dimensions and to perform integration at an intermediate level. Besides, we suggest to use responses of a recently proposed filterbank of object detectors, known as ObjectBank, as additional semantic high-level features. Here we show that our MKL-based framework together with the proposed object-specific features provide state-of-the-art performance as compared to SVM or AdaBoost-based saliency models. ","The task of generating natural language descriptions from images has received a lot of attention in recent years. Consequently, it is becoming increasingly important to evaluate such image captioning approaches in an automatic manner. In this paper, we provide an in-depth evaluation of the existing image captioning metrics through a series of carefully designed experiments. Moreover, we explore the utilization of the recently proposed Word Mover’s Distance (WMD) document metric for the purpose of image captioning. Our findings outline the differences and/or similarities between metrics and their relative robustness by means of extensive correlation, accuracy and distraction based evaluations. Our results also demonstrate that WMD provides strong advantages over other metrics.",0
"In the last few decades, significant achievements have been attained in predicting where humans look at images through different computational models. However, how to determine contributions of different visual features to overall saliency still remains an open problem. To overcome this issue, a recent class of models formulates saliency estimation as a supervised learning problem and accordingly apply machine learning techniques. In this paper, we also address this challenging problem and propose to use multiple kernel learning (MKL) to combine information coming from different feature dimensions and to perform integration at an intermediate level. Besides, we suggest to use responses of a recently proposed filterbank of object detectors, known as ObjectBank, as additional semantic high-level features. Here we show that our MKL-based framework together with the proposed object-specific features provide state-of-the-art performance as compared to SVM or AdaBoost-based saliency models. ","Завдання генерації описів природною мовою з зображень привернуло багато уваги в останні роки. Отже, стає все більш важливим автоматично оцінювати такі підходи до підписів до зображень. У цій статті ми надаємо поглиблену оцінку існуючих показників субтитрів до зображень за допомогою серії ретельно розроблених експериментів. Крім того, ми досліджуємо використання нещодавно запропонованої метрики документа Word Mover Distance (WMD) для підписів до зображень. Наші висновки окреслюють відмінності та/або подібності між показниками та їх відносною стійкістю за допомогою обширних оцінок на основі кореляції, точності та відволікання. Наші результати також демонструють, що ЗМЗ має значні переваги перед іншими показниками.",0
"За останні кілька десятиліть було досягнуто значних досягнень у передбаченні того, де люди дивляться на зображення, за допомогою різних обчислювальних моделей. Проте, як визначити внесок різних візуальних особливостей у загальну помітність, все ще залишається відкритою проблемою. Щоб подолати цю проблему, останній клас моделей формулює оцінку помітності як проблему контрольованого навчання та відповідно застосовує методи машинного навчання. У цьому документі ми також розглядаємо цю складну проблему та пропонуємо використовувати багатоядерне навчання (MKL) для об’єднання інформації, що надходить з різних вимірів функцій, і для виконання інтеграції на середньому рівні. Крім того, ми пропонуємо використовувати відгуки нещодавно запропонованого банку фільтрів детекторів об’єктів, відомого як ObjectBank, як додаткові семантичні функції високого рівня. Тут ми показуємо, що наш фреймворк на основі MKL разом із пропонованими об’єктно-специфічними функціями забезпечує найсучаснішу продуктивність порівняно з моделями помітності на основі SVM або AdaBoost.","The task of generating natural language descriptions from images has received a lot of attention in recent years. Consequently, it is becoming increasingly important to evaluate such image captioning approaches in an automatic manner. In this paper, we provide an in-depth evaluation of the existing image captioning metrics through a series of carefully designed experiments. Moreover, we explore the utilization of the recently proposed Word Mover’s Distance (WMD) document metric for the purpose of image captioning. Our findings outline the differences and/or similarities between metrics and their relative robustness by means of extensive correlation, accuracy and distraction based evaluations. Our results also demonstrate that WMD provides strong advantages over other metrics.",0
"За останні кілька десятиліть було досягнуто значних досягнень у передбаченні того, де люди дивляться на зображення, за допомогою різних обчислювальних моделей. Проте, як визначити внесок різних візуальних особливостей у загальну помітність, все ще залишається відкритою проблемою. Щоб подолати цю проблему, останній клас моделей формулює оцінку помітності як проблему контрольованого навчання та відповідно застосовує методи машинного навчання. У цьому документі ми також розглядаємо цю складну проблему та пропонуємо використовувати багатоядерне навчання (MKL) для об’єднання інформації, що надходить з різних вимірів функцій, і для виконання інтеграції на середньому рівні. Крім того, ми пропонуємо використовувати відгуки нещодавно запропонованого банку фільтрів детекторів об’єктів, відомого як ObjectBank, як додаткові семантичні функції високого рівня. Тут ми показуємо, що наш фреймворк на основі MKL разом із пропонованими об’єктно-специфічними функціями забезпечує найсучаснішу продуктивність порівняно з моделями помітності на основі SVM або AdaBoost.","Завдання генерації описів природною мовою з зображень привернуло багато уваги в останні роки. Отже, стає все більш важливим автоматично оцінювати такі підходи до підписів до зображень. У цій статті ми надаємо поглиблену оцінку існуючих показників субтитрів до зображень за допомогою серії ретельно розроблених експериментів. Крім того, ми досліджуємо використання нещодавно запропонованої метрики документа Word Mover Distance (WMD) для підписів до зображень. Наші висновки окреслюють відмінності та/або схожість між показниками та їх відносною надійністю за допомогою обширних оцінок на основі кореляції, точності та відволікання. Наші результати також демонструють, що ЗМЗ має значні переваги перед іншими показниками.",0
"Abstract  To detect visually salient elements of complex natural scenes, computational bottom-up saliency models commonly examine several feature channels such as color and orientation in parallel. They compute a separate feature map for each channel and then linearly combine these maps to produce a master saliency map. However, only a few studies have investigated how different feature dimensions contribute to the overall visual saliency. We address this integration issue and propose to use covariance matrices of simple image features (known as region covariance descriptors in the computer vision community; Tuzel, Porikli, & Meer, 2006) as meta-features for saliency estimation. As low-dimensional representations of image patches, region covariances capture local image structures better than standard linear filters, but more importantly, they naturally provide nonlinear integration of different features by modeling their correlations. We also show that first-order statistics of features could be easily incorporated to the proposed approach to improve the performance. Our experimental evaluation on several benchmark data sets demonstrate that the proposed approach outperforms the state-of-art models on various tasks including prediction of human eye fixations, salient object detection, and image-retargeting.","Multi-contrast MRI protocols increase the level of morphological information available for diagnosis. Yet, the number and quality of contrasts are limited in practice by various factors including scan time and patient motion. Synthesis of missing or corrupted contrasts from other high-quality ones can alleviate this limitation. When a single target contrast is of interest, common approaches for multi-contrast MRI involve either one-to-one or many-to-one synthesis methods depending on their input. One-to-one meth- ods take as input a single source contrast, and they learn a latent representation sensitive to unique fea- tures of the source. Meanwhile, many-to-one methods receive multiple distinct sources, and they learn a shared latent representation more sensitive to common features across sources. For enhanced image synthesis, we propose a multi-stream approach that aggregates information across multiple source im- ages via a mixture of multiple one-to-one streams and a joint many-to-one stream. The complementary feature maps generated in the one-to-one streams and the shared feature maps generated in the many- to-one stream are combined with a fusion block. The location of the fusion block is adaptively modified to maximize task-specific performance. Quantitative and radiological assessments on T1 ,- T2-, PD-weighted, and FLAIR images clearly demonstrate the superior performance of the proposed method compared to previous state-of-the-art one-to-one and many-to-one methods.",0
"Abstract  To detect visually salient elements of complex natural scenes, computational bottom-up saliency models commonly examine several feature channels such as color and orientation in parallel. They compute a separate feature map for each channel and then linearly combine these maps to produce a master saliency map. However, only a few studies have investigated how different feature dimensions contribute to the overall visual saliency. We address this integration issue and propose to use covariance matrices of simple image features (known as region covariance descriptors in the computer vision community; Tuzel, Porikli, & Meer, 2006) as meta-features for saliency estimation. As low-dimensional representations of image patches, region covariances capture local image structures better than standard linear filters, but more importantly, they naturally provide nonlinear integration of different features by modeling their correlations. We also show that first-order statistics of features could be easily incorporated to the proposed approach to improve the performance. Our experimental evaluation on several benchmark data sets demonstrate that the proposed approach outperforms the state-of-art models on various tasks including prediction of human eye fixations, salient object detection, and image-retargeting.","Мультиконтрастні протоколи МРТ підвищують рівень доступної для діагностики морфологічної інформації. Проте на практиці кількість і якість контрастів обмежена різними факторами, включаючи час сканування та рух пацієнта. Синтез відсутніх або пошкоджених контрастів з інших високоякісних може пом’якшити це обмеження. Коли цікавить одне цільове контрастування, загальні підходи до багатоконтрастної МРТ включають методи синтезу «один до одного» або «багато до одного» залежно від їх вхідних даних. Методи «один-до-одного» приймають як вхідні дані контраст одного джерела, і вони вивчають приховане представлення, чутливе до унікальних особливостей джерела. Тим часом методи «багато-до-одного» отримують кілька різних джерел, і вони вивчають спільне приховане представлення, більш чутливе до загальних функцій у джерелах. Для покращеного синтезу зображень ми пропонуємо багатопотоковий підхід, який агрегує інформацію з кількох вихідних зображень за допомогою суміші кількох потоків один до одного та спільного потоку багато до одного. Додаткові карти функцій, згенеровані в потоках «один-до-одного», і спільні карти функцій, згенеровані в потоці «багато-до-одного», поєднуються з блоком злиття. Розташування блоку злиття адаптивно змінюється, щоб максимально підвищити продуктивність для конкретного завдання. Кількісні та радіологічні оцінки T1, T2-, PD-зважених зображень і зображень FLAIR чітко демонструють чудову продуктивність запропонованого методу порівняно з попередніми сучасними методами один до одного та багато до одного.",0
"Анотація Щоб виявити візуально помітні елементи складних природних сцен, обчислювальні моделі помітності знизу вгору зазвичай досліджують кілька каналів ознак, таких як колір і орієнтація паралельно. Вони обчислюють окрему карту функцій для кожного каналу, а потім лінійно об’єднують ці карти, щоб отримати головну карту помітності. Однак лише кілька досліджень досліджували, як різні розміри функцій впливають на загальну візуальну помітність. Ми розглядаємо цю проблему інтеграції та пропонуємо використовувати коваріаційні матриці простих характеристик зображення (відомі як дескриптори коваріації регіонів у спільноті комп’ютерного зору; Tuzel, Porikli та Meer, 2006) як мета-ознаки для оцінки помітності. Будучи низьковимірними представленнями фрагментів зображення, коваріації регіонів фіксують локальні структури зображення краще, ніж стандартні лінійні фільтри, але, що більш важливо, вони природним чином забезпечують нелінійну інтеграцію різних функцій шляхом моделювання їхніх кореляцій. Ми також показуємо, що статистика функцій першого порядку може бути легко включена в запропонований підхід для покращення продуктивності. Наша експериментальна оцінка кількох еталонних наборів даних демонструє, що запропонований підхід перевершує сучасні моделі для різних завдань, включаючи прогнозування фіксації людського ока, виявлення помітних об’єктів і перенацілювання зображень.","Multi-contrast MRI protocols increase the level of morphological information available for diagnosis. Yet, the number and quality of contrasts are limited in practice by various factors including scan time and patient motion. Synthesis of missing or corrupted contrasts from other high-quality ones can alleviate this limitation. When a single target contrast is of interest, common approaches for multi-contrast MRI involve either one-to-one or many-to-one synthesis methods depending on their input. One-to-one meth- ods take as input a single source contrast, and they learn a latent representation sensitive to unique fea- tures of the source. Meanwhile, many-to-one methods receive multiple distinct sources, and they learn a shared latent representation more sensitive to common features across sources. For enhanced image synthesis, we propose a multi-stream approach that aggregates information across multiple source im- ages via a mixture of multiple one-to-one streams and a joint many-to-one stream. The complementary feature maps generated in the one-to-one streams and the shared feature maps generated in the many- to-one stream are combined with a fusion block. The location of the fusion block is adaptively modified to maximize task-specific performance. Quantitative and radiological assessments on T1 ,- T2-, PD-weighted, and FLAIR images clearly demonstrate the superior performance of the proposed method compared to previous state-of-the-art one-to-one and many-to-one methods.",0
"Анотація Щоб виявити візуально помітні елементи складних природних сцен, обчислювальні моделі помітності знизу вгору зазвичай досліджують кілька каналів ознак, таких як колір і орієнтація паралельно. Вони обчислюють окрему карту функцій для кожного каналу, а потім лінійно об’єднують ці карти, щоб отримати головну карту помітності. Однак лише кілька досліджень досліджували, як різні розміри функцій впливають на загальну візуальну помітність. Ми розглядаємо цю проблему інтеграції та пропонуємо використовувати коваріаційні матриці простих характеристик зображення (відомі як дескриптори коваріації регіонів у спільноті комп’ютерного зору; Tuzel, Porikli та Meer, 2006) як мета-ознаки для оцінки помітності. Будучи низьковимірними представленнями фрагментів зображення, коваріації регіонів фіксують локальні структури зображення краще, ніж стандартні лінійні фільтри, але, що більш важливо, вони природним чином забезпечують нелінійну інтеграцію різних функцій шляхом моделювання їхніх кореляцій. Ми також показуємо, що статистика функцій першого порядку може бути легко включена в запропонований підхід для покращення продуктивності. Наша експериментальна оцінка кількох еталонних наборів даних демонструє, що запропонований підхід перевершує сучасні моделі для різних завдань, включаючи прогнозування фіксації людського ока, виявлення помітних об’єктів і перенацілювання зображень.","Мультиконтрастні протоколи МРТ підвищують рівень доступної для діагностики морфологічної інформації. Проте на практиці кількість і якість контрастів обмежена різними факторами, включаючи час сканування та рух пацієнта. Синтез відсутніх або пошкоджених контрастів з інших високоякісних може пом’якшити це обмеження. Коли цікавить одне цільове контрастування, загальні підходи до багатоконтрастної МРТ включають методи синтезу «один до одного» або «багато до одного» залежно від їх вхідних даних. Методи «один-до-одного» приймають як вхідні дані контраст одного джерела, і вони вивчають приховане представлення, чутливе до унікальних особливостей джерела. Тим часом методи «багато-до-одного» отримують кілька різних джерел, і вони вивчають спільне приховане представлення, більш чутливе до загальних функцій у джерелах. Для покращеного синтезу зображень ми пропонуємо багатопотоковий підхід, який агрегує інформацію з кількох вихідних зображень за допомогою суміші кількох потоків один до одного та спільного потоку багато до одного. Додаткові карти функцій, згенеровані в потоках «один-до-одного», і спільні карти функцій, згенеровані в потоці «багато-до-одного», поєднуються з блоком злиття. Розташування блоку злиття адаптивно змінюється, щоб максимально підвищити продуктивність для конкретного завдання. Кількісні та радіологічні оцінки T1, T2-, PD-зважених зображень і зображень FLAIR чітко демонструють чудову продуктивність запропонованого методу порівняно з попередніми сучасними методами один до одного та багато до одного.",0
"In daily life, humans demonstrate astounding ability to remember images they see on magazines, commercials, TV, the web and so on, but automatic prediction of intrinsic memorability of images using computer vision and machine learning techniques was not investigated until a few years ago. However, despite these recent advances, none of the available approaches makes use of any attentional mechanism, a fundamental aspect of human vision, which selects relevant image regions for higher-level processing. Our goal in this paper is to explore the role of visual attention in understanding memorability of images. In particular, we present an attention-driven spatial pooling strategy for image memorability and show that the regions estimated by bottom-up and object-level saliency maps are more effective in predicting memorability than considering a fixed spatial pyramid structure as in the previous studies.","Giving machines the ability to imagine possible new objects or scenes from linguistic descriptions and produce their realistic renderings is arguably one of the most challenging problems in computer vision. Recent advances in deep generative models have led to new approaches that give promising results towards this goal. In this paper, we introduce a new method called DiCoMoGAN for manipulating videos with natural language, aiming to perform local and semantic edits on a video clip to alter the appearances of an object of interest. Our GAN architecture allows for better utilization of multiple observations by disentangling content and motion to enable controllable semantic edits. To this end, we introduce two tightly coupled networks: (i) a representation network for constructing a concise understanding of motion dynamics and temporally invariant content, and (ii) a translation network that exploits the extracted latent content representation to actuate the manipulation according to the target description. Our qualitative and quantitative evaluations demonstrate that DiCoMoGAN significantly outperforms existing frame-based methods, producing temporally coherent and semantically more meaningful results.",0
"In daily life, humans demonstrate astounding ability to remember images they see on magazines, commercials, TV, the web and so on, but automatic prediction of intrinsic memorability of images using computer vision and machine learning techniques was not investigated until a few years ago. However, despite these recent advances, none of the available approaches makes use of any attentional mechanism, a fundamental aspect of human vision, which selects relevant image regions for higher-level processing. Our goal in this paper is to explore the role of visual attention in understanding memorability of images. In particular, we present an attention-driven spatial pooling strategy for image memorability and show that the regions estimated by bottom-up and object-level saliency maps are more effective in predicting memorability than considering a fixed spatial pyramid structure as in the previous studies.","Надання машинам здатності уявляти можливі нові об’єкти чи сцени на основі лінгвістичних описів і виробляти їх реалістичне відтворення є, мабуть, однією з найскладніших проблем комп’ютерного зору. Останні досягнення в глибоких генеративних моделях привели до нових підходів, які дають багатообіцяючі результати для досягнення цієї мети. У цій статті ми представляємо новий метод під назвою DiCoMoGAN для обробки відео за допомогою природної мови, спрямований на виконання локальних і семантичних редагувань відеокліпу, щоб змінити зовнішній вигляд цікавого об’єкта. Наша архітектура GAN дозволяє краще використовувати кілька спостережень, розмежовуючи вміст і рух, щоб уможливити кероване семантичне редагування. З цією метою ми представляємо дві тісно пов’язані мережі: (i) репрезентативну мережу для побудови стислого розуміння динаміки руху та тимчасово незмінного вмісту, і (ii) мережу перекладу, яка використовує виділене латентне представлення вмісту для активації маніпуляції відповідно до цільовий опис. Наші якісні та кількісні оцінки демонструють, що DiCoMoGAN значно перевершує існуючі методи на основі фреймів, створюючи узгоджені в часі та семантично більш значущі результати.",0
"У повсякденному житті люди демонструють приголомшливу здатність запам’ятовувати зображення, які вони бачать у журналах, рекламі, на телебаченні, в Інтернеті тощо, але автоматичне передбачення внутрішньої запам’ятовуваності зображень за допомогою комп’ютерного зору та методів машинного навчання досліджувалося лише кілька років тому. Однак, незважаючи на ці нещодавні досягнення, жоден із доступних підходів не використовує жодного механізму уваги, фундаментального аспекту людського зору, який вибирає відповідні області зображення для обробки на вищому рівні. Наша мета в цій статті — дослідити роль візуальної уваги в розумінні запам’ятовуваності зображень. Зокрема, ми представляємо орієнтовану на увагу стратегію просторового об’єднання для запам’ятовуваності зображення та показуємо, що регіони, оцінені за допомогою карт помітності знизу вгору та на рівні об’єктів, є більш ефективними для прогнозування запам’ятовуваності, ніж розгляд фіксованої просторової пірамідальної структури, як у попередніх дослідженнях.","Giving machines the ability to imagine possible new objects or scenes from linguistic descriptions and produce their realistic renderings is arguably one of the most challenging problems in computer vision. Recent advances in deep generative models have led to new approaches that give promising results towards this goal. In this paper, we introduce a new method called DiCoMoGAN for manipulating videos with natural language, aiming to perform local and semantic edits on a video clip to alter the appearances of an object of interest. Our GAN architecture allows for better utilization of multiple observations by disentangling content and motion to enable controllable semantic edits. To this end, we introduce two tightly coupled networks: (i) a representation network for constructing a concise understanding of motion dynamics and temporally invariant content, and (ii) a translation network that exploits the extracted latent content representation to actuate the manipulation according to the target description. Our qualitative and quantitative evaluations demonstrate that DiCoMoGAN significantly outperforms existing frame-based methods, producing temporally coherent and semantically more meaningful results.",0
"У повсякденному житті люди демонструють приголомшливу здатність запам’ятовувати зображення, які вони бачать у журналах, рекламі, на телебаченні, в Інтернеті тощо, але автоматичне передбачення внутрішньої запам’ятовуваності зображень за допомогою комп’ютерного зору та методів машинного навчання досліджувалося лише кілька років тому. Однак, незважаючи на ці нещодавні досягнення, жоден із доступних підходів не використовує жодного механізму уваги, фундаментального аспекту людського зору, який вибирає відповідні області зображення для обробки на вищому рівні. Наша мета в цій статті — дослідити роль візуальної уваги в розумінні запам’ятовуваності зображень. Зокрема, ми представляємо орієнтовану на увагу стратегію просторового об’єднання для запам’ятовуваності зображення та показуємо, що регіони, оцінені за допомогою карт помітності знизу вгору та на рівні об’єктів, є більш ефективними для прогнозування запам’ятовуваності, ніж розгляд фіксованої просторової пірамідальної структури, як у попередніх дослідженнях.","Надання машинам здатності уявляти можливі нові об’єкти чи сцени на основі лінгвістичних описів і виробляти їх реалістичне відтворення є, мабуть, однією з найскладніших проблем комп’ютерного зору. Останні досягнення в глибоких генеративних моделях привели до нових підходів, які дають багатообіцяючі результати для досягнення цієї мети. У цій статті ми представляємо новий метод під назвою DiCoMoGAN для обробки відео за допомогою природної мови, спрямований на виконання локальних і семантичних редагувань відеокліпу, щоб змінити зовнішній вигляд цікавого об’єкта. Наша архітектура GAN дозволяє краще використовувати кілька спостережень, розмежовуючи вміст і рух, щоб уможливити кероване семантичне редагування. З цією метою ми представляємо дві тісно пов’язані мережі: (i) репрезентативну мережу для побудови стислого розуміння динаміки руху та тимчасово незмінного вмісту, і (ii) мережу перекладу, яка використовує виділене латентне представлення вмісту для активації маніпуляції відповідно до цільовий опис. Наші якісні та кількісні оцінки демонструють, що DiCoMoGAN значно перевершує існуючі методи на основі фреймів, створюючи узгоджені в часі та семантично більш значущі результати.",0
"In this paper, we address the issue of part-based tracking by proposing a new fragments-based tracker. The proposed tracker enhances the recently suggested FragTrack algorithm to employ an adaptive cue integration scheme. This is done by embedding the original tracker into a particle filter framework, asso- ciating a reliability value to each fragment that describes a different part of the target object and dynam- ically adjusting these reliabilities at each frame with respect to the current context. Particularly, the vote of each fragment contributes to the joint tracking result according to its reliability, and this allows us to achieve a better accuracy in handling partial occlusions and pose changes while preserving and even improving the efficiency of the original tracker. In order to demonstrate the performance and the effec- tiveness of the proposed algorithm we present qualitative and quantitative results on a number of chal- lenging video sequences","Automatic image synthesis research has been rapidly growing with deep networks getting more and more expressive. In the last couple of years, we have observed images of digits, indoor scenes, birds, chairs, etc. being automatically generated. The expressive power of image generators have also been enhanced by introducing several forms of conditioning variables such as object names, sentences, bounding box and key-point locations. In this work, we propose a novel deep conditional generative adversarial network architecture that takes its strength from the semantic layout and scene attributes integrated as conditioning variables. We show that our architecture is able to generate realistic outdoor scene images under different conditions, e.g. daynight, sunny-foggy, with clear object boundaries. ",0
"In this paper, we address the issue of part-based tracking by proposing a new fragments-based tracker. The proposed tracker enhances the recently suggested FragTrack algorithm to employ an adaptive cue integration scheme. This is done by embedding the original tracker into a particle filter framework, asso- ciating a reliability value to each fragment that describes a different part of the target object and dynam- ically adjusting these reliabilities at each frame with respect to the current context. Particularly, the vote of each fragment contributes to the joint tracking result according to its reliability, and this allows us to achieve a better accuracy in handling partial occlusions and pose changes while preserving and even improving the efficiency of the original tracker. In order to demonstrate the performance and the effec- tiveness of the proposed algorithm we present qualitative and quantitative results on a number of chal- lenging video sequences","Дослідження автоматичного синтезу зображень стрімко розвиваються, а глибокі мережі стають дедалі виразнішими. За останні пару років ми спостерігали автоматичне генерування зображень цифр, сцен у приміщенні, птахів, стільців тощо. Виражальну силу генераторів зображень також було покращено завдяки введенню кількох форм обумовлюючих змінних, таких як назви об’єктів, речення, обмежувальна рамка та розташування ключових точок. У цій роботі ми пропонуємо нову глибоку умовну генеруючу змагальну архітектуру мережі, яка бере свою силу від семантичного макету та атрибутів сцени, інтегрованих як змінні умови. Ми показуємо, що наша архітектура здатна генерувати реалістичні зображення зовнішньої сцени за різних умов, наприклад, вдень, у сонячну погоду з туманом, із чіткими межами об’єктів.",0
"У цій статті ми розглядаємо питання відстеження на основі частин, пропонуючи новий трекер на основі фрагментів. Запропонований трекер покращує нещодавно запропонований алгоритм FragTrack для використання адаптивної схеми інтеграції сигналів. Це робиться шляхом вбудовування оригінального трекера в структуру фільтра частинок, пов’язування значення надійності з кожним фрагментом, який описує іншу частину цільового об’єкта, і динамічного коригування цих надійностей у кожному кадрі відповідно до поточного контексту. Зокрема, голосування кожного фрагмента вносить свій внесок у результат спільного відстеження відповідно до його надійності, і це дозволяє нам досягти кращої точності в обробці часткових оклюзій і змін пози, зберігаючи та навіть покращуючи ефективність оригінального трекера. Щоб продемонструвати продуктивність та ефективність запропонованого алгоритму, ми представляємо якісні та кількісні результати на низці складних відеопослідовностей","Automatic image synthesis research has been rapidly growing with deep networks getting more and more expressive. In the last couple of years, we have observed images of digits, indoor scenes, birds, chairs, etc. being automatically generated. The expressive power of image generators have also been enhanced by introducing several forms of conditioning variables such as object names, sentences, bounding box and key-point locations. In this work, we propose a novel deep conditional generative adversarial network architecture that takes its strength from the semantic layout and scene attributes integrated as conditioning variables. We show that our architecture is able to generate realistic outdoor scene images under different conditions, e.g. daynight, sunny-foggy, with clear object boundaries. ",0
"У цій статті ми розглядаємо питання відстеження на основі частин, пропонуючи новий трекер на основі фрагментів. Запропонований трекер покращує нещодавно запропонований алгоритм FragTrack для використання адаптивної схеми інтеграції сигналів. Це робиться шляхом вбудовування оригінального трекера в структуру фільтра частинок, пов’язування значення надійності з кожним фрагментом, який описує іншу частину цільового об’єкта, і динамічного коригування цих надійностей у кожному кадрі відповідно до поточного контексту. Зокрема, голосування кожного фрагмента вносить свій внесок у результат спільного відстеження відповідно до його надійності, і це дозволяє нам досягти кращої точності в обробці часткових оклюзій і змін пози, зберігаючи та навіть покращуючи ефективність оригінального трекера. Щоб продемонструвати продуктивність та ефективність запропонованого алгоритму, ми представляємо якісні та кількісні результати на низці складних відеопослідовностей","Дослідження автоматичного синтезу зображень стрімко розвиваються, а глибокі мережі стають дедалі виразнішими. За останні пару років ми спостерігали автоматичне генерування зображень цифр, сцен у приміщенні, птахів, стільців тощо. Виражальну силу генераторів зображень також було покращено завдяки введенню кількох форм обумовлюючих змінних, таких як назви об’єктів, речення, обмежувальна рамка та розташування ключових точок. У цій роботі ми пропонуємо нову глибоку умовну генеруючу змагальну архітектуру мережі, яка бере свою силу від семантичного макету та атрибутів сцени, інтегрованих як змінні умови. Ми показуємо, що наша архітектура здатна генерувати реалістичні зображення зовнішньої сцени за різних умов, наприклад, вдень, у сонячну погоду з туманом, із чіткими межами об’єктів.",0
"Many researchers argue that fusing multiple cues increases the reliability and robustness of visual tracking. However, how the multi-cue integration is realized during tracking is still an open issue. In this work, we present a novel data fusion approach for multi-cue tracking using particle filter. Our method differs from previous approaches in a number of ways. First, we carry out the integration of cues both in making predictions about the target object and in verifying them through observations. Our second and more significant contribution is that both stages of integration directly depend on the dynamically changing reliabilities of visual cues. These two aspects of our method allow the tracker to easily adapt itself to the changes in the context, and accordingly improve the tracking accuracy by resolving the ambiguities","Capturing images under extremely low-light conditions poses significant challenges for the standard camera pipeline. Images become too dark and too noisy, which makes traditional enhancement techniques almost impossible to apply. Recently, learning-based approaches have shown very promising results for this task since they have substantially more expressive capabilities to allow for improved quality. Motivated by these studies, in this paper, we aim to leverage burst photography to boost the performance and obtain much sharper and more accurate RGB images from extremely dark raw images. The backbone of our proposed framework is a novel coarse-to-fine network architecture that generates high-quality outputs progressively. The coarse network predicts a low-resolution, denoised raw image, which is then fed to the fine network to recover fine-scale details and realistic textures. To further reduce the noise level and improve the color accuracy, we extend this network to a permutation invariant structure so that it takes a burst of low-light images as input and merges information from multiple images at the feature-level. Our experiments demonstrate that our approach leads to perceptually more pleasing results than the state-of-the-art methods by producing more detailed and considerably higher quality images.",0
"Many researchers argue that fusing multiple cues increases the reliability and robustness of visual tracking. However, how the multi-cue integration is realized during tracking is still an open issue. In this work, we present a novel data fusion approach for multi-cue tracking using particle filter. Our method differs from previous approaches in a number of ways. First, we carry out the integration of cues both in making predictions about the target object and in verifying them through observations. Our second and more significant contribution is that both stages of integration directly depend on the dynamically changing reliabilities of visual cues. These two aspects of our method allow the tracker to easily adapt itself to the changes in the context, and accordingly improve the tracking accuracy by resolving the ambiguities","Зйомка зображень в умовах надзвичайно слабкого освітлення створює значні труднощі для стандартної камери. Зображення стають надто темними та занадто шумними, що робить практично неможливим застосування традиційних методів покращення. Нещодавно підходи, що ґрунтуються на навчанні, показали багатообіцяючі результати для цього завдання, оскільки вони мають значно більше виразних можливостей для підвищення якості. Керуючись цими дослідженнями, у цій статті ми прагнемо використовувати серійну фотографію, щоб підвищити продуктивність і отримати набагато чіткіші та точніші RGB-зображення з надзвичайно темних необроблених зображень. Основою запропонованої нами структури є нова мережева архітектура від грубої до тонкої, яка поступово генерує високоякісні результати. Груба мережа передбачає необроблене зображення з низькою роздільною здатністю, знешумлене, яке потім передається в тонку мережу для відновлення дрібних деталей і реалістичних текстур. Щоб ще більше знизити рівень шуму та підвищити точність кольору, ми розширюємо цю мережу до структури, інваріантної до перестановок, щоб вона приймала серію зображень із слабким освітленням як вхідні дані та об’єднувала інформацію з кількох зображень на рівні функцій. Наші експерименти демонструють, що наш підхід дає сприятливіші результати, ніж найсучасніші методи, створюючи детальніші та значно якісніші зображення.",0
"Багато дослідників стверджують, що об’єднання кількох підказок підвищує надійність і стійкість візуального відстеження. Проте те, як реалізується інтеграція кількох підказок під час відстеження, залишається відкритим питанням. У цій роботі ми представляємо новий підхід об’єднання даних для відстеження кількох сигналів за допомогою фільтра частинок. Наш метод відрізняється від попередніх підходів кількома параметрами. По-перше, ми здійснюємо інтеграцію підказок як для створення прогнозів щодо цільового об’єкта, так і для їх перевірки за допомогою спостережень. Наш другий і більш значний внесок полягає в тому, що обидва етапи інтеграції безпосередньо залежать від надійності візуальних підказок, що динамічно змінюється. Ці два аспекти нашого методу дозволяють трекеру легко адаптуватися до змін у контексті та, відповідно, підвищити точність відстеження шляхом вирішення неоднозначностей.","Capturing images under extremely low-light conditions poses significant challenges for the standard camera pipeline. Images become too dark and too noisy, which makes traditional enhancement techniques almost impossible to apply. Recently, learning-based approaches have shown very promising results for this task since they have substantially more expressive capabilities to allow for improved quality. Motivated by these studies, in this paper, we aim to leverage burst photography to boost the performance and obtain much sharper and more accurate RGB images from extremely dark raw images. The backbone of our proposed framework is a novel coarse-to-fine network architecture that generates high-quality outputs progressively. The coarse network predicts a low-resolution, denoised raw image, which is then fed to the fine network to recover fine-scale details and realistic textures. To further reduce the noise level and improve the color accuracy, we extend this network to a permutation invariant structure so that it takes a burst of low-light images as input and merges information from multiple images at the feature-level. Our experiments demonstrate that our approach leads to perceptually more pleasing results than the state-of-the-art methods by producing more detailed and considerably higher quality images.",0
"Багато дослідників стверджують, що об’єднання кількох підказок підвищує надійність і стійкість візуального відстеження. Проте те, як реалізується інтеграція кількох підказок під час відстеження, залишається відкритим питанням. У цій роботі ми представляємо новий підхід об’єднання даних для відстеження кількох сигналів за допомогою фільтра частинок. Наш метод відрізняється від попередніх підходів кількома параметрами. По-перше, ми здійснюємо інтеграцію підказок як для створення прогнозів щодо цільового об’єкта, так і для їх перевірки за допомогою спостережень. Наш другий і більш значний внесок полягає в тому, що обидва етапи інтеграції безпосередньо залежать від надійності візуальних підказок, що динамічно змінюється. Ці два аспекти нашого методу дозволяють трекеру легко адаптуватися до змін у контексті та, відповідно, підвищити точність відстеження шляхом вирішення неоднозначностей.","Зйомка зображень в умовах надзвичайно слабкого освітлення створює значні труднощі для стандартної камери. Зображення стають надто темними та занадто шумними, що робить практично неможливим застосування традиційних методів покращення. Нещодавно підходи, що ґрунтуються на навчанні, показали багатообіцяючі результати для цього завдання, оскільки вони мають значно більше виразних можливостей для підвищення якості. Керуючись цими дослідженнями, у цій статті ми прагнемо використовувати серійну фотографію, щоб підвищити продуктивність і отримати набагато чіткіші та точніші RGB-зображення з надзвичайно темних необроблених зображень. Основою запропонованої нами структури є нова мережева архітектура від грубої до тонкої, яка поступово генерує високоякісні результати. Груба мережа передбачає необроблене зображення з низькою роздільною здатністю, знешумлене, яке потім передається в тонку мережу для відновлення дрібних деталей і реалістичних текстур. Щоб ще більше знизити рівень шуму та підвищити точність кольору, ми розширюємо цю мережу до структури, інваріантної до перестановок, щоб вона приймала серію зображень із слабким освітленням як вхідні дані та об’єднувала інформацію з кількох зображень на рівні функцій. Наші експерименти демонструють, що наш підхід дає сприятливіші результати, ніж найсучасніші методи, створюючи детальніші та значно якісніші зображення.",0
"This paper presents a new image segmentation framework which employs a shape prior in the form of an edge strength function to introduce a higher-level influence on the segmentation process. We formulate segmentation as the minimization of three coupled functionals, respectively, defining three processes: prior-guided segmentation, shape feature extraction and local deformation estimation. Particularly, the shape feature extraction process is in charge of estimating an edge strength function from the evolving object region. The local deformation estimation process uses this function to determine a meaningful correspondence between a given prior and the evolving object region, and the deformation map estimated in return supervises the segmentation by enforcing the evolving object boundary towards the prior shape.","In this paper, we present a new sampling-based alpha matting approach for the accurate estimation of foreground and background layers of an image. Previous samplingbased methods typically rely on certain heuristics in collecting representative samples from known regions, and thus their performance deteriorates if the underlying assumptions are not satisfied. To alleviate this, we take an entirely new approach and formulate sampling as a sparse subset selection problem where we propose to pick a small set of candidate samples that best explains the unknown pixels. Moreover, we describe a new dissimilarity measure for comparing two samples which is based on KL-divergence between the distributions of features extracted in the vicinity of the samples. The proposed framework is general and could be easily extended to video matting by additionally taking temporal information into account in the sampling process. Evaluation on standard benchmark data sets for image and video matting demonstrates that our approach provides more accurate results compared with the state-of-the-art methods.",0
"This paper presents a new image segmentation framework which employs a shape prior in the form of an edge strength function to introduce a higher-level influence on the segmentation process. We formulate segmentation as the minimization of three coupled functionals, respectively, defining three processes: prior-guided segmentation, shape feature extraction and local deformation estimation. Particularly, the shape feature extraction process is in charge of estimating an edge strength function from the evolving object region. The local deformation estimation process uses this function to determine a meaningful correspondence between a given prior and the evolving object region, and the deformation map estimated in return supervises the segmentation by enforcing the evolving object boundary towards the prior shape.","У цій статті ми представляємо новий підхід альфа-матування на основі вибірки для точної оцінки переднього та фонового шарів зображення. Попередні методи, засновані на вибірці, зазвичай покладаються на певні евристики при зборі репрезентативних вибірок із відомих регіонів, і тому їх продуктивність погіршується, якщо базові припущення не задовольняються. Щоб полегшити це, ми використовуємо абсолютно новий підхід і формулюємо вибірку як проблему відбору розрідженої підмножини, де ми пропонуємо вибрати невеликий набір вибірок-кандидатів, які найкраще пояснюють невідомі пікселі. Крім того, ми описуємо нову міру відмінності для порівняння двох зразків, яка базується на KL-розбіжності між розподілами ознак, виділених поблизу зразків. Запропонована структура є загальною і може бути легко розширена до відеоматінгу шляхом додаткового врахування тимчасової інформації в процесі вибірки. Оцінка стандартних контрольних наборів даних для матування зображень і відео демонструє, що наш підхід забезпечує більш точні результати порівняно з найсучаснішими методами.",0
"У цьому документі представлено нову структуру сегментації зображення, яка використовує попередні форми у формі функції міцності країв, щоб запровадити вплив вищого рівня на процес сегментації. Ми формулюємо сегментацію як мінімізацію трьох пов’язаних функціоналів відповідно, визначаючи три процеси: попередньо керовану сегментацію, виділення ознак форми та локальну оцінку деформації. Зокрема, процес виділення ознак форми відповідає за оцінку функції міцності країв з області об’єкта, що розвивається. Процес локальної оцінки деформації використовує цю функцію для визначення значущої відповідності між даним попереднім і еволюційною областю об’єкта, а карта деформації, оцінена у свою чергу, контролює сегментацію, примушуючи еволюцію межі об’єкта до попередньої форми.","In this paper, we present a new sampling-based alpha matting approach for the accurate estimation of foreground and background layers of an image. Previous samplingbased methods typically rely on certain heuristics in collecting representative samples from known regions, and thus their performance deteriorates if the underlying assumptions are not satisfied. To alleviate this, we take an entirely new approach and formulate sampling as a sparse subset selection problem where we propose to pick a small set of candidate samples that best explains the unknown pixels. Moreover, we describe a new dissimilarity measure for comparing two samples which is based on KL-divergence between the distributions of features extracted in the vicinity of the samples. The proposed framework is general and could be easily extended to video matting by additionally taking temporal information into account in the sampling process. Evaluation on standard benchmark data sets for image and video matting demonstrates that our approach provides more accurate results compared with the state-of-the-art methods.",0
"У цьому документі представлено нову структуру сегментації зображення, яка використовує попередні форми у формі функції міцності країв, щоб запровадити вплив вищого рівня на процес сегментації. Ми формулюємо сегментацію як мінімізацію трьох пов’язаних функціоналів відповідно, визначаючи три процеси: попередньо керовану сегментацію, виділення ознак форми та локальну оцінку деформації. Зокрема, процес виділення ознак форми відповідає за оцінку функції міцності країв з області об’єкта, що розвивається. Процес локальної оцінки деформації використовує цю функцію для визначення значущої відповідності між даним попереднім і еволюційною областю об’єкта, а карта деформації, оцінена у свою чергу, контролює сегментацію, примушуючи еволюцію межі об’єкта до попередньої форми.","У цій статті ми представляємо новий підхід альфа-матування на основі вибірки для точної оцінки переднього та фонового шарів зображення. Попередні методи, засновані на вибірці, зазвичай покладаються на певні евристики при зборі репрезентативних вибірок із відомих регіонів, і тому їх продуктивність погіршується, якщо базові припущення не задовольняються. Щоб полегшити це, ми застосували абсолютно новий підхід і сформулювали вибірку як проблему відбору розрідженої підмножини, де ми пропонуємо вибрати невеликий набір вибірок-кандидатів, які найкраще пояснюють невідомі пікселі. Крім того, ми описуємо нову міру відмінності для порівняння двох зразків, яка базується на KL-розбіжності між розподілами ознак, виділених поблизу зразків. Запропонована структура є загальною і може бути легко розширена до відеоматінгу шляхом додаткового врахування тимчасової інформації в процесі вибірки. Оцінка стандартних контрольних наборів даних для матування зображень і відео демонструє, що наш підхід забезпечує більш точні результати порівняно з найсучаснішими методами.",0
"We present a simple and robust feature preserving image regularization by letting local region measures modulate the diffusivity. The purpose of this modulation is to disambiguate low level cues in early vision. We interpret the Ambrosio-Tortorelli approximation of the Mumford-Shah model as a system with modulatory feedback and utilize this interpretation to integrate high level information into the regularization process. The method does not require any prior model or learning; the high level information is extracted from local regions and fed back to the regularization step. An important characteristic of the method is that both negative and positive feedback can be simultaneously used without creating oscillations. Experiments performed with both gray and color natural images demonstrate the potential of the method under difficult noise types, non-uniform contrast, existence of multi-scale patterns and textures.","Making sense of ever-growing amount of visual data available on the web is difficult, especially when considered in an unsupervised manner. As a step towards this goal, this study tackles a relatively less explored topic of generating structured summaries of large photo collections. Our framework relies on the notion of a story graph which captures the main narratives in the data and their relationships based on their visual, textual and spatio-temporal features. Its output is a directed graph with a set of possibly intersecting paths. Our proposed approach identifies coherent visual storylines and exploits sub-modularity to select a subset of these lines which covers the general narrative at most. Our experimental analysis reveals that extracted story graphs allow for obtaining better results when utilized as priors for photo album summarization. Moreover, our user studies show that our approach delivers better performance on next image prediction and coverage tasks than the state-of-the-art.",0
"We present a simple and robust feature preserving image regularization by letting local region measures modulate the diffusivity. The purpose of this modulation is to disambiguate low level cues in early vision. We interpret the Ambrosio-Tortorelli approximation of the Mumford-Shah model as a system with modulatory feedback and utilize this interpretation to integrate high level information into the regularization process. The method does not require any prior model or learning; the high level information is extracted from local regions and fed back to the regularization step. An important characteristic of the method is that both negative and positive feedback can be simultaneously used without creating oscillations. Experiments performed with both gray and color natural images demonstrate the potential of the method under difficult noise types, non-uniform contrast, existence of multi-scale patterns and textures.","Важко розібратися в постійно зростаючій кількості візуальних даних, доступних в Інтернеті, особливо якщо розглядати їх без нагляду. Як крок до цієї мети, це дослідження стосується відносно менш дослідженої теми створення структурованих підсумків великих колекцій фотографій. Наша структура спирається на поняття графіка історії, який фіксує основні наративи в даних та їхні зв’язки на основі їхніх візуальних, текстових і просторово-часових особливостей. Його виходом є орієнтований граф із набором можливо пересічних шляхів. Запропонований нами підхід визначає послідовні візуальні сюжетні лінії та використовує субмодульність, щоб вибрати підмножину цих ліній, яка максимально охоплює загальну розповідь. Наш експериментальний аналіз показує, що витягнуті графіки історій дозволяють отримати кращі результати, якщо їх використовувати як попередні для підсумовування фотоальбому. Крім того, наші дослідження користувачів показують, що наш підхід забезпечує кращу продуктивність у наступних задачах прогнозування зображення та покриття, ніж найсучасніший.",0
"Ми представляємо просту та надійну функцію, яка зберігає регулярність зображення, дозволяючи вимірюванням локальних регіонів модулювати дифузію. Метою цієї модуляції є усунення неоднозначності сигналів низького рівня в ранньому баченні. Ми інтерпретуємо наближення Амбросіо-Тортореллі моделі Мамфорда-Шаха як систему з модулюючим зворотним зв&#39;язком і використовуємо цю інтерпретацію для інтеграції інформації високого рівня в процес регулярізації. Метод не потребує попередньої моделі чи навчання; інформація високого рівня витягується з локальних регіонів і повертається на етап регулярізації. Важливою характеристикою методу є те, що як негативний, так і позитивний зворотний зв&#39;язок можуть використовуватися одночасно без створення коливань. Експерименти, проведені як з сірими, так і з кольоровими природними зображеннями, демонструють потенціал методу в умовах складних типів шуму, нерівномірного контрасту, наявності багатомасштабних візерунків і текстур.","Making sense of ever-growing amount of visual data available on the web is difficult, especially when considered in an unsupervised manner. As a step towards this goal, this study tackles a relatively less explored topic of generating structured summaries of large photo collections. Our framework relies on the notion of a story graph which captures the main narratives in the data and their relationships based on their visual, textual and spatio-temporal features. Its output is a directed graph with a set of possibly intersecting paths. Our proposed approach identifies coherent visual storylines and exploits sub-modularity to select a subset of these lines which covers the general narrative at most. Our experimental analysis reveals that extracted story graphs allow for obtaining better results when utilized as priors for photo album summarization. Moreover, our user studies show that our approach delivers better performance on next image prediction and coverage tasks than the state-of-the-art.",0
"Ми представляємо просту та надійну функцію, яка зберігає регулярізацію зображення, дозволяючи вимірюванням локальних регіонів модулювати дифузію. Метою цієї модуляції є усунення неоднозначності сигналів низького рівня в ранньому баченні. Ми інтерпретуємо наближення Амбросіо-Тортореллі моделі Мамфорда-Шаха як систему з модулюючим зворотним зв&#39;язком і використовуємо цю інтерпретацію для інтеграції інформації високого рівня в процес регулярізації. Метод не потребує попередньої моделі чи навчання; інформація високого рівня витягується з локальних регіонів і повертається на етап регулярізації. Важливою характеристикою методу є те, що як негативний, так і позитивний зворотний зв&#39;язок можуть використовуватися одночасно без створення коливань. Експерименти, проведені як з сірими, так і з кольоровими природними зображеннями, демонструють потенціал методу в умовах складних типів шуму, нерівномірного контрасту, наявності багатомасштабних візерунків і текстур.","Важко розібратися в постійно зростаючій кількості візуальних даних, доступних в Інтернеті, особливо якщо розглядати їх без нагляду. Як крок до цієї мети, це дослідження стосується відносно менш дослідженої теми створення структурованих підсумків великих колекцій фотографій. Наша структура спирається на поняття графіка історії, який фіксує основні наративи в даних та їхні зв’язки на основі їхніх візуальних, текстових і просторово-часових особливостей. Його виходом є орієнтований граф із набором можливо пересічних шляхів. Запропонований нами підхід визначає послідовні візуальні сюжетні лінії та використовує субмодульність, щоб вибрати підмножину цих ліній, яка максимально охоплює загальну розповідь. Наш експериментальний аналіз показує, що витягнуті графіки історій дозволяють отримати кращі результати, якщо їх використовувати як попередні для підсумовування фотоальбому. Крім того, наші дослідження користувачів показують, що наш підхід забезпечує кращу продуктивність у наступних задачах прогнозування зображення та покриття, ніж найсучасніший.",0
"We present a new skeletal representation along with a matching framework to address the deformable shape recognition problem. The disconnectedness arises as a result of excessive regularization that we use to describe a shape at an attainably coarse scale. Our motivation is to rely on the stable properties of the shape instead of inaccurately measured secondary details. The new representation does not suffer from the common instability problems of traditional connected skeletons and the matching process gives quite successful results on a diverse database of 2D shapes. An important difference of our approach from the conventional use of the skeleton is that we replace the local coordinate frame with a global euclidean frame supported by additional mechanisms to handle articulations and local boundary deformations. As a result, we can produce descriptions that are sensitive to any combination of changes in scale, position, orientation, and articulation, as well as invariant ones.","Magnetic resonance imaging (MRI) is used in many diagnostic applications as it has a high soft-tissue contrast and is a non-invasive medical imaging method. MR signal levels differs according to the parameters T1, T2 and PD that change with respect to the chemical structure of the tissues. However, long scan times might limit acquiring images from multiple contrasts or if the multi-contrasts images are acquired, the contrasts are noisy. To overcome this limitation of MRI, multi-contrast synthesis can be utilized. In this paper, we propose a deep learning method based on Channel-Exchanging-Network (CEN) for multi-contrast image synthesis. Demonstrations are provided on IXI dataset. The proposed model based on CEN is compared against alternative methods based on CNNs and GANs. Our results show that the proposed model achieves superior performance to the competing methods. ",0
"We present a new skeletal representation along with a matching framework to address the deformable shape recognition problem. The disconnectedness arises as a result of excessive regularization that we use to describe a shape at an attainably coarse scale. Our motivation is to rely on the stable properties of the shape instead of inaccurately measured secondary details. The new representation does not suffer from the common instability problems of traditional connected skeletons and the matching process gives quite successful results on a diverse database of 2D shapes. An important difference of our approach from the conventional use of the skeleton is that we replace the local coordinate frame with a global euclidean frame supported by additional mechanisms to handle articulations and local boundary deformations. As a result, we can produce descriptions that are sensitive to any combination of changes in scale, position, orientation, and articulation, as well as invariant ones.","Магнітно-резонансна томографія (МРТ) використовується в багатьох діагностичних програмах, оскільки вона має високий контраст м’яких тканин і є неінвазивним медичним методом візуалізації. Рівень сигналу МР відрізняється відповідно до параметрів T1, T2 і PD, які змінюються залежно від хімічної структури тканин. Однак тривалий час сканування може обмежити отримання зображень із кількох контрастів, або якщо отримані багатоконтрастні зображення, контрасти мають шум. Щоб подолати це обмеження МРТ, можна використовувати мультиконтрастний синтез. У цій статті ми пропонуємо метод глибокого навчання на основі мережі обміну каналами (CEN) для синтезу мультиконтрастного зображення. Демонстрації надаються на базі даних IXI. Запропонована модель на основі CEN порівнюється з альтернативними методами на основі CNN і GAN. Наші результати показують, що запропонована модель досягає кращої продуктивності порівняно з конкурентними методами.",0
"Ми представляємо нове скелетне представлення разом із відповідним каркасом для вирішення проблеми розпізнавання форми, що деформується. Роз’єднаність виникає в результаті надмірної регуляризації, яку ми використовуємо для опису форми в досяжно грубому масштабі. Наша мотивація полягає в тому, щоб покладатися на стабільні властивості форми замість неточно виміряних вторинних деталей. Нове представлення не страждає від поширених проблем нестабільності традиційних з’єднаних скелетів, і процес зіставлення дає досить успішні результати на різноманітній базі даних двовимірних форм. Важлива відмінність нашого підходу від звичайного використання скелета полягає в тому, що ми замінюємо локальну систему координат на глобальну евклідову систему, яка підтримується додатковими механізмами для обробки артикуляцій і локальних граничних деформацій. У результаті ми можемо створювати описи, чутливі до будь-якої комбінації змін у масштабі, положенні, орієнтації та артикуляції, а також інваріантні.","Magnetic resonance imaging (MRI) is used in many diagnostic applications as it has a high soft-tissue contrast and is a non-invasive medical imaging method. MR signal levels differs according to the parameters T1, T2 and PD that change with respect to the chemical structure of the tissues. However, long scan times might limit acquiring images from multiple contrasts or if the multi-contrasts images are acquired, the contrasts are noisy. To overcome this limitation of MRI, multi-contrast synthesis can be utilized. In this paper, we propose a deep learning method based on Channel-Exchanging-Network (CEN) for multi-contrast image synthesis. Demonstrations are provided on IXI dataset. The proposed model based on CEN is compared against alternative methods based on CNNs and GANs. Our results show that the proposed model achieves superior performance to the competing methods. ",0
"Ми представляємо нове скелетне представлення разом із відповідним каркасом для вирішення проблеми розпізнавання форми, що деформується. Роз’єднаність виникає в результаті надмірної регуляризації, яку ми використовуємо для опису форми в досяжно грубому масштабі. Наша мотивація полягає в тому, щоб покладатися на стабільні властивості форми замість неточно виміряних вторинних деталей. Нове представлення не страждає від поширених проблем нестабільності традиційних з’єднаних скелетів, і процес зіставлення дає досить успішні результати на різноманітній базі даних двовимірних форм. Важлива відмінність нашого підходу від звичайного використання скелета полягає в тому, що ми замінюємо локальну систему координат на глобальну евклідову систему, яка підтримується додатковими механізмами для обробки артикуляцій і локальних граничних деформацій. У результаті ми можемо створювати описи, чутливі до будь-якої комбінації змін у масштабі, положенні, орієнтації та артикуляції, а також інваріантні.","Магнітно-резонансна томографія (МРТ) використовується в багатьох діагностичних програмах, оскільки вона має високий контраст м’яких тканин і є неінвазивним медичним методом візуалізації. Рівень сигналу МР відрізняється відповідно до параметрів T1, T2 і PD, які змінюються залежно від хімічної структури тканин. Однак тривалий час сканування може обмежити отримання зображень із кількох контрастів, або якщо отримані багатоконтрастні зображення, контрасти мають шум. Щоб подолати це обмеження МРТ, можна використовувати мультиконтрастний синтез. У цій статті ми пропонуємо метод глибокого навчання на основі мережі обміну каналами (CEN) для синтезу мультиконтрастного зображення. Демонстрації надаються на базі даних IXI. Запропонована модель на основі CEN порівнюється з альтернативними методами на основі CNN і GAN. Наші результати показують, що запропонована модель досягає кращої продуктивності порівняно з конкурентними методами.",0
"As recently discussed by Bar, Kiryati, and Sochen in [3], the Ambrosio-Tortorelli approximation of the Mumford-Shah functional defines an extended line process regularization where the regularizer has an additional constraint introduced by the term ρ|∇v| . This term mildly forces some spatial organization by demanding that the edges are smooth. However, it does not force spatial coherence such as edge direction compatibility or edge connectivity, as in the traditional edge detectors such as Canny. Using the connection between regularization and diffusion filters, we incorporate further spatial structure into the regularization process of the Mumford-Shah model. The new model combines smoothing, edge detection and edge linking steps of the traditional approach to boundary detection. Importance of spatial coherence is best observed if the image noise is salt and pepper like. Proposed approach is able to deal with difficult noise cases without using non-smooth cost functions such as  in the data fidelity or regularizer. ","Humans are able to perceive, understand and reason about causal events. Developing models with similar physical and causal understanding capabilities is a long-standing goal of artificial intelligence. As a step towards this direction, we introduce CRAFT1 , a new video question answering dataset that requires causal reasoning about physical forces and object interactions. It contains 58K video and question pairs that are generated from 10K videos from 20 different virtual environments, containing various objects in motion that interact with each other and the scene. Two question categories in CRAFT include previously studied descriptive and counterfactual questions. Additionally, inspired by the Force Dynamics Theory in cognitive linguistics, we introduce a new causal question category that involves understanding the causal interactions between objects through notions like cause, enable, and prevent. Our results show that even though the questions in CRAFT are easy for humans, the tested baseline models, including existing state-of-the-art methods, do not yet deal with the challenges posed in our benchmark.",0
"As recently discussed by Bar, Kiryati, and Sochen in [3], the Ambrosio-Tortorelli approximation of the Mumford-Shah functional defines an extended line process regularization where the regularizer has an additional constraint introduced by the term ρ|∇v| . This term mildly forces some spatial organization by demanding that the edges are smooth. However, it does not force spatial coherence such as edge direction compatibility or edge connectivity, as in the traditional edge detectors such as Canny. Using the connection between regularization and diffusion filters, we incorporate further spatial structure into the regularization process of the Mumford-Shah model. The new model combines smoothing, edge detection and edge linking steps of the traditional approach to boundary detection. Importance of spatial coherence is best observed if the image noise is salt and pepper like. Proposed approach is able to deal with difficult noise cases without using non-smooth cost functions such as  in the data fidelity or regularizer. ","Люди здатні сприймати, розуміти та міркувати про причинно-наслідкові події. Розробка моделей із подібними фізичними та причинно-наслідковими можливостями розуміння є давньою метою штучного інтелекту. Як крок у цьому напрямку ми представляємо CRAFT1, новий набір даних із відповідями на відеозапитання, який вимагає причинно-наслідкових міркувань про фізичні сили та взаємодію об’єктів. Він містить 58 тис. відео та пар запитань, створених із 10 тис. відео з 20 різних віртуальних середовищ, що містять різні рухомі об’єкти, які взаємодіють один з одним і сценою. Дві категорії запитань у CRAFT включають раніше вивчені описові та контрфактичні запитання. Крім того, натхненний теорією динаміки сил у когнітивній лінгвістиці, ми вводимо нову категорію причинно-наслідкових питань, яка передбачає розуміння причинно-наслідкових взаємодій між об’єктами через такі поняття, як причина, можливість і запобігання. Наші результати показують, що, незважаючи на те, що запитання в CRAFT легкі для людей, перевірені базові моделі, включаючи існуючі найсучасніші методи, ще не справляються з проблемами, поставленими в нашому тесті.",0
"Як нещодавно обговорювали Бар, Кіріаті та Сочен у [3], апроксимація Амбросіо-Тортореллі функціоналу Мамфорда-Шаха визначає регулярізацію розширеного лінійного процесу, де регуляризатор має додаткове обмеження, введене членом ρ|∇v| . Цей термін м’яко вимагає певної просторової організації, вимагаючи гладкості країв. Однак він не забезпечує просторову когерентність, таку як сумісність напрямків країв або з’єднання країв, як у традиційних детекторах країв, таких як Canny. Використовуючи зв’язок між регуляризацією та дифузійними фільтрами, ми включаємо подальшу просторову структуру в процес регуляризації моделі Мамфорда-Шаха. Нова модель поєднує згладжування, визначення країв і з’єднання країв традиційного підходу до визначення меж. Важливість просторової когерентності найкраще спостерігається, якщо шум зображення схожий на сіль і перець. Запропонований підхід здатний працювати зі складними випадками шуму без використання негладких функцій вартості, таких як у точності даних або регуляризаторі.","Humans are able to perceive, understand and reason about causal events. Developing models with similar physical and causal understanding capabilities is a long-standing goal of artificial intelligence. As a step towards this direction, we introduce CRAFT1 , a new video question answering dataset that requires causal reasoning about physical forces and object interactions. It contains 58K video and question pairs that are generated from 10K videos from 20 different virtual environments, containing various objects in motion that interact with each other and the scene. Two question categories in CRAFT include previously studied descriptive and counterfactual questions. Additionally, inspired by the Force Dynamics Theory in cognitive linguistics, we introduce a new causal question category that involves understanding the causal interactions between objects through notions like cause, enable, and prevent. Our results show that even though the questions in CRAFT are easy for humans, the tested baseline models, including existing state-of-the-art methods, do not yet deal with the challenges posed in our benchmark.",0
"Як нещодавно обговорювали Бар, Кіріаті та Сочен у [3], апроксимація Амбросіо-Тортореллі функціоналу Мамфорда-Шаха визначає розширену регулярізацію лінійного процесу, де регулязатор має додаткове обмеження, введене членом ρ|∇v| . Цей термін м’яко вимагає певної просторової організації, вимагаючи гладкості країв. Однак він не забезпечує просторову когерентність, таку як сумісність напрямків країв або з’єднання країв, як у традиційних детекторах країв, таких як Canny. Використовуючи зв’язок між регуляризацією та дифузійними фільтрами, ми включаємо подальшу просторову структуру в процес регуляризації моделі Мамфорда-Шаха. Нова модель поєднує згладжування, визначення країв і з’єднання країв традиційного підходу до виявлення меж. Важливість просторової когерентності найкраще спостерігається, якщо шум зображення схожий на сіль і перець. Запропонований підхід здатний працювати зі складними випадками шуму без використання негладких функцій вартості, таких як у точності даних або регуляризаторі.","Люди здатні сприймати, розуміти та міркувати про причинно-наслідкові події. Розробка моделей із подібними фізичними та причинно-наслідковими можливостями розуміння є давньою метою штучного інтелекту. Як крок у цьому напрямку ми представляємо CRAFT1, новий набір даних із відповідями на відеозапитання, який вимагає причинно-наслідкових міркувань про фізичні сили та взаємодію об’єктів. Він містить 58 тис. відео та пар запитань, створених із 10 тис. відео з 20 різних віртуальних середовищ, що містять різні рухомі об’єкти, які взаємодіють один з одним і сценою. Дві категорії запитань у CRAFT включають раніше вивчені описові та контрфактичні запитання. Крім того, натхненний теорією динаміки сил у когнітивній лінгвістиці, ми вводимо нову категорію причинно-наслідкових питань, яка передбачає розуміння причинно-наслідкових взаємодій між об’єктами через такі поняття, як причина, можливість і запобігання. Наші результати показують, що, незважаючи на те, що запитання в CRAFT легкі для людей, перевірені базові моделі, включаючи існуючі найсучасніші методи, ще не справляються з проблемами, поставленими в нашому тесті.",0
"Local symmetry axis based schemes have been used for generic shape recognition as they lead to articulation insensitive representations. Despite their strengths, purely syntactic level of axial representations precludes the possibility of distinguishing a likely articulation from an unlikely one. In order to overcome this weakness, syntax should be combined with pragmatics and/or semantics. As a solution we propose a novel articulation space which enables inferences on the likelihood of possible articulations. Articulation priors can be constructed directly from examples (pragmatics) or set externally (semantics). We incorporate articulation priors to a skeletal matching scheme to arrive at an enriched axial representation which is sensitive to unlikely articulations but insensitive to likely ones","In recent years, many computational models for saliency prediction have been introduced. For dynamic scenes, the existing models typically combine different feature maps extracted from spatial and temporal domains either by following generic integration strategies such as averaging or winners take all or using machine learning techniques to set each feature’s importance. Rather than resorting to these fixed feature integration schemes, in this paper, we propose a novel weakly supervised dynamic saliency model called HedgeSal, which is based on a decision-theoretic online learning scheme. Our framework uses two pretrained deep static saliency models as experts to extract individual saliency maps from appearance and motion streams, and then generates the final saliency map by weighted decisions of all these models. As visual characteristics of dynamic scenes constantly vary, the models providing consistently good predictions in the past are automatically assigned higher weights, allowing each expert to adjust itself to the current conditions. We demonstrate the effectiveness of our model on the CRCNS, UCFSports and CITIUS datasets.",0
"Local symmetry axis based schemes have been used for generic shape recognition as they lead to articulation insensitive representations. Despite their strengths, purely syntactic level of axial representations precludes the possibility of distinguishing a likely articulation from an unlikely one. In order to overcome this weakness, syntax should be combined with pragmatics and/or semantics. As a solution we propose a novel articulation space which enables inferences on the likelihood of possible articulations. Articulation priors can be constructed directly from examples (pragmatics) or set externally (semantics). We incorporate articulation priors to a skeletal matching scheme to arrive at an enriched axial representation which is sensitive to unlikely articulations but insensitive to likely ones","В останні роки було введено багато обчислювальних моделей для прогнозування помітності. Для динамічних сцен існуючі моделі зазвичай поєднують різні карти функцій, отримані з просторових і часових доменів, дотримуючись загальних стратегій інтеграції, таких як усереднення або переможці отримують усе, або використовуючи методи машинного навчання для встановлення важливості кожної функції. Замість того, щоб вдаватися до цих схем інтеграції фіксованих функцій, у цій статті ми пропонуємо нову слабко контрольовану динамічну модель помітності під назвою HedgeSal, яка базується на схемі онлайн-навчання, що базується на теоретичному прийнятті рішень. Наш фреймворк використовує дві попередньо підготовлені глибокі статичні моделі помітності як експертів для отримання індивідуальних карт помітності з потоків зовнішнього вигляду та руху, а потім генерує остаточну карту помітності шляхом зважених рішень усіх цих моделей. Оскільки візуальні характеристики динамічних сцен постійно змінюються, моделям, що забезпечують незмінно хороші прогнози в минулому, автоматично призначаються вищі ваги, що дозволяє кожному експерту пристосуватися до поточних умов. Ми демонструємо ефективність нашої моделі на наборах даних CRCNS, UCFSports і CITIUS.",0
"Схеми, засновані на локальній осі симетрії, використовувалися для загального розпізнавання форми, оскільки вони призводили до представлень, нечутливих до артикуляції. Незважаючи на їхні сильні сторони, суто синтаксичний рівень осьових репрезентацій виключає можливість відрізнити ймовірну артикуляцію від малоймовірної. Щоб подолати цю слабкість, синтаксис слід поєднувати з прагматикою та/або семантикою. Як рішення ми пропонуємо новий артикуляційний простір, який дозволяє робити висновки про ймовірність можливих артикуляцій. Пріори артикуляції можуть бути побудовані безпосередньо з прикладів (прагматика) або задані зовні (семантика). Ми включаємо артикуляцію перед скелетною схемою відповідності, щоб досягти збагаченого осьового представлення, чутливого до малоймовірних артикуляцій, але нечутливого до ймовірних","In recent years, many computational models for saliency prediction have been introduced. For dynamic scenes, the existing models typically combine different feature maps extracted from spatial and temporal domains either by following generic integration strategies such as averaging or winners take all or using machine learning techniques to set each feature’s importance. Rather than resorting to these fixed feature integration schemes, in this paper, we propose a novel weakly supervised dynamic saliency model called HedgeSal, which is based on a decision-theoretic online learning scheme. Our framework uses two pretrained deep static saliency models as experts to extract individual saliency maps from appearance and motion streams, and then generates the final saliency map by weighted decisions of all these models. As visual characteristics of dynamic scenes constantly vary, the models providing consistently good predictions in the past are automatically assigned higher weights, allowing each expert to adjust itself to the current conditions. We demonstrate the effectiveness of our model on the CRCNS, UCFSports and CITIUS datasets.",0
"Схеми, засновані на локальній осі симетрії, використовувалися для загального розпізнавання форми, оскільки вони призводили до представлень, нечутливих до артикуляції. Незважаючи на їхні сильні сторони, суто синтаксичний рівень осьових репрезентацій виключає можливість відрізнити ймовірну артикуляцію від малоймовірної. Щоб подолати цю слабкість, синтаксис слід поєднувати з прагматикою та/або семантикою. Як рішення ми пропонуємо новий артикуляційний простір, який дозволяє робити висновки про ймовірність можливих артикуляцій. Пріори артикуляції можуть бути побудовані безпосередньо з прикладів (прагматика) або задані зовні (семантика). Ми включаємо артикуляцію перед скелетною схемою відповідності, щоб досягти збагаченого осьового представлення, чутливого до малоймовірних артикуляцій, але нечутливого до ймовірних","В останні роки було введено багато обчислювальних моделей для прогнозування помітності. Для динамічних сцен існуючі моделі зазвичай поєднують різні карти функцій, отримані з просторових і часових доменів, дотримуючись загальних стратегій інтеграції, таких як усереднення або переможці отримують усе, або використовуючи методи машинного навчання для встановлення важливості кожної функції. Замість того, щоб вдаватися до цих схем інтеграції фіксованих функцій, у цій статті ми пропонуємо нову слабко контрольовану динамічну модель помітності під назвою HedgeSal, яка базується на схемі онлайн-навчання, що базується на теоретичному прийнятті рішень. Наш фреймворк використовує дві попередньо підготовлені глибокі статичні моделі помітності як експертів для отримання індивідуальних карт помітності з потоків зовнішнього вигляду та руху, а потім генерує остаточну карту помітності шляхом зважених рішень усіх цих моделей. Оскільки візуальні характеристики динамічних сцен постійно змінюються, моделям, що забезпечують незмінно хороші прогнози в минулому, автоматично призначаються вищі ваги, що дозволяє кожному експерту пристосуватися до поточних умов. Ми демонструємо ефективність нашої моделі на наборах даних CRCNS, UCFSports і CITIUS.",0
"Many applications of computer vision requires segmenting out of an object of interest from a given image. Motivated by unlevel-sets formulation of Raviv, Kiryati and Sochen [8] and statistical formulation of Leventon, Grimson and Faugeras [6], we present a new image segmentation method which accounts for prior shape information. Our method depends on Ambrosio-Tortorelli approximation of Mumford-Shah functional. The prior shape is represented by a by-product of this functional, a smooth edge indicator function, known as the “edge strength function”, which provides a distance-like surface for the shape boundary. Our method can handle arbitrary deformations due to shape variability as well as plane Euclidean transformations. The method is also robust with respect to noise and missing parts. Furthermore, this formulation does not require simple closed curves as in a typical level set formulation.","Understanding and reasoning about cooking recipes is a fruitful research direction towards enabling machines to interpret procedural text. In this work, we introduce RecipeQA, a dataset for multimodal comprehension of cooking recipes. It comprises of approximately 20K instructional recipes with multiple modalities such as titles, descriptions and aligned set of images. With over 36K automatically generated question-answer pairs, we design a set of comprehension and reasoning tasks that require joint understanding of images and text, capturing the temporal flow of events and making sense of procedural knowledge. Our preliminary results indicate that RecipeQA will serve as a challenging test bed and an ideal benchmark for evaluating machine comprehension systems.",0
"Many applications of computer vision requires segmenting out of an object of interest from a given image. Motivated by unlevel-sets formulation of Raviv, Kiryati and Sochen [8] and statistical formulation of Leventon, Grimson and Faugeras [6], we present a new image segmentation method which accounts for prior shape information. Our method depends on Ambrosio-Tortorelli approximation of Mumford-Shah functional. The prior shape is represented by a by-product of this functional, a smooth edge indicator function, known as the “edge strength function”, which provides a distance-like surface for the shape boundary. Our method can handle arbitrary deformations due to shape variability as well as plane Euclidean transformations. The method is also robust with respect to noise and missing parts. Furthermore, this formulation does not require simple closed curves as in a typical level set formulation.","Розуміння та обґрунтування кулінарних рецептів є плідним напрямком дослідження, спрямованим на те, щоб дозволити машинам інтерпретувати процедурний текст. У цій роботі ми представляємо RecipeQA, набір даних для мультимодального розуміння кулінарних рецептів. Він складається з приблизно 20 тисяч навчальних рецептів із різними модальностями, такими як заголовки, описи та вирівняний набір зображень. Маючи понад 36 тисяч автоматично згенерованих пар запитань-відповідей, ми розробляємо набір завдань на розуміння й міркування, які вимагають спільного розуміння зображень і тексту, фіксації тимчасового потоку подій і осмислення процедурних знань. Наші попередні результати вказують на те, що RecipeQA слугуватиме складним випробувальним майданчиком і ідеальним еталоном для оцінки систем машинного розуміння.",0
"Багато застосувань комп’ютерного зору вимагають сегментування об’єкта інтересу з даного зображення. Спираючись на формулювання нерівневих наборів Равіва, Кір’яті та Сочена [8] та статистичне формулювання Левентона, Грімсона та Фогера [6], ми представляємо новий метод сегментації зображення, який враховує попередню інформацію про форму. Наш метод залежить від апроксимації Амбросіо-Тортореллі функціоналу Мамфорда-Шаха. Попередня форма представлена побічним продуктом цього функціоналу, функцією індикатора гладких країв, відомою як «функція сили краю», яка забезпечує поверхню, подібну до відстані, для межі форми. Наш метод може обробляти довільні деформації через мінливість форми, а також плоскі евклідові перетворення. Метод також надійний щодо шуму та відсутніх частин. Крім того, це формулювання не вимагає простих замкнутих кривих, як у типовому формулюванні набору рівнів.","Understanding and reasoning about cooking recipes is a fruitful research direction towards enabling machines to interpret procedural text. In this work, we introduce RecipeQA, a dataset for multimodal comprehension of cooking recipes. It comprises of approximately 20K instructional recipes with multiple modalities such as titles, descriptions and aligned set of images. With over 36K automatically generated question-answer pairs, we design a set of comprehension and reasoning tasks that require joint understanding of images and text, capturing the temporal flow of events and making sense of procedural knowledge. Our preliminary results indicate that RecipeQA will serve as a challenging test bed and an ideal benchmark for evaluating machine comprehension systems.",0
"Багато застосувань комп’ютерного зору вимагають сегментування об’єкта інтересу з даного зображення. Спираючись на формулювання нерівневих наборів Равіва, Кір’яті та Сочена [8] та статистичне формулювання Левентона, Грімсона та Фогера [6], ми представляємо новий метод сегментації зображення, який враховує попередню інформацію про форму. Наш метод залежить від апроксимації Амбросіо-Тортореллі функціонала Мамфорда-Шаха. Попередня форма представлена побічним продуктом цього функціоналу, функцією індикатора гладких країв, відомою як «функція сили краю», яка забезпечує поверхню, подібну до відстані, для межі форми. Наш метод може обробляти довільні деформації через мінливість форми, а також плоскі евклідові перетворення. Метод також надійний щодо шуму та відсутніх частин. Крім того, це формулювання не вимагає простих замкнутих кривих, як у типовому формулюванні набору рівнів.","Розуміння та обґрунтування кулінарних рецептів є плідним напрямком дослідження, спрямованим на те, щоб дозволити машинам інтерпретувати процедурний текст. У цій роботі ми представляємо RecipeQA, набір даних для мультимодального розуміння кулінарних рецептів. Він складається з приблизно 20 тисяч навчальних рецептів із різними модальностями, такими як заголовки, описи та вирівняний набір зображень. Маючи понад 36 тисяч автоматично згенерованих пар запитань-відповідей, ми розробляємо набір завдань на розуміння й міркування, які вимагають спільного розуміння зображень і тексту, фіксації тимчасового потоку подій і осмислення процедурних знань. Наші попередні результати вказують на те, що RecipeQA слугуватиме складним випробувальним майданчиком і ідеальним еталоном для оцінки систем машинного розуміння.",0
"It is now possible to design real-time, low-cost computer vision systems even in personal computers due to the recent advances in electronics and the computer industry. Due to this reason, it is feasible to develop computer-vision-based human-computer interaction systems. A vision-based continuous Graffiti™-like text entry system is presented. The user sketches characters in a Graffiti™-like alphabet in a continuous manner on a flat surface using a laser pointer. The beam of the laser pointer is tracked on the image sequences captured by a camera, and the corresponding written word is recognized from the extracted trace of the laser beam.","How to best integrate linguistic and perceptual processing in multi-modal tasks that involve language and vision is an important open problem. In this work, we argue that the common practice of using language in a topdown manner, to direct visual attention over high-level visual features, may not be optimal. We hypothesize that the use of language to also condition the bottom-up processing from pixels to high-level features can provide benefits to the overall performance. To support our claim, we propose a U-Net-based model and perform experiments on two language-vision dense-prediction tasks: referring expression segmentation and language-guided image colorization. We compare results where either one or both of the top-down and bottom-up visual branches are conditioned on language. Our experiments reveal that using language to control the filters for bottom-up visual processing in addition to top-down attention leads to better results on both tasks and achieves competitive performance. Our linguistic analysis suggests that bottom-up conditioning improves segmentation of objects especially when input text refers to low-level visual concepts",0
"It is now possible to design real-time, low-cost computer vision systems even in personal computers due to the recent advances in electronics and the computer industry. Due to this reason, it is feasible to develop computer-vision-based human-computer interaction systems. A vision-based continuous Graffiti™-like text entry system is presented. The user sketches characters in a Graffiti™-like alphabet in a continuous manner on a flat surface using a laser pointer. The beam of the laser pointer is tracked on the image sequences captured by a camera, and the corresponding written word is recognized from the extracted trace of the laser beam.","Як найкраще інтегрувати лінгвістичну та перцептивну обробку в мультимодальних завданнях, які включають мову та зір, є важливою відкритою проблемою. У цій роботі ми стверджуємо, що звичайна практика використання мови за принципом «зверху вниз» для спрямування візуальної уваги на візуальні функції високого рівня може бути неоптимальною. Ми припускаємо, що використання мови для обумовлення обробки знизу вгору від пікселів до функцій високого рівня може принести переваги загальній продуктивності. Щоб підтвердити наше твердження, ми пропонуємо модель на основі U-Net і проводимо експерименти з двома завданнями щільного прогнозування мовного бачення: сегментація виразу за посиланням і кольорування зображення під керуванням мови. Ми порівнюємо результати, де одна або обидві візуальні гілки зверху вниз і знизу вгору залежать від мови. Наші експерименти показують, що використання мови для керування фільтрами для візуальної обробки знизу вгору на додаток до уваги зверху вниз призводить до кращих результатів в обох завданнях і досягає конкурентоспроможності. Наш лінгвістичний аналіз показує, що кондиціонування знизу вгору покращує сегментацію об’єктів, особливо коли вхідний текст відноситься до візуальних концепцій низького рівня",0
"Завдяки останнім досягненням електроніки та комп’ютерної індустрії тепер можливо розробляти недорогі системи комп’ютерного зору в режимі реального часу навіть у персональних комп’ютерах. З цієї причини можливо розробити системи взаємодії людини з комп’ютером на основі комп’ютерного зору. Представлена безперервна система введення тексту, подібна до Graffiti™ на основі vision. Користувач безперервно малює символи алфавіту, схожого на Graffiti™, на плоскій поверхні за допомогою лазерної указки. Промінь лазерної вказівки відслідковується на послідовності зображень, знятих камерою, і відповідне написане слово розпізнається за виділеним слідом лазерного променя.","How to best integrate linguistic and perceptual processing in multi-modal tasks that involve language and vision is an important open problem. In this work, we argue that the common practice of using language in a topdown manner, to direct visual attention over high-level visual features, may not be optimal. We hypothesize that the use of language to also condition the bottom-up processing from pixels to high-level features can provide benefits to the overall performance. To support our claim, we propose a U-Net-based model and perform experiments on two language-vision dense-prediction tasks: referring expression segmentation and language-guided image colorization. We compare results where either one or both of the top-down and bottom-up visual branches are conditioned on language. Our experiments reveal that using language to control the filters for bottom-up visual processing in addition to top-down attention leads to better results on both tasks and achieves competitive performance. Our linguistic analysis suggests that bottom-up conditioning improves segmentation of objects especially when input text refers to low-level visual concepts",0
"Завдяки останнім досягненням електроніки та комп’ютерної індустрії тепер можливо розробляти недорогі системи комп’ютерного зору в режимі реального часу навіть у персональних комп’ютерах. З цієї причини можливо розробити системи взаємодії людини з комп’ютером на основі комп’ютерного зору. Представлена безперервна система введення тексту, подібна до Graffiti™ на основі vision. Користувач безперервно малює символи алфавіту, схожого на Graffiti™, на плоскій поверхні за допомогою лазерної указки. Промінь лазерної вказівки відстежується на послідовності зображень, знятих камерою, і відповідне написане слово розпізнається за вилученим слідом лазерного променя.","Як найкраще інтегрувати лінгвістичну та перцептивну обробку в мультимодальних завданнях, які включають мову та зір, є важливою відкритою проблемою. У цій роботі ми стверджуємо, що звичайна практика використання мови за принципом «зверху вниз» для спрямування візуальної уваги на візуальні функції високого рівня може бути неоптимальною. Ми припускаємо, що використання мови для обумовлення обробки знизу вгору від пікселів до функцій високого рівня може принести переваги загальній продуктивності. Щоб підтвердити наше твердження, ми пропонуємо модель на основі U-Net і проводимо експерименти з двома завданнями щільного прогнозування мовного бачення: сегментація виразу за посиланням і кольорування зображення під керуванням мови. Ми порівнюємо результати, де одна або обидві візуальні гілки зверху вниз і знизу вгору залежать від мови. Наші експерименти показують, що використання мови для керування фільтрами для візуальної обробки знизу вгору на додаток до уваги зверху вниз призводить до кращих результатів в обох завданнях і досягає конкурентоспроможності. Наш лінгвістичний аналіз показує, що кондиціонування знизу вгору покращує сегментацію об’єктів, особливо коли вхідний текст відноситься до візуальних концепцій низького рівня",0
"In this paper, a unistroke keyboard based on computer vision is described for the handicapped. The keyboard can be made of paper or fabric containing an image of a keyboard, which has an upside down U-shape. It can even be displayed on a computer screen. Each character is represented by a non-overlapping rectangular region on the keyboard image and the user enters a character by illuminating a character region with a laser pointer. The keyboard image is monitored by a camera and illuminated key locations are recognized. During the text entry process the user neither have to tum the laser light off nor raise the laser light from the keyboard. A disabled person who bas difficulty using hisiher hands may attach the laser pointer to an eyeglass and easily enter text by moving hisher head to point the laser beam on a character location. In addition, a mouse-like device can be developed based on the same principle. The user can move the cursor by moving the laser light on the computer screen which is monitored by a camera. ","In the last few decades, significant achievements have been attained in predicting where humans look at images through different computational models. However, how to determine contributions of different visual features to overall saliency still remains an open problem. To overcome this issue, a recent class of models formulates saliency estimation as a supervised learning problem and accordingly apply machine learning techniques. In this paper, we also address this challenging problem and propose to use multiple kernel learning (MKL) to combine information coming from different feature dimensions and to perform integration at an intermediate level. Besides, we suggest to use responses of a recently proposed filterbank of object detectors, known as ObjectBank, as additional semantic high-level features. Here we show that our MKL-based framework together with the proposed object-specific features provide state-of-the-art performance as compared to SVM or AdaBoost-based saliency models. ",0
"In this paper, a unistroke keyboard based on computer vision is described for the handicapped. The keyboard can be made of paper or fabric containing an image of a keyboard, which has an upside down U-shape. It can even be displayed on a computer screen. Each character is represented by a non-overlapping rectangular region on the keyboard image and the user enters a character by illuminating a character region with a laser pointer. The keyboard image is monitored by a camera and illuminated key locations are recognized. During the text entry process the user neither have to tum the laser light off nor raise the laser light from the keyboard. A disabled person who bas difficulty using hisiher hands may attach the laser pointer to an eyeglass and easily enter text by moving hisher head to point the laser beam on a character location. In addition, a mouse-like device can be developed based on the same principle. The user can move the cursor by moving the laser light on the computer screen which is monitored by a camera. ","За останні кілька десятиліть було досягнуто значних досягнень у передбаченні того, де люди дивляться на зображення, за допомогою різних обчислювальних моделей. Проте, як визначити внесок різних візуальних особливостей у загальну помітність, все ще залишається відкритою проблемою. Щоб подолати цю проблему, останній клас моделей формулює оцінку помітності як проблему контрольованого навчання та відповідно застосовує методи машинного навчання. У цьому документі ми також розглядаємо цю складну проблему та пропонуємо використовувати багатоядерне навчання (MKL) для об’єднання інформації, що надходить з різних вимірів функцій, і для виконання інтеграції на середньому рівні. Крім того, ми пропонуємо використовувати відгуки нещодавно запропонованого банку фільтрів детекторів об’єктів, відомого як ObjectBank, як додаткові семантичні функції високого рівня. Тут ми показуємо, що наш фреймворк на основі MKL разом із пропонованими об’єктно-специфічними функціями забезпечує найсучаснішу продуктивність порівняно з моделями помітності на основі SVM або AdaBoost.",0
"У цій статті описано одноштрихову клавіатуру на основі комп’ютерного зору для людей з обмеженими можливостями. Клавіатура може бути зроблена з паперу або тканини із зображенням клавіатури, яка має перевернуту U-подібну форму. Його можна навіть відобразити на екрані комп’ютера. Кожен символ представлений прямокутною областю, яка не перекривається, на зображенні клавіатури, і користувач вводить символ, підсвічуючи область символу лазерною вказівкою. Зображення клавіатури контролюється камерою, а підсвічені клавіші розпізнаються. Під час процесу введення тексту користувачеві не потрібно ні вимикати лазерне світло, ні піднімати його з клавіатури. Людина з обмеженими можливостями, якій важко користуватися своїми руками, може прикріпити лазерну указку до окулярів і легко вводити текст, рухаючи головою, щоб навести лазерний промінь на місце символу. Крім того, за таким же принципом можна розробити пристрій, схожий на мишу. Користувач може переміщати курсор, пересуваючи лазерне світло на екрані комп’ютера, який контролюється камерою.","In the last few decades, significant achievements have been attained in predicting where humans look at images through different computational models. However, how to determine contributions of different visual features to overall saliency still remains an open problem. To overcome this issue, a recent class of models formulates saliency estimation as a supervised learning problem and accordingly apply machine learning techniques. In this paper, we also address this challenging problem and propose to use multiple kernel learning (MKL) to combine information coming from different feature dimensions and to perform integration at an intermediate level. Besides, we suggest to use responses of a recently proposed filterbank of object detectors, known as ObjectBank, as additional semantic high-level features. Here we show that our MKL-based framework together with the proposed object-specific features provide state-of-the-art performance as compared to SVM or AdaBoost-based saliency models. ",0
"У цій статті описано одноштрихову клавіатуру на основі комп’ютерного зору для людей з обмеженими можливостями. Клавіатура може бути зроблена з паперу або тканини із зображенням клавіатури, яка має перевернуту U-подібну форму. Його можна навіть відобразити на екрані комп’ютера. Кожен символ представлений прямокутною областю, яка не перекривається, на зображенні клавіатури, і користувач вводить символ, підсвічуючи область символу лазерною вказівкою. Зображення клавіатури контролюється камерою, а підсвічені клавіші розпізнаються. Під час процесу введення тексту користувачеві не потрібно ні вимикати лазерне світло, ні піднімати його з клавіатури. Людина з обмеженими можливостями, якій важко користуватися своїми руками, може прикріпити лазерну указку до окулярів і легко вводити текст, рухаючи головою, щоб навести лазерний промінь на місце символу. Крім того, за таким же принципом можна розробити пристрій, схожий на мишу. Користувач може переміщати курсор, пересуваючи лазерне світло на екрані комп’ютера, який контролюється камерою.","За останні кілька десятиліть було досягнуто значних досягнень у передбаченні того, де люди дивляться на зображення, за допомогою різних обчислювальних моделей. Проте, як визначити внесок різних візуальних особливостей у загальну помітність, все ще залишається відкритою проблемою. Щоб подолати цю проблему, останній клас моделей формулює оцінку помітності як проблему контрольованого навчання та відповідно застосовує методи машинного навчання. У цьому документі ми також розглядаємо цю складну проблему та пропонуємо використовувати багатоядерне навчання (MKL) для об’єднання інформації, що надходить з різних вимірів функцій, і для виконання інтеграції на середньому рівні. Крім того, ми пропонуємо використовувати відгуки нещодавно запропонованого банку фільтрів детекторів об’єктів, відомого як ObjectBank, як додаткові семантичні функції високого рівня. Тут ми показуємо, що наш фреймворк на основі MKL разом із пропонованими об’єктно-специфічними функціями забезпечує найсучаснішу продуктивність порівняно з моделями помітності на основі SVM або AdaBoost.",0
"In MediaEval 2016, we focus on the image interestingness subtask which involves predicting interesting key frames of a video in the form of a movie trailer. We specifically propose three different deep models for this subtask. The first two models are based on fine-tuning two pretrained models, namely AlexNet and MemNet, where we cast the interestingness prediction as a regression problem. Our third deep model, on the other hand, depends on a triplet network which is comprised of three instances of the same feedforward network with shared weights, and trained according to a triplet ranking loss. Our experiments demonstrate that all these models provide relatively similar and promising results on the image interestingness subtask.","It is now possible to design real-time, low-cost computer vision systems even in personal computers due to the recent advances in electronics and the computer industry. Due to this reason, it is feasible to develop computer-vision-based human-computer interaction systems. A vision-based continuous Graffiti™-like text entry system is presented. The user sketches characters in a Graffiti™-like alphabet in a continuous manner on a flat surface using a laser pointer. The beam of the laser pointer is tracked on the image sequences captured by a camera, and the corresponding written word is recognized from the extracted trace of the laser beam.",0
"In MediaEval 2016, we focus on the image interestingness subtask which involves predicting interesting key frames of a video in the form of a movie trailer. We specifically propose three different deep models for this subtask. The first two models are based on fine-tuning two pretrained models, namely AlexNet and MemNet, where we cast the interestingness prediction as a regression problem. Our third deep model, on the other hand, depends on a triplet network which is comprised of three instances of the same feedforward network with shared weights, and trained according to a triplet ranking loss. Our experiments demonstrate that all these models provide relatively similar and promising results on the image interestingness subtask.","Завдяки останнім досягненням електроніки та комп’ютерної індустрії тепер можливо розробляти недорогі системи комп’ютерного зору в режимі реального часу навіть у персональних комп’ютерах. З цієї причини можливо розробити системи взаємодії людини з комп’ютером на основі комп’ютерного зору. Представлена безперервна система введення тексту, подібна до Graffiti™ на основі vision. Користувач безперервно малює символи алфавіту, схожого на Graffiti™, на плоскій поверхні за допомогою лазерної указки. Промінь лазерної вказівки відстежується на послідовності зображень, знятих камерою, і відповідне написане слово розпізнається за вилученим слідом лазерного променя.",0
"У MediaEval 2016 ми зосереджуємося на підзавданні щодо цікавості зображення, яке передбачає прогнозування цікавих ключових кадрів відео у формі трейлера фільму. Ми спеціально пропонуємо три різні глибокі моделі для цього підзавдання. Перші дві моделі базуються на тонкому налаштуванні двох попередньо навчених моделей, а саме AlexNet і MemNet, де ми використовуємо прогноз цікавості як проблему регресії. Наша третя глибока модель, з іншого боку, залежить від триплетної мережі, яка складається з трьох екземплярів тієї самої прямої мережі зі спільними вагами та навчена відповідно до втрати рейтингу триплетів. Наші експерименти демонструють, що всі ці моделі дають відносно схожі та багатообіцяючі результати щодо підзавдання щодо цікавості зображення.","It is now possible to design real-time, low-cost computer vision systems even in personal computers due to the recent advances in electronics and the computer industry. Due to this reason, it is feasible to develop computer-vision-based human-computer interaction systems. A vision-based continuous Graffiti™-like text entry system is presented. The user sketches characters in a Graffiti™-like alphabet in a continuous manner on a flat surface using a laser pointer. The beam of the laser pointer is tracked on the image sequences captured by a camera, and the corresponding written word is recognized from the extracted trace of the laser beam.",0
"У MediaEval 2016 ми зосереджуємося на підзавданні щодо цікавості зображення, яке передбачає прогнозування цікавих ключових кадрів відео у формі трейлера фільму. Ми спеціально пропонуємо три різні глибокі моделі для цього підзавдання. Перші дві моделі базуються на тонкому налаштуванні двох попередньо навчених моделей, а саме AlexNet і MemNet, де ми використовуємо прогноз цікавості як проблему регресії. Наша третя глибока модель, з іншого боку, залежить від триплетної мережі, яка складається з трьох екземплярів тієї самої прямої мережі зі спільними вагами та навчена відповідно до втрати рейтингу триплетів. Наші експерименти демонструють, що всі ці моделі дають відносно схожі та багатообіцяючі результати щодо підзавдання щодо цікавості зображення.","Завдяки останнім досягненням електроніки та комп’ютерної індустрії тепер можливо розробляти недорогі системи комп’ютерного зору в режимі реального часу навіть у персональних комп’ютерах. З цієї причини можливо розробити системи взаємодії людини з комп’ютером на основі комп’ютерного зору. Представлена безперервна система введення тексту, подібна до Graffiti™ на основі vision. Користувач безперервно малює символи алфавіту, схожого на Graffiti™, на плоскій поверхні за допомогою лазерної указки. Промінь лазерної вказівки відстежується на послідовності зображень, знятих камерою, і відповідне написане слово розпізнається за вилученим слідом лазерного променя.",0
Text1,Text2,Similarity
Text1,Текст2,Similarity
Текст1,Text2,Similarity
Текст1,Текст2,Similarity
"This paper describes our two-stage system1 for the Euphemism Detection shared task hosted by the 3rd Workshop on Figurative Language Processing in conjunction with EMNLP 2022. Euphemisms tone down expressions about sensitive or unpleasant issues like addiction and death. The ambiguous nature of euphemistic words or expressions makes it challenging to detect their actual meaning within a context. In the first stage, we seek to mitigate this ambiguity by incorporating literal descriptions into input text prompts to our baseline model. It turns out that this kind of direct supervision yields remarkable performance improvement. In the second stage, we integrate visual supervision into our system using visual imageries, two sets of images generated by a text-to-image model by taking terms and descriptions as input. Our experiments demonstrate that visual supervision also gives a statistically significant performance boost. Our system achieved the second place with an F1 score of 87.2%, only about 0.9% worse than the best submission.","Our two-stage system for the Euphemism Detection shared task hosted by the 3rd Workshop on Figurative Language Processing in conjunction with EMNLP 2022 is presented in this paper. It is difficult to identify the true meaning of euphemistic words or phrases due to their ambiguous nature when it comes to sensitive or unpleasant topics such as addiction and death. To reduce this ambiguity, we added literal descriptions to the input text prompts of our baseline model in the first stage, which resulted in significant performance improvement. In the second stage, visual supervision was integrated by using two sets of images generated by a text-to-image model that takes terms and descriptions as input. Our experiments showed that visual supervision gives a statistically significant performance boost. We achieved the second place with an F1 score of 87.2%, only about 0.9% worse than the best submission.",1
"This paper describes our two-stage system1 for the Euphemism Detection shared task hosted by the 3rd Workshop on Figurative Language Processing in conjunction with EMNLP 2022. Euphemisms tone down expressions about sensitive or unpleasant issues like addiction and death. The ambiguous nature of euphemistic words or expressions makes it challenging to detect their actual meaning within a context. In the first stage, we seek to mitigate this ambiguity by incorporating literal descriptions into input text prompts to our baseline model. It turns out that this kind of direct supervision yields remarkable performance improvement. In the second stage, we integrate visual supervision into our system using visual imageries, two sets of images generated by a text-to-image model by taking terms and descriptions as input. Our experiments demonstrate that visual supervision also gives a statistically significant performance boost. Our system achieved the second place with an F1 score of 87.2%, only about 0.9% worse than the best submission.","У цьому документі представлено нашу двоетапну систему для спільного завдання «Виявлення евфемізмів», організованого 3-м семінаром з обробки образної мови спільно з EMNLP 2022. Важко визначити справжнє значення евфемістичних слів або фраз через їх неоднозначну природу, коли мова йде про чутливі або неприємні теми, такі як залежність і смерть. Щоб зменшити цю неоднозначність, ми додали буквальні описи до вхідних текстових підказок нашої базової моделі на першому етапі, що призвело до значного покращення продуктивності. На другому етапі візуальний нагляд був інтегрований за допомогою двох наборів зображень, згенерованих моделлю перетворення тексту в зображення, яка приймає терміни та описи як вхідні дані. Наші експерименти показали, що візуальне спостереження дає статистично значущий приріст продуктивності. Ми зайняли друге місце з результатом Формули-1 87,2%, що лише на 0,9% гірше, ніж найкраще подання.",1
"У цьому документі описано нашу двоетапну систему1 для спільного завдання «Виявлення евфемізмів», організованого 3-м семінаром з обробки образної мови спільно з EMNLP 2022. Евфемізми пом’якшують вирази про чутливі або неприємні питання, такі як залежність і смерть. Неоднозначний характер евфемістичних слів або виразів ускладнює виявлення їх справжнього значення в контексті. На першому етапі ми намагаємося пом’якшити цю неоднозначність, включивши літеральні описи у вхідні текстові підказки нашої базової моделі. Виявляється, такий вид прямого контролю дає значне покращення продуктивності. На другому етапі ми інтегруємо візуальний нагляд у нашу систему за допомогою візуальних зображень, двох наборів зображень, створених за допомогою моделі тексту в зображення, використовуючи терміни та описи як вхідні дані. Наші експерименти демонструють, що візуальний контроль також дає статистично значуще підвищення продуктивності. Наша система посіла друге місце з результатом F1 87,2%, що лише приблизно на 0,9% гірше, ніж найкраще подання.","Our two-stage system for the Euphemism Detection shared task hosted by the 3rd Workshop on Figurative Language Processing in conjunction with EMNLP 2022 is presented in this paper. It is difficult to identify the true meaning of euphemistic words or phrases due to their ambiguous nature when it comes to sensitive or unpleasant topics such as addiction and death. To reduce this ambiguity, we added literal descriptions to the input text prompts of our baseline model in the first stage, which resulted in significant performance improvement. In the second stage, visual supervision was integrated by using two sets of images generated by a text-to-image model that takes terms and descriptions as input. Our experiments showed that visual supervision gives a statistically significant performance boost. We achieved the second place with an F1 score of 87.2%, only about 0.9% worse than the best submission.",1
"У цьому документі описано нашу двоетапну систему1 для спільного завдання «Виявлення евфемізмів», організованого 3-м семінаром з обробки образної мови спільно з EMNLP 2022. Евфемізми пом’якшують вирази щодо чутливих або неприємних питань, таких як залежність і смерть. Неоднозначний характер евфемістичних слів або виразів ускладнює виявлення їх справжнього значення в контексті. На першому етапі ми намагаємося пом’якшити цю неоднозначність, включивши літеральні описи у вхідні текстові підказки нашої базової моделі. Виявляється, такий вид прямого контролю дає значне покращення продуктивності. На другому етапі ми інтегруємо візуальне спостереження в нашу систему за допомогою візуальних зображень, двох наборів зображень, створених за допомогою моделі тексту в зображення, використовуючи терміни та описи як вхідні дані. Наші експерименти демонструють, що візуальний контроль також дає статистично значуще підвищення продуктивності. Наша система посіла друге місце з результатом F1 87,2%, що лише приблизно на 0,9% гірше, ніж найкраще подання.","У цьому документі представлено нашу двоетапну систему для спільного завдання «Виявлення евфемізмів», організованого 3-м семінаром з обробки образної мови спільно з EMNLP 2022. Важко визначити справжнє значення евфемістичних слів або фраз через їх неоднозначну природу, коли йдеться про чутливі чи неприємні теми, такі як залежність і смерть. Щоб зменшити цю неоднозначність, ми додали буквальні описи до вхідних текстових підказок нашої базової моделі на першому етапі, що призвело до значного покращення продуктивності. На другому етапі візуальний нагляд був інтегрований за допомогою двох наборів зображень, згенерованих моделлю тексту в зображення, яка приймає терміни та описи як вхідні дані. Наші експерименти показали, що візуальне спостереження дає статистично значущий приріст продуктивності. Ми зайняли друге місце з результатом Формули-1 87,2%, що лише на 0,9% гірше, ніж найкраще подання.",1
"Giving machines the ability to imagine possible new objects or scenes from linguistic descriptions and produce their realistic renderings is arguably one of the most challenging problems in computer vision. Recent advances in deep generative models have led to new approaches that give promising results towards this goal. In this paper, we introduce a new method called DiCoMoGAN for manipulating videos with natural language, aiming to perform local and semantic edits on a video clip to alter the appearances of an object of interest. Our GAN architecture allows for better utilization of multiple observations by disentangling content and motion to enable controllable semantic edits. To this end, we introduce two tightly coupled networks: (i) a representation network for constructing a concise understanding of motion dynamics and temporally invariant content, and (ii) a translation network that exploits the extracted latent content representation to actuate the manipulation according to the target description. Our qualitative and quantitative evaluations demonstrate that DiCoMoGAN significantly outperforms existing frame-based methods, producing temporally coherent and semantically more meaningful results.","Developing machines that can generate realistic renderings of new objects or scenes from linguistic descriptions is one of the most difficult tasks in computer vision. Recently, deep generative models have been utilized to produce promising results. In this paper, we propose a new method, DiCoMoGAN, for manipulating videos with natural language, to modify the look of a particular object. Our GAN architecture permits better utilization of multiple observations by separating content and motion to allow for controllable semantic edits. To accomplish this, two networks are used: (i) a representation network for creating a succinct understanding of motion dynamics and content that is temporally invariant, and (ii) a translation network that uses the latent content representation to effect the manipulation according to the target description. Our qualitative and quantitative evaluations show that DiCoMoGAN significantly surpasses existing frame-based methods, producing results that are temporally coherent and semantically more meaningful.",1
"Giving machines the ability to imagine possible new objects or scenes from linguistic descriptions and produce their realistic renderings is arguably one of the most challenging problems in computer vision. Recent advances in deep generative models have led to new approaches that give promising results towards this goal. In this paper, we introduce a new method called DiCoMoGAN for manipulating videos with natural language, aiming to perform local and semantic edits on a video clip to alter the appearances of an object of interest. Our GAN architecture allows for better utilization of multiple observations by disentangling content and motion to enable controllable semantic edits. To this end, we introduce two tightly coupled networks: (i) a representation network for constructing a concise understanding of motion dynamics and temporally invariant content, and (ii) a translation network that exploits the extracted latent content representation to actuate the manipulation according to the target description. Our qualitative and quantitative evaluations demonstrate that DiCoMoGAN significantly outperforms existing frame-based methods, producing temporally coherent and semantically more meaningful results.","Розробка машин, які можуть генерувати реалістичні візуалізації нових об’єктів або сцен на основі лінгвістичних описів, є одним із найскладніших завдань комп’ютерного зору. Останнім часом глибокі генеративні моделі були використані для отримання багатообіцяючих результатів. У цій статті ми пропонуємо новий метод, DiCoMoGAN, для обробки відео за допомогою природної мови, щоб змінити вигляд певного об’єкта. Наша архітектура GAN дозволяє краще використовувати кілька спостережень, відокремлюючи вміст і рух, щоб забезпечити кероване семантичне редагування. Щоб досягти цього, використовуються дві мережі: (i) мережа представлення для створення короткого розуміння динаміки руху та вмісту, незмінного за часом, і (ii) мережа перекладу, яка використовує представлення латентного вмісту для здійснення маніпуляцій відповідно до цільовий опис. Наші якісні та кількісні оцінки показують, що DiCoMoGAN значно перевершує існуючі методи на основі кадрів, створюючи результати, які є часово узгодженими та семантично більш значущими.",1
"Надання машинам здатності уявляти можливі нові об’єкти чи сцени на основі лінгвістичних описів і виробляти їх реалістичне відтворення є, мабуть, однією з найскладніших проблем комп’ютерного зору. Останні досягнення в глибоких генеративних моделях привели до нових підходів, які дають багатообіцяючі результати для досягнення цієї мети. У цій статті ми представляємо новий метод під назвою DiCoMoGAN для обробки відео за допомогою природної мови, спрямований на виконання локальних і семантичних редагувань відеокліпу, щоб змінити зовнішній вигляд цікавого об’єкта. Наша архітектура GAN дозволяє краще використовувати кілька спостережень, розмежовуючи вміст і рух, щоб уможливити кероване семантичне редагування. З цією метою ми представляємо дві тісно пов’язані мережі: (i) репрезентативну мережу для побудови стислого розуміння динаміки руху та тимчасово незмінного вмісту, і (ii) мережу перекладу, яка використовує виділене латентне представлення вмісту для активації маніпуляції відповідно до цільовий опис. Наші якісні та кількісні оцінки демонструють, що DiCoMoGAN значно перевершує існуючі методи на основі фреймів, створюючи узгоджені в часі та семантично більш значущі результати.","Developing machines that can generate realistic renderings of new objects or scenes from linguistic descriptions is one of the most difficult tasks in computer vision. Recently, deep generative models have been utilized to produce promising results. In this paper, we propose a new method, DiCoMoGAN, for manipulating videos with natural language, to modify the look of a particular object. Our GAN architecture permits better utilization of multiple observations by separating content and motion to allow for controllable semantic edits. To accomplish this, two networks are used: (i) a representation network for creating a succinct understanding of motion dynamics and content that is temporally invariant, and (ii) a translation network that uses the latent content representation to effect the manipulation according to the target description. Our qualitative and quantitative evaluations show that DiCoMoGAN significantly surpasses existing frame-based methods, producing results that are temporally coherent and semantically more meaningful.",1
"Надання машинам здатності уявляти можливі нові об’єкти чи сцени на основі лінгвістичних описів і виробляти їх реалістичне відтворення є, мабуть, однією з найскладніших проблем комп’ютерного зору. Останні досягнення в глибоких генеративних моделях привели до нових підходів, які дають багатообіцяючі результати для досягнення цієї мети. У цій статті ми представляємо новий метод під назвою DiCoMoGAN для обробки відео за допомогою природної мови, спрямований на виконання локальних і семантичних редагувань відеокліпу, щоб змінити зовнішній вигляд цікавого об’єкта. Наша архітектура GAN дозволяє краще використовувати кілька спостережень, розмежовуючи вміст і рух, щоб уможливити кероване семантичне редагування. З цією метою ми представляємо дві тісно пов’язані мережі: (i) репрезентативну мережу для побудови стислого розуміння динаміки руху та тимчасово незмінного вмісту, і (ii) мережу перекладу, яка використовує виділене латентне представлення вмісту для активації маніпуляції відповідно до цільовий опис. Наші якісні та кількісні оцінки демонструють, що DiCoMoGAN значно перевершує існуючі методи на основі фреймів, створюючи узгоджені в часі та семантично більш значущі результати.","Розробка машин, які можуть генерувати реалістичні візуалізації нових об’єктів або сцен на основі лінгвістичних описів, є одним із найскладніших завдань комп’ютерного зору. Останнім часом глибокі генеративні моделі були використані для отримання багатообіцяючих результатів. У цій статті ми пропонуємо новий метод, DiCoMoGAN, для обробки відео за допомогою природної мови, щоб змінити вигляд певного об’єкта. Наша архітектура GAN дозволяє краще використовувати кілька спостережень, відокремлюючи вміст і рух, щоб забезпечити кероване семантичне редагування. Щоб досягти цього, використовуються дві мережі: (i) мережа представлення для створення короткого розуміння динаміки руху та вмісту, незмінного за часом, і (ii) мережа перекладу, яка використовує представлення латентного вмісту для здійснення маніпуляцій відповідно до цільовий опис. Наші якісні та кількісні оцінки показують, що DiCoMoGAN значно перевершує існуючі методи на основі кадрів, створюючи результати, які є часово узгодженими та семантично більш значущими.",1
"Flow-based generative super-resolution (SR) models learn to produce a diverse set of feasible SR solutions, called the SR space. Diversity of SR solutions increases with the temperature (τ ) of latent variables, which introduces random variations of texture among sample solutions, resulting in visual artifacts and low fidelity. In this paper, we present a simple but effective image ensembling/fusion approach to obtain a single SR image eliminating random artifacts and improving fidelity without significantly compromising perceptual quality. We achieve this by benefiting from a diverse set of feasible photorealistic solutions in the SR space spanned by flow models. We propose different image ensembling and fusion strategies which offer multiple paths to move sample solutions in the SR space to more desired destinations in the perception-distortion plane in a controllable manner depending on the fidelity vs. perceptual quality requirements of the task at hand. Experimental results demonstrate that our image ensembling/fusion strategy achieves more promising perception-distortion tradeoff compared to sample SR images produced by flow models and adversarially trained models in terms of both quantitative metrics and visual quality.","Flow-based generative super-resolution (SR) models learn to generate a wide range of SR solutions, referred to as the SR space. As the temperature (τ ) of latent variables increases, the diversity of SR solutions also increases, resulting in visible artifacts and low accuracy. In this paper, we present a straightforward yet effective image ensembling/fusion approach to obtain a single SR image that eliminates random artifacts and boosts fidelity without significantly reducing perceptual quality. We take advantage of the wide range of realistic solutions in the SR space generated by flow models. We propose several image ensembling and fusion strategies that provide multiple paths to move sample solutions in the SR space to the desired locations in the perception-distortion plane depending on the task's fidelity vs. perceptual quality requirements. Experimental results demonstrate that our image ensembling/fusion strategy offers a better perception-distortion tradeoff compared to sample SR images produced by flow models and adversarially trained models in terms of both quantitative metrics and visual quality.",1
"Flow-based generative super-resolution (SR) models learn to produce a diverse set of feasible SR solutions, called the SR space. Diversity of SR solutions increases with the temperature (τ ) of latent variables, which introduces random variations of texture among sample solutions, resulting in visual artifacts and low fidelity. In this paper, we present a simple but effective image ensembling/fusion approach to obtain a single SR image eliminating random artifacts and improving fidelity without significantly compromising perceptual quality. We achieve this by benefiting from a diverse set of feasible photorealistic solutions in the SR space spanned by flow models. We propose different image ensembling and fusion strategies which offer multiple paths to move sample solutions in the SR space to more desired destinations in the perception-distortion plane in a controllable manner depending on the fidelity vs. perceptual quality requirements of the task at hand. Experimental results demonstrate that our image ensembling/fusion strategy achieves more promising perception-distortion tradeoff compared to sample SR images produced by flow models and adversarially trained models in terms of both quantitative metrics and visual quality.","Моделі generative super-resolution (SR) на основі потоку навчаються генерувати широкий спектр рішень SR, які називаються простором SR. Зі збільшенням температури (τ ) латентних змінних також збільшується різноманітність рішень SR, що призводить до видимих артефактів і низької точності. У цій статті ми представляємо простий, але ефективний підхід до ансамблювання/злиття зображень для отримання єдиного зображення SR, яке усуває випадкові артефакти та підвищує точність без значного зниження якості сприйняття. Ми використовуємо широкий спектр реалістичних рішень у просторі SR, створених моделями потоку. Ми пропонуємо кілька стратегій поєднання зображень і злиття, які забезпечують кілька шляхів для переміщення зразків рішень у просторі SR до бажаних місць у площині сприйняття-спотворення залежно від вимог до точності завдання та вимог до якості сприйняття. Експериментальні результати демонструють, що наша стратегія ансамблювання/злиття зображень пропонує кращий компроміс між сприйняттям і спотворенням порівняно з зразками SR-зображень, створених потоковими моделями та навченими моделями з точки зору як кількісних показників, так і візуальної якості.",1
"Моделі generative super-resolution (SR) на основі потоку навчаються виробляти різноманітний набір можливих рішень SR, які називають простором SR. Різноманітність розчинів SR збільшується з температурою (τ) прихованих змінних, що вводить випадкові варіації текстури серед розчинів зразків, що призводить до візуальних артефактів і низької точності. У цій статті ми представляємо простий, але ефективний підхід до ансамблювання/злиття зображень для отримання єдиного зображення SR, усуваючи випадкові артефакти та покращуючи точність без значного погіршення якості сприйняття. Ми досягаємо цього, використовуючи переваги різноманітного набору можливих фотореалістичних рішень у просторі SR, охопленому моделями потоку. Ми пропонуємо різні стратегії поєднання зображень і злиття, які пропонують кілька шляхів для переміщення зразків рішень у просторі SR до більш бажаних пунктів призначення в площині сприйняття-спотворення контрольованим способом залежно від вимог щодо точності та якості сприйняття для поточного завдання. Експериментальні результати демонструють, що наша стратегія ансамблювання/злиття зображень досягає більш перспективного компромісу між сприйняттям і спотворенням порівняно з зразками SR-зображень, створених потоковими моделями та конкурентно навченими моделями як з точки зору кількісних показників, так і якості зображення.","Flow-based generative super-resolution (SR) models learn to generate a wide range of SR solutions, referred to as the SR space. As the temperature (τ ) of latent variables increases, the diversity of SR solutions also increases, resulting in visible artifacts and low accuracy. In this paper, we present a straightforward yet effective image ensembling/fusion approach to obtain a single SR image that eliminates random artifacts and boosts fidelity without significantly reducing perceptual quality. We take advantage of the wide range of realistic solutions in the SR space generated by flow models. We propose several image ensembling and fusion strategies that provide multiple paths to move sample solutions in the SR space to the desired locations in the perception-distortion plane depending on the task's fidelity vs. perceptual quality requirements. Experimental results demonstrate that our image ensembling/fusion strategy offers a better perception-distortion tradeoff compared to sample SR images produced by flow models and adversarially trained models in terms of both quantitative metrics and visual quality.",1
"Моделі generative super-resolution (SR) на основі потоку вчаться створювати різноманітний набір можливих рішень SR, які називають простором SR. Різноманітність розчинів SR збільшується з температурою (τ) прихованих змінних, що вводить випадкові варіації текстури серед розчинів зразків, що призводить до візуальних артефактів і низької точності. У цій статті ми представляємо простий, але ефективний підхід до ансамблювання/злиття зображень для отримання єдиного зображення SR, усуваючи випадкові артефакти та покращуючи точність без значного погіршення якості сприйняття. Ми досягаємо цього, використовуючи переваги різноманітного набору можливих фотореалістичних рішень у просторі SR, охопленому моделями потоку. Ми пропонуємо різні стратегії поєднання зображень і злиття, які пропонують кілька шляхів для переміщення зразків рішень у просторі SR до більш бажаних пунктів призначення в площині сприйняття-спотворення контрольованим способом залежно від вимог щодо точності та якості сприйняття для поточного завдання. Експериментальні результати демонструють, що наша стратегія ансамблювання/злиття зображень досягає більш перспективного компромісу між сприйняттям і спотворенням порівняно з зразками SR-зображень, створених потоковими моделями та конкурентно навченими моделями як з точки зору кількісних показників, так і якості зображення.","Моделі generative super-resolution (SR) на основі потоку навчаються генерувати широкий спектр рішень SR, які називаються простором SR. Зі збільшенням температури (τ ) прихованих змінних також збільшується різноманітність рішень SR, що призводить до видимих артефактів і низької точності. У цій статті ми представляємо простий, але ефективний підхід до ансамблювання/злиття зображень для отримання єдиного зображення SR, яке усуває випадкові артефакти та підвищує точність без значного зниження якості сприйняття. Ми використовуємо широкий спектр реалістичних рішень у просторі SR, створених моделями потоку. Ми пропонуємо кілька стратегій поєднання зображень і злиття, які забезпечують кілька шляхів для переміщення зразків рішень у просторі SR до бажаних місць у площині сприйняття-спотворення залежно від вимог до точності завдання та вимог до якості сприйняття. Експериментальні результати демонструють, що наша стратегія ансамблювання/злиття зображень пропонує кращий компроміс між сприйняттям і спотворенням порівняно з зразками SR-зображень, створених потоковими моделями та навченими моделями з точки зору як кількісних показників, так і візуальної якості.",1
"Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 444 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI’s GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit “breakthrough” behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting. ","Language models show improvement and new capabilities with size increase. These are not well understood, so to be prepared for future developments and to avoid negative effects, we must know the current and near-future abilities and limitations of these models. To do this, BIG-bench was created with 204 tasks from 444 authors at 132 institutions, covering linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and more. It focuses on tasks that are believed to be too difficult for existing language models. We tested OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, with sizes ranging from millions to hundreds of billions of parameters. Furthermore, human experts completed all tasks to set a strong baseline. Results showed that performance and calibration increase with size, but are still low in absolute terms (and compared to the experts). Performance was similar across model types, but sparsity was beneficial. Tasks that improve gradually usually have large knowledge or memorization components, while tasks that show dramatic improvement at a certain size usually involve multiple steps or components, or have fragile metrics. Social bias usually rises with scale in ambiguous contexts, but can be lessened with prompting.",1
